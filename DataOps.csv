Title,Content
Optimisation des tests ETL pour une meilleure qualité et fiabilité des données,"<p>Avez-vous déjà pris une décision commerciale sur la base de données inexactes ou incomplètes ? Dans le monde actuel axé sur les données, la qualité et la fiabilité de vos données peuvent faire la différence entre votre stratégie commerciale et la vôtre. Les entreprises s’appuyant de plus en plus sur les données pour guider leurs décisions, il est plus important que jamais de s’assurer que ces données sont exactes, cohérentes et fiables. C’est là qu’entrent en jeu les tests ETL (Extract, Transform, Load).</p><p>Les processus ETL sont essentiels pour déplacer des données provenant de diverses sources vers un système unifié, mais sans tests rigoureux, même de petites erreurs dans ces processus peuvent entraîner des problèmes de qualité des données importants. Ce blog met en évidence les meilleures pratiques, les défis courants et les solutions innovantes pour optimiser les tests ETL, vous aidant à préserver l'intégrité de vos données et à prendre des décisions en toute confiance.</p><h3>Libérer la puissance des tests ETL</h3><p>Dans le domaine de la gestion des données, les tests ETL constituent la pierre angulaire de l'assurance qualité des données. Leur importance ne peut être surestimée, car ils servent de gardien de l'intégrité des données tout au long du pipeline de données. Voici quelques raisons pour lesquelles les tests ETL sont essentiels :</p><li>Validation de l'exactitude des données : vérifie méticuleusement que la transformation des données est exécutée correctement, garantissant que les données chargées dans les systèmes cibles sont exactes et fiables.</li><li>Conformité aux règles métier : confirme que les données adhèrent aux règles et normes métier prédéfinies, préservant ainsi la cohérence au sein de l'organisation.</li><li>Assurance de l'intégration des données : valide l'intégration transparente des données provenant de sources disparates, essentielle pour une veille économique et des analyses complètes.</li><li>Détection et prévention des erreurs : en identifiant les problèmes au début du pipeline de données, les tests ETL empêchent la propagation d’erreurs pouvant conduire à des décisions commerciales coûteuses.</li><li>Support de gouvernance des données : il joue un rôle essentiel dans le maintien des politiques de gouvernance des données en garantissant la qualité, la sécurité et la conformité des données tout au long du processus ETL.</li><p>« D’ici 2028, les outils basés sur GenAI seront capables d’écrire 70 % des tests logiciels, réduisant ainsi le besoin de tests manuels et améliorant la couverture des tests, la convivialité des logiciels et la qualité du code ». IDC.</p><h3>Défis pour atteindre la qualité des données</h3><p>Bien que la recherche de données de haute qualité grâce aux tests ETL soit cruciale, les organisations se retrouvent souvent confrontées à un paysage complexe rempli d'obstacles. Ces défis découlent de la nature complexe des écosystèmes de données modernes, du rythme rapide des changements technologiques et des demandes toujours croissantes d'informations basées sur les données. Comprendre ces obstacles est la première étape vers l'élaboration de stratégies robustes pour les surmonter et atteindre le niveau de qualité des données souhaité. Les principaux défis des tests ETL auxquels les organisations sont confrontées sont les suivants :</p><li>Volume et diversité des données : les méthodes de test traditionnelles ont souvent du mal à gérer des téraoctets ou des pétaoctets de données, ce qui entraîne des cycles de test prolongés et des contraintes de ressources. De plus, les données se présentent sous différents formats structurés, semi-structurés et non structurés, chacun nécessitant des approches de traitement et de validation différentes.</li><li>Transformations complexes : les transformations de données complexes sont difficiles à tester de manière approfondie, en particulier lorsqu'elles impliquent plusieurs règles métier et cas limites. Les transformations conditionnelles complexes créent une multitude de chemins de données possibles, ce qui rend difficile le test complet de tous les scénarios.</li><li>Contraintes de temps : la demande croissante de traitement des données en temps réel ou quasi réel exerce une pression sur les équipes de test pour valider la qualité des données à grande vitesse. Équilibrer le temps consacré au développement et aux tests conduit souvent à des compromis dans la couverture des tests.</li><li>Différences d'environnement : les variations de puissance de traitement, de mémoire ou de stockage entre les environnements de test et de production peuvent masquer des problèmes de qualité des données liés aux performances. De même, les incohérences dans les versions ou les configurations logicielles entre les environnements peuvent entraîner un comportement inattendu en production.</li><li>Manque de visibilité de bout en bout : il peut être extrêmement difficile de suivre les données via des processus ETL complexes pour identifier la cause profonde des problèmes de qualité. En outre, une surveillance inadéquate de l'ensemble du pipeline de données peut entraîner des angles morts où les problèmes de qualité des données passent inaperçus.</li><li>Évolution du paysage des données : l'ajout fréquent de nouvelles sources de données nécessite des mises à jour constantes des processus ETL et des cas de test correspondants. L'évolution des réglementations en matière de confidentialité et de conformité des données nécessite des ajustements continus des procédures de traitement et de test des données.</li><h3>Stratégies pour obtenir des données de type production</h3><p>Pour garantir l’efficacité des tests ETL, il est essentiel de travailler avec des données qui ressemblent étroitement aux données de production. Voici quelques stratégies pour obtenir efficacement des données de type production :</p><li>Sous-ensemble de données : créez des sous-ensembles représentatifs de données de production qui conservent les caractéristiques et les complexités de l'ensemble de données complet.</li><li>Masquage des données : mettez en œuvre des techniques de masquage des données robustes pour protéger les informations sensibles tout en préservant les propriétés statistiques des données.</li><li>Génération de données synthétiques : utilisez des algorithmes avancés pour générer des données synthétiques qui reflètent les modèles et les distributions des données de production.</li><li>Actualisation incrémentielle des données : mettez à jour les données de test avec de nouvelles données de production pour garantir la pertinence et capturer de nouveaux modèles de données.</li><li>Copies de données virtuelles : exploitez les technologies de virtualisation de bases de données pour créer des copies légères et à jour des données de production à des fins de test.</li><li>Profilage et analyse des données : effectuez un profilage approfondi des données pour comprendre les caractéristiques des données de production et les reproduire dans des environnements de test.</li><h3>Bonnes pratiques pour les tests ETL</h3><li>Établissez des objectifs de test clairs : définissez des objectifs spécifiques et mesurables pour chaque phase de test afin de garantir une couverture complète.</li><li>Implémenter le contrôle de version : utilisez des systèmes de contrôle de version pour suivre les modifications dans les processus ETL et les cas de test, facilitant ainsi le dépannage et les restaurations.</li><li>Automatisez les tests répétitifs : exploitez les outils d'automatisation des tests pour exécuter des tests de routine, libérant ainsi des ressources pour des scénarios de test plus complexes.</li><li>Priorisez les cas de test : concentrez-vous sur les éléments de données critiques et les zones à haut risque pour maximiser l'impact des efforts de test.</li><li>Mettre en œuvre des tests continus : intégrez les tests tout au long du cycle de développement pour découvrir et résoudre les problèmes le plus tôt possible.</li><li>Documentez minutieusement : conservez une documentation détaillée des cas de test, des résultats et de tout problème de qualité des données découvert pendant les tests.</li><li>Collaborer entre les équipes : Favorisez une collaboration étroite entre les ingénieurs de données, les testeurs et</li><h3>Types de tests ETL</h3><p>Les tests ETL sont essentiels pour garantir l'exactitude et l'intégrité des données lors de leur transfert de leur source d'origine à leur destination. Ce processus implique une série de contrôles et de validations pour détecter les erreurs, les incohérences et d'autres problèmes lors des étapes d'extraction, de transformation et de chargement des données. Pour gérer efficacement cela, les tests ETL sont classés en différents types, chacun ciblant des aspects spécifiques du pipeline de données.</p><h3>Avantages des tests ETL automatisés</h3><p>Les tests ETL automatisés sont devenus une véritable révolution pour les entreprises. Ils offrent une solution puissante aux défis liés à la garantie de la qualité des données à grande échelle. Alors que les entreprises sont confrontées à des volumes de données croissants, à des transformations complexes et à la nécessité d'obtenir des informations rapides, l'automatisation des tests se distingue par son efficacité et sa fiabilité. En exploitant des outils et des technologies de pointe, les tests ETL automatisés accélèrent non seulement le processus de test, mais améliorent également sa précision et son exhaustivité. Grâce à l'automatisation, les entreprises peuvent bénéficier des avantages suivants :</p><li>Couverture de test accrue : permet des tests plus complets sur une plus large gamme de scénarios et de variations de données.</li><li>Exécution plus rapide : les tests automatisés peuvent être exécutés rapidement et fréquemment, permettant une identification rapide des problèmes.</li><li>Cohérence et fiabilité : élimine les erreurs humaines et garantit une exécution cohérente des cas de test.</li><li>Évolutivité : offre la possibilité d'augmenter ou de diminuer la capacité sans effort, ce qui est particulièrement avantageux pour gérer les charges de pointe ou les exigences de test fluctuantes.</li><li>Efficacité et flexibilité améliorées : en automatisant les tâches répétitives, les testeurs peuvent se concentrer sur des activités de test plus complexes et à forte valeur ajoutée. Cela permet le déploiement et la gestion dynamiques des machines virtuelles, du stockage et des réseaux, accélérant ainsi le processus de test et facilitant l'itération et l'expérimentation rapides.</li><li>Détection précoce des problèmes : des tests automatisés continus tout au long du processus de développement permettent d'identifier les problèmes plus tôt, réduisant ainsi le coût des correctifs.</li><li>Rapports améliorés : les outils de test automatisés fournissent souvent des rapports et des analyses détaillés, offrant des informations plus approfondies sur les résultats et les tendances des tests.</li><li>Maintenance plus facile : les tests automatisés bien conçus sont plus faciles à mettre à jour et à entretenir à mesure que les processus ETL évoluent.</li><h3>Assurez l'avenir de votre stratégie de données avec un partenaire de test ETL compétent</h3><p>L’optimisation des tests ETL n’est pas seulement une nécessité technique ; c’est un impératif stratégique pour toute organisation qui s’appuie sur les données pour prendre des décisions. AgreeYa est l’une de ces options qui aident les organisations en proposant des stratégies de test robustes, en surmontant les défis courants et en tirant parti de l’automatisation.</p>"
Naviguer dans les eaux agiles : pourquoi l'intégration de Copilot exige des ajustements méthodologiques,"<p>Le Congrès américain a récemment interdit à son personnel d’utiliser l’IA Copilot de Microsoft, un chatbot intégré à grand modèle de langage qui permet l’automatisation des produits Microsoft tels que Word, Excel, PowerPoint, Outlook et Teams, en invoquant des problèmes de sécurité. Et ils ne sont pas les seuls à penser ainsi, car de nombreux professionnels oscillent entre l’enthousiasme et la crainte lorsqu’il est question de l’IA. En attendant, Microsoft a vanté avec assurance Copilot, promettant que la puissance de son IA réduira le travail quotidien de gestion d’une entreprise. L’entreprise est passée du simple discours sur l’IA à l’intégration de celle-ci dans toutes les couches de sa pile technologique. Son introduction récente de Copilot Runtime permet même aux développeurs d’utiliser l’IA dans leurs propres programmes, il n’est donc pas surprenant que la plupart des directeurs des systèmes d’information expérimentent Copilot. Ses promesses de productivité et l’élargissement du champ d’application et les économies de coûts qui en résultent pour un réinvestissement potentiel sont trop alléchantes pour être ignorées. Microsoft montre certainement l’exemple, et la seule question est de savoir comment les autres devraient suivre. Au cours de la dernière décennie, les organisations agiles ont surpassé les autres en prenant et en gérant les décisions plus rapidement. L'adoption de l'agilité dans le domaine informatique a entraîné des changements dans la manière dont les infrastructures, les applications, les données et les compétences sont produites, consommées ou les deux. Les éléments fondamentaux de l'agilité, tels que la collaboration, l'automatisation et les améliorations continues, sont les principales sources d'innovation pertinentes pour les applications, le développement et le déploiement.</p><p>Les événements récents ont perturbé la progression agile dans le domaine informatique. L’ère de flexibilité induite par la COVID-19 dans la manière, le moment et le lieu de travail remet en question le statu quo dans nos modes de collaboration. En outre, l’IA générative et les complexités associées à la gouvernance informatique ont également bouleversé la progression agile. Par conséquent, une économie incrémentale a été créée qui oblige chaque entreprise à prendre les opportunités et les défis plus au sérieux.</p><p>L'agilité distribuée est déjà en pratique depuis un certain temps. Les réalités du travail à distance et hybride ne sont que des extensions de ce que nous avons déjà vu dans les équipes distribuées. Cependant, les promesses de productivité d'outils tels que Copilot sont nouvelles, donc supposer que les pratiques Agile actuelles fonctionneront avec les pratiques GenAI est une erreur. Alors que pouvons-nous faire ?</p><p>Voici quelques réflexions sur la manière d’intégrer la méthodologie agile dans le cadre de l’adoption de Copilot.</p><h3>Étendre DevOps pour inclure la représentation de DataOps et MLOps</h3><p>Étant donné l’importance des données ainsi que des modèles d’IA et d’apprentissage automatique, les équipes DevOps doivent inclure des représentants des équipes DataOps et MLOps (ModelOps est un sous-ensemble de MLOps). Ce n’est qu’à ce moment-là que l’objectif de rapprocher la « production » et les « opérations » peut être atteint. À première vue, le remplacement peut sembler être la plus grande menace de l’IA, mais son premier acte sera plutôt de révéler et d’approfondir les fissures dans la collaboration.</p><h3>L’intelligence logicielle est plus importante que jamais</h3><p>Ne pas comprendre les systèmes d’application de manière globale avant de produire automatiquement le code en production sera désastreux. L’informatique d’entreprise est un mélange d’applications et d’actifs informatiques IA et non IA. De plus, l’« explicabilité » du code ne peut être obtenue que lorsque l’intelligence logicielle sur le code produit par GenAI est atteinte. En fin de compte, ce n’est pas l’exactitude fonctionnelle, mais l’adéquation architecturale qui compte le plus pour débloquer des améliorations de productivité. L’IA évolue rapidement, mais laisser la vitesse prendre une trop grande priorité ouvre la voie à l’échec.</p><h3>La conformité continue et la sécurité continue sont tout aussi importantes</h3><p>L’une des principales préoccupations des outils GenAI concerne les vulnérabilités que le code généré automatiquement peut introduire dans l’informatique des entreprises, ce qui est la principale raison pour laquelle le Congrès américain a interdit l’utilisation de Copilot. Il est important de procéder aux ajustements appropriés au niveau des modules pour la conformité et la sécurité, afin qu’ils soient conçus et livrés en continu, plutôt que d’être vérifiés et assurés périodiquement. La réglementation est connue pour être à l’origine de l’innovation, et les entreprises doivent délibérément anticiper les difficultés futures.</p><h3>Augmenter les portes de qualité dans votre pipeline CI/CD pour les assistants IA</h3><p>Les principes fondateurs de l'open source (transparence, inspection et adaptation) peuvent être étendus aux produits GenAI. L'« inspection » ne doit pas seulement couvrir les aspects qualité, performance, sécurité et UX du code fourni par les outils, mais également l'adéquation architecturale au sein de l'informatique de l'entreprise.</p><h3>Mesurez le succès et soyez transparent sur vos lacunes</h3><p>L’impact de l’IA peut être flou, mais certains résultats doivent être mesurables pour justifier son adoption. L’élaboration d’indicateurs de performance clés spécifiques à l’IA peut contribuer à consolider le rôle du copilote au sein de l’équipe. Trouver les bons indicateurs à mesurer est de la plus haute importance et constitue en soi un défi de taille.</p><p>Dans l’analyse détaillée de l’impact de l’IA, il est également important d’accepter ouvertement les défauts. Le système est, bien sûr, imparfait par nature, et ces imperfections doivent être suivies et corrigées. L’IA évoluant si rapidement, de nombreux problèmes seront probablement résolus à court terme. Notez les défauts et faites-en un examen régulier.</p><h3>L’inadéquation des compétences aura un impact sur les promesses de productivité</h3><p>Les outils ne sont efficaces que si ceux qui les utilisent le savent. Un développeur expérimenté peut démontrer un niveau de productivité supérieur à la moyenne avec un assistant IA, mais un développeur inexpérimenté peut rapidement créer plus de problèmes que de solutions. Former les communautés de développement et d'assurance qualité à maîtriser les outils et les directives de gouvernance du code et des tests nécessite d'ajuster le modèle opérationnel Agile.</p><p>N’oubliez pas que les équipes DevOps sont bien plus que des développeurs, les équipes DataOps bien plus que des ingénieurs de données et les équipes ModelOps bien plus que des data scientists. Les compétences interdisciplinaires des équipes DevOps vont considérablement évoluer lorsque l’IA fera partie de la conversation. À mesure que les frontières entre ces disciplines s’estomperont, ceux qui seront prêts à s’adapter se hisseront au sommet.</p><p>L’adaptation de la méthodologie aux défis perçus ne doit pas limiter les avantages potentiels que GenAI peut produire. S’il est utilisé correctement, GenAI peut contribuer à l’hyper-automatisation des tâches de développement et d’assurance qualité, à l’évaluation des options de conception grâce au prototypage rapide, à la simplification du processus de documentation, à la surveillance de l’environnement de production pour prévoir les goulots d’étranglement des performances, etc.</p><p>Si nous n’adaptons pas nos méthodes agiles pour répondre à ces nouvelles réalités et dégager de la valeur plus rapidement, le « time to market » et les avantages en termes de coûts associés seront mal perçus. Les changements dans la méthodologie agile sont inévitables, car GenAI et Agile offrent de réels avantages concurrentiels. Ne vous laissez pas aller.</p>"
CISA : une faille critique de Jenkins exploitée dans des attaques de ransomware,"<p>Une faille de sécurité critique dans le populaire serveur d’automatisation open source Jenkins figure sur la liste des vulnérabilités connues de la Cybersecurity and Infrastructure Security Agency (CISA) après avoir été exploitée dans un ransomware et d’autres attaques.</p><p>L'agence de cybersécurité la plus importante du gouvernement américain a ajouté le bug - identifié comme CVE-2024-23897 et avec un score de gravité CVSS de 9,8 sur 10 - à son catalogue de vulnérabilités exploitées connues, ce qui met en garde les agences fédérales contre la nécessité de sécuriser leurs serveurs Jenkins, bien que la CISA ait également averti toutes les organisations exploitant de tels serveurs de s'assurer qu'ils sont sécurisés.</p><p>La vulnérabilité dans l'interface de ligne de commande (CLI) de Jenkins est une faille de parcours de chemin causée par une faiblesse dans l'analyseur de commandes args4j, qui peut être exploitée par des acteurs malveillants pour obtenir l'exécution de code à distance (RCE) et pour lire des fichiers arbitraires sur le serveur Jenkins.</p><p>Le serveur Jenkins basé sur Java, maintenu par CloudBees et la communauté Jenkins, est utilisé par les développeurs dans leur intégration continue et leur développement continu (CI/CD) et automatise les étapes du cycle de vie du développement logiciel, y compris le développement et le déploiement. L'outil, soutenu par des sociétés telles qu'Amazon Web Services (AWS), GitHub et JFrog, compte plus d'un million d'utilisateurs.</p><h3>Une faille devient publique et est corrigée</h3><p>Yaniv Nizry, chercheur en vulnérabilité chez SonarSource, développeur de logiciels open source, a été le premier à signaler cette faille de sécurité en janvier, soulignant qu’avec une part de marché d’environ 44 %, « la popularité de Jenkins est évidente. Cela signifie que l’impact potentiel des vulnérabilités de sécurité de Jenkins est important ».</p><p>Un correctif a été publié en janvier avec Jenkins 2.442, LTS 2.426.3 en désactivant la fonctionnalité d'analyse de commandes, les responsables expliquant que Jenkins est livré avec une CLI intégrée pour accéder à Jenkins à partir d'un environnement de script ou de shell. Il utilise la bibliothèque args4j pour analyser les arguments et les options de commande sur le contrôleur Jenkins lors du traitement des commandes CLI.</p><p>« Cet analyseur de commandes possède une fonctionnalité qui remplace un caractère @ suivi d'un chemin de fichier dans un argument par le contenu du fichier (expandAtFiles) », ont écrit les responsables. « Cela permet aux attaquants de lire des fichiers arbitraires sur le système de fichiers du contrôleur Jenkins en utilisant l'encodage de caractères par défaut du processus du contrôleur Jenkins. »</p><h3>Les cybercriminels se lancent</h3><p>Des preuves de concept (POC) auraient commencé à émerger peu après la publication du correctif par Jenkins. Les chercheurs de Trend Micro ont signalé en mars qu'ils avaient observé plusieurs attaques exploitant la faille, 28 des 44 adresses IP sources des attaques provenant des Pays-Bas, les autres de pays comme Singapour et l'Allemagne. La plupart des cibles se trouvaient en Afrique du Sud.</p><p>Ils ont également constaté des cas où des exploits RCE étaient échangés.</p><p>D’autres chercheurs ont découvert des attaques plus récentes exploitant la vulnérabilité de Jenkins. En juillet, CloudSEK a signalé une attaque de la chaîne d’approvisionnement contre Born Group, une agence internationale de conseil et d’expérience client basée à New York, par le groupe de menaces IntelBroker, spécialisé dans les violations de données, l’extorsion et la vente d’accès à des systèmes compromis.</p><p>Les chercheurs de CloudSEK ont déclaré qu'IntelBroker avait exploité CVE-2024-23897 pour obtenir un accès initial via un serveur Jenkins vulnérable avant d'accéder au référentiel GitHub de Born Group.</p><h3>Attaque de ransomware en Inde</h3><p>Plus tôt ce mois-ci, les chercheurs du Juniper Threat Lab ont écrit sur une attaque de ransomware contre Brontoo Technology Solutions, une société de services et de conseil informatique en Inde qui collabore avec C-Edge Technologies, une coentreprise entre Tata Consultancy Services et la State Bank of India. Juniper et CloudSEK ont attribué l'attaque au groupe de ransomware RansomXXX, qui existe depuis 2018, opère depuis la Russie ou l'Europe de l'Est et cible les agences gouvernementales, les banques et les organismes de santé.</p><p>L’attaque a perturbé les paiements de détail dans les banques indiennes. Une fois encore, les acteurs malveillants ont obtenu un accès initial à l’environnement informatique de Brontoo via la vulnérabilité Jenkins.</p><p>« Cette vulnérabilité permet à un utilisateur non authentifié de lire les premières lignes de n’importe quel fichier du système de fichiers », ont écrit les chercheurs. « Elle existe parce que la fonction intégrée de l’analyseur de commandes n’a pas été désactivée par défaut. Si elle est exploitée avec succès, cette vulnérabilité peut entraîner la fuite de fichiers et de données sensibles, l’exécution potentielle de commandes et permettre une attaque par ransomware. »</p>"
Le succès d'AIOps nécessite des données de télémétrie Internet synthétiques,"<p>Toute forme d’intelligence artificielle (IA) n’est efficace que si les données utilisées pour la former sont exactes. Si les entreprises souhaitent appliquer efficacement l’IA aux opérations informatiques (ITOps), elles doivent collecter autant de données de télémétrie que possible.</p><p>Les équipes informatiques découvrent souvent que leur plateforme AIOps a été formée sur une base restreinte de données de télémétrie. Ces données peuvent avoir été collectées, par exemple, à partir d’une plateforme DevOps qui n’a pas une visibilité complète sur l’environnement informatique distribué dans lequel s’exécute leur application. En l’absence de données de télémétrie synthétiques collectées à partir d’une plateforme de surveillance des performances Internet (IPM) pour collecter ces données de télémétrie, il est tout simplement peu probable que les algorithmes d’apprentissage automatique qui sont au cœur de toute plateforme AIOps parviennent à faire émerger les meilleures recommandations pour optimiser les expériences applicatives.</p><p>Le défi réside dans la nature probabiliste de l’IA. La pertinence des recommandations qui en découlent est déterminée par la qualité des données qui ont été exposées au modèle d’IA. Les données réelles des utilisateurs, par exemple, peuvent être rares, voire inexistantes. Bien entendu, si les données de télémétrie n’ont jamais été partagées avec le modèle d’IA, il est extrêmement peu probable que les recommandations de l’IA améliorent l’expérience des applications.</p><h3>Gagner en visibilité grâce aux données synthétiques</h3><p>Les équipes informatiques doivent s’assurer que les données utilisées pour former le modèle d’IA reflètent les environnements de production dans lesquels les applications sont déployées. Sinon, quel que soit le niveau de développement d’un modèle d’IA, les données qui entrent sont toujours synonymes de données qui sortent. Avant qu’une équipe informatique n’adopte une plateforme AIOps, elle doit connaître la provenance des données du modèle d’IA sous-jacent. Si le pool de données de formation de l’IA est limité, les recommandations générées le seront également. Les équipes informatiques ne vont pas faire confiance aux plateformes AIOps qui leur conseillent de prendre des mesures spécifiques sur la base de données partielles ou incomplètes. Et elles ne devraient pas le faire.</p><p>Au lieu de cela, les équipes supposeront que chaque résultat doit être vérifié avant que l’étape suivante d’un processus ne soit autorisée. Après tout, la seule chose pire que de se tromper en matière d’informatique et d’IA est de se tromper à une échelle catastrophique. Bien entendu, continuer à gérer l’informatique de manière séquentielle va sans doute à l’encontre de l’objectif d’investir dans une plateforme AIOps censée gérer les tâches en parallèle.</p><p>Étant donné la dépendance des applications modernes aux services Internet, tout effort visant à appliquer l’IA à la gestion informatique sans inclure de données de télémétrie Internet synthétiques conduira à un résultat sous-optimal. En incluant ce type de télémétrie, les informations fournies aux équipes DevOps leur permettront de garantir que les indicateurs clés de performance (KPI) sont atteints et maintenus.</p><h3>Plusieurs modèles d'IA</h3><p>Il est peu probable qu’un seul modèle d’IA puisse tout contrôler. Dans de nombreux cas, les plateformes de réseau, de sécurité et de gestion des services informatiques (ITSM) auront déjà appliqué l’IA aux données de télémétrie qu’elles collectent en temps réel. Les résultats de ces modèles d’IA seront ensuite partagés avec les plateformes AIOps pour automatiser une série de tâches de bout en bout qui, auparavant, nécessitaient que les équipes informatiques orchestrent les flux de travail sur plusieurs îlots d’automatisation.</p><p>Les équipes DevOps doivent donc évaluer l’efficacité de ce qui sera bientôt un réseau de modèles d’IA. Il en existe tellement, chacun étant ou sera conçu pour automatiser des tâches spécifiques, comme l’analyse du trafic Internet pour identifier la source des goulots d’étranglement qui pourraient n’avoir qu’un impact intermittent sur une application. Armée de ces informations, la plateforme AIOps peut alors générer systématiquement des recommandations utiles auxquelles les équipes DevOps peuvent faire confiance. Elles peuvent ensuite laisser les outils appliquer automatiquement la suggestion sur la manière dont, par exemple, le trafic Internet doit être réacheminé pour maintenir les objectifs de niveau de service (SLO).</p><h3>Réaliser la promesse de l’IA</h3><p>À mesure que les plateformes AIOps s’améliorent, elles peuvent réduire considérablement la charge de travail que les équipes DevOps rencontrent régulièrement. Les équipes peuvent passer des semaines à essayer de déterminer la cause profonde d’un problème qui, une fois découvert, peut prendre quelques minutes à résoudre. Le défi est que la source du problème n’a souvent pas grand-chose à voir avec ce que l’équipe DevOps contrôle immédiatement, comme c’est le cas lorsque, par exemple, la latence créée par un service Internet a un impact négatif sur les performances de l’application. Cependant, ces informations devraient permettre aux équipes DevOps d’envoyer des demandes d’assistance qui identifient mieux la source exacte d’un problème de service Internet que leur fournisseur devrait être en mesure de résoudre plus rapidement. Tout aussi important, l’équipe DevOps peut passer aux problèmes sur lesquels elle a un contrôle plus direct.</p><p>Une grande partie du stress que subit une équipe DevOps provient du fait qu’elle ne connaît pas la véritable cause d’un problème qui, malgré tous ses efforts, continue de générer une série d’alertes en continu. L’AIOps promet de réduire ce stress en simplifiant d’abord la corrélation des causes, puis en automatisant la correction. Cette promesse ne sera toutefois jamais pleinement tenue si les données sur lesquelles repose l’entraînement du modèle d’IA ne fournissent pas suffisamment d’informations pour prendre une décision véritablement éclairée.</p>"
Sumo Logic élimine les frais d'ingestion pour les données du journal d'observabilité,"<p>Sumo Logic a annoncé cette semaine qu'il ne facturerait plus de frais pour l'ingestion de données de journaux dans sa plateforme d'observabilité afin d'encourager les équipes DevOps à appliquer l'analyse plus en profondeur.</p><p>Michael Cucchi, vice-président du marketing produit chez Sumo Logic, a déclaré que le plan de licences flexibles de Sumo Logic élimine ce qui équivaut à une taxe pour l'utilisation d'une plateforme d'observabilité. Les licences flexibles sont immédiatement disponibles pour les nouveaux clients et seront proposées aux clients Sumo Logic existants plus tard dans l'année. Il n'y a pas de frais mensuels cachés, de restrictions de fonctionnalités, de compromis de performances ou de limitations d'utilisateurs appliqués dans le cadre de ce plan.</p><p>Le plan de licence ne s'applique qu'aux données de log, mais les équipes DevOps pourront ingérer toutes les données structurées, semi-structurées et non structurées pour un coût inférieur que Sumo Logic s'efforce de stocker plus efficacement, a déclaré Cucchi. L'objectif global est de mettre à disposition une approche disruptive de la gestion des licences qui réduit le coût total de l'observabilité, a-t-il ajouté.</p><p>Le stockage des données de journalisation va devenir de plus en plus crucial à mesure que les entreprises commenceront à appliquer plusieurs types de modèles d’intelligence artificielle (IA) pour automatiser davantage les opérations informatiques, a-t-il ajouté. Ces modèles d’IA promettent de rendre les plateformes d’observabilité plus accessibles, car les algorithmes seront capables de faire apparaître les problèmes qui doivent être résolus. Historiquement, la valeur d’une plateforme d’observabilité a été liée à la compétence d’une équipe DevOps dans l’utilisation de son langage de requête.</p><p>Les équipes informatiques qui ont adopté des plateformes d’observabilité ont, à des degrés divers, limité la quantité de données qu’elles collectent et conservent pour éviter que les coûts de stockage n’explosent. Le défi est que, à mesure que les équipes DevOps déploient davantage d’applications basées sur des microservices, la quantité de données de journal générées a explosé. Déterminer la cause profonde d’un problème impliquant ces applications peut s’avérer problématique si les données de journal ne sont pas facilement disponibles. Les données de journal sont, après tout, l’unité atomique de l’observabilité, a noté Cucchi.</p><p>En général, les environnements informatiques deviennent trop complexes pour être gérés par des humains sans l’aide de plateformes d’observabilité complétées par des modèles d’IA, a noté Cucchi. Chaque environnement d’application est assez unique, il est donc essentiel que ces modèles d’IA soient exposés à autant de données que possible pour garantir des résultats optimaux.</p><p>Il n’est pas certain que les plateformes d’observabilité puissent un jour remplacer la plupart des outils de surveillance sur lesquels les équipes informatiques s’appuient aujourd’hui pour suivre un ensemble de mesures prédéfinies. Cependant, une chose est sûre : les plateformes d’observabilité offrent au moins l’occasion de rationaliser certains de ces outils. Le plus grand défi consiste à trouver des moyens de financer l’acquisition d’une plateforme d’observabilité en premier lieu, ce qui crée ensuite une opportunité de consolider ces outils par la suite.</p><p>Étant donné la complexité des plateformes informatiques hautement distribuées, ce n’est qu’une question de temps avant que la plupart des équipes informatiques ne disposent d’une certaine capacité d’observabilité. Bien entendu, l’observabilité a toujours été un principe fondamental de DevOps. Le problème est que les outils utilisés pour y parvenir sont des plateformes de surveillance héritées qui n’offrent pas le niveau de profondeur requis pour gérer avec succès les environnements d’application modernes.</p><p>Source de l'image : Joshua Sortino via Unsplash.</p>"
Techstrong Research PulseMeter : la mise en cache transforme les performances des applications,"<p>La demande d’accès instantané aux données et d’expériences numériques fluides n’a jamais été aussi forte. Le récent rapport PulseMeter de Techstrong Research, « Database Caching Hits the Mainstream », se penche sur l’adoption de la mise en cache des bases de données et son rôle essentiel dans l’amélioration des performances et de l’évolutivité des applications.</p><p>Alors que le volume et l'utilisation des données augmentent à un rythme sans précédent, les développeurs de logiciels et les professionnels des bases de données recherchent constamment des solutions innovantes pour améliorer la lecture/écriture afin d'offrir une efficacité et des performances optimales. La mise en cache des bases de données est passée de son statut de niche à celui de stratégie courante pour les organisations qui souhaitent accroître les performances des applications, relever les défis de l'évolutivité et améliorer la résilience des applications et des bases de données dans des conditions de fonctionnement fluctuantes.</p><p>Les entreprises numériques et les attentes des clients rendent les applications plus nécessaires pour fournir un accès aux données en temps réel ou quasi réel. Cette nécessité est particulièrement aiguë dans les applications mobiles et Web, les environnements de données distribués, l'architecture de microservices cloud-native et les cas d'utilisation à forte demande comme les jeux, qui nécessitent tous des niveaux de performance et de fiabilité des bases de données sans précédent.</p><p>Le rapport PulseMeter souligne que l’amélioration de la latence et des performances grâce à la mise en cache des bases de données peut considérablement accroître les revenus et favoriser des expériences numériques positives. À l’inverse, l’absence de mise en œuvre de ces améliorations peut faire la différence entre le succès et l’échec d’une organisation. La mise en cache des bases de données est passée d’une technique spécialisée pour des cas d’utilisation spécifiques à un aspect fondamental de l’optimisation des performances des bases de données et des applications. Elle est désormais considérée comme une technologie fiable et éprouvée qui répond aux défis croissants liés au volume de données et aux exigences analytiques des applications contemporaines.</p><p>Les données recueillies par Techstrong Research auprès de DevOps, de développeurs de logiciels, de professionnels des bases de données, d’ingénieurs SRE, d’ingénieurs de plateforme et d’autres parties prenantes révèlent des informations essentielles sur l’adoption et l’impact de la technologie de mise en cache. 64,8 % des répondants utilisent déjà la mise en cache, et 13,8 % supplémentaires évaluent son intégration dans leurs opérations. Cette adoption significative souligne le rôle essentiel de la mise en cache dans l’infrastructure informatique actuelle, avec des solutions de pointe comme Amazon ElastiCache, Redis et NGINX détenant collectivement plus de 40 % des parts de marché.</p><p>Parmi les technologies à l’origine de l’essor de la mise en cache des bases de données, Redis est bien connu pour sa polyvalence et ses performances. Disponible à la fois en version open source et commerciale, Redis est parfaitement adapté à un large éventail de cas d’utilisation, de la mise en cache de base aux structures de données avancées, aux opérations, aux analyses en temps réel et à la gestion des files d’attente. Ce qui distingue Redis, ce sont les améliorations de performances qu’il offre, permettant des applications en temps réel dans les jeux, les services financiers, la santé et d’autres secteurs. Le fort taux d’adoption de Redis, comme le souligne le rapport PulseMeter de Techstrong Research, souligne son rôle central dans l’acceptation et la mise en œuvre généralisées des technologies de mise en cache des bases de données.</p><p>L’étude identifie les performances et la fiabilité comme les principales considérations pour les entreprises qui explorent de nouvelles technologies de mise en cache. De plus, il existe une demande croissante d’expertise externe pour obtenir une conception et une mise en œuvre optimales de la mise en cache, ce qui reflète la complexité et les connaissances spécialisées requises pour naviguer efficacement dans ce domaine.</p><p>Le rapport PulseMeter de Techstrong Research souligne le rôle essentiel de la mise en cache des bases de données dans la prise en charge des applications en temps réel et de l’expérience numérique. Alors que les entreprises sont confrontées aux défis de la transformation numérique et à la croissance exponentielle des données, les technologies de mise en cache s’imposent comme des outils essentiels pour améliorer les performances des bases de données, garantir l’évolutivité et répondre aux exigences toujours croissantes des applications et services modernes. Ce passage d’une solution de niche à une nécessité grand public marque une évolution significative des stratégies de gestion des bases de données, soulignant la nécessité d’une innovation et d’une expertise continues dans l’application des technologies de mise en cache.</p><p>Remarque : Redis a sponsorisé le rapport PulseMeter de Techstrong Research, « La mise en cache des bases de données devient courante ».</p>"
JFrog étend ses efforts d'intégration MLOps via Qwak Alliance,"<p>JFrog a annoncé aujourd'hui l'intégration de sa plateforme DevSecOps avec une plateforme d'opérations d'apprentissage automatique gérées (MLOps) de Qwak pour faire progresser la collaboration entre les équipes créant et déployant plusieurs classes d'artefacts logiciels.</p><p>Cette alliance fait suite à une alliance similaire avec Amazon Sagemaker annoncée le mois dernier, qui intégrait également un service géré pour la création de modèles d'intelligence artificielle (IA) fournis par Amazon Web Services (AWS) avec la plateforme JFrog Software Supply Chain.</p><p>Les deux plates-formes d'opérations d'apprentissage automatique (MLOps) fournissent aux équipes de science des données une pile complète d'outils organisés nécessaires à la création de modèles d'IA à partir de zéro, plutôt qu'à la personnalisation de modèles d'IA déjà créés.</p><p>Gal Marder, vice-président exécutif de la stratégie chez JFrog, a déclaré que l'intégration avec la plateforme Qwak permet de gérer les artefacts logiciels créés par les équipes MLOps aux côtés du reste des artefacts logiciels qu'une équipe DevSecOps gère déjà. Cette approche permet également de détecter et de bloquer l'utilisation de modèles ML malveillants en plus de garantir que les modèles sont conformes aux politiques de l'entreprise et aux exigences réglementaires, a-t-il noté.</p><p>Comme il devient évident que davantage de modèles d'IA seront directement intégrés aux applications, la nécessité d'intégrer les flux de travail DevOps aux plateformes d'opérations d'apprentissage automatique (MLOps) utilisées pour créer des modèles d'IA devient de plus en plus prononcée, a déclaré Marder.</p><p>La plateforme JFrog Software Supply Chain peut être utilisée pour fournir aux scientifiques de données et aux développeurs une source unique de vérité pour gérer en toute sécurité les artefacts logiciels à l'aide d'un référentiel commun afin de favoriser une plus grande collaboration entre les équipes qui, dans la plupart des cas, déterminent encore la meilleure façon de collaborer, a-t-il ajouté.</p><p>Le défi est que les équipes de data science forment et déploient généralement des modèles d’IA tous les quelques mois, tandis que les équipes DevSecOps mettent souvent à jour les applications plusieurs fois par mois. Les équipes de data science et DevSecOps ont donc généralement des cultures distinctes, mais à long terme, les flux de travail DevOps et MLOps finiront par fusionner, a déclaré Marder.</p><p>La plupart des entreprises tentent encore de déterminer la meilleure façon d’exploiter l’IA à l’aide de leurs propres données. À terme, les modèles d’IA sont invoqués via des interfaces de programmation d’applications (API). Cependant, ce n’est désormais qu’une question de temps avant que davantage de modèles d’IA soient intégrés aux applications pour améliorer les performances globales. Le défi est que les modèles d’IA ne peuvent pas être mis à jour de la même manière que les autres artefacts logiciels sont corrigés. La gestion des versions des modèles d’IA nécessitera donc un ensemble de contrôles différent lorsqu’un modèle remplace un autre. JFrog, par exemple, a développé des fonctionnalités de gestion des versions qui peuvent être appliquées aux modèles d’IA dans le contexte d’un flux de travail DevOps.</p><p>Les plateformes MLOps ne manquent pas, les équipes DevOps doivent donc s’attendre à voir une vague d’alliances se former entre les fournisseurs de ces plateformes. Il est moins évident de savoir dans quelle mesure ces alliances pourraient conduire à des fusions et acquisitions entre les fournisseurs de ces plateformes.</p><p>D’une manière ou d’une autre, les modèles d’IA arrivent dans les workflows DevSecOps. La seule question à résoudre est de savoir comment gérer au mieux leur déploiement aux côtés de tous les autres types d’artefacts logiciels qui circulent déjà dans les pipelines DevOps existants.</p>"
Ce que l'évolution du matériel spécialisé pour l'IA et le ML signifie pour DevOps,"<p>L’adoption généralisée des technologies d’intelligence artificielle (IA) et d’apprentissage automatique (ML) accélère l’évolution du matériel informatique, essentiel pour automatiser les processus complexes et améliorer la précision de la prise de décision. Cette accélération est cruciale pour faire progresser l’informatique et le traitement des données, en particulier dans les différents segments des pipelines de données IA/ML.</p><p>Cette demande croissante incite un large éventail de fabricants de puces, des acteurs établis aux concurrents émergents, à innover et à jouer un rôle de premier plan dans le développement de solutions de traitement plus rapides et plus efficaces. L’objectif principal ? Concevoir des puces qui optimisent le transfert de données, améliorent la gestion de la mémoire et renforcent l’efficacité énergétique. Ces avancées sont loin d’être progressives ; elles sont essentielles pour répondre aux exigences croissantes des applications sophistiquées d’IA et de ML.</p><p>L’évolution du paysage matériel de DevOps nécessite une plus grande unification et une plus grande automatisation des différentes applications de l’infrastructure d’IA spécialisée. Avec la diversification des architectures de puces, la création de piles d’applications axées sur la portabilité, les performances et la facilité d’accès devient encore plus cruciale. La capacité à adopter de manière transparente plusieurs architectures sera essentielle pour trouver le bon équilibre entre les différentes capacités techniques des personnes qui souhaitent interagir avec la technologie.</p><p>Cet article explore les implications de ces avancées matérielles sur les processus DevOps au sein de l'IA et du ML. En outre, nous explorons les stratégies que les équipes DevOps peuvent envisager pour garantir que les applications restent efficaces et portables sur différentes architectures de puces.</p><p>Par le passé, le matériel à usage général, notamment les GPU et les CPU, était la base de nombreuses charges de travail. Cependant, un changement clair vers le matériel spécialisé est en cours dans l’intelligence artificielle et l’apprentissage automatique, où les exigences en matière de formation et d’inférence ont augmenté de manière exponentielle. Ces applications se heurtent souvent aux limitations de performances inhérentes au matériel traditionnel, en partie en raison des contraintes modernes de la loi de Moore. Par conséquent, il existe un besoin croissant de matériel capable de gérer des tâches d’IA spécifiques avec une efficacité et une rapidité accrues. Par exemple, dans certains scénarios d’apprentissage automatique, l’utilisation de matériel prenant en charge les calculs à virgule flottante en simple précision peut accélérer les processus sans avoir besoin de la précision fournie par les calculs en double précision.</p><p>Même si NVIDIA reste une force dominante sur le marché des puces d’IA, la concurrence s’intensifie et diverses entreprises proposent des alternatives innovantes. Et ce ne sont pas seulement les habituels Intel ou AMD qui sont à l’origine de cette concurrence. Google a également fait des progrès avec ses unités de traitement Tensor (TPU). Amazon a récemment annoncé Trainium2, une nouvelle puce d’IA conçue spécifiquement pour la formation des systèmes d’IA. Cette puce, qui devrait concurrencer Maia de Microsoft et les TPU de Google, souligne la tendance croissante des grandes entreprises technologiques à développer des puces d’IA personnalisées. Au-delà de ces géants, des startups telles que Cerebras, SambaNova Systems, Graphcore et Tenstorrent apportent de nouvelles solutions matérielles d’IA.</p><p>À mesure que le matériel spécialisé devient de plus en plus répandu, la communauté DevOps devra gérer de nouveaux défis, notamment la portabilité des performances. La portabilité des performances consiste à garantir que les applications fonctionnent efficacement et fonctionnent bien sur différentes architectures informatiques avec un minimum de modifications, voire aucune.</p><p>L’informatique cognitive (la catégorie plus large de l’IA et du ML) varie en complexité, en fonction des algorithmes, des modèles et des exigences uniques des créateurs en matière de fonctionnalités spécifiques au matériel. Si une version adaptée à l’architecture optimisera certainement les performances sur les plateformes respectives, elle complique le processus visant à garantir une expérience logicielle cohérente sur différents matériels.</p><p>Le défi de la conception d’un système consiste à optimiser l’environnement pour une efficacité maximale, en particulier lorsque la nature précise de la charge de travail est inconnue de ceux qui sont responsables de la conception et du support des systèmes.</p><p>Bien entendu, les pipelines d’intégration continue et de déploiement continu (CI/CD) doivent également faire l’objet de considérations importantes et connexes. Les subtilités des pipelines CI/CD sont amplifiées lorsque l’on recherche la portabilité des performances. La nécessité de valider les performances logicielles sur plusieurs configurations matérielles introduit une matrice de tests plus élaborée et peut allonger les cycles de déploiement, affectant directement les exigences de mise sur le marché. Soudain, les charges de travail franchissent désormais les limites traditionnelles de l’infrastructure informatique et des hautes performances/supercalculateurs autrefois définies par les technologies de microservices et de traitement par lots ; elles ne font désormais qu’un dans un pipeline CI/CD.</p><p>À mesure que les entreprises adoptent du matériel spécialisé, il existe un risque d’augmentation parallèle du nombre de spécialistes se concentrant uniquement sur un type de matériel ou sur un cas d’utilisation d’application. Si une telle expertise peut favoriser l’innovation et l’optimisation d’une plateforme particulière, elle crée également de la complexité et un risque de silos de connaissances et de complexités inutiles pour les équipes opérationnelles et les clients qui utilisent ces systèmes.</p><p>La portabilité des performances et les concepts étroitement liés, tels que les performances indépendantes du matériel et l'efficacité multiplateforme, sont de plus en plus importants pour les équipes DevOps. À mesure que le paysage technologique évolue, la question urgente devient : comment l'industrie et les équipes DevOps peuvent-elles gérer cette évolution en toute transparence ?</p><p>Les recherches et le développement en cours joueront sans aucun doute un rôle clé. Par exemple, le ministère américain de l’Énergie (DoE) étudie de nouvelles méthodologies pour soutenir son projet de calcul exascale. Il s’agit notamment d’affiner les bibliothèques de logiciels existantes, d’élaborer de nouveaux modèles de programmation et de développer de nouveaux outils qui pourraient éventuellement influencer les pratiques DevOps plus larges. D’autres chercheurs développent des couches d’abstraction logicielle, visant à simplifier l’adaptation de code générique à des configurations matérielles spécifiques.</p><p>Au-delà des nouveaux outils et méthodologies qui peuvent provenir des efforts actuels de R&D, il existe de nombreux outils et processus existants qui se prêtent à l’amélioration de la portabilité des performances, notamment :</p><li>Conteneurisation : les conteneurs encapsulent les applications et leurs dépendances de manière à garantir leur exécution cohérente dans différents environnements. Les outils open source comme SingularityCE avec compatibilité Open Container Initiative (OCI) peuvent aider à standardiser et à simplifier le déploiement sur différentes configurations matérielles, favorisant ainsi la portabilité des performances pour le calcul haute performance et la gestion traditionnelle de l'infrastructure informatique.</li><li>Analyse comparative et profilage : pour garantir la portabilité des performances, il est impératif de comprendre le comportement des logiciels sur différentes architectures. Les outils d’analyse comparative fournissent des mesures quantitatives des performances, tandis que les outils de profilage offrent des informations sur le comportement des logiciels, aidant ainsi les développeurs à identifier les goulots d’étranglement et les domaines nécessitant une optimisation.</li><li>Bibliothèques de portabilité de code : en plus des bibliothèques comme OpenCL qui permettent l'exécution de logiciels sur divers matériels, les avancées récentes dans les technologies de conteneurs complètent cette capacité. Par exemple, les améliorations apportées aux interfaces des périphériques de conteneur, telles que celles de SingularityCE, rationalisent l'intégration de ressources spécifiques au matériel. Ce développement aide les équipes DevOps à optimiser les logiciels pour divers matériels sans réécritures approfondies de la base de code, illustrant les outils prenant en charge la diversité matérielle et l'agilité logicielle dans tous les aspects de l'informatique cognitive.</li><p>Bien entendu, en plus de tous ces outils et stratégies, les méthodologies agiles resteront essentielles car elles privilégient le développement itératif, le feedback/l’amélioration continue et l’adaptabilité, autant d’éléments importants pour les configurations matérielles et logicielles en évolution rapide.</p><p>À l’heure où nous embrassons les nouvelles frontières de l’IA et du ML, le rôle des équipes DevOps dans la navigation dans un paysage matériel en constante évolution devient de plus en plus vital. Au cœur de ce parcours se trouve le formidable défi de la portabilité des applications, un défi qui nécessite une expertise technique et un changement stratégique vers l’adaptabilité. C’est là que les conteneurs apparaissent comme des outils indispensables qui garantissent une expérience et des performances applicatives cohérentes sur diverses plateformes et gèrent les subtilités de la portabilité.</p><p>De même, l’adoption de méthodologies agiles va au-delà de l’adhésion aux processus ; elle incarne un état d’esprit de flexibilité et de réactivité, essentiel dans les changements technologiques rapides. Ces approches, loin d’être des solutions temporaires, font partie intégrante d’une stratégie visant à libérer le potentiel de l’IA. Alors que les équipes DevOps continuent de relever ces défis, leur succès dépendra de leur adaptabilité et de leur volonté d’explorer et d’intégrer les technologies établies et émergentes, en particulier celles qui excellent en termes d’évolutivité et d’efficacité. Cette exploration et cette intégration proactives seront la clé de la survie et de la prospérité dans ce paysage technologique dynamique. Et pourtant, alors que nous avons évoqué ce problème concernant les applications scientifiques, l’impact se fait également sentir du côté AIOps, qui utilise du matériel spécifique à l’IA avec des pratiques AIOps. Bien que notre objectif ici était de discuter des complexités de la création et de l’administration de systèmes d’intelligence artificielle et d’apprentissage automatique, nous n’avons pas encore abordé la conversation sur la sécurité, en particulier avec l’informatique confidentielle, qui est un sujet pour une autre fois. Bon informatique.</p>"
Comment les équipes DevOps peuvent-elles utiliser les données de Customer Intelligence ?,"<p>L’intelligence client consiste à collecter et à analyser des données sur les comportements, les préférences et les besoins des clients. Elle aide les entreprises à mieux comprendre leurs clients et à adapter leurs produits et services pour répondre à leurs demandes. Les données sont collectées à partir de diverses sources, telles que les commentaires des clients, les analyses Web, les médias sociaux, l’historique des achats et les mesures d’utilisation des logiciels. Les informations tirées de l’intelligence client peuvent être utilisées pour éclairer la stratégie commerciale, le développement de produits et les efforts marketing.</p><p>L’objectif de la veille client est de comprendre en détail les besoins, les préférences et les habitudes des clients. Ces informations peuvent être utilisées pour prédire le comportement futur, améliorer le service client et stimuler les ventes. Par exemple, si une société de logiciels sait qu’un segment de clientèle particulier accède principalement à ses logiciels sur des appareils mobiles, l’organisation peut améliorer son support mobile ou créer des applications mobiles dédiées à ce segment de clientèle.</p><p>Une équipe DevOps peut exploiter les données recueillies à partir de l’intelligence client de plusieurs manières.</p><h3>Prioriser le développement des fonctionnalités en fonction des commentaires des clients</h3><p>Lors du développement de nouvelles fonctionnalités, il est utile d’analyser les commentaires des clients pour comprendre quelles fonctionnalités ils trouvent les plus utiles et les hiérarchiser pour le développement. Par exemple, si les commentaires suggèrent que les clients souhaitent une interface utilisateur plus intuitive, l’équipe peut donner la priorité aux améliorations de l’interface utilisateur.</p><p>En se concentrant sur les fonctionnalités que les clients trouvent les plus utiles, les équipes DevOps peuvent améliorer la convivialité et la fonctionnalité de leurs produits. Cela peut conduire à une plus grande satisfaction et fidélité des clients et, en fin de compte, à une augmentation des ventes. De plus, en donnant la priorité au développement de fonctionnalités en fonction des commentaires des clients, les équipes DevOps peuvent éviter de perdre du temps et des ressources sur des fonctionnalités que les clients n’apprécient pas.</p><h3>Adaptation de la conception UX et UI en fonction des préférences et des comportements des clients</h3><p>En analysant les données sur la façon dont les clients interagissent avec leurs produits, les équipes peuvent identifier les points faibles et les domaines à améliorer. Par exemple, si les données montrent que les clients ont des difficultés à naviguer sur le site Web, l'équipe peut repenser la navigation pour la rendre plus conviviale.</p><p>Alternativement, si les clients abandonnent leur panier à un certain moment du processus de paiement, l’équipe peut enquêter et apporter les améliorations nécessaires.</p><h3>Utilisation des données client pour piloter les fonctionnalités de personnalisation et de personnalisation</h3><p>La personnalisation consiste à adapter l'expérience client à l'utilisateur individuel, tandis que la customisation consiste à donner aux clients la possibilité d'adapter le produit ou le service à leurs besoins spécifiques.</p><p>Par exemple, en analysant les données sur l’historique de navigation et d’achat d’un client, une équipe pourrait développer une fonctionnalité qui recommande des produits en fonction du comportement passé du client. Ou, si un client achète fréquemment un certain type de produit, l’équipe pourrait créer une fonctionnalité qui lui permet de personnaliser ce produit à sa guise.</p><h3>Exploiter l'analyse prédictive pour prévoir les besoins et les tendances futurs des clients</h3><p>L'analyse prédictive consiste à utiliser des données historiques pour prédire des événements futurs. Dans le contexte de DevOps, cela peut signifier utiliser les données clients pour prévoir les besoins et les tendances futurs des clients.</p><p>Par exemple, si les données montrent un intérêt croissant pour les produits écologiques, l’équipe pourrait anticiper cette tendance et commencer à développer des produits plus respectueux de l’environnement. L’analyse prédictive peut également aider les équipes à identifier les problèmes potentiels avant qu’ils ne deviennent des problèmes, ce qui leur permet de les résoudre et d’améliorer l’expérience client de manière proactive.</p><p>Voici quelques bonnes pratiques que les équipes DevOps peuvent utiliser pour tirer le meilleur parti de leurs données d’intelligence client.</p><h3>Intégrez les commentaires des clients tôt et souvent</h3><p>Les commentaires des clients issus d’enquêtes, des réseaux sociaux et des interactions avec le service client fournissent des informations précieuses sur les besoins des clients, leurs difficultés et leurs préférences.</p><p>En intégrant ces commentaires dans le processus DevOps, les équipes peuvent mieux comprendre les besoins des clients et développer des produits ou des services qui répondent efficacement à ces besoins. L'intégration précoce des commentaires des clients permet aux équipes de s'adapter et d'apporter des modifications plus rapidement, économisant ainsi du temps et des ressources.</p><p>Il est également important de recueillir régulièrement des retours d’information. Les goûts et les préférences des clients évoluent en permanence, et leurs retours reflètent ces changements. L’intégration fréquente de renseignements sur les clients dans le processus de développement garantit que le travail de l’équipe DevOps reste en phase avec l’évolution du paysage client.</p><h3>Tirer parti des outils d'analyse</h3><p>Les outils d'analyse peuvent aider les équipes DevOps à analyser de grands volumes de données clients et à en extraire des informations exploitables. Ces informations peuvent ensuite être utilisées pour éclairer la prise de décision et stimuler l'innovation.</p><p>Certains outils excellent dans la visualisation des données, aidant les équipes à comprendre des ensembles de données complexes à l'aide de graphiques et de diagrammes. D'autres outils excellent dans l'analyse prédictive, aidant les équipes à prévoir les tendances futures en fonction des données historiques.</p><p>Le choix du bon outil d’analyse dépend des besoins spécifiques de l’équipe DevOps et de la nature des données de renseignement client dont elle dispose. L’essentiel est d’exploiter pleinement ses capacités pour extraire le plus de valeur possible des données de renseignement client.</p><h3>Assurer la qualité et la pertinence des données</h3><p>Toutes les données ne sont pas égales et si elles sont de mauvaise qualité ou non pertinentes, elles peuvent conduire à des décisions erronées et à un gaspillage de ressources.</p><p>La qualité des données fait référence à l’exactitude, à l’exhaustivité, à la cohérence et à la fiabilité des données. Les équipes DevOps doivent mettre en œuvre des contrôles de qualité des données rigoureux pour garantir que les données de renseignement client qu’elles utilisent sont de haute qualité.</p><p>La pertinence des données, quant à elle, fait référence à leur applicabilité à la tâche à accomplir. Toutes les données de veille client ne sont pas pertinentes pour tous les projets DevOps. Les équipes doivent sélectionner soigneusement les données les plus pertinentes pour leur projet spécifique et ignorer le reste.</p><h3>Équilibrer les données quantitatives et qualitatives</h3><p>Alors que les données quantitatives fournissent des chiffres précis et des faits concrets, les données qualitatives offrent des informations plus approfondies sur les attitudes, les perceptions et les comportements des clients.</p><p>Les données quantitatives peuvent aider les équipes DevOps à identifier les tendances, à mesurer les performances et à suivre les progrès au fil du temps. Cependant, elles ne parviennent souvent pas à expliquer pourquoi certaines tendances se produisent ou pourquoi les performances évoluent.</p><p>Cela peut également aider les équipes DevOps à comprendre les raisons qui se cachent derrière les chiffres. Cela peut fournir des informations sur les raisons pour lesquelles les clients se comportent comme ils le font et sur ce qu'ils pensent et ressentent vraiment à propos d'un produit ou d'un service. L'équilibre entre les données quantitatives et qualitatives donne aux équipes DevOps une vision plus globale du paysage client, leur permettant de prendre des décisions plus éclairées.</p><h3>Collaboration interfonctionnelle</h3><p>Les données d'intelligence client sont précieuses pour toutes les équipes de l'entreprise, pas seulement pour DevOps. En collaborant avec d'autres équipes, telles que le marketing, les ventes et le service client, l'équipe DevOps peut obtenir des informations et des perspectives supplémentaires.</p><p>La collaboration interfonctionnelle favorise également une culture de prise de décision basée sur les données dans toute l’organisation. Lorsque toutes les équipes utilisent les informations client pour éclairer leur travail, l’organisation dans son ensemble devient plus centrée sur le client, plus agile et plus innovante.</p><p>L’utilisation réussie des données de renseignement client dans le développement logiciel permet aux équipes de rester réactives aux besoins des clients et d’augmenter la valeur de leurs produits. Les équipes DevOps doivent intégrer les commentaires des clients tôt et souvent, exploiter les outils d’analyse, garantir la qualité et la pertinence des données, équilibrer les données quantitatives et qualitatives et favoriser la collaboration interfonctionnelle. En suivant ces bonnes pratiques, les équipes DevOps peuvent utiliser les données de renseignement client pour stimuler l’innovation et créer des produits et services qui trouvent un véritable écho auprès des clients.</p><p>Source de l'image : Josh Sortino via Unsplash</p>"
Guide DevOps sur le profilage Java,"<p>Le profilage Java est une technique utilisée pour comprendre le comportement détaillé d'une application Java. Il consiste à surveiller et à mesurer divers aspects de l'exécution d'un programme, tels que l'utilisation de la mémoire, l'utilisation du processeur, l'exécution des threads et la récupération de place.</p><p>Le profilage Java peut être utilisé à différentes étapes du cycle de vie du développement logiciel (SDLC). Au cours du développement, il peut aider à identifier les goulots d'étranglement et les points chauds de performances, qui peuvent ensuite être optimisés pour de meilleures performances. Dans la phase de test, le profilage peut être utilisé pour vérifier que l'application fonctionne comme prévu sous charge. Enfin, en production, le profilage peut être utilisé pour surveiller les performances de l'application et détecter les problèmes potentiels avant qu'ils n'affectent les utilisateurs.</p><p>Les outils de profilage Java fournissent des informations sur la machine virtuelle Java (JVM) et l'application qui y est exécutée. Ils permettent aux développeurs de surveiller l'exécution des threads, la création d'objets, la récupération de place et de nombreux autres aspects du fonctionnement de la JVM. En utilisant un profileur Java, les développeurs peuvent acquérir une compréhension approfondie des caractéristiques de performances de l'application et identifier les domaines potentiels d'optimisation.</p><h3>Impact sur l'efficacité et l'évolutivité des applications</h3><p>L'optimisation des performances joue un rôle essentiel dans DevOps en améliorant l'efficacité et l'évolutivité des applications. Les applications efficaces utilisent moins de ressources, ce qui réduit les coûts et permet d'avoir plus d'utilisateurs ou des charges de travail plus importantes. L'évolutivité est la capacité d'une application à gérer des charges de travail accrues sans diminution des performances. En identifiant et en éliminant les goulots d'étranglement des performances, les équipes DevOps peuvent s'assurer que les applications évoluent efficacement à mesure que la demande augmente.</p><h3>Relation entre performance et expérience utilisateur</h3><p>Les performances sont directement liées à l’expérience utilisateur. Une application lente ou peu réactive peut frustrer les utilisateurs et les amener à abandonner complètement l’application. En s’assurant que les applications fonctionnent bien, les équipes DevOps peuvent améliorer la satisfaction et la rétention des utilisateurs. De plus, en surveillant les performances des applications en production, les équipes DevOps peuvent identifier et résoudre de manière proactive les problèmes avant qu’ils n’affectent les utilisateurs.</p><h3>Gestion des coûts et optimisation des ressources</h3><p>L’optimisation des performances peut également contribuer à la gestion des coûts et à l’optimisation des ressources. En optimisant les performances des applications, les équipes DevOps peuvent réduire la quantité de ressources informatiques nécessaires à l’exécution de l’application. Cela peut se traduire par des économies de coûts importantes, en particulier dans les environnements cloud où les coûts sont directement liés à l’utilisation des ressources.</p><p>De plus, en comprenant les modèles d’utilisation des ressources de l’application, les équipes DevOps peuvent prendre des décisions plus éclairées concernant l’allocation des ressources et la planification des capacités. Cela peut aider à éviter le sur-provisionnement, qui conduit à un gaspillage de ressources, ou le sous-provisionnement, qui peut entraîner de mauvaises performances de l’application.</p><h3>Fuites de mémoire</h3><p>Les fuites de mémoire constituent un problème de performances courant dans les applications Java. Une fuite de mémoire se produit lorsqu'une application alloue continuellement de la mémoire mais ne parvient pas à la libérer lorsqu'elle n'est plus nécessaire. Au fil du temps, cela peut entraîner des exceptions OutOfMemoryError et provoquer le blocage de l'application.</p><p>Le profilage Java peut aider à identifier les fuites de mémoire en surveillant l’utilisation de la mémoire par l’application au fil du temps. Si l’utilisation de la mémoire augmente continuellement même lorsque l’application est inactive, cela peut indiquer une fuite de mémoire. Les outils de profilage peuvent également fournir des informations sur les objets qui consomment le plus de mémoire, ce qui peut aider à localiser la source de la fuite.</p><h3>Problèmes de conflit et de synchronisation des threads</h3><p>Les conflits de threads et les problèmes de synchronisation peuvent avoir un impact significatif sur les performances des applications Java. Les conflits de threads se produisent lorsque plusieurs threads tentent d'accéder simultanément à une ressource partagée, ce qui les oblige à attendre et entraîne une baisse des performances. Les problèmes de synchronisation, tels que les blocages et les livelocks, peuvent entraîner le blocage des threads, les empêchant de progresser.</p><p>Le profilage Java peut aider à détecter les conflits de threads et les problèmes de synchronisation en surveillant l'état et l'exécution des threads. Les outils de profilage peuvent afficher les threads en cours d'exécution, en attente ou bloqués et peuvent fournir une trace de pile de chaque thread, ce qui peut aider à identifier la cause des conflits ou des problèmes de synchronisation.</p><h3>Frais généraux de collecte des ordures</h3><p>La récupération de place est un aspect essentiel de la gestion de la mémoire de Java, mais elle peut également être une source importante de perte de performances. Pendant la récupération de place, la JVM interrompt l'exécution de l'application pour récupérer la mémoire des objets qui ne sont plus utilisés. Si la récupération de place se produit trop fréquemment ou prend trop de temps, cela peut entraîner des pauses de l'application et une baisse des performances.</p><p>Le profilage Java peut aider à comprendre et à optimiser le comportement du garbage collection. Les outils de profilage peuvent fournir des informations détaillées sur les événements de garbage collection, tels que leur fréquence, leur durée et la quantité de mémoire récupérée. Ces informations peuvent aider les développeurs à ajuster la configuration du garbage collector pour minimiser son impact sur les performances de l'application.</p><h3>Manque de ressources</h3><p>Le manque de ressources est un autre problème de performances courant dans les applications Java. Il se produit lorsqu'un système ou un processus ne parvient pas à obtenir un accès suffisant aux ressources, ce qui entraîne une baisse des performances ou une défaillance. Dans les applications Java, le manque de ressources peut être causé par des facteurs tels qu'une mémoire insuffisante, un processeur insuffisant, un espace disque insuffisant ou une bande passante réseau insuffisante.</p><p>Le profilage Java peut aider à détecter et à résoudre les problèmes de pénurie de ressources. En surveillant l'utilisation des ressources, les outils de profilage peuvent identifier le moment où les ressources deviennent rares et fournir des informations sur les facteurs contribuant à la pénurie de ressources. Ces informations peuvent aider les développeurs à optimiser l'utilisation des ressources et à garantir que l'application dispose de suffisamment de ressources pour fonctionner efficacement.</p><p>Les outils de profilage Java fournissent généralement les fonctionnalités suivantes :</p><li>Capacité à détecter les fuites de mémoire : les fuites de mémoire peuvent constituer un problème majeur dans les applications Java, car elles peuvent entraîner une erreur de manque de mémoire, perturbant le bon fonctionnement de l'application. Les outils de profilage peuvent aider à identifier ces fuites, permettant aux développeurs de les corriger avant qu'elles ne causent des problèmes plus graves.</li><li>Profilage du processeur : cela permet aux développeurs de voir combien de temps processeur chaque méthode de leur application consomme. En identifiant les méthodes qui consomment une quantité disproportionnée de temps processeur, les développeurs peuvent optimiser leur code pour améliorer ses performances.</li><li>Profilage détaillé des threads : cela peut aider les développeurs à identifier les problèmes de synchronisation, les blocages et autres problèmes potentiels dans les applications multithread. En fournissant ces informations essentielles, les outils de profilage peuvent aider les développeurs à créer des applications plus efficaces et plus performantes.</li><p>Voici quelques bonnes pratiques pour tirer le meilleur parti de vos efforts de profilage Java.</p><h3>Identifier les indicateurs clés de performance</h3><p>Tout d’abord, il est essentiel d’identifier les indicateurs de performance clés les plus pertinents pour votre application. Les indicateurs que vous choisissez dépendent de la nature de votre application et de ses exigences de performance. Parmi les indicateurs courants, citons le temps de réponse, l’utilisation du processeur, l’utilisation de la mémoire et l’activité de récupération de place. En identifiant ces indicateurs clés, vous pouvez utiliser vos outils de profilage pour les surveiller et identifier les problèmes potentiels.</p><h3>Intégration du profilage dans les pipelines CI/CD</h3><p>Une autre bonne pratique consiste à intégrer le profilage dans vos pipelines d’intégration continue/déploiement continu (CI/CD). Cela vous permet de détecter les problèmes de performances dès le début du processus de développement, avant qu’ils ne deviennent plus difficiles à résoudre. En profilant régulièrement votre application pendant le développement, vous pouvez vous assurer que les modifications que vous apportez n’affectent pas ses performances.</p><h3>Profilage dans des scénarios réels</h3><p>Bien qu’il puisse être utile de profiler votre application dans des conditions contrôlées, il est également important de le faire dans des scénarios réels. Cela signifie profiler votre application lorsqu’elle est sous charge, lorsqu’elle traite des données réelles et lorsqu’elle s’exécute sur les mêmes configurations matérielles et logicielles qu’en production. En procédant ainsi, vous obtiendrez une image beaucoup plus précise des performances de votre application dans le monde réel.</p><h3>Collaboration et partage des connaissances</h3><p>Enfin, le profilage Java ne doit pas être une activité solitaire. Il est important de partager vos découvertes et vos idées avec votre équipe et de collaborer pour trouver des solutions aux problèmes de performances que vous identifiez. Ce faisant, vous pouvez créer une culture de sensibilisation aux performances au sein de votre équipe et vous assurer que tout le monde travaille vers l’objectif commun de créer des applications Java efficaces et performantes.</p><p>Le profilage Java est un élément essentiel du processus de développement logiciel. En comprenant ses principales fonctionnalités et en suivant les meilleures pratiques, vous pouvez l’utiliser pour créer des applications efficaces, performantes et capables de répondre aux exigences du monde moderne. Ne sous-estimez donc pas la puissance du profilage : adoptez-le et regardez vos applications Java atteindre de nouveaux sommets de performances.</p>"
Dynatrace étend la portée et la portée des données de sa plateforme d'observabilité,"<p>Lors de son événement Perform 2024 aujourd'hui, Dynatrace a dévoilé un Dynatrace OpenPipeline qui permet d'appliquer des analyses à plusieurs types de sources de données en temps réel.</p><p>La société a également dévoilé une offre d'observabilité des données qui peut être utilisée pour vérifier la qualité et la lignée des données exposées au moteur d'intelligence artificielle (IA) Davis au cœur de la plateforme d'observabilité Dynatrace afin de réduire les faux positifs qui pourraient autrement générer une alerte, en plus de contribuer à réduire le volume de données qui pourraient devoir être stockées.</p><p>Enfin, Dynatrace a annoncé aujourd’hui qu’elle étendait la portée de sa plateforme d’observabilité aux grands modèles de langage (LLM) utilisés pour créer des plateformes d’IA génératives. Cette capacité permettra, par exemple, de simplifier le suivi de la consommation de jetons utilisés pour donner accès à ces modèles.</p><p>Steve Tack, vice-président senior des produits chez Dynatrace, a déclaré que Dynatrace OpenPipeline permettra aux organisations de rationaliser la collecte de données d'une manière qui leur permettra d'appliquer l'observabilité plus largement en appliquant des algorithmes de traitement de flux à des pétaoctets de données.</p><p>Prévue pour être disponible dans 90 jours, la fonctionnalité Dynatrace OpenPipeline permet aux équipes informatiques d'ingérer et d'acheminer les données d'observabilité, de sécurité et d'événements commerciaux à partir de n'importe quelle source et dans n'importe quel format au point d'ingestion, y compris les données non structurées qui sont automatiquement converties en un format exploitable. Ces données peuvent ensuite être enrichies pour permettre des analyses plus approfondies.</p><p>Cela offre également aux équipes informatiques davantage de contrôle sur les données qu'elles analysent, stockent ou excluent des analyses, ce qui contribue à réduire le coût total de l'observabilité, a noté Tack.</p><p>Enfin, cela offre aux équipes informatiques la possibilité d’appliquer des contrôles de sécurité et de confidentialité plus personnalisables à la manière dont ces données sont utilisées, a-t-il ajouté.</p><p>Dynatrace adopte une approche multimodale de l’IA qui englobe des modèles prédictifs, causaux et génératifs. Collectivement, cette approche simplifie l’identification de la cause profonde des problèmes, l’identification des anomalies susceptibles de perturber les services et la rationalisation de la gestion des incidents, par exemple en fournissant des résumés des événements dans un format en langage naturel.</p><p>On ne sait pas encore dans quelle mesure l’observabilité sera appliquée au-delà des données collectées pour gérer les flux de travail DevOps et d’autres processus informatiques, mais il est clair qu’il existe une forte corrélation entre les événements informatiques et les résultats commerciaux à mesure que les organisations deviennent plus dépendantes des logiciels. À mesure que l’IA devient de plus en plus utilisée, il devrait devenir plus simple d’appliquer l’analyse à un éventail beaucoup plus large de types de données pour faire apparaître la relation entre les événements informatiques et les processus commerciaux. En effet, Dynatrace simplifie l’application des meilleures pratiques d’ingénierie des données pour collecter, gérer et analyser ces données, a noté Tack.</p><p>En attendant, les équipes DevOps doivent revoir la manière dont leurs pipelines sont actuellement construits pour simplifier la capture de toutes les données pertinentes. Si les données de télémétrie collectées à partir de la plateforme DevOps sont essentielles pour garantir que le développement et la livraison des applications se déroulent le plus rapidement possible, ces données ne sont qu’un facteur dans une équation plus vaste. Le défi et l’opportunité à présent consistent à déterminer la meilleure façon d’appliquer l’IA pour corréler tout cela.</p>"
Prévisions technologiques pour les infrastructures en 2024,"<p>Ganesh Srinivasan, associé chez Venrock, est co-auteur de cet article.</p><p>2023 a été une année de montagnes russes sans précédent ; de la mort de l’étalement des piles de données modernes à la naissance de l’IA générative, nous ne sommes qu’au début d’une nouvelle ère dans « l’art du possible ». Nous vous garantissons que 2024 ne sera pas une déception.</p><p>À l’approche d’une nouvelle année, c’est le moment idéal pour examiner ce que nous prévoyons être les plus grands développements de l’année à venir. Voici ce qui, selon nous, va se passer en 2024 :</p><p>1. Le règne d’OpenAI remis en question</p><p>Avec les apprentissages émergents dans les architectures de réseaux neuronaux de base qui ont conduit au transformateur et à la domination d'OpenAI, il est probable que leur sortie imminente de GPT5 sera dépassée dans des tests de performance spécifiques par un nouvel entrant sur la base d'architectures plus efficaces, de capacités multimodales améliorées, d'une meilleure compréhension contextuelle du monde et d'un apprentissage par transfert amélioré. Ces nouveaux modèles seront construits sur la recherche émergente dans les réseaux spatiaux, les structures de graphes et les combinaisons de divers réseaux neuronaux qui conduiront à des capacités plus efficaces, polyvalentes et puissantes.</p><p>2. Apple : le nouveau leader de l'IA générative</p><p>L’un des acteurs les plus importants dans le domaine de l’IA générative commence tout juste à montrer ses cartes. 2024 sera l’année où Apple lancera son premier ensemble de fonctionnalités d’IA générative, libérant ainsi le véritable potentiel d’une architecture fermée d’IA en périphérie avec un accès complet à vos données personnelles – prouvant qu’Apple est en fait l’entreprise la plus importante dans la course à l’IA générative.</p><p>3. Construire en donnant la priorité au client</p><p>La dernière décennie a vu l’abandon des clients lourds au profit du rendu et du calcul côté serveur. Mais le monde revient au client. Les expériences mobiles devront fonctionner en mode hors ligne. Les expériences en temps réel nécessitent des transactions à très faible latence. L’exécution des LLM devra de plus en plus s’exécuter sur l’appareil pour augmenter les performances et réduire les coûts.</p><p>4. La mort de l’étalement des infrastructures de données</p><p>La croissance rapide des besoins en infrastructures de données des entreprises a conduit à une multiplication des solutions ponctuelles, des catalogues de données à la gouvernance des données, en passant par l'extraction inverse, la transformation, le chargement et les alternatives Airflow, jusqu'aux bases de données vectorielles et à un autre lakehouse. Le pendule reviendra vers des plateformes unifiées et moins de silos pour réduire le coût total de possession et les frais généraux d'exploitation d'ici 2024.</p><p>5. Approche de l'hiver de l'IA</p><p>En 2023, l’IA générative pourrait être qualifiée d’« art du possible », 2024 étant le véritable test pour voir si les prototypes se transforment en cas d’utilisation en production. Le pic du cycle de battage médiatique étant probablement atteint ici, 2024 connaîtra la phase de désillusion où les entreprises découvriront où l’IA générative peut créer un impact positif sur les marges et où les coûts l’emportent sur les avantages.</p><p>6. La menace de la désinformation</p><p>Si les modèles de diffusion d’images et de vidéos ont ouvert une nouvelle ère pour la création numérique et l’expression artistique, il ne fait aucun doute que leur côté obscur n’a pas encore fait de ravages. À l’approche d’une élection présidentielle, les modèles de diffusion en tant que machine de désinformation politique émergeront pour devenir la prochaine arme de désinformation majeure.</p><p>7. La percée de l’IA dans le monde réel</p><p>L’ère du « champ des rêves » pour l’IA sera révolue et 2024 marquera une avancée majeure pour les cas d’utilisation commerciale de l’IA, en particulier dans le monde physique. L’utilisation de l’IA pour les modalités du monde physique nous permettra de changer et d’interagir avec les machines et les appareils qui nous entourent, de la compréhension de la télémétrie des machines pour la détection des problèmes à la distillation d’informations à partir de milliers de flux de capteurs en temps réel.</p><p>8. S3 : La menace grandissante pour Snowflake</p><p>AWS S3 est sur le point de devenir le plus grand concurrent de Snowflake. Avec un écosystème d’outils en pleine croissance émergeant à mesure que les couches de calcul au-dessus de S3 tirent parti de leurs coûts ultra-faibles, de leur haute disponibilité, de la prise en charge de la zone de disponibilité et du nouveau service de latence à un chiffre en millisecondes, le « problème de coût de Snowflake » de tout le monde est sur le point de se voir offrir un nouvel ensemble d’alternatives attrayantes.</p><p>9. L'essor de Flink et de l'analyse opérationnelle</p><p>Si l’analyse commerciale a été à l’origine de la première vague d’évolution de l’infrastructure de données, l’analyse opérationnelle sera le moteur de la prochaine vague, avec Apache Flink au cœur de celle-ci. Les entreprises commenceront à adopter en masse des fonctionnalités en temps réel et en streaming pour commencer à utiliser les données afin de prendre des décisions opérationnelles. Cela créera une nouvelle valeur dans des domaines tels que la logistique, les transactions financières, la gestion des stocks et la détection des fraudes.</p><p>10. La pile d'IA du développeur</p><p>En 2024, l’impact le plus significatif de l’IA dans le monde numérique sera de débloquer la productivité des développeurs. De la génération de code au débogage en passant par la gestion des builds, l’IA ouvrira un nouveau niveau de productivité des développeurs que nous n’avons pas vu depuis l’avènement du cloud.</p><p>Si vous êtes d'accord, pas d'accord ou souhaitez partager vos propres prédictions pour 2024 avec nous, veuillez nous contacter sur X à @ethanjb et @gan3sh.</p>"
Naviguer dans le labyrinthe du cloud hybride : surmonter les obstacles à l'adoption,<h3>Types d'organisations adaptées aux solutions de cloud hybride</h3><h3>Protocoles d'intégration standardisés</h3><h3>Stratégies de sécurité dynamiques</h3><h3>Solutions pour exploiter les avantages du cloud hybride</h3><h3>L'évolution du paysage des architectures de cloud hybride</h3>
Naviguer dans l'écosystème multi-cloud,"<p>L'infrastructure multicloud, une approche dans laquelle les organisations utilisent les services de plusieurs fournisseurs de cloud, gagne rapidement du terrain dans le monde de la technologie.</p><p>Dans cet article, nous allons décortiquer le concept d’infrastructure multicloud, en examinant comment il permet aux entreprises de diversifier leurs écosystèmes cloud pour des performances améliorées, une plus grande flexibilité et une meilleure gestion des risques.</p><h3>Comprendre l'écosystème multi-cloud</h3><p>Les environnements multi-cloud représentent une approche stratégique du cloud computing où les entreprises utilisent les services de plusieurs fournisseurs de cloud pour répondre à divers besoins opérationnels.</p><p>Ce concept diffère du cloud hybride, qui combine généralement des infrastructures de cloud privé et public, dans le but d'offrir un équilibre entre les deux dans un seul environnement unifié.</p><p>En revanche, une stratégie multicloud implique l’utilisation de services cloud distincts (par exemple, un fournisseur pour le stockage, un autre pour la puissance de calcul et un autre pour l’analyse) sans nécessairement les intégrer dans un système cohérent. Cette approche permet aux organisations de tirer parti des atouts et des offres uniques de divers fournisseurs de cloud, optimisant ainsi leur infrastructure informatique pour plus d’efficacité, de rentabilité et de performances.</p><h3>Les avantages d’une approche multi-cloud</h3><p>L’adoption d’une approche multicloud apporte plusieurs avantages clés, les plus importants étant une résilience et une flexibilité améliorées et l’évitement du verrouillage des fournisseurs.</p><p>En répartissant les ressources et les charges de travail sur plusieurs environnements cloud, les entreprises peuvent atténuer les risques associés au recours à un seul fournisseur cloud, comme les pannes de service ou la perte de données. Cette diversité garantit un fonctionnement continu et de solides capacités de reprise après sinistre.</p><p>De plus, une stratégie multi-cloud permet aux entreprises de choisir les meilleurs services proposés par chaque fournisseur de cloud, en adaptant leur infrastructure informatique à des besoins spécifiques et à des exigences opérationnelles. Cette approche sur mesure permet aux entreprises d'optimiser à la fois les performances et les coûts. Différents fournisseurs peuvent proposer des tarifs compétitifs pour des services spécifiques, ce qui permet aux entreprises de choisir les solutions les plus rentables en fonction de leurs besoins.</p><p>De plus, en évitant le blocage des fournisseurs, les entreprises conservent la liberté de changer de fournisseur en fonction de l'évolution des besoins ou de l'arrivée de nouveaux services plus avantageux. Cette utilisation stratégique de plusieurs clouds conduit finalement à une infrastructure informatique plus efficace, plus agile et plus rentable.</p><h3>Principaux défis du multi-cloud</h3><p>La gestion multicloud s'accompagne de son lot de défis, notamment en termes de complexité, de sécurité des données et de conformité. Les équipes DevOps sont souvent confrontées à la complexité de la gestion de services cloud disparates, chacun doté de fonctionnalités et d'interfaces uniques, ce qui rend l'intégration transparente et le fonctionnement cohérent une tâche exigeante.</p><p>La sécurité des données est une autre préoccupation majeure, car la protection des informations sensibles sur plusieurs plateformes nécessite des mesures de sécurité robustes et adaptables. En outre, la conformité aux différentes réglementations dans différents environnements cloud ajoute un niveau de complexité supplémentaire.</p><p>Ces défis nécessitent que les équipes DevOps soient techniquement compétentes et expertes en planification stratégique et en coordination pour gérer efficacement un écosystème multicloud.</p><h3>Bonnes pratiques en matière de déploiement et de gestion multi-cloud</h3><p>Lors de la gestion et du déploiement au sein d'un écosystème multi-cloud, les équipes DevOps peuvent grandement bénéficier des cinq meilleures pratiques suivantes :</p><p>1. Sélection stratégique des services cloud : évaluez et choisissez soigneusement les services cloud en fonction des exigences spécifiques de la charge de travail, des mesures de performance et de la rentabilité pour garantir un déploiement optimal. 2. Surveillance et gestion centralisées : utilisez des outils centralisés pour surveiller et gérer différents environnements cloud afin de maintenir la cohérence et le contrôle et de rationaliser les opérations. 3. Posture de sécurité cohérente : appliquez des politiques et des pratiques de sécurité uniformes sur toutes les plateformes cloud pour protéger l'intégrité des données et vous conformer aux normes réglementaires. 4. Automatisation : automatisez les tâches répétitives et routinières sur différents clouds pour une utilisation efficace des ressources, une réduction des erreurs et des cycles de déploiement plus rapides. 5. Gestion et optimisation des coûts : surveillez et gérez en permanence les dépenses cloud sur toutes les plateformes pour éviter les dépassements de coûts, garantir une utilisation efficace des ressources et suivre les meilleures pratiques d'optimisation des coûts cloud.</p><h3>Élaboration d'une stratégie multi-cloud : une perspective DevOps</h3><p>Du point de vue DevOps, la création d’une stratégie multicloud complète implique d’aligner l’infrastructure cloud sur les objectifs commerciaux et les principes fondamentaux de DevOps.</p><p>La première étape consiste à définir clairement les objectifs de l'entreprise et à identifier comment les différents environnements cloud peuvent les prendre en charge. Il convient ensuite de sélectionner les services cloud qui correspondent le mieux aux besoins spécifiques de chaque charge de travail, en tenant compte de facteurs tels que les performances, les coûts et les exigences de conformité. L'intégration des processus d'intégration continue et de déploiement continu (CI/CD) est essentielle dans une configuration multicloud.</p><p>Cette intégration garantit que les logiciels peuvent être développés, testés et déployés rapidement et de manière fiable sur différentes plateformes cloud. La stratégie doit également inclure la mise en œuvre d'outils d'automatisation et de surveillance robustes pour maintenir la cohérence et l'efficacité dans l'ensemble du cloud.</p><p>En se concentrant sur ces éléments, une équipe DevOps peut développer une stratégie multi-cloud qui non seulement s’aligne sur les objectifs de l’organisation, mais adhère également à l’éthique agile et efficace des méthodologies DevOps, garantissant un fonctionnement et une évolutivité transparents dans l’environnement multi-cloud.</p><h3>Pour conclure</h3><p>En conclusion, une bonne maîtrise de l’écosystème multi-cloud est essentielle à la réussite de DevOps, car il offre une flexibilité, une évolutivité et une résilience inégalées dans la gestion de diverses ressources cloud. Les professionnels DevOps sont encouragés à adopter des stratégies multi-cloud car elles améliorent l’agilité opérationnelle et ouvrent des perspectives d’innovation et d’amélioration de la prestation de services.</p>"
Mise en œuvre de l'analyse des données dans des environnements multi-cloud,"<p>L'analyse des données multicloud est plus complexe et exige une approche nuancée pour garantir des opérations fluides et des informations précises. La coordination des analyses entre plusieurs fournisseurs de cloud pose divers problèmes, nécessitant une réflexion approfondie sur la stratégie de mise en œuvre. De la gestion des coûts à la variabilité des performances et à l'orchestration des données, nous explorerons chaque défi en détail et fournirons des informations exploitables sur la manière de surmonter efficacement ces obstacles, garantissant une infrastructure d'analyse de données à toute épreuve dans les solutions multicloud.</p><h3>Interopérabilité et normes</h3><p>Les problèmes d'interopérabilité se posent dans l'analyse de données multicloud en raison des différences entre les API et les normes des différents fournisseurs de cloud. Le manque d'uniformité des formats de données et des protocoles de communication entrave la collaboration et l'échange de données. Pour y remédier, les organisations doivent privilégier les services cloud conformes aux normes ouvertes et plaider en faveur d'initiatives d'interopérabilité à l'échelle du secteur. En outre, créez une couche d'abstraction ou un middleware qui fait office de passerelle entre les différents fournisseurs de cloud. Cette couche d'abstraction permet de normaliser les interactions, de faciliter l'échange de données fluide et de réduire les dépendances vis-à-vis des API spécifiques des fournisseurs. Ainsi, vous améliorez la flexibilité de votre environnement multicloud, facilitant l'intégration et le basculement entre différentes plateformes cloud.</p><h3>Verrouillage du fournisseur</h3><p>Les entreprises risquent de devenir trop dépendantes d’un seul fournisseur, ce qui limite leur flexibilité et rend la migration complexe. L’utilisation de fonctionnalités ou d’outils spécialisés d’un fournisseur peut être confrontée à des difficultés lors de la transition vers un autre, ce qui limite la capacité à choisir des solutions plus rentables ou plus adaptées et peut entraîner une augmentation des coûts de changement. Pour atténuer ce problème, il convient d’adopter une stratégie multicloud, privilégiant les technologies standardisées et les API ouvertes, permettant aux entreprises de répartir les charges de travail entre plusieurs fournisseurs de cloud et d’éviter d’être liées à l’écosystème d’un fournisseur spécifique, améliorant ainsi la flexibilité et la prise de décision stratégique. Concevez des architectures qui exploitent les couches d’abstraction et évitent un couplage étroit avec des services spécifiques au fournisseur. Cette approche garantit la flexibilité, permettant à votre entreprise de choisir et de basculer entre les fournisseurs de cloud en fonction des performances, des coûts et des considérations stratégiques tout en minimisant le risque de dépendance vis-à-vis d’un fournisseur.</p><h3>Gestion des coûts</h3><p>Chaque fournisseur a des modèles de tarification et des mécanismes de facturation uniques, ce qui rend difficile pour les organisations d’estimer et de comparer avec précision les coûts. La nature dynamique des charges de travail d’analyse de données, associée aux dépenses de transfert de données entre les clouds, contribue à la complexité de la budgétisation. Pour y faire face, implémentez un outil de gestion centralisée des coûts du cloud pour surveiller les dépenses entre les fournisseurs. De plus, examinez et optimisez régulièrement l’utilisation des ressources pour identifier les instances sous-utilisées ou surprovisionnées. Vous pouvez exploiter les instances réservées ou repérer les instances pour un calcul rentable et implémenter l’automatisation pour la mise à l’échelle en fonction de la demande, en veillant à ce que les ressources soient allouées efficacement. Enfin, négociez les prix avec les fournisseurs de cloud et étudiez les options de remise pour garantir des conditions favorables pour les engagements à long terme.</p><h3>Variabilité des performances</h3><p>Les fournisseurs disposant d'une infrastructure, de configurations réseau et de niveaux de service uniques, des niveaux de performances incohérents peuvent survenir et avoir un impact sur la fiabilité et la prévisibilité des résultats d'analyse. Pour atténuer ces conséquences, effectuez des tests de performances approfondis dans différents environnements cloud, optimisez les requêtes pour des plates-formes spécifiques et envisagez la répartition de la charge de travail en fonction des points forts des fournisseurs. L'utilisation d'outils de surveillance des performances permet de suivre et d'analyser les variations, ce qui permet d'effectuer des ajustements en temps opportun. Le choix des fournisseurs de cloud en fonction d'exigences de performances spécifiques et l'établissement d'accords de niveau de service (SLA) de performance atténuent également l'impact de la variabilité des performances dans les analyses de données multicloud.</p><h3>Orchestration des ressources</h3><p>En raison des différences entre les outils d'orchestration, les API et les mécanismes de gestion des ressources utilisés par les fournisseurs de cloud, des problèmes de compatibilité se produisent souvent. L'hétérogénéité rend complexe la garantie d'un déploiement, d'une mise à l'échelle et d'une gestion transparents des charges de travail d'analyse. Les organisations peuvent envisager d'adopter des outils et des cadres d'orchestration indépendants du cloud pour faire abstraction des différences sous-jacentes entre les fournisseurs de cloud, ce qui permet une gestion cohérente des ressources. L'adoption des pratiques d'infrastructure en tant que code (IaC) et de l'automatisation rationalise également l'orchestration des ressources, ce qui permet aux organisations de déployer et de gérer les charges de travail d'analyse plus efficacement dans divers environnements cloud.</p><h3>Mouvement et latence des données</h3><p>Les architectures de réseau variées, les distances géographiques et les différents mécanismes de transfert de données contribuent à accroître la latence et à poser des problèmes de performances potentiels. Le transfert d’ensembles de données importants entre clouds entraîne des coûts supplémentaires et peut entraîner des inefficacités opérationnelles. Pour éviter cela, les organisations doivent optimiser les processus de transfert de données en exploitant des protocoles de transfert efficaces, en envisageant l’informatique de pointe pour la proximité des sources de données et en répartissant stratégiquement les charges de travail en fonction de l’emplacement des données. La mise en œuvre de stratégies de mise en cache et l’utilisation de réseaux de diffusion de contenu (CDN) peuvent contribuer à atténuer les problèmes de latence, garantissant ainsi un environnement d’analyse de données multicloud plus réactif et plus rentable.</p><h3>En note de bas de page</h3><p>En conclusion, la mise en œuvre de l’analyse de données dans des environnements multi-cloud présente un ensemble de défis uniques. L’adoption de normes ouvertes, l’adoption d’outils indépendants du cloud et la promotion d’une culture d’amélioration continue sont des éléments essentiels pour les surmonter. Une surveillance continue, des mises à jour régulières des cadres de gouvernance et une prise de décision stratégique alignée sur les spécificités et les objectifs de votre entreprise sont essentielles pour tirer le meilleur parti de l’analyse de données multi-cloud. Vous pouvez également bénéficier de conseils en cloud computing pour obtenir une stratégie sur mesure auprès d’experts.</p>"
Exécuter MongoDB sur AWS : un guide pratique,"<p>MongoDB est un programme de base de données orienté document disponible en source. MongoDB est une solution de base de données NoSQL qui utilise des documents de type JSON avec des schémas facultatifs. De nombreuses organisations choisissent d'exécuter MongoDB dans le cloud Amazon Web Services (AWS) pour améliorer l'évolutivité et la fiabilité de leur déploiement MongoDB.</p><p>AWS propose une gamme de services pouvant être intégrés à MongoDB, notamment une puissance de calcul et de multiples options de stockage. L'exécution de MongoDB sur AWS peut être effectuée de différentes manières :</p><li>Vous pouvez installer et gérer manuellement MongoDB sur une instance AWS.</li><li>Vous pouvez utiliser MongoDB Atlas, une base de données en tant que service entièrement gérée par les créateurs de MongoDB, qui prend automatiquement en charge vos opérations de base de données.</li><p>Dans cet article, nous aborderons les avantages de l’exécution de MongoDB sur AWS et montrerons comment démarrer avec MongoDB Atlas sur AWS.</p><h3>Flexibilité des infrastructures</h3><p>Avec MongoDB sur AWS, vous pouvez choisir l'instance qui correspond le mieux à votre charge de travail. Par exemple, si vous avez besoin de plus de puissance de calcul, vous pouvez sélectionner une instance optimisée pour le calcul. Si votre charge de travail nécessite plus de mémoire, vous pouvez opter pour une instance optimisée pour la mémoire. Cette flexibilité s'étend également au stockage, AWS proposant des options de stockage SSD et HDD pour votre base de données MongoDB.</p><h3>Évolutivité</h3><p>Que vous ayez affaire à une petite application ou à une opération à grande échelle, AWS vous permet d'augmenter ou de réduire rapidement vos capacités pour gérer les changements d'exigences ou les pics de popularité. La fonction de mise à l'échelle automatique d'AWS, combinée aux capacités de partitionnement de MongoDB, vous offre une solution de base de données qui peut évoluer avec votre entreprise.</p><h3>Sécurité intégrée</h3><p>L'exécution de MongoDB sur AWS offre également des mesures de sécurité robustes. AWS fournit diverses fonctionnalités de sécurité, telles qu'AWS Identity and Access Management (IAM), qui vous permet de définir les autorisations et les politiques des utilisateurs. Cela signifie que vous pouvez avoir un contrôle précis sur qui peut accéder à vos bases de données MongoDB et sur les actions qu'ils peuvent effectuer.</p><h3>Sauvegarde et récupération après sinistre</h3><p>Enfin, la combinaison de MongoDB et d'AWS offre des options robustes pour la sauvegarde et la reprise après sinistre. AWS propose Amazon S3 pour le stockage d'objets, connu pour sa durabilité et bien adapté aux charges de travail NoSQL. Vous pouvez également utiliser AWS Snapshots pour créer des sauvegardes de vos bases de données MongoDB. En cas de sinistre, vous pouvez rapidement lancer davantage de ressources dans AWS pour assurer la continuité des activités.</p><p>MongoDB Atlas est un service de base de données entièrement géré basé sur le cloud proposé par MongoDB Inc. Il fournit une suite intégrée de fonctionnalités avancées telles que les sauvegardes automatisées, la surveillance, la mise à l'échelle et la sécurité pour exécuter MongoDB dans le cloud.</p><p>MongoDB Atlas gère les aspects opérationnels de l'exécution de MongoDB, éliminant ainsi le besoin d'interventions manuelles. Voici quelques-unes des principales caractéristiques de MongoDB Atlas :</p><li>Service géré : MongoDB Atlas décharge l'utilisateur des tâches d'installation, de configuration, de maintenance et de mise à l'échelle, ce qui en fait une option incontournable pour ceux qui souhaitent se concentrer davantage sur les opérations de base de données et moins sur l'administration.</li><li>Indépendant de la plateforme : bien que MongoDB Atlas fonctionne parfaitement sur AWS, il est également disponible sur d'autres plateformes cloud, notamment Google Cloud Platform (GCP) et Microsoft Azure. Les utilisateurs peuvent choisir leur fournisseur de cloud et leur région préférés.</li><li>Sécurité : MongoDB Atlas est livré avec des mesures de sécurité intégrées, notamment un cryptage de bout en bout, un peering VPC et des mécanismes d'authentification de niveau entreprise.</li><li>Sauvegardes automatisées : Atlas fournit des sauvegardes continues avec récupération à un instant donné. Cela signifie que vous pouvez restaurer vos données à n'importe quelle seconde du passé sans temps d'arrêt significatif.</li><li>Informations sur les performances : MongoDB Atlas offre une surveillance et des alertes en temps réel. Les utilisateurs peuvent obtenir des informations sur les requêtes, l'utilisation des ressources et d'autres mesures directement à partir du tableau de bord Atlas.</li><p>Voici les étapes de haut niveau nécessaires à la migration d'une base de données MongoDB auto-hébergée vers MongoDB Atlas sur AWS Cloud. Pour plus de détails, consultez les instructions prescriptives d'Amazon.</p><h3>Première étape : découverte et évaluation</h3><p>La première phase consiste à comprendre votre configuration MongoDB actuelle et à préparer la migration. Commencez par évaluer votre environnement MongoDB existant, notamment la taille de la base de données, la configuration et les mesures de performances. Cette évaluation vous aidera à déterminer la taille et la configuration appropriées pour votre cluster MongoDB Atlas sur AWS. Il est également essentiel d’analyser les dépendances de votre application sur la base de données pour comprendre l’impact de la migration sur votre application.</p><p>Au cours de cette phase, vous devez également identifier les éventuels défis ou limitations qui pourraient survenir pendant la migration. Il peut s'agir de problèmes de compatibilité, de problèmes d'intégrité des données ou d'implications en termes de performances. En abordant ces problèmes en amont, vous pouvez atténuer les risques et planifier une migration plus fluide.</p><h3>Deuxième étape : configurer la sécurité et la conformité</h3><p>La sécurité et la conformité sont primordiales dans tout processus de migration. Commencez par configurer les paramètres réseau dans MongoDB Atlas pour garantir une connectivité sécurisée. Cela implique généralement la configuration d'un peering VPC ou d'AWS Direct Connect entre votre environnement AWS et MongoDB Atlas.</p><p>Ensuite, concentrez-vous sur la configuration des fonctionnalités de sécurité dans MongoDB Atlas. Celles-ci incluent la configuration du chiffrement au repos et en transit, la gestion des identités et des accès, ainsi que les fonctionnalités d’audit et de surveillance. Assurez-vous que ces paramètres sont conformes aux politiques de sécurité et aux exigences de conformité de votre organisation.</p><p>Il est également essentiel de comprendre le modèle de responsabilité partagée dans AWS et MongoDB Atlas. Tandis que MongoDB Atlas gère la sécurité du cloud, vous êtes responsable de la sécurité dans le cloud, ce qui inclut la sécurisation de vos données et le contrôle des accès.</p><h3>Troisième étape : migrer les données</h3><p>Une fois les préparatifs terminés, vous pouvez commencer à migrer vos données vers MongoDB Atlas. MongoDB Atlas propose un service de migration en direct, qui vous permet de migrer vos données avec un temps d'arrêt minimal. Pour utiliser ce service, vous devrez fournir des détails sur votre environnement MongoDB source et lancer la migration via l'interface MongoDB Atlas.</p><p>Surveillez de près le processus de migration pour vous assurer que les données sont transférées avec précision et efficacité. Il est conseillé d’effectuer des contrôles de validation pendant et après la migration pour garantir l’intégrité des données. Soyez également prêt à résoudre rapidement tout problème qui pourrait survenir au cours de cette phase.</p><h3>Étape 4 : Configurer l’intégration opérationnelle</h3><p>Une fois vos données migrées, l’étape suivante consiste à intégrer MongoDB Atlas à vos processus opérationnels. Cela implique de mettre à jour la configuration de votre application pour qu’elle pointe vers le nouveau cluster MongoDB Atlas. Vous devrez mettre à jour les chaînes de connexion à la base de données de votre application et tester minutieusement l’application pour vous assurer qu’elle interagit correctement avec MongoDB Atlas.</p><p>De plus, configurez la surveillance et les alertes dans MongoDB Atlas pour suivre les performances et l'état de votre base de données. MongoDB Atlas fournit divers outils et mesures qui peuvent vous aider à surveiller efficacement votre base de données.</p><p>Enfin, pensez à mettre en œuvre les modifications nécessaires à vos procédures de sauvegarde et de récupération. MongoDB Atlas propose des solutions de sauvegarde automatisées, mais vous devez les aligner sur vos politiques de protection des données et tester votre processus de sauvegarde et de récupération pour vous assurer qu'il répond à vos exigences.</p><p>En conclusion, l’exécution de MongoDB sur AWS offre une combinaison de flexibilité, d’évolutivité, de sécurité et d’options robustes de sauvegarde et de reprise après sinistre. En tirant parti de l’infrastructure diversifiée d’AWS, vous pouvez personnaliser votre environnement MongoDB pour répondre à des exigences de charge de travail spécifiques, qu’il s’agisse d’exigences centrées sur le calcul, la mémoire ou le stockage.</p><p>Le processus de migration vers MongoDB Atlas sur AWS implique la découverte et l'évaluation, la configuration de la sécurité et de la conformité, la migration des données et l'intégration opérationnelle. En suivant ce processus, les organisations peuvent migrer et optimiser efficacement leurs déploiements MongoDB dans le cloud AWS, en profitant des avantages d'un environnement de base de données entièrement géré, évolutif et sécurisé.</p>"
Comment les supergraphes contribuent à consolider l'accès aux données,"<p>GraphQL, un langage de requête pour les API, est devenu un outil de plus en plus courant pour le développement de nouveaux outils basés sur le Web. GraphQL peut aider à rationaliser l'accès aux données en fournissant toutes les ressources dont un développeur a besoin dans une seule requête. Par rapport à l'approche REST traditionnelle de l'accès aux données basées sur le Web, GraphQL dispose d'un schéma plus standardisé et plus navigable et d'une expérience de développement sans doute meilleure.</p><p>Mais GraphQL ne se contente pas d’améliorer la convivialité des développeurs pour un modèle de données unique. S’il est utilisé pour regrouper diverses sources de données et API dans un graphique unifié, GraphQL pourrait devenir « un schéma unique pour les gouverner toutes » pour les entreprises. Certains qualifient cette architecture de supergraphe.</p><p>J’ai récemment rencontré Tanmai Gopal, PDG de Hasura, pour explorer le concept de supergraphe et voir comment il peut contribuer à rendre les données plus accessibles aux entreprises. Ci-dessous, nous examinerons l’expansion du concept de supergraphe et les avantages du déploiement d’une telle stratégie.</p><p>Dans les grandes architectures logicielles, les équipes produit doivent avoir accès à diverses sources de données issues de différents domaines. « Il existe un besoin d’expériences intégratives, qui deviennent de plus en plus importantes pour l’utilisateur final », explique Gopal. Par exemple, une application de commerce électronique peut être composée de données de catalogue de produits, de fonctionnalités de connexion, de données de paiement, d’informations d’expédition et d’autres informations nécessaires.</p><p>Cependant, il existe des obstacles à l’extraction de données provenant de ces différents domaines. Différents domaines sont souvent conçus sous forme de microservices, pris en charge par une équipe dédiée et utilisant un portail de développement et un schéma d’API sur mesure pour externaliser les services, a déclaré Gopal. Les frictions liées à l’intégration des API et l’absence d’un registre commun de services pourraient ralentir le développement.</p><p>Non seulement les microservices qui exposent des points de terminaison discrets posent des problèmes, mais les frontières entre interne et externe sont floues, ce qui nécessite une approche zero-trust. Et si nous commençons à considérer les données comme un produit, explique Gopal, nous devons accéder à plusieurs domaines de manière sécurisée, cohérente et évolutive.</p><p>Un supergraphe peut résoudre certaines de ces difficultés, a expliqué Gopal. En exposant des graphes provenant de différents services de domaine, vous pouvez automatiquement unifier les données dans un supergraphe, auquel différentes équipes peuvent accéder. Et l'avantage de l'utilisation de GraphQL est que l'acte de documentation est l'acte de construction de l'API, ce qui signifie que la description et le schéma sont plus standardisés.</p><p>En regroupant des services disparates dans un registre centralisé, vous pourriez travailler sur plusieurs domaines de manière systématique mais fédérée, a déclaré Gopal. L'implémentation d'une couche ici pourrait également être un bon domaine pour appliquer une identité ou une authentification basée sur les rôles.</p><p>Andrew Carlson, architecte principal chez Apollo GraphQL, a également proposé d’utiliser GraphQL pour centraliser le contrôle d’accès aux données. « Lorsqu’il est utilisé comme couche pour agréger et orchestrer les API existantes », a déclaré Carlson. « C’est un emplacement idéal dans notre architecture pour centraliser le contrôle d’accès et l’autorisation jusqu’au niveau du terrain, offrant une observabilité au niveau du terrain pour savoir quels clients demandent quelles données. »</p><p>Gopal a partagé une poignée d’autres avantages spécifiques liés à l’utilisation de GraphQL dans ce contexte :</p><li>Vous aide à écrire moins de code : comme GraphQL vous permet d'extraire le champ de votre choix, l'alignement sur GraphQL en tant qu'interface commune pourrait réduire le nombre de requêtes nécessaires à l'intégration des données. De plus, certains avantages liés à l'expérience du développeur, tels que la saisie semi-automatique, pourraient améliorer la découverte des schémas et présenter des avantages exponentiels lorsque les sources sont entrelacées dans un graphique combiné.</li><li>Augmente l’agilité pour le nouveau développement : la nécessité de comprendre rapidement le schéma global et son apparence devient particulièrement importante, notamment pour les applications d’IA, a déclaré Gopal. L’utilisation d’un schéma de supergraphes apporte un gain d’agilité considérable, a-t-il ajouté, notamment en permettant aux équipes de développement de logiciels d’extraire le contexte dont elles ont besoin pour mettre rapidement les choses en mouvement.</li><li>Améliore la découverte des données et des services : les équipes de développement logiciel sont confrontées à une prolifération d’outils et le nombre d’API utilisées au sein d’une organisation ne cesse d’augmenter. Un supergraphe peut permettre un maillage de données traversable, ce qui facilite grandement la découverte et, par conséquent, la capacité d’innovation. « L’avantage de la capacité à comprendre et à utiliser des données provenant de différents domaines est énorme », a déclaré Gopal.</li><li>Unifie les équipes de différents domaines : un autre avantage potentiel du concept de supergraphe est l'amélioration de la collaboration entre les services. Aider les équipes à travailler ensemble pourrait améliorer la création de rapports et réduire les risques. Ce nouveau modèle de fonctionnement partagé permet de faire le ménage lors de la mise à jour du code et du schéma, a déclaré Gopal.</li><p>L’idée de regrouper des sources et des formats de données disparates dans une couche standard et unifiée est un concept séduisant. « L’impact en termes d’agilité est énorme », a déclaré Gopal. Surtout, ajoute-t-il, si vous avez la possibilité d’automatiser la création d’un supergraphe autour de différents styles de bases de données existantes.</p><p>Cependant, l’investissement dans un supergraphe peut prendre du temps. Les avantages se débloquent au fil du temps, mais vous aurez besoin d’un champion interne pour le défendre au départ et le concrétiser, prévient-il. En outre, les entreprises peuvent hésiter à se lancer à fond dans GraphQL en raison d’investissements antérieurs dans d’autres types de technologies.</p><p>Bien que GraphQL devienne de plus en plus populaire, divers styles d’API sont encore couramment utilisés et continueront d’être pris en charge pendant un certain temps. Par exemple, une étude réalisée en 2023 par la société de gestion et de test d’API Postman a révélé que l’utilisation de GraphQL avait éclipsé le format d’API principal de SOAP. Pourtant, REST reste le style d’API dominant, utilisé par 86 % des répondants, suivi par Webhooks (36 %), GraphQL (29 %) et SOAP (26 %).</p><p>L'étape suivante consiste donc à rendre le supergraphe indépendant de GraphQL, a déclaré Gopal. Au lieu de cela, il devrait pouvoir fonctionner avec différents protocoles puisque la plupart des entreprises utilisent plusieurs styles.</p>"
KubeCon 2023 : Télémétrie et gestion des données,"<p>Orateur 1 : C'est Techstrong TV.</p><p>Alan Shimel : Salut à tous. Nous sommes de retour à KubeCon et c'est très animé. C'est un espace immense avec beaucoup de monde qui se promène, alors nous espérons que vous aimerez ça. C'est un peu différent de ce que nous faisons habituellement. Habituellement, les gens ne peuvent pas voir l'arrière-plan parce que les caméras sont orientées dans cette direction, mais nous avons généralement le fond typique que vous voyez sur certains de ces autres stands. Mais nous voulions donner aux gens une idée de ce que nous sommes en vie ici.</p><p>Tucker Callaway : Nous sommes en plein dedans.</p><p>Alan Shimel : Nous sommes en plein milieu de cette période. C’est un peu comme si vous regardiez un match de foot le dimanche soir. C’est donc ce que nous recherchions. Mais quoi qu’il en soit, laissez-moi vous présenter Tucker Callaway. Tucker est le PDG d’une entreprise appelée Mezmo. Certains d’entre vous en ont peut-être entendu parler, je crois que c’était Log DNA.</p><p>Tucker Callaway : Avant. Ouais.</p><p>Alan Shimel : Et ils ont changé de cap. C'était il y a environ quatre ans ?</p><p>Tucker Callaway : Il y a environ deux ans maintenant.</p><p>Alan Shimel : Il y a deux ans.</p><p>Tucker Callaway : Je suis ici depuis quatre ans, mais le grand changement a eu lieu il y a deux ans, j'ai changé de nom.</p><p>Alan Shimel : Rebaptisé Mezmo et avec une mission légèrement différente, disons. Et puis j'ai eu l'occasion de retrouver Tucker à RSA la dernière fois, je crois que c'était en avril dernier.</p><p>Tucker Callaway : Avril. Ouais.</p><p>Alan Shimel : Cette année, c'est en mai. Je pense que c'était à peu près à cette époque l'année dernière. Et pour ceux d'entre vous qui veulent voir cette interview, elle est en fait disponible sur Techstrong TV. Si vous recherchez RSA 2023, elle y est. Mais alors Tucker, avant d'aborder ce sujet, revenons en arrière. Mezmo. Tout le monde ici n'est pas au courant. Écoutez, tout le monde ici ne connaissait pas non plus Log DNA, alors ne vous inquiétez pas.</p><p>Tucker Callaway : C'est vrai. Ouais.</p><p>Alan Shimel : Commençons par là. Parlons de la mission.</p><p>Tucker Callaway : Oui, la mission de Mezmo est essentiellement d’aider les gens avec leurs données de télémétrie. Nous avons donc constaté qu’il y a en fait trois problèmes principaux dans le domaine des données de télémétrie. Il y a trop de données de télémétrie. Elles ne sont pas au bon format. Elles ne sont pas au bon endroit. C’était donc en quelque sorte l’un de nos principes fondateurs pour résoudre ce problème.</p><p>Alan Shimel : Oui, je pense ça.</p><p>Tucker Callaway : Résoudre le problème.</p><p>Alan Shimel : Ces trois-là étaient solides comme un roc.</p><p>Tucker Callaway : Difficile de débattre.</p><p>Alan Shimel : Difficile de débattre de la question. C’est la bonne façon de le dire. Et puis, très franchement, ces trois problèmes se compliquent mutuellement.</p><p>Tucker Callaway : Ils le font.</p><p>Alan Shimel : C’est comme si vous aviez trop de données. C’est un gros problème, mais nous avons trop de données que nous ne savons pas comment classer, ce qui aggrave exponentiellement le problème. Donc, ce sont…</p><p>Tucker Callaway : Des problèmes de composition. Aucun doute là-dessus.</p><p>Alan Shimel : Aucun doute là-dessus. Donc, lorsque nous étions chez RSA, vous aviez l’impression d’être sur le point de prendre conscience de quelque chose, n’est-ce pas ? Vous alliez aider les clients. Pourquoi ne le faites-vous pas ? Eh bien, vous racontez l’histoire mieux que moi.</p><p>Tucker Callaway : Comme nous l’avons évoqué plus tôt, nous venions de lancer cette nouvelle fonctionnalité de produit, le pipeline de télémétrie chez RSA. Nous avons depuis lors de nombreux clients formidables et une expérience formidable. Lorsque nous réfléchissons à la valeur fondamentale de ce qu’il apporte, nous pensons qu’il aide les gens à gérer les coûts, à obtenir de meilleures informations et à faire respecter la conformité. Ce qui nous a surpris au cours des derniers mois, c’est à quel point les gens comprennent peu leurs données.</p><p>En fait, nous avons consacré beaucoup de temps et d’efforts en tant qu’industrie à maîtriser nos applications et notre infrastructure, mais nous n’avons pas encore vraiment maîtrisé nos données de télémétrie, et c’est là que se situe la grande opportunité que nous voyons. Mais pour obtenir la valeur que j’ai décrite, les gens n’avaient pas besoin de mieux comprendre leurs données. Et cela a donc été une grande révélation pour nous.</p><p>Nous venons de lancer notre fonctionnalité de profilage des données qui aide les utilisateurs à catégoriser et à comprendre les données de base, à gérer les schémas de leurs données, ce qui constitue une base très solide pour les principes d’exploitation des données appliqués aux données de télémétrie. Ensuite, après avoir résolu ce problème, nous avons découvert que les utilisateurs comprenaient désormais leurs données et qu’ils étaient capables de les optimiser parce qu’ils les comprenaient. Ils ont également réalisé que le contexte et les données de télémétrie évoluent rapidement. Par exemple, vous ne souhaitez pas stocker toutes ces données, car vous n’en avez pas besoin jusqu’à ce que vous en ayez vraiment besoin.</p><p>Alan Shimel : Je vais en prendre un peu, ils veulent stocker les données parce que les gens sont des thésauriseurs de données.</p><p>Tucker Callaway : L'accumulateur de données. Ouais.</p><p>Alan Shimel : C’est quand ils doivent payer la facture pour stocker toutes ces données qu’ils se disent : « Attendez une seconde, je n’ai pas besoin de toutes ces données. Pas à ce prix-là. »</p><p>Tucker Callaway : Il faut que quelque chose cède quelque part.</p><p>Alan Shimel : Exactement.</p><p>Tucker Callaway : Nous avons donc réalisé que si nous prenons en compte cette valeur, nous pensons que comprendre, optimiser et répondre, nous avons dit que l’étape de compréhension et d’optimisation de la réponse était très importante pour nous. Ainsi, lorsqu’un nouveau contexte arrive, le pipeline peut modifier son comportement et ses optimisations pour tenir compte du flux actuel de données et des besoins actuels. Ainsi, si vous réduisez l’échantillonnage à 20 % et que vous voyez un événement, vous voyez les performances, vous pouvez réhydrater, retraiter, modifier dynamiquement la réactivité du pipeline et faire parvenir toutes les données au bon endroit pour les bonnes personnes, ce qui permet de résoudre le problème des coûts, mais vous donne également la visibilité complète dont vous avez besoin.</p><p>Alan Shimel : Si l’on devait réaliser une étude de cas dans une école de commerce sur ce sujet, quelqu’un qui n’est pas issu de ce secteur dirait : « Hé, le simple fait que vous ayez découvert que la plupart de ces organisations ne savent même pas quelle quantité de données elles possèdent serait suffisant pour construire une entreprise autour de cela. »</p><p>Tucker Callaway : Oui, j’aimerais le penser.</p><p>Alan Shimel : Non, c’est logique. C’est logique, mais ce n’est pas le cas, c’est le seuil. Parce que je pense que vous avez mis le doigt dessus, Tucker, c’est qu’une fois qu’ils ont reconnu cela, ils ont une toute nouvelle perspective de réalisation : « Mon Dieu, j’ai toutes ces données, maintenant je veux commencer à les comprendre. Waouh. Je vois des choses que je n’aurais jamais pensé voir. Peut-être que je pensais intuitivement que c’était le cas, mais j’ai compris maintenant. »</p><p>Tucker Callaway : Nous aimons penser que c’est fascinant, n’est-ce pas ?</p><p>Alan Shimel : Ok, j'adore. Je suis fasciné par Mezmo.</p><p>Tucker Callaway : Je ne pouvais pas ne pas prendre la photo, mais c'était juste-</p><p>Alan Shimel : Ouais, c'était ton coup.</p><p>Tucker Callaway : C'est le dérivé du nom.</p><p>Alan Shimel : Je me sens utilisé. Je me sens utilisé.</p><p>Tucker Callaway : Je m'y suis remis un peu.</p><p>Alan Shimel : Vous êtes fasciné et vous vous dites : « Oh mon Dieu, que puis-je faire avec ça ? » Et c’est vraiment devenu votre métier. Et je suppose que c’est là que vous en êtes maintenant.</p><p>Tucker Callaway : La prochaine étape pour nous est clairement celle-là. Oui, et je pense qu’il y a deux façons de procéder. Il y a la question : « Que dois-je en faire ? Aidez-moi, dites-moi quoi faire. Donnez-moi les meilleures pratiques de tous vos clients, que dois-je faire ? » Nous sortirons donc probablement plus tard au printemps quelque chose qui vous permettra de simuler à quoi ces données pourraient ressembler une fois l’optimisation appliquée, afin que vous puissiez commencer à prendre ces décisions plus efficacement.</p><p>Mais la deuxième phase de cette démarche ne sera pas non plus suffisante pour certaines organisations qui souhaitent considérer leurs données comme un avantage stratégique. Elles vont donc chercher des moyens d’agir et d’extraire davantage d’informations de ces données. Nous envisageons cela à travers des recettes. Nous vous proposerons les offres standard, mais vous aurez ensuite la possibilité d’y accéder et de les modifier sous forme de code et de faire ce que vous devez faire pour tirer davantage de valeur de ces données. Si vous êtes prêt pour des mouvements de yoga avancés dans l’espace des données.</p><p>Alan Shimel : Je l'ai, je l'ai, je l'ai. J'ai une formation en sécurité.</p><p>Tucker Callaway : Ouais.</p><p>Alan Shimel : J’ai lancé une entreprise de sécurité en 2001, et c’était juste au moment où nous passions de l’IDS à l’IPS. Donc de la détection d’intrusion à la prévention d’intrusion. Même chose avec la gestion des vulnérabilités. Nous sommes passés de la simple recherche de vulnérabilités à l’application de correctifs, de mesures correctives, pas toujours. Qui ne voudrait pas de ça ? C’est évident, n’est-ce pas ? Ne me dites pas que je suis attaqué. Bloquez l’attaque. Au moment où vous me le dites, l’attaque a eu lieu. Ne me dites pas simplement que vous avez trouvé une vulnérabilité, assurez-vous que je ne suis pas exploité. Mais une chose amusante que j’ai apprise est que beaucoup de gens disent : « Allez-y doucement. Je ne peux pas me le permettre, car parfois la vulnérabilité, l’attaque ou les données sur lesquelles vous voulez que j’agisse peuvent affecter quelque chose que je considère comme plus précieux. »</p><p>Tucker Callaway : Oui, c'est vrai.</p><p>Alan Shimel : Et j’aime y aller doucement quand il s’agit de laisser un programme, une application ou un produit faire des choses. Donnez-moi juste mon menu et je déciderai de ce que je veux faire. Bien sûr, c’était avant l’avènement de l’IA, du ML et de beaucoup d’automatisations. Et très franchement, la vitesse des affaires était probablement un peu plus lente à l’époque. Voyez-vous un stade où les gens vont vouloir cela ? Donnez-moi juste les meilleures pratiques et je déciderai quand, comment et où je veux les mettre en œuvre. Ou bien allons-nous directement à « Hé mec, fais en sorte que cela se réalise pour moi ».</p><p>Tucker Callaway : Je pense qu’il y a une étape à franchir lorsque nous pensons à la gestion des données, à la télémétrie, à la gestion des données en général. Le mot qui est le plus important pour nous est la confiance, n’est-ce pas ? C’est une chose d’optimiser les données, mais il faut avoir confiance que les données sont constamment optimisées et traitées de la bonne manière pour tous les différents besoins que vous avez décrits.</p><p>Je pense donc que la première étape consiste à faire une suggestion avec un humain dans la boucle. Vous voulez appuyer sur ce bouton pour dire « go », puis lorsque vous établissez la confiance dans ces données et que vous savez que les algorithmes fonctionnent, vous les laissez aller. Probablement avec quelques garde-fous sur le côté, je vais les laisser aller, mais je suis [inaudible 00:09:52] juste au cas où. Et ensuite vous commencerez à l’optimiser de plus en plus. Et je pense que naturellement, presque comme une plate-forme de données en temps réel, le système d’exploitation commencera à évoluer.</p><p>Alan Shimel : La clé ici est la confiance.</p><p>Tucker Callaway : La clé est la confiance.</p><p>Alan Shimel : Pour instaurer la confiance envers les autres. En tant que PDG, comment instaurer cette confiance ?</p><p>Tucker Callaway : Eh bien, la confiance est toujours une question difficile.</p><p>Alan Shimel : Je suis intéressé, dites-moi.</p><p>Tucker Callaway : Oui. Je pense que comme pour tout le reste dans la vie, la confiance est instaurée par une prestation cohérente. C’est pourquoi je pense que l’intervention humaine dans la boucle est importante. Elle doit être durable. Cela ne peut pas être un moment précis, cela ne peut pas être la toute première mise en œuvre. C’est quelque chose que vous gagnez au fil du temps, où les gens peuvent compter sur vous ou sur les données ou sur la façon dont vous traitez les données pour vous amener au résultat souhaité. Ce n’est donc vraiment qu’en fournissant des résultats au client qu’il fera confiance aux systèmes et à toutes ces choses.</p><p>Alan Shimel : Cela a du sens ?</p><p>Tucker Callaway : Ouais.</p><p>Alan Shimel : Très bien. Je dois faire un peu de ménage.</p><p>Tucker Callaway : Ok, faisons-le.</p><p>Alan Shimel : Les gens qui veulent en savoir plus sur Mezmo.</p><p>Tucker Callaway : Rendez-vous sur le site Web. Nous avons une offre pour profiler les données de n’importe qui gratuitement dès maintenant.</p><p>Alan Shimel : Vraiment ?</p><p>Tucker Callaway : Oui. Venez sur mezmo.com ou arrêtez-vous à notre stand.</p><p>Alan Shimel : MEZM-O.com.</p><p>Tucker Callaway : M-E-Z-M-O.com</p><p>Alan Shimel : Là-bas, dans le monde de la télévision ou si vous entendez un coupon.</p><p>Tucker Callaway : Venez au stand. J'ai oublié le numéro, mais il est là-bas.</p><p>Alan Shimel : Vous savez quoi, si vous regardez sur le « je ne l’ai pas ». Si vous regardez au dos de votre carte d’identité, vous pouvez effectivement chercher des choses comme ça.</p><p>Tucker Callaway : Oui, c'est là-bas. Si vous vous êtes connecté au wifi ici à KubeCon, c'est Mezmo Data.</p><p>Alan Shimel : J’ai remarqué ça.</p><p>Tucker Callaway : C'est le wifi.</p><p>Alan Shimel : Je me suis dit : « Waouh, quelle chose cool. » Ouais.</p><p>Tucker Callaway : Vous nous avez vus. Venez nous voir. Le profilage de vos données vous donnera un aperçu et une compréhension de cela et vous parlera des prochaines étapes.</p><p>Alan Shimel : Très bien, allez voir ça sur Mezmo. Tucker, merci.</p><p>Tucker Callaway : Merci de m'avoir invité.</p><p>Alan Shimel : En fait, on se voit. Serez-vous présent chez AWS ?</p><p>Tucker Callaway : Oui, nous serons là. Oui, je serai là. Oui.</p><p>Alan Shimel : Nous allons faire des vidéos, mais pas sur le salon. Ce sera un peu plus calme dans une suite privée. Et puis, bien sûr, vous serez à la RSA.</p><p>Tucker Callaway : Nous serons à RSA.</p><p>Alan Shimel : Nous serons là aussi.</p><p>Tucker Callaway : Nous ne serons peut-être pas présents sur le salon AWS simplement à cause de-</p><p>Alan Shimel : C'est un peu fou, non ? Un peu cher.</p><p>Tucker Callaway : Ouais.</p><p>Alan Shimel : Oui, je vous entends. C’est pour ça que nous serons au studio.</p><p>Tucker Callaway : On se voit là-bas.</p><p>Alan Shimel : Viens, je vais prendre un café.</p><p>Tucker Callaway : Cela semble bien.</p><p>Alan Shimel : Mais RSA sera présent à Broadcast Alley et nous ferons notre truc DevSecOps lundi. Et le genre de folie RSA habituelle.</p><p>Tucker Callaway : J'ai hâte d'y être.</p><p>Alan Shimel : Absolument.</p><p>Tucker Callaway : D'accord.</p><p>Alan Shimel : Consultez Mezmo.com.</p><p>Tucker Callaway : Merci à tous.</p><p>Alan Shimel : Tucker Callaway ici sur Tech Drunk TV. Nous allons faire une pause. Je suppose qu'il est presque l'heure du déjeuner ici, mais il y a encore du monde.</p><p>Tucker Callaway : Presque.</p><p>Alan Shimel : Restez à l’écoute. Nous sommes là toute la journée. Nous reviendrons. Au revoir.</p>"
L'observabilité des données et son importance : tout ce que vous devez savoir,"<p>Dans le monde actuel, où la technologie est omniprésente, les entreprises s’appuient sur des volumes de données presque incompréhensibles pour dicter leurs opérations et leurs décisions commerciales. Les entreprises se concentrent sur la création de multiples référentiels et pipelines de données pour traiter, stocker, gérer et utiliser les données provenant de diverses sources. Compte tenu de la taille et de la complexité croissantes de l’environnement de données d’entreprise, il devient de plus en plus difficile de garantir l’exactitude et l’exhaustivité des données. Alors, comment comprendre simultanément les performances des données sur l’ensemble de l’infrastructure informatique ? La réponse est l’observabilité des données.</p><h3>Qu'est-ce que l'observabilité des données ?</h3><p>L'observabilité des données est un processus qui vise à vous alerter sur la fiabilité et la santé de vos données tout en fournissant les informations et les analyses nécessaires pour identifier et résoudre les problèmes avant qu'ils n'affectent l'ensemble de votre organisation.</p><p>La surveillance des opérations dans la base de données sert de mécanisme de défense proactif contre les menaces potentielles de sécurité. Cela garantit des données précises, complètes, sécurisées et précieuses et élimine les temps d'arrêt des données. Plongeons plus en détail dans l'infrastructure d'observabilité et explorons certaines des meilleures plateformes d'observabilité des données qui peuvent vous aider à optimiser et à sécuriser vos opérations de données.</p><h3>Cadre d'observabilité des données</h3><p>Nous savons que l’observabilité repose sur trois piliers : les journaux, les traces et les métriques.</p><p>Cependant, l’observabilité des données repose sur cinq piliers qui, ensemble, fournissent des informations clés sur la fiabilité et la qualité de vos données.</p><p>L’intégration de chaque pilier peut vous aider à élaborer une stratégie d’observabilité efficace.</p><p>1. Récence : également appelée « fraîcheur », la récence consiste à confirmer si les données sont à jour. Elle analyse également les éventuels écarts temporels inhabituels dans les tableaux de données. Cela permet d'éviter les problèmes de rapidité dans les pipelines de données.</p><p>2. Volume : Le volume consiste à vérifier si la quantité de données contenues dans la base de données correspond aux seuils prévus. Cela permet de garantir que les ensembles de données sont complets.</p><p>3. Distribution : cette mesure permet de mesurer la qualité des données au niveau du champ et de confirmer si les valeurs des données se situent dans les plages attendues. Des fluctuations inattendues dans les modèles de distribution indiquent un problème de données.</p><p>4. Schéma : il s'agit de surveiller et d'auditer les modifications apportées aux tables de données et à l'organisation des données pour rechercher des signes de données endommagées. Les schémas sont extrêmement importants, car les modifications de la structure des données sources sont souvent la cause des temps d'arrêt des données.</p><p>5. Lineage : Lineage collecte les métadonnées et fournit une image complète du paysage de données de votre organisation, y compris les sources en amont, en aval et les équipes qui peuvent accéder aux données à chaque étape. Ce processus aide les équipes de données à résoudre les problèmes liés aux ruptures de données.</p><h3>Avantages de l'observabilité des données</h3><p>L’observabilité des données offre une meilleure visibilité sur l’état interne de votre système informatique, notamment son comportement, ses performances et ses interactions avec d’autres systèmes. Cela peut être bénéfique de plusieurs manières :● Facilite l’analyse des causes profondes : grâce à une visibilité et une surveillance des données de bout en bout sur une infrastructure informatique multicouche, l’observabilité des données permet aux équipes de données de repérer rapidement les problèmes dans les ensembles de données avec moins d’efforts. Cela augmente également les chances d’identifier de nouveaux problèmes, quelle que soit leur origine.</p><p>● Délai moyen de détection et de résolution plus rapide : comme l'observabilité des données surveille une large gamme de résultats, les équipes de données peuvent trier et déboguer activement leurs systèmes. En fournissant des informations précieuses sur la manière dont les données interagissent et se déplacent au sein de l'architecture informatique, l'observabilité des données aide les équipes à repérer les problèmes qu'elles ignoraient, ce qui se traduit par un délai moyen de détection (MTTD) et un délai moyen de résolution (MTTR) plus rapides.</p><p>● Automatisation de la gestion de la sécurité : l'observabilité des données offre non seulement une visibilité en temps réel sur la posture de sécurité, mais facilite également l'automatisation de certaines parties du processus de tri. Cela permet de détecter instantanément les problèmes d'intégrité des données ou les temps d'arrêt des données.</p><h3>Défis d'observabilité des données</h3><p>En fonction de leur architecture informatique existante, les organisations peuvent être confrontées aux défis suivants en matière d'observabilité des données :● Silos de données : en raison de la présence de plusieurs agents, d'outils de surveillance des silos et de sources de données disparates, il devient difficile de comprendre les interdépendances entre les applications, les canaux numériques et les différents clouds.</p><p>● Intégration à l'écosystème de données complet : pour que l'observabilité des données fonctionne, l'outil doit disposer d'informations sur l'ensemble du pipeline de données et sur les serveurs, bases de données, logiciels et applications concernés. Cependant, certaines organisations peuvent trouver difficile de connecter tous les systèmes à une plateforme d'observabilité des données.</p><p>● Instrumentation et configuration manuelles : les outils d'observabilité des données visent à normaliser les données de télémétrie et les directives de journalisation afin de corréler efficacement les informations. Cependant, comme les grandes organisations gèrent plusieurs sources de données (des centaines, voire des milliers), les données de ces sources peuvent avoir des normes différentes. Cela nécessite un effort manuel pour normaliser les données.</p><h3>Observabilité des données vs. Gouvernance des données</h3><p>Dans le contexte actuel de digitalisation, où la sécurité des données est une préoccupation mondiale, la gouvernance des données fait l’objet d’une attention croissante. La gouvernance des données permet de définir les politiques et les procédures nécessaires pour contrôler la manière dont une organisation collecte, analyse, stocke, partage et utilise ses données.</p><p>Un programme de gouvernance des données solide garantit la disponibilité, l'intégrité, la facilité d'utilisation et la sécurité des données. Il élimine les problèmes d'intégration des données, les silos de données et la mauvaise qualité des données, résolvant ainsi les défis de l'observabilité des données. Les ensembles de données étant désormais évolutifs avec davantage de tables, davantage de sources de données et davantage de complexité, les ingénieurs et développeurs de données sont sous pression pour répondre aux exigences de cohérence, de disponibilité et de sécurité des données. Tout temps d'arrêt peut entraîner un gaspillage de ressources et de temps tout en détériorant la confiance dans la prise de décision.</p><p>L'observabilité des données, ainsi que la gouvernance des données, aident les organisations à gérer les problèmes de sécurité et de qualité des données de manière simplifiée.</p><h3>Observabilité des données et qualité des données</h3><p>La qualité des données mesure l'exhaustivité et l'exactitude des ensembles de données afin de déterminer s'ils peuvent être utilisés dans des applications analytiques et opérationnelles. L'observabilité des données, quant à elle, permet aux organisations de détecter et de résoudre les problèmes dans le pipeline de données de manière efficace et rapide.</p><p>Pour une gestion efficace des données, une entreprise doit prendre en compte ces deux éléments. Auparavant, la vérification et le nettoyage manuels des ensembles de données permettaient de déterminer la qualité des données. Mais aujourd'hui, bon nombre de ces tâches ont été automatisées grâce aux piles de données modernes.</p><p>En conséquence, l’accent est passé de la qualité des données à l’observabilité des données. Grâce à l’observabilité des données, les entreprises peuvent surveiller et résoudre efficacement les problèmes de leur pipeline de données. Sans cela, elles risquent de s’appuyer sur des données incomplètes ou inexactes pour prendre des décisions. Cela peut entraîner des erreurs coûteuses.</p><h3>Trouver les bons outils d'observabilité des données</h3><p>L'observabilité des données est sans aucun doute une caractéristique essentielle de toute entreprise qui utilise des données. Cependant, tous les outils d'observabilité des données ne sont pas aussi bénéfiques pour votre entreprise. Recherchez les caractéristiques suivantes lors du choix d'une plateforme d'observabilité des données : ● Compatibilité : l'outil doit être compatible avec vos lacs de données, bases de données et solutions de stockage cloud.</p><p>● Autonome : la technologie autonome réagit aux stimuli sans aucune intervention humaine. Cela est essentiel dans un outil d'observabilité des données car cela permet de détecter rapidement les anomalies et de répondre instantanément aux alertes.</p><p>● Rapide : un outil d'observabilité des données doit vous aider à identifier les erreurs le plus tôt possible. La meilleure plateforme surveille en permanence l'état des données depuis leur ajout à l'écosystème jusqu'à la fin de leur cycle de vie. La détection précoce des erreurs permet d'éviter les problèmes avant qu'ils ne deviennent critiques.</p><p>● Sophistiqué : les meilleures plateformes exploitent l’apprentissage automatique (ML) et l’intelligence artificielle (IA) pour identifier les problèmes difficiles à trouver.</p><p>● Autres fonctionnalités : L'outil doit être capable de collecter, d'échantillonner, d'examiner et de traiter des données de télémétrie provenant de plusieurs sources de données. Il doit servir de référentiel de données centralisé, offrir des services complets de surveillance des données et permettre la visualisation des données.</p><p>En fin de compte, la bonne plateforme d’observabilité des données dépend des besoins d’ingénierie d’observabilité de votre organisation et de son architecture informatique unique.</p><h3>Principaux outils d'observabilité des données</h3><p>Les plateformes d'observabilité des données sont encore une catégorie de produits en plein essor. La bonne nouvelle est que plus d'une demi-douzaine d'entreprises spécialisées dans l'observabilité des données proposent désormais des outils commerciaux dotés d'excellentes fonctionnalités. Il s'agit notamment des éléments suivants :</p><h3>Fournisseurs d'observabilité, aperçu, avantages et inconvénients</h3><p>Monte Carlo Data, dont le siège social se trouve à San Francisco, est le créateur du premier outil d’observabilité des données de bout en bout du secteur.</p><p>● Fournit des capacités complètes d'observabilité des données.● Offre un niveau élevé de fonctionnalités, notamment des alertes automatisées, des catalogues de données, etc.● Prend en charge une configuration entièrement automatisée.</p><p>● Des volumes de données élevés peuvent entraîner des problèmes d'interface utilisateur.● D'énormes quantités de variables restreintes par différentes contraintes peuvent entraîner une inefficacité informatique.</p><p>Bigeye est une plateforme d'observabilité des données de pointe qui permet aux équipes d'améliorer, de mesurer et de communiquer des données de qualité de manière claire et rapide à n'importe quelle échelle.</p><p>● Une interface facile à utiliser qui facilite la configuration des données tout en garantissant la cohérence et la précision.● Fournit de puissantes capacités d'intégration d'API.● Comprend un tableau de bord polyvalent avec suivi et surveillance en temps réel des mesures de qualité des données par plusieurs personnes.</p><p>● Cela peut être coûteux pour les petites organisations.● L'outil peut parfois planter, nécessitant des améliorations de performances.</p><p>Acceldata propose des outils pour Hadoop, les services cloud et les entreprises qui incluent la fiabilité des données de bout en bout, la surveillance du pipeline de données et l'observabilité des données multicouches.</p><p>● Fournit des contrôles de fiabilité des données entièrement automatisés.● Offre une interface glisser-déposer pour analyser les pipelines de données sur plusieurs couches et plates-formes.</p><p>● La configuration initiale peut être complexe et difficile.● L'ajout et la suppression de nœuds nécessitent une intervention humaine.</p><p>Databand est une société IBM qui offre des capacités proactives pour identifier et résoudre les problèmes de données dès les premières étapes du cycle de développement.</p><p>● Offre une visibilité sur l'ensemble de la pile. Cela signifie que vous pouvez obtenir une vue d'ensemble de toutes les tâches de données du début à la fin. ● Offre des DataOps standardisés et une lignée de données de bout en bout, garantissant la fiabilité et l'exactitude des données.</p><p>● Le programme nécessite une quantité importante d’espace, ce qui rend son installation sur le système de l’utilisateur difficile. ● Il nécessite des mises à jour logicielles constantes, ce qui le rend beaucoup plus lourd.</p><p>Datafold possède une capacité unique à détecter, enquêter et hiérarchiser de manière proactive les erreurs de qualité des données avant qu'elles n'affectent la production.</p><p>● Vous permet de transformer les requêtes SQL en alertes intelligentes, vous tenant informé de tout problème pouvant survenir.● Automatise les tests de régression en intégrant le processus CI via GitLab et GitHub.</p><p>● Options d'intégration limitées.● Ne fournit aucun support pour l'analyse des données et la science des données.</p><h3>Réflexions finales</h3><p>L’observabilité des données est l’épine dorsale de la capacité des ingénieurs de données à être agiles avec leurs produits. Si vous souhaitez moderniser vos pratiques de gestion des données et améliorer la qualité de vos données, l’observabilité des données est la voie à suivre. Sans elle, votre équipe de données ne peut pas compter sur ses outils et son infrastructure, car les problèmes ne peuvent pas être détectés efficacement et rapidement.</p>"
Observabilité : la tour de guet centrale dont votre informatique a besoin pour tout voir,"<p>Si vous devez garder quelque chose en sécurité, il est naturel de vouloir le garder sous clé. Cependant, même garder quelque chose dans un coffre-fort, loin des regards indiscrets et des mauvaises intentions, peut ne pas suffire à vous offrir la tranquillité d’esprit. Après tout, comment pouvez-vous avoir la tranquillité d’esprit de savoir que votre objet est en sécurité si vous ne vérifiez pas régulièrement qu’il n’a pas été verrouillé, cambriolé ou cassé ?</p><p>Maintenant, multipliez par dix le volume et la valeur des éléments surveillés, et vous aurez une idée des inefficacités et des angoisses quotidiennes auxquelles les professionnels de l’informatique sont confrontés.</p><p>Au lieu de bijoux, ces équipes protègent, gèrent et entretiennent quelque chose de bien plus précieux : les données qui sous-tendent tout, des transactions commerciales aux performances du système, en passant par la détection des menaces et la prestation de services, et bien plus encore. Pour compliquer encore les choses, les écosystèmes numériques d’aujourd’hui sont plus complexes, distribués et disparates que jamais.</p><p>Les bases de données sont essentielles à la gestion d’un service informatique performant. Il est temps de mettre un terme à l’idée largement répandue dans le secteur selon laquelle elles sont une boîte noire insondable. Les bases de données représentent en effet le composant le plus difficile à observer, à ajuster, à gérer et à faire évoluer des écosystèmes informatiques. Mais la base de données n’est pas une boule magique en termes de processus internes : elle est un outil indispensable pour gérer les processus internes. Nous n’avons plus besoin de secouer le jouet mystérieux et d’accepter l’une des six réponses qu’il est censé fournir.</p><p>Le fait est que les spécialistes des bases de données et les équipes informatiques ont besoin d’une vision claire de la télémétrie des performances des bases de données s’ils veulent espérer maintenir la santé, la stabilité et l’évolutivité de leurs services.</p><h3>Outils d'observabilité</h3><p>C'est là que les outils d'observabilité peuvent jouer un rôle transformateur. S'inspirant de la tour de guet panoptique omnisciente, l'observabilité élimine les recoins sombres du donjon de la base de données en offrant une vue complète de l'ensemble des piles technologiques cloud natives, sur site et hybrides.</p><p>Le mot panoptique dérive du mot grec panoptes, qui signifie « tout voir ». Le modèle panoptique comprend une tour de garde centrale éclairée au centre d'une pièce circulaire, ce qui permet des lignes de vue à 360 degrés qui permettent aux gardes d'observer chaque cellule environnante à partir d'un seul point d'observation centralisé.</p><p>Le philosophe anglais Jeremy Bentham n’avait aucun moyen d’anticiper les défis auxquels sont confrontés les professionnels des bases de données d’aujourd’hui lorsqu’il a imaginé le panoptique dans les années 1700. Ce concept allait poser les bases d’un système pénitentiaire conçu autour d’un principe fondamental : permettre au nombre minimum de gardiens de surveiller efficacement le nombre maximum de détenus. Le système de Bentham fonctionne également comme une stratégie pour garantir la santé et les performances des bases de données critiques de votre organisation.</p><p>Le panoptique a été conçu pour que chaque cellule puisse être surveillée à partir d'un point central, ce qui garantit non seulement une sécurité plus efficace et plus simple, mais aussi une préservation des ressources en termes de temps, de main-d'œuvre et d'efforts. Considérez vos équipes ITOps, base de données, DataOps et DevOps comme les gardiens de vos performances informatiques. La surveillance traditionnelle peut être comparée à la patrouille d'une prison traditionnelle : des rangées de cellules donnant sur un couloir qui est patrouillé selon un calendrier ou utilisé pour repérer et traiter les mauvais comportements dès qu'ils sont remarqués.</p><p>Comment peuvent-ils être sûrs de concentrer leurs efforts au bon endroit ? Ou être sûrs que le bloc cellulaire A ne déclenchera pas d’évasion alors qu’ils interrompent une dispute dans le bloc cellulaire B ?</p><p>En bref : ils ne peuvent pas. Il y a de fortes chances que l’équipe soit en train de courir d’une panne à l’autre dans un mode de lutte contre les incendies permanent, soit qu’elle s’appuie sur des contrôles ponctuels aléatoires pour quelque chose d’aussi vital que la santé de nos systèmes informatiques.</p><p>L'observabilité, par opposition à la surveillance, place les équipes dans cette tour de guet panoptique centrale, leur permettant non seulement de tout voir, mais aussi de le voir dans le contexte d'une vue d'ensemble. Cette visibilité complète et cette observation permanente permettent aux équipes informatiques d'identifier les problèmes critiques au fur et à mesure qu'ils surviennent, même ceux causés par des dépendances complexes entre la base de données, le système d'exploitation, le sous-système de stockage et le réseau.</p><h3>Problèmes de performances de la base de données</h3><p>En outre, les problèmes de performances des bases de données sont le point de départ de goulots d’étranglement coûteux ou de pannes graves qui peuvent entraver la capacité de votre entreprise à être compétitive ou à se développer. Sans surveillance et observabilité complètes et précises des bases de données, les opérations informatiques ont du mal à déterminer avec précision la cause profonde des problèmes de performances d’une application. Cela augmente le risque de temps d’arrêt, de perte de données et de mauvaise expérience client.</p><p>La découverte de la cause profonde des problèmes de performances est une tâche critique pour le système, que vous dirigiez simplement votre entreprise, que vous déployiez un nouveau code ou que vous évoluiez dans le cadre de vos opérations. L’observabilité offre également aux équipes la rare opportunité d’anticiper les problèmes de performances, car vous pouvez voir les anomalies de performances avant une interruption de service plutôt que de vous fier à une correction en temps réel.</p><p>Avec les méthodes traditionnelles, les équipes DevOps et informatiques doivent analyser manuellement les données qui leur sont présentées, les corréler au problème et localiser l’erreur avant de pouvoir enfin commencer à la résoudre. En revanche, l’observabilité collecte des données pour fournir des informations sur ce qui ne fonctionne pas comme prévu et pourquoi. Cela permet aux équipes d’adopter une approche proactive pour résoudre les temps d’arrêt et les goulots d’étranglement et les prévenir de manière proactive.</p><p>Dans le monde du logiciel, cela se traduit directement par plus de temps consacré à ce qui compte : développer une nouvelle valeur commerciale au sein de vos applications et infrastructures, alimenter l’innovation et dépasser les attentes des clients.</p><p>Étant donné la complexité des bases de données modernes, qui est devenue et continue de devenir telle, équiper votre entreprise des bonnes solutions d’observabilité peut améliorer l’efficacité et les performances et éviter à vos équipes de se retrouver dans un travail fastidieux, fastidieux et sujet aux erreurs.</p>"
Couchbase ajoute une base de données en colonnes à l'environnement DBaaS,"<p>Lors de la conférence AWS re:Invent 2023 qui s'est tenue aujourd'hui, Couchbase, Inc. a annoncé avoir ajouté une base de données en colonnes basée sur le format de fichier JSON à son portefeuille de bases de données en tant que service (DBaaS) pour permettre aux organisations de créer des applications d'analyse en temps réel.</p><p>Scott Anderson, vice-président senior de la gestion des produits et des opérations commerciales chez Couchbase, a déclaré que la base de données en colonnes Capella est conçue pour être intégrée à la base de données de documents basée sur JSON de la société via un protocole de changement de base de données (DCP) que Couchbase a intégré dans son environnement DBaaS.</p><p>Déployé sur les services cloud d'Amazon Web Services (AWS), l'objectif global est de réduire le niveau de friction que les équipes informatiques rencontreraient autrement lors de l'intégration d'une base de données de documents avec une base de données en colonnes optimisée pour traiter les analyses à l'aide de colonnes plutôt que des lignes généralement associées à une base de données relationnelle.</p><p>Cette approche donne aux équipes informatiques la possibilité d'utiliser une architecture de base de données sans schéma qui offre une alternative à l'utilisation de bases de données disparates qui devraient être déployées puis intégrées à l'aide de processus d'extraction, de transformation et de chargement (ETL) pour déplacer les données d'une base de données à l'autre, a noté Anderson.</p><p>Capella columnar utilise également le même langage SQL++ que la base de données de documents de l'entreprise pour rationaliser les requêtes en plus de prendre en charge Capella iQ, un copilote basé sur l'intelligence artificielle générative (IA) qui permet de créer des requêtes en langage naturel.</p><p>De plus, Capella columnar fournit des connecteurs pour déplacer des données depuis Amazon DynamoDB, Amazon DocumentDB, Amazon Relational Database Service (Amazon RDS) et d'autres sources de données déployées sur le cloud AWS.</p><p>À plus long terme, Couchbase prévoit également d'ajouter des fonctionnalités vectorielles à son environnement DBaaS pour simplifier l'extension sécurisée des modèles d'IA génératifs, a déclaré Anderson.</p><p>En général, les bases de données documentaires ont été largement adoptées car elles offrent aux développeurs une alternative aux plates-formes de bases de données existantes qu'ils peuvent télécharger et déployer eux-mêmes. La plate-forme DBaaS de Capella simplifie encore davantage ce processus via un service géré par lequel Couchbase assume la responsabilité opérationnelle de la gestion des bases de données. La base de données en colonnes étend cette capacité en utilisant une base de données en colonnes qui peut piloter les capacités d'analyse pour offrir une expérience d'application plus personnalisée, a noté Anderson.</p><p>On ne sait pas encore dans quelle mesure les entreprises finiront par s’appuyer sur des plateformes de type « as-a-service », mais depuis le début de la pandémie de COVID-19, le recours à cette approche de consommation des ressources informatiques s’est considérablement accru. En conséquence, la manière dont les équipes informatiques internes sont organisées évolue, les tâches de niveau inférieur étant soit automatisées, soit gérées par une équipe informatique externe qui travaille pour un fournisseur.</p><p>Quelle que soit la manière dont les bases de données sont gérées, il reste nécessaire d'intégrer les processus d'exploitation des données (DataOps) utilisés pour les gérer aux flux de travail DevOps utilisés pour créer des applications modernes. Chaque organisation devra déterminer dans quelle mesure il est économiquement judicieux pour elle de gérer ces données ou de faire appel à un fournisseur de services externe. Le défi lorsqu'on fait appel à un fournisseur de services externe est, comme toujours, d'intégrer ce service dans un environnement informatique existant.</p>"
Cinq excellentes opportunités d'emploi DevOps,"<p>DevOps.com fournit désormais un rapport hebdomadaire sur les emplois DevOps à travers lequel les opportunités pour les professionnels DevOps seront mises en évidence pour mieux servir notre public.</p><p>Notre objectif en ces temps économiques difficiles est de permettre aux professionnels DevOps de faire progresser plus facilement leur carrière.</p><p>Bien entendu, le vivier de talents DevOps disponibles est encore relativement limité, donc lorsqu’un professionnel DevOps assume un nouveau rôle, cela a tendance à créer une opportunité pour les autres.</p><p>Les cinq offres d’emploi partagées cette semaine sont sélectionnées en fonction de l’entreprise qui cherche à embaucher, du segment industriel vertical et, naturellement, de l’échelle salariale proposée.</p><p>Nous nous engageons également à fournir des informations supplémentaires sur l’état du marché du travail DevOps. En attendant, voici quelques informations à prendre en compte :</p><p>CareerBuilder.com</p><p>ZoomSan Jose, CalifornieResponsable de programme technique - DevOps134 300 $</p><p>LinkedIn</p><p>GrammarlySpringdale, Caroline du SudIngénieur logiciel - infrastructure cloud123 000 $ à 253 000 $</p><p>Dés.com</p><p>Amtex EnterprisesIrving, TexasIngénieur DevOps en automatisation120 000 $</p><p>SimplyHired.com</p><p>KUBRATempe, ArizonaIngénieur DevOps senior115 000 à 146 000 $</p><p>Indeed.com</p><p>McDonald’s CorpChicago, IllinoisIngénieur DevOps101 000 à 127 000 $</p>"
Redgate automatise l'extraction des données de test à partir des bases de données de production,"<p>Redgate a ajouté aujourd'hui un outil Redgate Test Data Management (TDM) qui automatise l'extraction de données de test masquées à partir de bases de données exécutées dans des environnements de production.</p><p>David Gummer, directeur des produits chez Redgate, a déclaré que cette capacité élimine les tâches manuelles qui empêchaient auparavant les tests ou entraînaient l'utilisation par inadvertance d'informations personnelles identifiables (PPI) pour tester les applications.</p><p>Redgate TDM classe et masque automatiquement les données résidant dans les bases de données SQL Server, PostgreSQL, MySQL ou Oracle avant de les utiliser pour tester une application. Il produit ensuite une copie ou un clone de ces données qui représente une fraction de la taille de l'original, a déclaré Gummer.</p><p>Cette approche crée un processus reproductible pour les équipes DevOps qui peut résister à un audit réglementaire des processus de développement d’applications, a-t-il ajouté. En fait, avec l’attention accrue portée aux problèmes de chaîne d’approvisionnement en logiciels, de plus en plus d’auditeurs examinent spécifiquement la manière dont les données sont sécurisées pendant le processus de développement d’applications.</p><p>Conçu pour être invoqué via une interface de ligne de commande (CLI), dans un environnement de développement intégré ou via une interface graphique, Redgate TDM réduit les frictions qui se produisent actuellement lorsque les équipes DevOps demandent des données de test aux administrateurs de base de données (DBA), a déclaré Gummer. De nombreux administrateurs de base de données sont soit trop occupés pour répondre à ces demandes, soit réticents à donner accès à des données sensibles qui conduiraient l'organisation à enfreindre une obligation de conformité.</p><p>Les développeurs ont alors souvent recours à des données anonymes qui ne reflètent pas réellement l’environnement dans lequel leur application sera finalement déployée.</p><p>La quantité de données dont les développeurs ont besoin pour créer des applications ne fera qu'augmenter. En l'absence de moyen efficace d'automatiser le processus d'accès à ces données, il faut finalement plus de temps pour créer et déployer des applications stables. Dans de nombreux cas, les applications ne sont tout simplement pas testées aussi minutieusement qu'elles devraient l'être avant d'être déployées dans un environnement de production, ce qui entraîne des retours en arrière qui pourraient autrement être évités.</p><p>Les progrès de l’intelligence artificielle (IA) facilitent la création de scripts de test, mais une grande partie du processus de configuration des tests repose encore sur des processus manuels qui augmentent le niveau global de travail.</p><p>À l’heure où de plus en plus d’entreprises tentent d’accroître la productivité de leurs développeurs, l’automatisation de tâches relativement banales telles que l’extraction de données de test peut avoir un impact considérable. Les développeurs veulent consacrer le plus de temps possible à l’écriture de code. Tout le reste est une tâche qui les empêche d’atteindre cet objectif. Bien qu’il existe un désir général de responsabiliser les développeurs, cela ne signifie pas nécessairement qu’ils souhaitent que la charge cognitive associée à la création d’applications augmente. En fait, l’une des raisons pour lesquelles l’ingénierie de plateforme est devenue une méthodologie de gestion des flux de travail DevOps à grande échelle est de réduire le niveau actuel de charge cognitive que subissent les développeurs aujourd’hui.</p><p>Bien entendu, les équipes DevOps sont, par nature, impitoyablement déterminées à automatiser autant de processus que possible. Le problème est que les goulots d'étranglement dans ces processus conspirent pour ralentir la vitesse globale à laquelle les applications peuvent être créées et déployées malgré les meilleures intentions de toutes les parties impliquées.</p>"
Grafana Labs acquiert Asserts.ai pour amener l'IA au service de l'observabilité,"<p>Lors de son événement ObservabilityCON, Grafana Labs a annoncé aujourd'hui l'acquisition d'Asserts.ai pour automatiser les configurations et la personnalisation des tableaux de bord.</p><p>En outre, l’entreprise présente un aperçu de la possibilité d’appliquer l’intelligence artificielle (IA) à la gestion des incidents pour simplifier la recherche de la cause profonde d’un problème. Sift est un assistant de diagnostic dans Grafana Cloud qui analyse automatiquement les métriques, les journaux et les données de traçage, tandis que Grafana Incident est un outil d’IA génératif qui résume les chronologies des incidents en un seul clic, crée des métadonnées pour les tableaux de bord et simplifie l’écriture des requêtes PromQL.</p><p>Grafana Labs met également à disposition un module d'observabilité des applications pour Grafana Cloud afin de fournir une vue plus holistique des environnements informatiques.</p><p>Enfin, Grafana Beyla, un projet d’auto-instrumentation open source qui utilise le filtrage de paquets Berkeley étendu (eBPF), est désormais également disponible. Cet outil permet aux équipes DevOps de collecter des données de télémétrie pour un environnement informatique à partir d’un environnement sandbox exécuté dans le micro-noyau d’un système d’exploitation. Cette approche simplifie l’instrumentation automatique d’un environnement informatique, mais il existe des cas où les équipes DevOps géreront des applications complexes qui nécessiteront toujours qu’elles collectent des données de télémétrie via l’espace utilisateur d’une application.</p><p>Richi Hartmann, directeur de la communauté de Grafana Labs, a déclaré que ces fonctionnalités supplémentaires simplifieront l’application de l’observabilité dans des environnements informatiques de plus en plus complexes. Par exemple, les technologies d’IA développées par Assert.ai permettront aux équipes DevOps de commencer à envoyer des données à Grafana Labs qui permettront au service cloud d’identifier les applications et l’infrastructure utilisées. Les modèles d’IA pourront alors générer automatiquement un tableau de bord personnalisé pour cet environnement que les équipes DevOps pourront étendre à leur guise, a déclaré Hartmann.</p><p>En général, les algorithmes d’apprentissage automatique et l’IA générative commencent à être plus largement appliqués à l’observabilité. L’objectif ultime est d’identifier automatiquement les problèmes de manière à réduire la charge cognitive nécessaire à la gestion d’environnements informatiques complexes tout en facilitant le lancement de requêtes permettant d’identifier les goulots d’étranglement susceptibles d’avoir un impact négatif sur les performances et la disponibilité des applications.</p><p>On ne sait pas exactement dans quelle mesure les outils d’observabilité pourraient éliminer le besoin d’outils de surveillance qui suivent des mesures prédéfinies, mais la plupart des équipes DevOps utiliseront probablement un mélange des deux dans un avenir proche.</p><p>Dans le même temps, les environnements informatiques deviennent de plus en plus complexes à mesure que divers types d’applications cloud natives sont déployés aux côtés d’applications monolithiques existantes qui sont continuellement mises à jour. Le défi est que la taille globale des équipes DevOps ne s’agrandit pas, d’où un besoin accru d’outils pour rationaliser la gestion des flux de travail DevOps.</p><p>L’IA jouera naturellement un rôle plus important pour permettre aux organisations d’atteindre cet objectif, mais il est peu probable qu’elle remplace le besoin d’ingénieurs DevOps, a déclaré Hartmann.</p><p>À l’inverse, de nombreuses équipes DevOps se tourneront naturellement vers des organisations qui mettent à leur disposition les outils dont elles ont besoin pour réussir. Aujourd’hui, un nombre bien trop important de tâches manuelles entraînent une augmentation du turnover au fur et à mesure que les équipes DevOps s’épuisent. Les organisations qui souhaitent embaucher et conserver les meilleurs ingénieurs DevOps devront investir dans l’IA.</p><p>Bien entendu, DevOps a toujours eu pour objectif d’automatiser impitoyablement autant de tâches manuelles que possible. L’IA n’est que la dernière d’une série d’avancées qui, au fil du temps, continuent de rendre DevOps plus accessible aux professionnels de l’informatique de tous niveaux.</p>"
Comment les logiciels open source favorisent la science climatique à fort impact,"<p>Selon les Centres nationaux d’information environnementale de la NOAA (NCEI), les neuf dernières années ont été les neuf plus chaudes depuis le début de l’enregistrement des données en 1880, et 2023 est en passe d’établir un nouveau record. Prendre les décisions difficiles nécessaires pour faire face à cette crise mondiale nécessite d’analyser et de comprendre d’énormes quantités de données à l’aide de méthodes de calcul sophistiquées. Heureusement, les innovations issues de la communauté des logiciels open source contribuent à révolutionner la science du climat, en permettant aux chercheurs de tirer rapidement des enseignements des données.</p><h3>Assurer une collaboration essentielle</h3><p>La science moderne repose largement sur des techniques complexes de calcul et d’analyse numérique. Cependant, la quantité massive de données de mesure et de simulation disponibles dans des domaines comme la science du climat rend les approches traditionnelles impraticables. Avec des données à l’échelle du pétaoctet, les chercheurs ne peuvent plus espérer télécharger ou obtenir des copies physiques d’ensembles de données entiers, et leurs postes de travail locaux ou leurs ordinateurs portables ne peuvent plus gérer des ensembles de données de cette taille une fois qu’ils sont arrivés. Il est également impossible de déplacer les scientifiques vers les données, car les chercheurs du monde entier doivent utiliser les mêmes données.</p><p>Au lieu de cela, les scientifiques et les ingénieurs de Pangeo.io et d’autres collaborations mondiales ont développé un ensemble robuste d’outils open source pour travailler efficacement avec de grands ensembles de données distants, en contournant les restrictions des logiciels propriétaires qui se limitent aux petits ensembles de données installés localement. Au lieu d’applications commerciales monolithiques ou de bases de code Fortran inflexibles, les chercheurs modernes utilisent des approches flexibles et légères pour distribuer les calculs sur des ressources de cloud computing partagées entre de nombreux collaborateurs. Ces innovations sont essentielles pour la science du climat, car tout retard supplémentaire pourrait conduire à des défis catastrophiques dans la lutte contre le réchauffement climatique.</p><h3>Obtenir des données plus rapides et plus flexibles pour un meilleur rendement à grande échelle</h3><p>L’écosystème scientifique de Python, en particulier, a gagné en importance et est au premier plan des réflexions sur les plateformes open source les mieux adaptées à la mise en œuvre de solutions climatiques. Les nouveaux étudiants diplômés connaissent plus souvent Python que tout autre langage de programmation, maintenant que Python est devenu le langage de programmation le plus populaire au monde. L’écosystème open source de Python comprend une grande variété de packages, chacun se concentrant sur un domaine de fonctionnalité spécifique. Ces composants évoluent rapidement et introduisent régulièrement de nouvelles fonctionnalités tout en maintenant la compatibilité via des protocoles standard. Contrairement à une approche monolithique et conçue de manière centralisée, l’écosystème ouvert de Python offre plus de flexibilité lors de la mise à l’échelle des données et des solutions.</p><p>Dans les domaines à fort impact comme la science climatique, l’objectif est de permettre aux scientifiques et aux chercheurs de se concentrer sur les questions spécifiques à leur domaine plutôt que de s’emmêler dans les subtilités de la manipulation des données et de la gestion des ressources. La pile numérique Python d’outils open source, combinée à la possibilité d’accéder aux données disponibles dans le cloud à grande échelle, facilite cette division du travail. Les scientifiques peuvent se concentrer sur l’écriture de logiciels pour les parties d’analyse spécifiques à leur expertise, ce qui signifie des progrès plus rapides vers des solutions et des produits climatiques.</p><p>Avec les approches modernes de cloud computing, il est plus logique de déplacer les calculs vers les données plutôt que l’inverse (le code étant beaucoup plus petit et plus portable que les données). Avec une plateforme comme Jupyter fonctionnant sur un cluster distant et des noyaux Python distants comme environnement d’exécution, les scientifiques peuvent exploiter la même infrastructure qui héberge des ensembles de données massifs dans des magasins d’objets cloud. Les améliorations apportées à la technologie logicielle et aux formats de données simplifient également l’accès à des segments spécifiques de fichiers très volumineux, tels que les relevés de température associés à une ville particulière parmi les données du monde entier. L’accès aux données devient rapide, rentable et évolutif, contournant les flux de travail traditionnels impliquant le téléchargement répété de grands ensembles de données constitués principalement de données non nécessaires à une analyse particulière.</p><p>Pour la science climatique en particulier, cette approche flexible de calcul à distance signifie également une collaboration plus facile entre développeurs et scientifiques pour créer des innovations qui pourraient faire avancer les études et les solutions climatiques. Les technologies de calcul distribué comme Dask, Ray et Spark permettent un calcul parallèle et hors cœur, ce qui permet de traiter des ensembles de données plus volumineux à des vitesses plus élevées. Il existe désormais plusieurs services distants basés sur Jupyter, permettant même au processus Python principal (le « noyau ») de s’exécuter à distance tout en utilisant le navigateur comme plate-forme de développement interactive. Comme tous ces outils sont ouverts, les chercheurs peuvent appliquer des technologies émergentes telles que l’apprentissage automatique (ML) et les techniques d’intelligence artificielle (IA) dès qu’elles sont développées sans attendre que les fabricants de logiciels et de matériel informatique rattrapent leur retard.</p><h3>Création de visualisations de données et de tableaux de bord</h3><p>Pour que les humains comprennent et valident les données et les étapes impliquées dans l’analyse, il faut une visualisation à chaque étape, ce qui est difficile lorsque les données sont trop volumineuses pour être transférées localement pour être affichées. De nouveaux outils open source comme Datashader et VegaFusion offrent un rendu côté serveur même des plus grands ensembles de données, en rendant efficacement les ensembles de données distribués dans une image visualisable, puis en transférant uniquement cette image vers le navigateur local d’un scientifique pour l’affichage. Matplotlib, HoloViews, Altair, Plotly et hvPlot en Python, ainsi que Makie.jl pour Julia, permettent désormais d’intégrer le rendu côté serveur dans des tracés entièrement interactifs dans un navigateur Web, permettant aux chercheurs d’explorer les ensembles de données et d’identifier des fonctionnalités intéressantes ou de détecter des problèmes de qualité des données ou des étapes d’analyse. Les outils de tableau de bord Panel et Dash en Python permettent de combiner ces tracés dans des applications Web complètes pour que les chercheurs puissent partager leurs analyses interactives, le tout sans aucun outil propriétaire ni restriction (voir cette application d’éolienne de l’USGS comme exemple).</p><p>Dans un domaine où le besoin d’innovation est si urgent, les technologies open source sont cruciales pour fournir la créativité et la rapidité nécessaires aux climatologues pour lutter contre le réchauffement climatique. La communauté scientifique compte de plus en plus sur la mise en œuvre collaborative et transparente de l’innovation.</p><p>Le Dr Martin Durant, ingénieur logiciel chez Anaconda, a contribué à cet article.</p>"
Comment optimiser les plateformes de données pour une efficacité optimale,"<p>Les données sont l’élément vital des entreprises modernes. Presque tout ce avec quoi nous interagissons dans notre vie numérique implique une gestion des données sous une forme ou une autre. Et les grandes entreprises doivent souvent gérer de nombreuses opérations liées aux données non optimisées, telles que des requêtes inefficaces, des charges de travail de données non optimisées ou de grands lacs de données non gouvernés.</p><p>J'ai récemment discuté avec Salim Syed, vice-président et responsable de l'ingénierie Slingshot de Capital One Software, sur l'optimisation des opérations de données pour maximiser l'efficacité. Ci-dessous, nous examinerons comment la gestion des données évolue lors de la migration vers le cloud et examinerons certaines méthodes concrètes que les responsables techniques peuvent utiliser pour optimiser les DataOps.</p><p>Le premier et le plus grand obstacle à surmonter lors de l’optimisation des opérations de données est sans doute la transition d’une ancienne façon de penser la gestion des données sur site vers un nouveau modèle axé sur le cloud, a expliqué Syed, car les deux paradigmes ne pourraient pas être plus différents.</p><p>La simple réplication des comportements sur site peut être néfaste pour la gestion des bases de données dans le cloud, car elle permet une puissance illimitée par rapport à l’informatique sur site. « Lorsque vous passez au cloud, les entraves se brisent », a déclaré Syed. Les technologies cloud permettent une évolutivité infinie, mais une telle puissance s’accompagne d’un degré de responsabilité nettement plus élevé.</p><p>Un autre problème potentiel est la structure organisationnelle. Une équipe centralisée supervisant de nombreux contrôles pourrait finir par freiner l’innovation, a-t-il déclaré. En revanche, fédérer trop d’équipes de plateformes pourrait avoir un effet de cloisonnement. C’est pourquoi il a recommandé de définir des politiques et des réglementations de manière centralisée et de les rendre faciles à mettre en œuvre grâce à des outils en libre-service.</p><p>Capital One a été l’un des premiers à adopter le cloud. Syed a expliqué que Capital One a commencé son parcours de transformation technologique en 2012 et a quitté ses derniers centres de données en 2020. Il n’est pas surprenant qu’en tant qu’entreprise financière numérique basée sur le cloud, elle traite des volumes de données extrêmement importants. Par exemple, des milliers d’analystes s’appuient sur des données financières et d’autres sources pour générer leurs informations.</p><p>L’entreprise doit donc constamment trouver un équilibre entre l’autonomisation et l’efficacité opérationnelle pour interagir avec de grandes quantités de données, a déclaré Syed. Dans ce contexte, il a partagé quelques conseils qu’ils ont utilisés pour améliorer l’efficacité de leurs opérations basées sur les données.</p><h3>Évitez les requêtes inefficaces</h3><p>Les requêtes de base de données avaient autrefois un impact minime sur la consommation de ressources lorsque vous étiez propriétaire de votre infrastructure. Mais tout cela change à l'ère du cloud. Par exemple, des requêtes mal écrites et inefficaces programmées pour analyser des milliards de lignes peuvent entraîner des efforts de calcul inutiles et des dépenses élevées dans le cloud. Par conséquent, la conception de requêtes avec des plages plus intelligentes peut aider à éviter le gaspillage.</p><p>Syed a expliqué que chez Capital One, son équipe a développé un conseiller de requêtes capable d'examiner les modèles antérieurs et de formuler des recommandations, telles que la correction de la plage, la jonction de requêtes ou la réduction du nombre de lignes. En examinant les modèles de requêtes antérieurs et en examinant les métadonnées associées, le système peut recommander des requêtes mieux construites qui améliorent l'efficacité.</p><h3>Optimiser les ensembles de données</h3><p>En plus d'optimiser les requêtes, les ingénieurs de bases de données doivent également veiller à optimiser les jeux de données eux-mêmes. En effet, la manière dont vous structurez vos tables et le modèle de données lui-même est encore plus critique dans le monde du cloud, a déclaré Syed.</p><p>En outre, le modèle d’accès doit être réfléchi à l’avance. Bien que certains scénarios nécessitent un traitement en temps réel, cela peut être coûteux et n’est pas toujours nécessaire. Par conséquent, Syed recommande de comprendre votre modèle d’accès pour le chargement des données et ses implications financières associées.</p><h3>Avoir une stratégie de conservation des données</h3><p>Ensuite, il est facile de laisser les données s’accumuler au fil du temps. Mais une politique de type « configurez-la et oubliez-la » pour les données n’est pas une stratégie judicieuse pour le cloud. Au lieu de laisser des pétaoctets d’informations s’accumuler et générer des frais de stockage excessifs, il est préférable d’élaguer vos lacs de données lorsque cela est possible afin qu’ils ne soient pas conservés indéfiniment, a déclaré Syed, et de définir à l’avance des politiques de conservation du stockage et des délais de suppression.</p><h3>Surveiller les inefficacités</h3><p>Syed reconnaît qu’un certain degré d’inefficacité dans les opérations de données sera inévitable. Il peut s’agir par exemple d’une instance laissée en fonctionnement par accident ou de la mise en service de grands clusters pour effectuer des fonctions de base. Néanmoins, les ingénieurs peuvent prendre certaines mesures pour remédier aux inefficacités au fur et à mesure qu’elles surviennent. Il s’agit notamment d’identifier les inefficacités en temps réel et de les corréler à des changements réalisables. Il encourage les dirigeants à créer des opportunités d’apprentissage à partir de ces événements et à diffuser les connaissances en interne.</p><h3>Adaptez la technologie au cas d'utilisation concerné</h3><p>Enfin, et c'est peut-être le plus important, utilisez-vous la bonne technologie pour le cas d'utilisation concerné ? Il existe aujourd'hui de nombreux types de bases de données parmi lesquels choisir, des bases de données relationnelles comme MySQL ou PostgreSQL aux bases de données non relationnelles comme NoSQL ou MongoDB. Associer la bonne technologie de données au cas d'utilisation concerné constituera une étape importante vers l'amélioration de l'efficacité.</p><p>En outre, Syed a recommandé d’adapter la taille des calculs à la charge de travail. Par exemple, pour son équipe chez Capital One, Syed a expliqué que la politique consiste à ne pas avoir de calculs de grande taille dans des environnements de niveau inférieur, car il ne devrait pas être nécessaire de traiter beaucoup de données à cet endroit.</p><p>Les stratégies ci-dessus peuvent être adoptées pour éviter le gaspillage de ressources. De plus, ces tactiques d’optimisation des processus de données pourraient également aider les FinOps, qui cherchent à optimiser les opérations pour réduire les dépenses informatiques. Mais il faut souvent revoir l’efficacité des données : il faut être vigilant lorsque de nouvelles charges de travail sont introduites, prévient Syed.</p><p>Source de l'image : joshua-sortino-LqKhnDzSF-8-unsplash.jpg</p>"
Mesurer les progrès du projet OpenTelemetry,"<p>Le plus grand compliment que l’on puisse faire à une norme technologique est de dire qu’elle est « ennuyeuse ».</p><p>Les normes de la couche transport comme TCP et UDP sont ennuyeuses. Ce sont les protocoles sous-jacents qui alimentent à peu près tout ce que nous faisons sur Internet. REST est ennuyeux. Il gère des milliards de transactions API à travers les microservices distribués du monde entier.</p><p>Les normes d’infrastructure deviennent ennuyeuses lorsqu’elles deviennent des infrastructures courantes qui « fonctionnent tout simplement ». Elles offrent aux créateurs et aux mainteneurs de logiciels open source des fondations communes sur lesquelles innover. Elles donnent aux utilisateurs l’assurance que leurs choix technologiques ne mènent pas à des impasses. Regardez toute évolution majeure de l’informatique, elle est toujours soutenue par des normes ennuyeuses.</p><p>Lorsque OpenTelemetry a fait ses débuts à KubeCon en 2019, les données de télémétrie pour les applications dans les systèmes distribués étaient loin d’être ennuyeuses. Deux projets concurrents, OpenTracing et OpenCensus, tentaient de regrouper les meilleures pratiques autour de la discipline encore naissante du traçage distribué. Ils étaient « très similaires dans le sens où ils visaient à unifier l’instrumentation des applications et à faire de l’observabilité une fonctionnalité intégrée dans chaque application moderne » (Merging OpenTracing and OpenCensus: Goals and Non-Goals). Mais le fait d’avoir deux communautés fragmentées autour de ces deux projets a rendu difficile l’avancement du traçage distribué. C’était donc un événement majeur lorsque les deux projets ont fusionné et que OpenTelemetry est né.</p><p>Voyons comment la norme OpenTelemetry rend tout cela possible.</p><h3>Conventions sémantiques standard et un « modèle mental » unique</h3><p>Les conventions sémantiques sont importantes pour normaliser la manière dont nous décrivons les opérations courantes afin de disposer d'une télémétrie cohérente pour des actions similaires. Les développeurs qui instrumentent des applications doivent pouvoir se référer à des définitions sémantiques claires pour comprendre comment instrumenter tout, des pilotes de base de données aux instructions SQL, et comment utiliser ces données de télémétrie pour l'analyse.</p><p>Par exemple, les conventions sémantiques d’OpenTelemetry définissent l’emplacement dans les métadonnées où le nom du processus doit être stocké, mais pas son nom. De même, si je déploie mon application sur Kubernetes chez un fournisseur de cloud quelque part, je m’attends à ce que mes données de télémétrie contiennent le pod et l’espace de noms Kubernetes d’où elles proviennent, ainsi que le cluster et la région.</p><p>OpenTelemetry standardise également les métadonnées pour les étiquettes qui couvrent plusieurs langues. Ainsi, si j’ai un service et une requête HTTP, peu importe qu’ils soient en Java, en Python ou dans tout autre langage : avec OpenTelemetry, je peux simplement commencer à interroger les données sans avoir à me soucier du langage sous-jacent.</p><p>Il apporte un modèle mental cohérent sur la manière dont les différents types de données de télémétrie (traces, métriques, journaux) sont liés : OpenTelemetry leur donne la même base, donc comprendre comment une période est modélisée fournit des connaissances qui peuvent être réutilisées lors de la compréhension des métriques et des journaux. Il peut être utilisé par n'importe quel microservice et n'importe quel service ou appareil connecté à HTTP ou gRPC. Pour n'importe quel appareil réseau, OpenTelemetry fournit aux développeurs des conventions sémantiques qui apportent de la cohérence aux rapports de données de télémétrie sur tout type d'événement. Ainsi, tout ingénieur travaillant avec OpenTelemetry peut comparer les latences et les requêtes entre les services, même s'ils sont écrits dans des langages différents, grâce aux conventions sémantiques.</p><h3>Protéger les utilisateurs contre la dépendance vis-à-vis d'un fournisseur</h3><p>Il y a dix ans, les fournisseurs de surveillance et de journalisation fournissaient des agents à leurs clients. À l’époque, ils n’appelaient pas cela observabilité. Mais ils disaient au marché que si vous voulez les meilleures données de télémétrie, vous devez utiliser notre agent. Chaque fournisseur avait son propre agent.</p><p>L’ajout de ces agents était risqué et prenait beaucoup de temps. Une fois qu’un fournisseur avait intégré ses agents dans l’environnement d’un client, il était difficile de les modifier et il fallait soit payer le fournisseur, soit payer votre équipe d’ingénieurs pour supprimer cette instrumentation au profit d’une autre.</p><p>OpenTelemetry a véritablement banalisé la définition de la télémétrie de telle manière que nous n’avons plus besoin d’utiliser des agents de fournisseurs. Cela signifie que les utilisateurs peuvent décider quel fournisseur utiliser pour les outils d’observabilité et ont la possibilité de passer à différents fournisseurs par la suite. Avec OpenTelemetry, ils n’ont plus besoin d’instrumenter les applications, de redémarrer les applications et de se lancer dans des cycles d’opérations risqués pour modifier la destination de leurs données de télémétrie.</p><h3>Économies de communauté – Échelle des moteurs et des intégrations</h3><p>OpenTelemetry a réuni un ensemble très large et diversifié de contributeurs en un laps de temps relativement court. Il est devenu le deuxième projet CNCF le plus populaire, derrière Kubernetes.</p><p>Le projet bénéficie de contributions tant du côté de l'instrumentation que de celui de la collecte. Les spécifications constituent un autre domaine important dans lequel le projet évolue, notamment la spécification API/SDK, le modèle de données et le protocole OTel (OTLP). Le fait qu'OpenTelemetry bénéficie d'une large participation de l'écosystème des fournisseurs de journalisation, de surveillance et d'observabilité, qui comprend bon nombre de ses plus grands contributeurs, accélère tout cela.</p><p>Il nous reste encore du chemin à parcourir. Mais au cours de l’année dernière, il est devenu évident qu’OpenTelemetry est si communément considéré comme la norme de traçage des données que les créateurs et mainteneurs de langage, de bibliothèque et de base de données s’équipent pour OpenTelemetry. C’est le signe ultime de maturité pour tout projet ou norme, lorsque sa popularité a suscité l’intérêt des mainteneurs qui conçoivent autour de la spécification et l’utilisent de manière native.</p><h3>Quelle est la prochaine étape pour OpenTelemetry ?</h3><p>À l’avenir, je pense que nous allons assister à une conséquence inattendue : OpenTelemetry fera passer l’industrie des données opérationnelles à davantage d’intelligence des données. OpenTelemetry facilite grandement l’instrumentation de la façon dont les utilisateurs interagissent avec les systèmes et permet aux développeurs d’ajouter des mesures commerciales et de les comparer au comportement réel des utilisateurs.</p><p>OpenTelemetry a également un grand potentiel de mariage avec eBPF et des projets connexes comme Cilium. Ces abstractions qui rendent le noyau Linux « programmable » ajoutent un pool de données de télémétrie beaucoup plus fines qui peuvent être extraites dans les journaux des applications pendant l’exécution.</p><p>Tout cela alimente le concept de « profils » qu’OpenTelemetry met en œuvre. Alors que le traçage distribué met l’accent sur la coupe horizontale de la façon dont une transaction spécifique s’exécute ou sur tous les services liés par une seule requête, un profil est une coupe verticale différente qui met l’accent sur l’état des ressources comme la mémoire, le processeur et plus encore à un moment donné. Ces profils sont des collections qui sont stockées en continu et une autre grande tranche de données de télémétrie qui sera encapsulée par la norme OpenTelemetry.</p>"
Comment résoudre vos problèmes d'observabilité,"<p>L’observabilité devient un élément clé des pratiques DevOps contemporaines. Même les services qui ne faisaient pas traditionnellement partie de DevOps voient les avantages d’être placés sous les auspices des équipes d’observabilité. En 2023, cependant, les organisations constatent que le chemin vers l’adoption est plus cahoteux que prévu. Voici sept des plus grands défis auxquels les équipes DevOps sont confrontées en matière d’observabilité et quelques suggestions pour les atténuer.</p><h3>Augmentation du MTTR</h3><p>Le MTTR (ou temps moyen de récupération) est le temps nécessaire pour remettre un système en marche après une panne ou un bug. Un MTTR plus long signifie davantage de temps d’arrêt et un service de moins bonne qualité pour les clients. Fait inquiétant, le rapport DevOps Pulse indique que le MTTR moyen est en augmentation. Cette année, 73 % des personnes interrogées ont déclaré un MTTR de plusieurs heures, contre seulement 64 % l’année dernière.</p><p>Le MTTR résulte souvent d'une incapacité à diagnostiquer les incidents en raison de silos de données qui nuisent à l'observabilité. Cela peut être amélioré par une plateforme d'observabilité qui permet aux ingénieurs d'avoir une vue d'ensemble.</p><h3>Coûts des données de télémétrie</h3><p>Parallèlement à l’augmentation du MTTR, de nombreuses organisations doivent faire face aux coûts engendrés par les volumes élevés de données de télémétrie. C’est un problème de taille. Une enquête IDC menée auprès de 200 entreprises a révélé que 53 % des personnes interrogées ont souligné les coûts liés au stockage des données de journal.</p><p>Le problème est en grande partie dû à un modèle de tarification à plusieurs niveaux, obsolète. De nombreux fournisseurs facturent par Go de données, donc si vos volumes de données fluctuent, vos coûts de données fluctuent également. Ils ont également des modèles de tarification peu clairs, ce qui signifie que de nombreuses organisations ont du mal à savoir ce qu'elles paient. Chez Coralogix, nous avons créé un nouveau modèle commercial qui coûte un tiers du coût des solutions de stockage de journaux standard.</p><h3>Propagation des outils</h3><p>Pour comprendre pourquoi cela constitue un défi, nous devons répondre à la question suivante : qu'est-ce que l'observabilité ? Une observabilité efficace nécessite l'intégration des données de tous les aspects de votre application. Étant donné que de nombreuses organisations mettent en œuvre la surveillance avec plusieurs outils, elles souffrent d'une prolifération d'outils. Cela a pour effet de cloisonner les données de télémétrie, ce qui rend plus difficile la corrélation des données et l'extraction d'informations sur les performances du système.</p><p>Il existe de nombreuses options pour limiter la prolifération des outils, comme l’évaluation approfondie des coûts et des avantages d’un outil avant de l’inclure dans votre stratégie DevOps. La solution la plus efficace est un outil « à écran unique » qui combine les informations de manière synoptique sur un seul tableau de bord.</p><h3>Complexité de Kubernetes</h3><p>Elastic rapporte que les entreprises se tournent de plus en plus vers des solutions basées sur le cloud telles que Kubernetes pour leur DevOps. Kubernetes peut dynamiser les entreprises grâce à sa capacité à faire évoluer dynamiquement l'infrastructure en fonction des besoins, éliminant ainsi les frais généraux liés aux serveurs dédiés.</p><p>Cependant, Kubernetes est complexe et comporte son lot de défis. L’architecture évolutive de Kubernetes est issue de la conteneurisation, un paradigme dans lequel les applications sont hébergées dans des objets appelés conteneurs. Cela signifie que le développement dans Kubernetes nécessite la capacité de travailler avec de nombreuses plaques tournantes.</p><p>Une bonne façon de lutter contre ce phénomène est d’améliorer la formation au sein des organisations. De plus, la suppression des silos permet aux différentes équipes de transférer leurs connaissances.</p><h3>Défis de sécurité</h3><p>La popularité de Kubernetes pose des problèmes de sécurité. Il peut s'agir d'une escalade de privilèges, lorsqu'un utilisateur parvient à obtenir des privilèges tels que l'accès en écriture, et de mauvaises configurations de sécurité, lorsque les développeurs oublient de modifier les configurations par défaut non sécurisées.</p><p>Il existe plusieurs stratégies pour atténuer les risques de sécurité de Kubernetes. Il s'agit notamment de définir des rôles dans des espaces de noms particuliers, d'utiliser des maillages de services et d'améliorer la sécurité avec Kubernetes Operator de Coralogix.</p><p>Au-delà de Kubernetes, il y a le problème plus vaste de l’intégration de la sécurité dans une stratégie d’observabilité, qui devient un défi pour un nombre croissant d’entreprises informatiques. Pour y remédier, de plus en plus d’entreprises commencent à intégrer l’observabilité et la surveillance de la sécurité sous un même toit. Des solutions telles que les mesures d’infrastructure et d’application peuvent améliorer la sécurité et la surveillance.</p><h3>Plateformes de mise à l'échelle</h3><p>Pour relever les défis posés par l’augmentation des coûts des données et la complexité croissante du cloud, les entreprises se tournent vers des solutions open source. Mais ces solutions présentent également leurs propres défis. Selon le rapport DevOps Pulse, environ 30 % des entreprises interrogées ont rencontré des problèmes de gestion de l’infrastructure, de mise à l’échelle et de mise à niveau des composants concernés. Étant donné que de nombreuses plateformes open source nécessitent des connaissances spécialisées pour leur maintenance, les entreprises ont du mal à trouver les compétences et l’expertise nécessaires.</p><p>Des outils tels qu'OpenTelemetry peuvent faciliter la mise à l'échelle en s'intégrant à des plateformes telles que Coralogix.</p><h3>Dépannage des performances du pipeline de données</h3><p>La mise en œuvre de l'observabilité nécessite de disposer d'un pipeline fiable et performant pour les données de télémétrie. Cependant, les organisations qui utilisent des plateformes open source ont souvent du mal à surveiller et à dépanner les performances de leur pipeline de données. Cela peut nuire à l'observabilité car les données de télémétrie sont de moindre qualité.</p><p>L'ingénieur de données Abraham Alcantara propose dix étapes clés pour résoudre avec succès les problèmes des pipelines de données. Il s'agit notamment d'identifier le logiciel et l'infrastructure du pipeline de données, de reproduire et d'isoler les problèmes et d'automatiser les scénarios de problèmes. Une autre stratégie consiste à appliquer l'apprentissage automatique, comme celui utilisé par Coralogix.</p>"
Maximiser les informations sur les données : comment OLAP renforce les piles de données modernes,"<p>À l’ère du big data et du cloud computing, le traitement analytique en ligne (OLAP) est tombé en désuétude. À mesure que les entrepôts de données modernes ont évolué pour prendre en charge des volumes de données plus importants et revendiquer des capacités de traitement plus rapides, l’OLAP est tombé en désuétude, car on a commencé à penser qu’il aurait du mal à gérer l’explosion combinatoire. L’accent a été mis sur les moteurs d’interrogation d’exécution. Au lieu d’utiliser des cubes OLAP prédéfinis ou pré-agrégés, de nombreux outils de business intelligence (BI) et d’analyse ont utilisé leurs propres applications de modélisation de données qui fonctionnaient directement avec les données brutes.</p><p>Cependant, ces derniers temps, on a réalisé que l'OLAP, lorsqu'il est mis en œuvre de manière intelligente, peut être très bénéfique, même dans le contexte d'une échelle de données massive et du cloud computing. L'OLAP moderne exploite des technologies et des optimisations avancées qui surmontent toutes les limitations antérieures. En plus d'être rentable, en particulier pour les implémentations dans le cloud, il offre de réels avantages en termes de performances.</p><p>Les systèmes OLAP modernes sont conçus pour gérer efficacement de grands ensembles de données. L'un des principaux atouts d'OLAP réside dans sa capacité à fournir des informations en temps réel ou quasi réel, permettant aux entreprises de prendre des décisions basées sur les données avec une plus grande agilité. Le modèle de données multidimensionnel des cubes OLAP facilite la réponse instantanée aux requêtes, permettant aux utilisateurs d'extraire des informations précieuses à partir d'ensembles de données complexes sans compromettre les performances des requêtes.</p><h3>Considérez ces avantages de l’utilisation d’OLAP dans les piles de données modernes</h3><p>Performances : Alors que les volumes de données continuent de croître, OLAP reste une technologie puissante pour extraire des informations précieuses et exploiter le véritable potentiel des piles de données et des environnements cloud modernes. Le modèle de données multidimensionnel d'OLAP avec cubes de pré-agrégation permet un traitement et une analyse efficaces des données, offrant des informations plus rapides et des réponses aux requêtes en moins d'une seconde. Les algorithmes modernes améliorent encore les performances, permettant aux entreprises de gérer des charges de travail de données à grande échelle sans se soucier des problèmes d'explosion des données.</p><p>Fonctionnalités OLAP uniques : certains domaines, comme la finance, ont un besoin inhérent de préserver des fonctionnalités OLAP cruciales, même avec les exigences croissantes en matière de stockage de données et de calcul, pour lesquelles ils migrent vers des piles de données modernes et des environnements cloud. Le passage au cloud permet l'évolutivité des données, ce qui permet de relever les défis liés à la gestion de vastes volumes de données. Cependant, ils ne peuvent pas se permettre de faire des compromis sur les fonctionnalités OLAP essentielles telles que les hiérarchies avancées, comme les relations parent-enfant, et les hiérarchies alternatives avec des cumuls pondérés.</p><p>Ces fonctionnalités sont indispensables, en particulier dans le domaine de l’analyse financière, quelle que soit la modernité et l’avancée de la nouvelle pile de données. En exploitant OLAP dans le cloud, les entreprises peuvent continuer à analyser et à explorer efficacement les données avec des hiérarchies complexes, garantissant ainsi des informations financières précises et approfondies sans sacrifier la flexibilité et la puissance d’analyse qu’offre OLAP.</p><p>Architecture cloud et rentabilité : OLAP dans l’architecture cloud représente une solution puissante et avant-gardiste qui combine les avantages de la technologie OLAP avec l’évolutivité et la rentabilité du cloud computing. En exploitant les ressources du cloud, OLAP peut traiter et analyser efficacement de grands volumes de données, répondant ainsi aux exigences croissantes des entreprises modernes.</p><p>Une fois les cubes OLAP créés et renseignés, ils fournissent une représentation hautement efficace et agrégée des données. Ces structures pré-agrégées accélèrent les performances des requêtes, permettant aux équipes d'accéder aux informations en temps réel ou quasi réel. Les équipes BI peuvent intégrer de manière transparente les outils existants au modèle OLAP, favorisant ainsi un environnement d'analyse cohérent et familier. Il permet aux décideurs d'explorer les données de manière interactive, d'approfondir les détails et d'obtenir des informations précieuses.</p><p>L’un des principaux avantages de l’OLAP dans le cloud est sa rentabilité. La technologie OLAP, qui repose principalement sur la pré-agrégation, réduit considérablement le coût de calcul des requêtes d’exécution, car les réponses aux requêtes métier sont déjà précalculées et stockées dans une structure agrégée. Au moment de l’interrogation, les outils OLAP doivent rechercher les bonnes réponses dans la structure agrégée (et non dans le stockage de données dans le cloud) et renvoyer la réponse à la requête, ce qui entraîne un coût de calcul bien inférieur au coût de calcul de l’agrégation d’exécution. Cela permet véritablement de créer une seule fois, d’interroger plusieurs fois. En outre, les entreprises peuvent tirer parti de l’élasticité du cloud pour augmenter ou réduire les ressources de calcul et de stockage en fonction de la fluctuation de leurs besoins en données. Cette adaptabilité permet aux entreprises de gérer les pics de charge de travail sans surprovisionner et de réduire les dépenses inutiles.</p><p>Couche sémantique universelle : une approche USL garantit des définitions et des analyses de données cohérentes dans toute l'organisation, réduisant ainsi les écarts de données et améliorant la gouvernance des données. Une compréhension commune des données permet une meilleure communication entre les équipes et se traduit par une mise en œuvre fluide d'un processus décisionnel basé sur les données.</p><p>En même temps, l'utilisation d'un USL signifie que plusieurs outils de Business Intelligence et plateformes de reporting peuvent se connecter au même modèle sémantique et l'exploiter. Ainsi, différentes parties de l'organisation peuvent continuer à travailler avec leurs interfaces BI habituelles tout en accédant aux données sous-jacentes communes.</p><p>OLAP se prête naturellement à la fourniture d'une couche sémantique universelle puissante. Le concept de dimensions, de mesures, de hiérarchies et de calculs constitue la base d'un modèle sémantique OLAP. Les modèles sémantiques OLAP prennent également en charge les calculs complexes, les expressions personnalisées ou les formules requises pour une logique métier complexe. Ces composants offrent un moyen puissant de structurer et d'organiser les données, les rendant plus accessibles et intuitives pour les utilisateurs professionnels qui ont besoin d'en tirer des informations.</p><p>Si l'on considère les données de vente typiques, les catégories de produits, les régions géographiques et les périodes de temps peuvent être dimensionnées. Les mesures, en revanche, seraient le chiffre d'affaires, les marges bénéficiaires, les quantités vendues ou tout autre indicateur clé de performance. Les hiérarchies permettent d'analyser les données à différents niveaux de granularité. Les utilisateurs peuvent effectuer une synthèse ascendante ou descendante pour afficher les données à différents intervalles de temps tels que l'année, le trimestre, le mois, le jour, etc.</p><p>Les modèles USL et OLAP sont conçus pour protéger les utilisateurs professionnels de la modélisation et des schémas de données complexes sous-jacents. Ils peuvent travailler avec des filtres, des dimensions et des mesures qui sont des représentations des données axées sur l'entreprise, utilisant des termes commerciaux familiers tout en faisant abstraction des subtilités techniques. Ils peuvent visualiser et explorer les données via des hiérarchies, des filtres et des mesures intuitifs alignés sur leurs processus métier et leurs KPI.</p><h3>Conclusion</h3><p>L’OLAP dans l’architecture cloud représente une solution d’avenir, rapide et rentable. Elle tire parti de l’évolutivité du cloud et du modèle de paiement à l’utilisation pour traiter efficacement de vastes volumes de données, tandis que ses structures pré-agrégées et ses fonctionnalités en temps réel permettent aux équipes de business intelligence d’obtenir des informations précieuses de manière transparente. Grâce à son agilité, l’OLAP dans le cloud est bien placé pour relever les défis du futur, en offrant aux entreprises un avantage concurrentiel grâce à une prise de décision basée sur les données.</p>"
La fiabilité des données doit être déplacée vers la gauche,"<p>Les pipelines de données alimentent et transforment les données destinées à la consommation et deviennent de plus en plus complexes. À mesure que les entreprises continuent d’ajouter des sources de données, elles courent le risque de rompre leurs pipelines de données en raison d’erreurs de données, d’une logique défectueuse ou de ressources insuffisantes pour le traitement. Le défi pour toute équipe de données moderne est d’établir la fiabilité des données le plus tôt possible dans le parcours des données et de créer des pipelines de données optimisés capables de fonctionner et d’évoluer pour répondre aux besoins commerciaux et techniques de l’entreprise. Dans le contexte de l’observabilité des données, le « shift left » fait référence à une approche proactive qui intègre des pratiques d’observabilité dès les premières étapes du cycle de vie des données. Ce concept est emprunté aux méthodologies de développement logiciel et met l’accent sur la résolution des problèmes potentiels et la garantie de la qualité dès le départ.</p><p>Le shifting left implique l'intégration des pratiques et des outils d'observabilité dans le pipeline et l'infrastructure de données dès le début. Cette approche évite de traiter l'observabilité comme une réflexion après coup ou de l'appliquer uniquement à des étapes ultérieures pour identifier et résoudre les problèmes de fiabilité, d'intégrité et de performance des données le plus tôt possible dans le processus de développement, minimisant ainsi la probabilité que des problèmes se propagent en aval.</p><h3>L’impact économique des données de mauvaise qualité</h3><p>Pour gérer efficacement les données et optimiser les pipelines de données, il est essentiel de détecter et de traiter les incidents de données le plus tôt possible dans la chaîne d'approvisionnement.</p><p>La « règle 1 x 10 x 100 » s’applique à de nombreux processus, notamment au développement de logiciels. Elle stipule que le coût de résolution d’un problème augmente de manière exponentielle, en fonction du stade auquel il est détecté. La règle peut être étendue aux pipelines de données et aux chaînes d’approvisionnement, où elle suggère que la détection et la rectification d’un problème dans la zone d’arrivée des données (c’est-à-dire là où les données sources sont alimentées) entraînent un coût de 1 $, tandis que l’identification et la résolution du problème dans la zone de transformation (c’est-à-dire là où les données sont transformées dans leur format final) augmentent le coût à 10 $. Si le problème n’est détecté que dans la zone de consommation (où les données sont dans leur format final et sont consultées par les utilisateurs), le coût augmente considérablement pour atteindre 100 $.</p><h3>Décalage vers la gauche Fiabilité des données</h3><p>Le processus d'identification des problèmes de pipeline de données est devenu plus difficile pour les équipes de données à mesure que les chaînes d'approvisionnement de données continuent d'évoluer et de gagner en complexité, principalement en raison des facteurs suivants :</p><p>● Extension des sources : le nombre de sources de données (telles que Databricks, Snowflake, Redshift, Teradata et bien d'autres) intégrées à la chaîne logistique a considérablement augmenté. Les organisations intègrent désormais des données provenant d'un plus large éventail de sources internes et externes, ce qui contribue à la complexité de la gestion et du traitement des données. ● Logique de transformation des données avancée : la logique et les algorithmes utilisés pour transformer les données au sein de la chaîne logistique sont devenus plus sophistiqués. Des transformations et des calculs complexes sont appliqués aux données brutes pour obtenir des informations utiles, ce qui nécessite des systèmes et des processus robustes pour gérer les subtilités impliquées. ● Traitement gourmand en ressources : les exigences de traitement des données au sein de la chaîne logistique ont considérablement augmenté. Avec des volumes de données plus importants et des opérations plus complexes, les organisations doivent allouer des ressources informatiques importantes, telles que des serveurs, des capacités de stockage et de traitement, pour gérer efficacement la charge de travail de traitement des données.</p><p>En abordant les incidents de données de manière proactive à un stade précoce, les organisations minimisent l’impact potentiel et les coûts associés aux problèmes de données. Cela garantit non seulement la fiabilité et l’exactitude des données consommées par les utilisateurs, mais préserve également l’intégrité des processus et des prises de décision en aval. En fin de compte, l’approche de la fiabilité des données par glissement vers la gauche favorise l’efficacité, réduit les coûts et améliore la qualité et la fiabilité globales des données.</p><h3>Comment les équipes de données peuvent-elles se déplacer vers la gauche ?</h3><p>Pour effectuer un déplacement efficace vers la gauche dans votre solution de fiabilité des données, certaines fonctionnalités sont cruciales ; celles-ci incluent :</p><p>● Vérifications précoces de la fiabilité des données : effectuez des tests de fiabilité des données avant que les données n'entrent dans l'entrepôt de données et le data lakehouse. Cela garantit que les données erronées sont identifiées et filtrées tôt dans les pipelines de données, empêchant leur propagation vers les zones de transformation et de consommation.● Prise en charge des plateformes de données en mouvement : activez la prise en charge des plateformes de données telles que Kafka et surveillez les pipelines de données dans les tâches Spark ou les orchestrations Airflow, ce qui permet une surveillance et une mesure en temps réel des pipelines de données.● Prise en charge des fichiers : effectuez des vérifications sur différents types de fichiers et capturez les événements de fichiers pour déterminer quand des vérifications incrémentielles doivent être effectuées.● Disjoncteurs : intégrez des API qui intègrent les résultats des tests de fiabilité des données dans vos pipelines de données, ce qui peut interrompre le flux de données lorsque des données erronées sont détectées. En empêchant la propagation de données erronées, les disjoncteurs protègent les processus en aval contre toute atteinte.● Isolation des données : identifiez et isolez les lignes de données erronées, empêchant ainsi leur traitement continu.● Rapprochement des données : implémentez des fonctionnalités de rapprochement des données pour garantir la cohérence et la synchronisation des données sur plusieurs emplacements.</p><p>En déplaçant vers la gauche vos pratiques de fiabilité des données, vous permettez aux équipes de données de se concentrer sur l'innovation, de réduire les coûts, d'améliorer la confiance dans les données et de favoriser des processus métier agiles. L'adoption de cette approche rend les données plus exploitables et résilientes et offre un écosystème de données plus robuste et plus fiable qui favorise la prise de décision éclairée et favorise la réussite organisationnelle.</p>"
Stratégies de bases de données multi-locataires pour charges de travail mixtes et SLA en temps réel,"<p>Le multi-hébergement, qui consiste à utiliser une ou plusieurs instances d’une ou plusieurs applications dans un environnement d’infrastructure partagé unique, n’est pas un concept nouveau. Utilisé à l’origine dans le matériel mainframe, le partage de temps permettait de découper une seule et même machine informatique massive et de l’utiliser par plusieurs applications sur une seule instance matérielle. Cela simplifiait à la fois le paysage matériel ainsi que les compétences et les ressources nécessaires à la gestion des environnements. Avec l’essor du logiciel en tant que service (SaaS) dans les environnements de cloud privé et public modernes d’aujourd’hui, le multi-hébergement est désormais une fonction courante et essentielle. Il permet aux entreprises de cloud d’évoluer et aux clients d’économiser de l’argent en mutualisant essentiellement les ressources. Comme de plus en plus d’applications exigent de plus en plus de données, le multi-hébergement est désormais une fonction essentielle au niveau de la couche de base de données.</p><p>Mais lorsque de nombreuses applications différentes s'exécutent sur la même infrastructure et accèdent à la même base de données, comment éviter qu'une application n'impacte les performances des données d'une autre ? Et comment déterminer, à partir de l'infrastructure de base de données sous-jacente, quels locataires obtiennent quelle infrastructure et à quel niveau de performances ?</p><p>Voici un aperçu des forces motrices derrière les besoins actuels en données en temps réel et de leur impact sur la gestion de l’architecture de données multi-locataire.</p><h3>Les accords de niveau de service sont au cœur de la gestion des données multi-locataires</h3><p>Tout le monde souhaite des performances plus rapides et des systèmes plus résilients. Nous sommes impatients, c'est pourquoi des performances fiables et en temps réel offrent des avantages commerciaux importants et compétitifs. Cependant, tous les locataires n'ont pas besoin du même niveau de performances. Les besoins réels en termes de performances peuvent être suffisamment rapides pour un utilisateur ou une machine sans nécessiter le chemin d'exécution le plus rapide possible avec les ressources disponibles.</p><p>Les accords de niveau de service (SLA) régissent en fin de compte les ressources fournies à un locataire. Le temps réel peut devoir être mesuré en moins de millisecondes dans un cas (par exemple, pour une transaction boursière) mais en moins de secondes dans d'autres cas (par exemple, dans un transfert de fonds Zelle). C'est là que les développeurs, les architectes et les opérateurs s'associent à une entreprise pour comprendre les besoins réels de ses utilisateurs ou de son système et pour créer un système capable de fournir le SLA en utilisant le minimum de ressources possible.</p><h3>Charges de travail mixtes et problème du « voisin bruyant »</h3><p>Dès que les charges de travail sont partagées, il existe un risque qu'une application s'empare rapidement des ressources et les utilise d'une manière qui affecte les autres applications. Souvent appelées « voisines bruyantes », les applications impactées peuvent considérablement interférer avec les SLA de temps de réponse en temps réel lorsque les applications multi-locataires partagent une base d'infrastructure. Par exemple, un nœud d'un cluster de base de données devient fortement chargé et les performances des autres nœuds commencent à être affectées, ce qui entraîne le non-respect des SLA.</p><p>En règle générale, plusieurs machines virtuelles (VM) sont hébergées sur une seule machine physique ou un seul nœud, mais partagent les ressources système (c’est-à-dire le processeur, le réseau, le stockage, etc.) avec les autres VM. Il est possible que les applications surutilisent ces ressources partagées, ce qui a un impact sur d’autres applications et parfois même sur la base de données elle-même. Une telle surutilisation de la couche d’infrastructure (dans ce cas, le partage des ressources de calcul, du stockage, des files d’attente réseau, etc.) entraîne une baisse des performances de la base de données et du système de bout en bout. C’est comme brancher un appareil de 100 ampères sur un circuit de 50 ampères. Tous les disjoncteurs se déclenchent et l’ensemble du système s’arrête brutalement !</p><h3>Stratégies pour maximiser les performances de la base de données multi-locataire</h3><p>La configuration et la gestion de la base de données elle-même (et pas seulement de l'infrastructure) dans une optique de multi-location peuvent optimiser les performances et éviter des problèmes tels que des voisins bruyants. L'isolement des bases de données dans votre architecture multi-locataire peut contribuer à minimiser les problèmes non liés à la base de données qui peuvent être à l'origine de mauvaises performances et de non-respect des accords de niveau de service.</p><p>Commencez par sélectionner des bases de données qui permettent aux applications d'isoler leur accès aux données en fonction d'ensembles (ou de tables dans le jargon relationnel) auxquels peuvent être associées des limites de stockage. En combinant ces limites avec des limites de débit pour les transactions par utilisateur, les applications peuvent se voir garantir leur quota de ressources d'exécution. En utilisant les configurations de quota par application pour des éléments tels que le stockage et les transactions par seconde (TPS), la base de données peut évaluer la capacité totale disponible dans le système et s'assurer que suffisamment de ressources sont réservées et disponibles pour garantir que les SLA de toutes les applications sont respectés à tout moment.</p><p>Notez que les événements inattendus qui provoquent un rééquilibrage des données au sein de la base de données distribuée, tels que les pannes de nœuds et de réseau, nécessitent des ressources supplémentaires. Le système de base de données soustrait cette capacité de ressources de ce qui est disponible pour les applications afin que le système ne soit pas surchargé en cas de panne.</p><p>Les entreprises ont également besoin d’une très bonne automatisation et d’une très bonne observabilité pour résoudre rapidement les problèmes immédiats et avoir la visibilité nécessaire sur les tendances à long terme qui ont un impact sur les performances et le respect des SLA. Sinon, au moment où un humain remarque un problème, le SLA peut déjà avoir été manqué et l’entreprise en subir les conséquences.</p><p>Une bonne observabilité peut vous aider à comprendre comment les données sont utilisées et à créer ces automatisations. À mesure que de plus en plus d'applications accèdent au même ensemble de données et l'utilisent de différentes manières, vous avez besoin d'outils d'observabilité modernes intégrés (ou interopérables) à la base de données sous-jacente.</p><p>Avec une bonne compréhension de la manière dont vos applications riches en données interagissent avec d’autres locataires dans des environnements à charge mixte, vous pouvez commencer à déployer ces stratégies pour vous assurer de respecter tous les SLA requis et de faire en sorte que toutes ces données fonctionnent efficacement pour l’entreprise.</p>"
Comment créer une plateforme de données pour des analyses ad hoc en libre-service,"<p>En fait, il existe de nombreuses situations dans lesquelles les ingénieurs DevOps, les équipes SRE et d’autres services axés sur l’observabilité doivent explorer des volumes de données rapidement et de manière flexible. Un SRE essayant de déterminer la cause profonde de la latence peut avoir besoin d’analyser le mouvement des données entre les points de terminaison pour trouver celui qui fonctionne mal. Un autre ingénieur peut avoir besoin de comparer les performances historiques et actuelles pour trouver un comportement anormal. Enfin, un ingénieur DevOps peut avoir à analyser les mesures de performance des utilisateurs pour comprendre l’ampleur d’un ralentissement, qu’il soit mondial ou régional.</p><p>Chaque cas d'utilisation nécessite des tableaux de bord capables de fournir des données détaillées via une grande variété de visualisations, notamment des cartes choroplèthes, des zones de pile, des graphiques à secteurs et des graphiques à barres. Idéalement, un tableau de bord permettra aux équipes d'isoler les dimensions, d'appliquer des filtres et d'explorer les données pour obtenir des informations plus approfondies.</p><p>Malheureusement, tous les tableaux de bord ne peuvent pas prendre en charge ce type d’analyse rapide et flexible. Beaucoup utilisent encore des technologies obsolètes, qui manquent de la flexibilité, de l’agilité et de l’évolutivité nécessaires pour explorer les données de manière transparente. Beaucoup ont également été conçus sans l’urgence que requièrent les données d’aujourd’hui. Après tout, au cours des années précédentes, la plupart des utilisations des tableaux de bord (comme les rapports internes) n’étaient pas sensibles au temps.</p><p>D'autres tableaux de bord sont limités par des modèles, qui offrent un éventail limité de widgets, d'outils et de fonctionnalités d'exploration. Ces tableaux de bord peuvent sembler peu maniables et lents, inadaptés à une situation en évolution rapide comme une panne d'application.</p><h3>Exigences</h3><p>Pour véritablement tenir la promesse d’une analyse ad hoc en libre-service sur d’énormes ensembles de données, les équipes doivent travailler en temps réel et ont donc besoin d’une base de données capable de répondre rapidement.</p><p>Lorsqu’une application tombe en panne, un SRE peut ne pas savoir quoi rechercher et doit donc parcourir rapidement de nombreuses données. Alors que certains tableaux de bord s’appuient sur des solutions de contournement pour des requêtes plus rapides, telles que les pré-agrégations, le précalcul ou les cumuls, cela n’est pas possible dans ce cas, car le SRE ne saura tout simplement pas quoi rechercher. Après tout, il ne peut pas nécessairement prédire ce qui va se passer, et même s’il travaille sur des hypothèses issues de pannes précédentes, le problème actuel pourrait être très différent.</p><p>Les tableaux de bord doivent donc proposer de nombreuses fonctionnalités et visualisations. Les utilisateurs doivent pouvoir filtrer les données par heure, isoler des variables comme la localisation et zoomer sur des intervalles de temps spécifiques en quelques clics. Les tableaux de bord doivent également prendre en charge divers types de données, notamment des relations parent-enfant complexes et des colonnes imbriquées.</p><p>De plus, les tableaux de bord doivent offrir une vision approfondie. Une plateforme de diffusion multimédia en ligne peut avoir besoin d'évaluer les indicateurs utilisateur (tels que la latence ou les temps de chargement) sur différents appareils, systèmes d'exploitation et régions pour affiner les performances. Un fournisseur de cloud doit surveiller son matériel physique pour détecter les températures élevées, les commutateurs ou périphériques réseau lents et d'autres anomalies.</p><p>Les données étant désormais si importantes pour la réussite, de plus en plus de personnes au sein d’une entreprise, notamment les data scientists, les chefs de produit et les utilisateurs externes, ont besoin d’informations basées sur les données. Dans ces situations, les tableaux de bord doivent gérer l’augmentation du trafic utilisateur et de l’activité de requête, en maintenant des réponses rapides même en cas de charge. Cela est particulièrement important si l’on considère qu’une opération mono-utilisateur (comme un zoom) nécessitera plusieurs requêtes sur le backend pour s’exécuter.</p><p>Une base de données doit également évoluer de manière transparente. Si l’environnement d’une organisation génère des millions d’événements par heure, cela équivaut à des milliards d’événements par jour ou par semaine, ce qui constitue un défi pour toute base de données à ingérer, stocker, analyser et interroger. En fait, de nombreuses bases de données ne peuvent pas fournir de temps de réponse rapides tout en gérant de grands ensembles de données et un volume de requêtes élevé. Par exemple, les bases de données transactionnelles (OLTP) peuvent souvent effectuer des requêtes rapidement, mais ne peuvent pas exécuter d’analyses à grande échelle, tandis que les bases de données analytiques (OLAP) peuvent analyser des volumes massifs de données, mais pas à grande vitesse.</p><h3>Apache Druid pour l'exploration indépendante et ad hoc des données</h3><p>C'est là qu'intervient Apache Druid open source. En combinant l'évolutivité et les analyses avancées d'une base de données OLAP avec la vitesse d'une base de données OLTP, Druid offre une exploration rapide et en temps réel des données.</p><p>Une fois les données ingérées, Druid les rend immédiatement disponibles pour l'interrogation et l'analyse, éliminant ainsi la nécessité de regrouper ou d'agréger les données d'une manière ou d'une autre. De plus, Druid s'intègre nativement aux technologies de streaming comme Apache Kafka et Amazon Kinesis, éliminant ainsi le besoin de solutions de contournement ou de connecteurs.</p><p>Druid permet des visualisations interactives, offrant des temps de réponse en millisecondes, permettant une exploration plus polyvalente, élargissant la gamme de dimensions et de filtres disponibles, et même maintenant des vitesses inférieures à la seconde face à l'augmentation des volumes d'utilisateurs et de requêtes.</p><p>L'architecture unique de Druid permet également une mise à l'échelle facile. En séparant les tâches clés entre différents types de nœuds (nœuds de données pour le stockage, nœuds maîtres pour l'ingestion et la disponibilité des données et nœuds de requête pour l'exécution des requêtes et le renvoi des résultats via la méthode de dispersion/collecte), Druid garantit que les nœuds peuvent être mis à l'échelle indépendamment en fonction des besoins. Par la suite, Druid rééquilibre également automatiquement le trafic pour garantir des performances constantes.</p><h3>Salesforce : l'histoire d'un succès druidique</h3><p>Salesforce est un pionnier dans le domaine de la gestion de la relation client (CRM), sert 150 000 clients dans le monde et génère des milliards de revenus annuels.</p><p>L'équipe Edge Intelligence est la division de Salesforce qui s'occupe de la tâche colossale consistant à ingérer, traiter, filtrer, agréger et interroger l'intégralité de leurs données de journal, soit des milliards à des milliards de lignes par jour. Chaque minute, Salesforce ingère 200 millions de mesures, tandis que chaque jour, Salesforce traite cinq milliards d'événements quotidiens à l'échelle mondiale. Au total, Salesforce accumule des dizaines de pétaoctets de données dans son magasin transactionnel, cinq pétaoctets de journaux dans ses centres de données et près de 200 pétaoctets dans son stockage Hadoop.</p><p>Les équipes Salesforce utilisent Druid pour obtenir des informations en temps réel sur les performances des produits et les expériences des utilisateurs, en explorant instantanément de grands ensembles de données. De l'ingénieur au responsable de compte, tout le monde peut interroger une grande variété de dimensions, de filtres et d'agrégations pour mieux comprendre les tendances, résoudre les problèmes qui surviennent et définir des stratégies pour l'avenir.</p><p>En utilisant les capacités de compactage de Druid, Salesforce a également réduit le nombre de lignes Druid de 82 %, ce qui a entraîné une réduction globale de son empreinte de stockage de 47 % et une accélération de ses temps de réponse aux requêtes de 30 %.</p>"
Microsoft tue Python 3.7 ¦ … et VBScript ¦ Exascale ARM sur Jupiter,"<p>Bienvenue dans The Long View, où nous passons en revue l'actualité de la semaine et en tirons l'essentiel. Déterminons ce qui compte vraiment.</p><p>Cette semaine : VS Code abandonne le support de Python 3.7, Windows abandonne VBScript et l'Europe prévoit le supercalculateur ARM le plus rapide.</p><p>Tout d’abord cette semaine : Microsoft abandonne la prise en charge de Python 3.7 dans l’extension Python de Visual Studio Code. Cela continuera probablement à fonctionner pendant un certain temps, cependant (l’accent est mis sur le « probablement »).</p><h3>Analyse : Le langage de script obsolète est obsolète</h3><p>Si vous utilisez toujours la version 3.7, pourquoi ? Il est temps de passer à autre chose : la version 3.12 est la nouvelle tendance. Même la version 3.8 est en sursis.</p><p>Priya Walia : Microsoft fait ses adieux à Python 3.7</p><p>« L’influence croissante du langage Python »Python 3.7, bien qu’ayant atteint sa fin de vie en juin, reste une version très populaire parmi les développeurs. … Microsoft s’attend à ce que l’extension continue de fonctionner de manière non officielle avec Python 3.7 dans un avenir prévisible, mais rien ne garantit que tout fonctionnera correctement sans le soutien d’un support officiel. … Le récent lancement par Microsoft de scripts Python dans Excel souligne l’influence croissante du langage Python dans divers domaines. Cette initiative ouvre de nouvelles perspectives aux développeurs Python pour travailler avec des données dans le célèbre logiciel de tableur. Cependant, tout ne se passe pas sans heurts, car de récentes failles de sécurité dans certains packages Python ont posé des problèmes.</p><p>Python ? N’est-ce pas un langage de jeu ? Ce lâche anonyme dit le contraire :</p><p>Ha, dites ça à Instagram, Spotify, Nextdoor, Disqus, BitBucket, DropBox, Pinterest ou YouTube. Ou au domaine de la science des données, aux mathématiciens ou à la communauté de l'intelligence artificielle.... Notre production actuelle exécute la version 3.10, mais nous avons hâte de la déplacer vers Python 3.11 (la version 3.12 étant un peu trop récente) en raison des augmentations de vitesse allant jusqu'à 60 %. ... Si vous êtes encore quelque part avant la version 3.11, essayez de passer directement à la version 3.11.6.... Les principales améliorations... sont des améliorations de l'interpréteur et du compilateur pour créer un bytecode plus rapide pour l'exécution, parfois de nouvelles fonctionnalités pour écrire du code plus efficacement et des correctifs occasionnels pour supprimer l'ambiguïté. J'exécute Python en production depuis quatre ans maintenant, en migrant de la version 3.8 -> 3.9 -> 3.10 et bientôt vers la version 3.11 et jusqu'à présent, nous n'avons jamais eu à apporter de modifications à notre base de code pour fonctionner avec une nouvelle mise à jour du langage.</p><p>Et Sodul dit que la réputation de Python de ne pas être compatible avec les versions antérieures n'est pas nouvelle :</p><p>La plupart des codes écrits pour Python 3.7 fonctionneront parfaitement dans la version 3.12. … Nous effectuons des mises à jour une fois par an et la plupart des problèmes que nous rencontrons sont liés à des SDK tiers qui ont trop d’opinions sur leurs propres dépendances. Nous effectuons des changements radicaux, mais nous trouvons surtout des bugs préexistants qui sont découverts grâce à une meilleure annotation de type, ce qui est essentiel dans les projets Python de plus grande envergure.</p><p>Microsoft abandonne également VBScript dans le client Windows. Il continuera probablement à fonctionner pendant un certain temps en tant que fonctionnalité à la demande (je souligne le « probablement »).</p><h3>Analyse : Le langage de script obsolète est obsolète</h3><p>Si vous utilisez toujours VBScript, pourquoi ? Il est temps de passer à autre chose : PowerShell est la nouvelle tendance, et il est même multiplateforme.</p><p>Sergiu Gatlan : Microsoft va supprimer VBScript dans Windows</p><p>« Campagnes de malware » VBScript (également connu sous le nom de Visual Basic Script ou Microsoft Visual Basic Scripting Edition) est un langage de programmation similaire à Visual Basic ou Visual Basic for Applications (VBA) et a été introduit il y a près de 30 ans, en août 1996. Il… intègre des scripts actifs dans les environnements Windows et communique avec les applications hôtes via Windows Script.… Bien que cela ne soit pas officiellement expliqué, la décision de Microsoft de déprécier VBScript fait probablement… partie d’une stratégie plus large visant à atténuer la prévalence croissante des campagnes de malware exploitant diverses fonctionnalités de Windows et Office pour les infections. Des acteurs malveillants ont utilisé VBScript pour distribuer des malwares, notamment des souches notoires comme Lokibot, Emotet, Qbot et, plus récemment, le malware DarkGate, entre autres.</p><p>La nostalgie n’est plus ce qu’elle était. Avec les larmes aux yeux, voici un whoopdedo :</p><p>Je suis un peu nostalgique à ce sujet. Pas à cause de VBScript… mais à cause d’ActiveState qui fournissait des interpréteurs Perl, TCP et Python fonctionnant avec Windows Script Host. Je pouvais créer et appeler des interfaces COM ainsi que la bibliothèque CPAN parmi lesquelles choisir… Cela faisait partie du rêve d’OLE que les utilisateurs disposent de nombreux petits scripts automatisant leurs flux de travail orientés composants. Au lieu de cela, nous avons eu un codage culte du cargo et d’innombrables vulnérabilités car, pour emprunter une expression aux gens de l’IoT, le « S » de COM/OLE signifie sécurité.</p><p>La seule constante est le changement. xzelldx prédit beaucoup de plaisir pour DevOps et l'informatique :</p><p>Cela va casser des choses bizarrement. … J’imagine que cela va se transformer en une sorte de mini problème de l’an 2000, où la plupart des gens ne sauront pas que cela touche quelque chose jusqu’à ce que cela se produise. … Maintenant, je peux exécuter une recherche .vbs et déterminer ce qui utilise quoi et si je peux amener la personne qui le possède à comprendre le problème.</p><p>Ces Européens sournois prévoient de construire un supercalculateur incroyablement rapide, qui fonctionnerait avec des puces ARM.</p><h3>Analyse : ExaFLOPS en vue</h3><p>Les conceptions Neoverse d’Arm semblent vraiment attrayantes pour les constructeurs HPC. C’est également la base de l’AWS Graviton3. Comme toujours, tout est une question de performances par watt.</p><p>Ryan Whitwam : Le supercalculateur exascale fonctionnera sur ARM</p><p>« Début 2024 » L’un des supercalculateurs les plus puissants du monde sera bientôt en ligne en Europe, mais ce n’est pas seulement la vitesse brute qui rendra le supercalculateur Jupiter spécial. Contrairement à la plupart des supercalculateurs du Top 500, le système exascale Jupiter s’appuiera sur des cœurs ARM au lieu de pièces x86. … Une fois terminé, Jupiter sera tout en haut de la liste des supercalculateurs. … Jupiter est un projet de l’entreprise commune européenne pour le calcul haute performance (EuroHPC JU), qui … a choisi d’utiliser le processeur Rhea de SiPearl … basé sur l’architecture ARM. … Rhea est basé sur la conception du processeur Neoverse V1 d’ARM, qui a été développé spécifiquement pour les applications de calcul haute performance (HPC) avec 72 cœurs. Il prend en charge la mémoire à large bande passante HBM2e, ainsi que la DDR5, et le cache plafonne à un impressionnant 160 Mo.… Le système sera doté du module Booster de Nvidia, qui comprend des GPU et des interconnexions à bande passante ultra-élevée Mellanox. Le groupe n'a pas annoncé quelles puces Nvidia… mais la génération actuelle de H100 semble être une valeur sûre. … L'assemblage pourrait commencer dès le début de l'année 2024.</p><p>La seule constante est le changement (je l'ai peut-être mentionné plus tôt). Darinbob explique l'abandon du x86 :</p><p>Français À l’époque du calcul parallèle ou des supercalculateurs, les familles x86 étaient correctes, mais pas géniales. … Les temps ont évolué, mais il semble que la raison pour laquelle les gens utilisent des puces compatibles Intel x86/64 pour les supercalculateurs est que c’est tout ce que les gens connaissent désormais. … Les puces modernes ont des problèmes dans les supercalculateurs : le processeur de votre PC est en fait une grosse pile de puces avec plusieurs couches, il y a beaucoup de choses qui gênent les performances et c’est là pour la compatibilité, etc. … Les processeurs x86-64 modernes sont des machines RISC, ils sont juste cachés derrière une dominatrice CISC. … L’une des raisons pour lesquelles les GPU sont beaucoup utilisés dans ce domaine est que la conception (et la programmation !) des GPU est fortement orientée vers les calculs optimisés, pas vers des applications Web à usage général, etc. Le pipeline massif, la superscaling et le traitement vectoriel sont intégrés, exactement les composants que vous voulez avec les supercalculateurs.</p><p>Mais ARM est-il prêt pour le DevOps classique ? Voici l'expérience personnelle de customizable :</p><p>Nous avons déplacé plusieurs serveurs PostgreSQL, dont un grand utilisant 32 vCPU, vers des instances équivalentes basées sur ARM, et les performances étaient à peu près les mêmes, mais bien sûr, les instances ARM sont moins chères.</p><p>[Vous êtes viré – NDLR] Vous lisez The Long View de Richi Jennings. Vous pouvez le contacter à @RiCHi, @richij ou à [email protected].</p><p>Image : @flaviewxvx (via Unsplash ; nivelée et recadrée)</p>"
Créer des applications cloud évolutives avec MySQL,"<p>Les systèmes de bases de données relationnelles tels que MySQL sont depuis longtemps le choix de prédilection pour gérer des tâches de traitement de données volumineuses et complexes. Mais, avec l'introduction de nouvelles technologies comme NoSQL, de nombreux développeurs ont commencé à se demander si les bases de données relationnelles traditionnelles avaient toujours l'avantage. Cependant, lors de la création d'applications cloud hautement évolutives, les bases de données relationnelles comme MySQL ont toujours l'avantage sur NoSQL. Voici cinq raisons pour lesquelles MySQL reste supérieur à NoSQL pour soutenir les développeurs d'applications cloud à grande échelle.</p><h3>1. Activation des requêtes complexes</h3><p>Le langage de requête structuré (SQL) est le langage le plus largement adopté pour les bases de données relationnelles, et MySQL ne fait pas exception. SQL offre aux développeurs une plate-forme puissante pour créer des requêtes complexes et des rapports ad hoc, gérer efficacement de grands ensembles de données et créer des tables et des index pour l'optimisation des données. En revanche, NoSQL n'offre généralement que des capacités de requête et de manipulation de données rudimentaires, ce qui s'avère souvent inefficace lors de la gestion d'ensembles de données complexes. La plupart des analystes de données connaissent également SQL, c'est donc un outil simple qu'ils peuvent utiliser pour obtenir des informations.</p><h3>2. Évolutivité et fiabilité</h3><p>MySQL est réputé pour son évolutivité lors de la gestion de charges de données massives et est particulièrement bien adapté aux applications basées sur le cloud qui nécessitent des performances élevées et constantes. MySQL garantit le maintien de la cohérence des données. MySQL est l'option de référence en matière d'évolutivité et de fiabilité pour les applications cloud de plus grande envergure. Des entreprises telles que Facebook, GitHub et WordPress ont bâti des entreprises d'une valeur de plusieurs milliards de dollars sur MySQL, et il a été testé et validé dans certaines des charges de travail les plus difficiles et les plus exigeantes de toutes les bases de données.</p><h3>3. Réplication haute performance à grande échelle</h3><p>La réplication MySQL offre une solution fiable pour les applications cloud qui doivent évoluer. Contrairement à d’autres bases de données, les capacités de réplication de MySQL offrent une sécurité efficace des données, ce qui en fait un choix solide pour les applications qui nécessitent une disponibilité et une durabilité élevées. De plus, MySQL permet des mises à niveau de version transparentes sans perturber les opérations, garantissant que le système dispose toujours des dernières fonctionnalités et des derniers correctifs de sécurité. La fonction de mise à l’échelle de lecture de MySQL améliore les performances en répartissant la charge de lecture sur plusieurs instances, ce qui la rend idéale pour les applications cloud à grande échelle. Avec une expérience éprouvée à grande échelle, la réplication MySQL est le choix optimal pour les applications qui souhaitent croître et évoluer efficacement.</p><h3>4. Haute disponibilité et durabilité</h3><p>MySQL offre une haute disponibilité et une durabilité améliorées, garantissant que les données critiques sont toujours disponibles, même pendant les opérations de maintenance et de reprise après sinistre. En effet, MySQL est entièrement conforme à ACID (atomicité, cohérence, isolation, durabilité) et offre un support complet pour les transactions. Même en cas de défaillance du système, la transaction peut être annulée et les données ne sont pas perdues. Avec les bases de données NoSQL, les données sont principalement écrites en mémoire et n'offrent pas le même degré de durabilité ou de cohérence.</p><h3>5. Facilité de mise en œuvre et de débogage</h3><p>MySQL est facile à installer, à implémenter et à déboguer par rapport à NoSQL. MySQL est pris en charge par une large communauté de développeurs et d'administrateurs de bases de données qui l'utilisent pour une large gamme d'applications. MySQL est également open source et gratuit, ce qui en fait un excellent choix pour les entreprises de toutes tailles. Les systèmes NoSQL nécessitent l'installation d'outils, de bibliothèques et d'autres composants supplémentaires pour leur mise en œuvre, ce qui les rend plus complexes et plus chronophages.</p><p>MySQL offre de nombreux avantages aux développeurs d'applications cloud à grande échelle, allant de son puissant langage SQL pour les requêtes complexes à sa haute disponibilité et sa flexibilité pour la gestion des jointures multi-tables. NoSQL reste une option viable dans certains cas, mais les développeurs qui créent des applications cloud à grande échelle s'en tiennent principalement à MySQL en raison de sa nature évolutive et fiable, de sa mise en œuvre facile et de ses capacités de gestion des requêtes flexibles. Le choix du bon moteur de base de données est essentiel pour toute organisation qui s'appuie fortement sur des applications volumineuses et gourmandes en données. MySQL continue de résister à l'épreuve du temps lorsqu'il s'agit de garantir les performances et d'obtenir des résultats cohérents.</p>"
Cisco acquiert Splunk pour créer une centrale d'observabilité,"<p>Cisco a annoncé aujourd'hui avoir signé un accord définitif pour acquérir Splunk pour 28 milliards de dollars en espèces. L'accord ajoutera Splunk, l'une des plateformes les plus utilisées pour la gestion des opérations informatiques, au portefeuille de Cisco, rejoignant ainsi la plateforme d'observabilité AppDynamics en plus d'une large gamme d'équipements d'infrastructure informatique.</p><p>Chuck Robbins, PDG de Cisco, a déclaré aux analystes du secteur que l'entité fusionnée constituerait l'une des plus grandes sociétés de logiciels au monde. La plateforme Splunk et AppDynamics crée une plateforme d'observabilité complète couvrant les applications, les réseaux et les environnements de cloud computing hybrides, a ajouté M. Robbins.</p><p>En combinant AppDynamics avec les données collectées par la plateforme Splunk, l'entreprise fusionnée sera en mesure de fournir de meilleures informations sur les opérations informatiques que toute autre entreprise, a ajouté Robbins.</p><p>Selon Gary Steele, PDG de Splunk, cette acquisition représente la prochaine étape naturelle et logique pour Splunk et renforce son partenariat à long terme avec Cisco pour garantir la sécurité et la résilience des environnements informatiques. Les deux entreprises seront également en mesure d'appliquer l'intelligence artificielle (IA) plus efficacement aux opérations informatiques, a-t-il ajouté.</p><p>Au cours de son dernier trimestre, Splunk a déclaré un chiffre d'affaires de 911 millions de dollars, dont 445 millions de dollars, soit 29 % du total, provenant du cloud. Le chiffre d'affaires total de Splunk devrait se situer entre 3,925 et 3,95 milliards de dollars pour l'exercice 2024.</p><p>Splunk avait déjà acquis SignalFX, un fournisseur d'une plateforme d'observabilité, en 2019. On ne sait pas encore comment la plateforme pourrait être intégrée à la plateforme AppDynamics, mais Robbins a déclaré qu'il y a peu de chevauchement entre les plateformes d'observabilité que les deux sociétés fournissent actuellement.</p><p>L’acquisition de Splunk intervient à un moment où il devient de plus en plus évident que le besoin d’observabilité s’étend bien au-delà des flux de travail DevOps, car les organisations deviennent de plus en plus dépendantes des logiciels.</p><p>Chaque organisation informatique devra déterminer combien de temps elle attendra la clôture de l’acquisition de Splunk avant d’investir davantage dans l’observabilité. Historiquement, les acquisitions de cette envergure s’accompagnent de hausses de prix pour aider à financer la fusion une fois l’accord conclu.</p><p>En attendant, les responsables informatiques et DevOps vont probablement revoir la façon dont l’informatique est gérée dans les mois à venir. Splunk est devenu célèbre à une époque où la plupart des charges de travail s’exécutaient dans des environnements informatiques sur site. Alors que la majeure partie des charges de travail continuent de s’exécuter dans ces environnements, de nombreuses charges de travail déployées dans le cloud sont gérées à l’aide de plateformes conçues spécifiquement pour ces environnements. Splunk a plaidé en faveur d’une approche de cloud computing hybride qui suppose que la gestion des environnements sur site et dans le cloud deviendra plus unifiée.</p><p>En théorie, les difficultés économiques poussent de plus en plus d’entreprises à emprunter cette voie pour réduire le coût total de gestion de l’informatique. Mais il est tout aussi possible d’étendre les outils cloud aux environnements informatiques sur site que d’étendre les outils traditionnels au cloud. Malgré cette issue inévitable, les concurrents de Splunk passeront la majeure partie de l’année prochaine à proposer des changements de plateforme avant et après la clôture de l’accord.</p>"
Il n’existe pas de stratégie universelle de mobilité des données,"<p>Que vous soyez une entreprise du Fortune 500 ou une jeune start-up, chaque entreprise doit s’attendre à une chose : le changement. C’est particulièrement vrai en matière de gestion des données. Même les opérations de démarrage devront, tôt ou tard, changer la manière et l’endroit où elles exploitent leurs données. Après tout, chaque entreprise, y compris celles qui sont lancées avec la technologie la plus avancée, repose sur une série de choix et de compromis. À mesure que les technologies et les priorités commerciales évoluent, votre stratégie de mobilité des données doit également évoluer.</p><p>Dans un monde où les variables sont en constante évolution, quelle est la bonne stratégie de mobilité des données ? Il n’existe pas de solution universelle. Il est toutefois utile de commencer par une base solide sur laquelle construire votre stratégie.</p><p>Avec la plateforme adéquate qui permet la flexibilité et le changement, il est temps de construire le cadre structurel de votre stratégie de mobilité des données. Lors de la création du cadre de votre stratégie de mobilité des données, il y a quelques considérations à garder à l’esprit, notamment l’utilisation souhaitée du stockage sur site, votre stratégie de cloud hybride et vos capacités de « supercalcul », ainsi que le réseau de stockage (SAN) nécessaire en fonction des besoins de votre organisation. Examinons ce que cela pourrait signifier pour vous :</p><h3>Développer une stratégie de cloud hybride</h3><p>La réalité est que le cloud hybride est désormais réalisable pour la plupart des organisations. Selon le rapport 2023 State of the Cloud de Flexera, 72 % des organisations ont adopté une stratégie de cloud hybride. Votre stratégie de mobilité des données doit en tenir compte.</p><p>Avec un environnement de cloud hybride approprié, l’expérience utilisateur doit être transparente, quel que soit l’endroit où se trouvent vos données, que ce soit sur site, dans un cloud privé ou dans le cloud public. À mesure que le potentiel d’allocation de ressources plus dynamique augmente, votre stratégie de mobilité des données devient intrinsèquement plus critique. La mobilité des données, en effet, équivaut désormais à la mobilité informatique.</p><h3>Exploiter le « supercalculateur » dans le cloud</h3><p>La mobilité des données devient encore plus impérative à mesure que les entités ont de plus en plus la possibilité de tirer parti des capacités de « supercalcul ».</p><p>La puissance de calcul du cloud est aujourd’hui considérable et, grâce à la tarification dynamique du cloud, même les petites entreprises peuvent en profiter. Selon une estimation, le marché du calcul haute performance basé sur le cloud devrait atteindre 55 milliards de dollars d’ici 2030, avec un TCAC de 7 % entre 2023 et 2030.</p><p>Par exemple, si vous êtes une start-up FinTech, vous pourriez payer à la minute pour bénéficier de capacités de calcul hautes performances afin de tester votre algorithme de trading. Auparavant, ces puissantes capacités n’étaient accessibles qu’aux plus gros clients (pensez à la NASA) et s’accompagnaient de coûts astronomiques.</p><h3>Les solutions sur site ne sont pas près de disparaître</h3><p>Même si les services et les fonctionnalités du cloud deviennent plus accessibles et plus précieux, les opérations sur site ne sont pas près de disparaître. Les clients ont toutes sortes de raisons de conserver leurs données sur site, que ce soit pour des raisons de conformité, de coûts ou de désir de tirer parti de l’infrastructure existante. Certaines charges de travail, quant à elles, n’ont tout simplement pas autant de sens sur le cloud.</p><p>En réalité, même les solutions les plus convaincantes ne peuvent pas éliminer les opérations sur site, car les écosystèmes commerciaux et les industries évoluent de manière organique. Pensez-y : de nombreuses entreprises utilisent encore des lignes fixes et les employés utilisent toujours des ordinateurs de bureau. Près d’un demi-siècle après que les spécialistes du marketing ont inventé le terme de « bureau sans papier », il a fallu une pandémie de grande ampleur pour que nos outils de collaboration deviennent enfin, pour ainsi dire, sans papier.</p><h3>Fibre Channel et iSCSI</h3><p>En matière de mobilité des données, comment une organisation décide-t-elle d’adopter la fibre optique, l’iSCSI ou les deux ? Les organisations gérant des charges de travail qui nécessitent une faible latence et d’autres capacités réseau vont investir massivement dans ce domaine.</p><p>Traditionnellement, une grande partie de la gestion des données repose sur la technologie Fiber Channel. Avec iSCSI, une organisation n’a pas besoin d’investir dans une infrastructure physique distincte, comme des commutateurs ou des câbles. De plus, iSCSI est devenu plus attrayant pour les entités qui déplacent leurs charges de travail vers le cloud. Toute une gamme de produits, de Pure Storage, NetApp et même des plateformes cloud comme Microsoft Azure, proposent un stockage basé sur iSCSI dans le cloud. Votre stratégie de mobilité des données doit tenir compte de la manière de passer de la technologie Fiber Channel à iSCSI et inversement.</p><h3>Le changement est la seule constante</h3><p>En fin de compte, personne ne peut prédire toutes les innovations technologiques majeures qui auront un impact sur son entreprise à l’avenir. Pour rester agile, vous avez besoin d’une stratégie de mobilité des données que vous pouvez exécuter avec l’équipe dont vous disposez, que son expertise principale soit dans le stockage, la mise en réseau ou ailleurs. Cela peut être particulièrement difficile dans les grandes organisations, avec des équipes qui doivent planifier méticuleusement leurs projets et rester responsables de leurs budgets. Aujourd’hui plus que jamais, il est important de disposer d’une base solide sur laquelle construire une stratégie adaptée à vos besoins.</p>"
