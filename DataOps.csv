Title,Content
Optimisation des tests ETL pour une meilleure qualité et fiabilité des données,"<p>Avez-vous déjà pris une décision commerciale sur la base de données inexactes ou incomplètes ? Dans le monde actuel axé sur les données, la qualité et la fiabilité de vos données peuvent faire la différence entre votre stratégie commerciale et la vôtre. Les entreprises s’appuyant de plus en plus sur les données pour guider leurs décisions, il est plus important que jamais de s’assurer que ces données sont exactes, cohérentes et fiables. C’est là qu’entrent en jeu les tests ETL (Extract, Transform, Load).</p><p>Les processus ETL sont essentiels pour déplacer des données provenant de diverses sources vers un système unifié, mais sans tests rigoureux, même de petites erreurs dans ces processus peuvent entraîner des problèmes de qualité des données importants. Ce blog met en évidence les meilleures pratiques, les défis courants et les solutions innovantes pour optimiser les tests ETL, vous aidant à préserver l'intégrité de vos données et à prendre des décisions en toute confiance.</p><h3>Libérer la puissance des tests ETL</h3><p>Dans le domaine de la gestion des données, les tests ETL constituent la pierre angulaire de l'assurance qualité des données. Leur importance ne peut être surestimée, car ils servent de gardien de l'intégrité des données tout au long du pipeline de données. Voici quelques raisons pour lesquelles les tests ETL sont essentiels :</p><li>Validation de l'exactitude des données : vérifie méticuleusement que la transformation des données est exécutée correctement, garantissant que les données chargées dans les systèmes cibles sont exactes et fiables.</li><li>Conformité aux règles métier : confirme que les données adhèrent aux règles et normes métier prédéfinies, préservant ainsi la cohérence au sein de l'organisation.</li><li>Assurance de l'intégration des données : valide l'intégration transparente des données provenant de sources disparates, essentielle pour une veille économique et des analyses complètes.</li><li>Détection et prévention des erreurs : en identifiant les problèmes au début du pipeline de données, les tests ETL empêchent la propagation d’erreurs pouvant conduire à des décisions commerciales coûteuses.</li><li>Support de gouvernance des données : il joue un rôle essentiel dans le maintien des politiques de gouvernance des données en garantissant la qualité, la sécurité et la conformité des données tout au long du processus ETL.</li><p>« D’ici 2028, les outils basés sur GenAI seront capables d’écrire 70 % des tests logiciels, réduisant ainsi le besoin de tests manuels et améliorant la couverture des tests, la convivialité des logiciels et la qualité du code ». IDC.</p><h3>Défis pour atteindre la qualité des données</h3><p>Bien que la recherche de données de haute qualité grâce aux tests ETL soit cruciale, les organisations se retrouvent souvent confrontées à un paysage complexe rempli d'obstacles. Ces défis découlent de la nature complexe des écosystèmes de données modernes, du rythme rapide des changements technologiques et des demandes toujours croissantes d'informations basées sur les données. Comprendre ces obstacles est la première étape vers l'élaboration de stratégies robustes pour les surmonter et atteindre le niveau de qualité des données souhaité. Les principaux défis des tests ETL auxquels les organisations sont confrontées sont les suivants :</p><li>Volume et diversité des données : les méthodes de test traditionnelles ont souvent du mal à gérer des téraoctets ou des pétaoctets de données, ce qui entraîne des cycles de test prolongés et des contraintes de ressources. De plus, les données se présentent sous différents formats structurés, semi-structurés et non structurés, chacun nécessitant des approches de traitement et de validation différentes.</li><li>Transformations complexes : les transformations de données complexes sont difficiles à tester de manière approfondie, en particulier lorsqu'elles impliquent plusieurs règles métier et cas limites. Les transformations conditionnelles complexes créent une multitude de chemins de données possibles, ce qui rend difficile le test complet de tous les scénarios.</li><li>Contraintes de temps : la demande croissante de traitement des données en temps réel ou quasi réel exerce une pression sur les équipes de test pour valider la qualité des données à grande vitesse. Équilibrer le temps consacré au développement et aux tests conduit souvent à des compromis dans la couverture des tests.</li><li>Différences d'environnement : les variations de puissance de traitement, de mémoire ou de stockage entre les environnements de test et de production peuvent masquer des problèmes de qualité des données liés aux performances. De même, les incohérences dans les versions ou les configurations logicielles entre les environnements peuvent entraîner un comportement inattendu en production.</li><li>Manque de visibilité de bout en bout : il peut être extrêmement difficile de suivre les données via des processus ETL complexes pour identifier la cause profonde des problèmes de qualité. En outre, une surveillance inadéquate de l'ensemble du pipeline de données peut entraîner des angles morts où les problèmes de qualité des données passent inaperçus.</li><li>Évolution du paysage des données : l'ajout fréquent de nouvelles sources de données nécessite des mises à jour constantes des processus ETL et des cas de test correspondants. L'évolution des réglementations en matière de confidentialité et de conformité des données nécessite des ajustements continus des procédures de traitement et de test des données.</li><h3>Stratégies pour obtenir des données de type production</h3><p>Pour garantir l’efficacité des tests ETL, il est essentiel de travailler avec des données qui ressemblent étroitement aux données de production. Voici quelques stratégies pour obtenir efficacement des données de type production :</p><li>Sous-ensemble de données : créez des sous-ensembles représentatifs de données de production qui conservent les caractéristiques et les complexités de l'ensemble de données complet.</li><li>Masquage des données : mettez en œuvre des techniques de masquage des données robustes pour protéger les informations sensibles tout en préservant les propriétés statistiques des données.</li><li>Génération de données synthétiques : utilisez des algorithmes avancés pour générer des données synthétiques qui reflètent les modèles et les distributions des données de production.</li><li>Actualisation incrémentielle des données : mettez à jour les données de test avec de nouvelles données de production pour garantir la pertinence et capturer de nouveaux modèles de données.</li><li>Copies de données virtuelles : exploitez les technologies de virtualisation de bases de données pour créer des copies légères et à jour des données de production à des fins de test.</li><li>Profilage et analyse des données : effectuez un profilage approfondi des données pour comprendre les caractéristiques des données de production et les reproduire dans des environnements de test.</li><h3>Bonnes pratiques pour les tests ETL</h3><li>Établissez des objectifs de test clairs : définissez des objectifs spécifiques et mesurables pour chaque phase de test afin de garantir une couverture complète.</li><li>Implémenter le contrôle de version : utilisez des systèmes de contrôle de version pour suivre les modifications dans les processus ETL et les cas de test, facilitant ainsi le dépannage et les restaurations.</li><li>Automatisez les tests répétitifs : exploitez les outils d'automatisation des tests pour exécuter des tests de routine, libérant ainsi des ressources pour des scénarios de test plus complexes.</li><li>Priorisez les cas de test : concentrez-vous sur les éléments de données critiques et les zones à haut risque pour maximiser l'impact des efforts de test.</li><li>Mettre en œuvre des tests continus : intégrez les tests tout au long du cycle de développement pour découvrir et résoudre les problèmes le plus tôt possible.</li><li>Documentez minutieusement : conservez une documentation détaillée des cas de test, des résultats et de tout problème de qualité des données découvert pendant les tests.</li><li>Collaborer entre les équipes : Favorisez une collaboration étroite entre les ingénieurs de données, les testeurs et</li><h3>Types de tests ETL</h3><p>Les tests ETL sont essentiels pour garantir l'exactitude et l'intégrité des données lors de leur transfert de leur source d'origine à leur destination. Ce processus implique une série de contrôles et de validations pour détecter les erreurs, les incohérences et d'autres problèmes lors des étapes d'extraction, de transformation et de chargement des données. Pour gérer efficacement cela, les tests ETL sont classés en différents types, chacun ciblant des aspects spécifiques du pipeline de données.</p><h3>Avantages des tests ETL automatisés</h3><p>Les tests ETL automatisés sont devenus une véritable révolution pour les entreprises. Ils offrent une solution puissante aux défis liés à la garantie de la qualité des données à grande échelle. Alors que les entreprises sont confrontées à des volumes de données croissants, à des transformations complexes et à la nécessité d'obtenir des informations rapides, l'automatisation des tests se distingue par son efficacité et sa fiabilité. En exploitant des outils et des technologies de pointe, les tests ETL automatisés accélèrent non seulement le processus de test, mais améliorent également sa précision et son exhaustivité. Grâce à l'automatisation, les entreprises peuvent bénéficier des avantages suivants :</p><li>Couverture de test accrue : permet des tests plus complets sur une plus large gamme de scénarios et de variations de données.</li><li>Exécution plus rapide : les tests automatisés peuvent être exécutés rapidement et fréquemment, permettant une identification rapide des problèmes.</li><li>Cohérence et fiabilité : élimine les erreurs humaines et garantit une exécution cohérente des cas de test.</li><li>Évolutivité : offre la possibilité d'augmenter ou de diminuer la capacité sans effort, ce qui est particulièrement avantageux pour gérer les charges de pointe ou les exigences de test fluctuantes.</li><li>Efficacité et flexibilité améliorées : en automatisant les tâches répétitives, les testeurs peuvent se concentrer sur des activités de test plus complexes et à forte valeur ajoutée. Cela permet le déploiement et la gestion dynamiques des machines virtuelles, du stockage et des réseaux, accélérant ainsi le processus de test et facilitant l'itération et l'expérimentation rapides.</li><li>Détection précoce des problèmes : des tests automatisés continus tout au long du processus de développement permettent d'identifier les problèmes plus tôt, réduisant ainsi le coût des correctifs.</li><li>Rapports améliorés : les outils de test automatisés fournissent souvent des rapports et des analyses détaillés, offrant des informations plus approfondies sur les résultats et les tendances des tests.</li><li>Maintenance plus facile : les tests automatisés bien conçus sont plus faciles à mettre à jour et à entretenir à mesure que les processus ETL évoluent.</li><h3>Assurez l'avenir de votre stratégie de données avec un partenaire de test ETL compétent</h3><p>L’optimisation des tests ETL n’est pas seulement une nécessité technique ; c’est un impératif stratégique pour toute organisation qui s’appuie sur les données pour prendre des décisions. AgreeYa est l’une de ces options qui aident les organisations en proposant des stratégies de test robustes, en surmontant les défis courants et en tirant parti de l’automatisation.</p>"
Naviguer dans les eaux agiles : pourquoi l'intégration de Copilot exige des ajustements méthodologiques,"<p>Le Congrès américain a récemment interdit à son personnel d’utiliser l’IA Copilot de Microsoft, un chatbot intégré à grand modèle de langage qui permet l’automatisation des produits Microsoft tels que Word, Excel, PowerPoint, Outlook et Teams, en invoquant des problèmes de sécurité. Et ils ne sont pas les seuls à penser ainsi, car de nombreux professionnels oscillent entre enthousiasme et crainte lorsqu’il est question de l’IA. En attendant, Microsoft a vanté avec assurance Copilot, promettant que la puissance de son IA réduira le travail quotidien de gestion d’une entreprise. L’entreprise est passée du simple discours sur l’IA à l’intégration de celle-ci dans toutes les couches de sa pile technologique. Son introduction récente de Copilot Runtime permet même aux développeurs d’utiliser l’IA dans leurs propres programmes, il n’est donc pas surprenant que la plupart des directeurs des systèmes d’information expérimentent Copilot. Ses promesses de productivité et l’élargissement du champ d’application et les économies de coûts qui en résultent pour un réinvestissement potentiel sont trop alléchantes pour être ignorées. Microsoft montre certainement l’exemple, et la seule question est de savoir comment les autres devraient suivre. Au cours de la dernière décennie, les organisations agiles ont surpassé les autres en prenant et en gérant les décisions plus rapidement. L'adoption de l'agilité dans le domaine informatique a entraîné des changements dans la manière dont les infrastructures, les applications, les données et les compétences sont produites, consommées ou les deux. Les éléments fondamentaux de l'agilité, tels que la collaboration, l'automatisation et les améliorations continues, sont les principales sources d'innovation pertinentes pour les applications, le développement et le déploiement.</p><p>Les événements récents ont perturbé la progression agile dans le domaine informatique. L’ère de flexibilité induite par la COVID-19 dans la manière, le moment et le lieu de travail remet en question le statu quo dans nos modes de collaboration. En outre, l’IA générative et les complexités associées à la gouvernance informatique ont également bouleversé la progression agile. Par conséquent, une économie incrémentale a été créée qui oblige chaque entreprise à prendre les opportunités et les défis plus au sérieux.</p><p>L'agilité distribuée est déjà en pratique depuis un certain temps. Les réalités du travail à distance et hybride ne sont que des extensions de ce que nous avons déjà vu dans les équipes distribuées. Cependant, les promesses de productivité d'outils tels que Copilot sont nouvelles, donc supposer que les pratiques Agile actuelles fonctionneront avec les pratiques GenAI est une erreur. Alors que pouvons-nous faire ?</p><p>Voici quelques réflexions sur la manière d’intégrer la méthodologie agile dans le cadre de l’adoption de Copilot.</p><h3>Étendre DevOps pour inclure la représentation de DataOps et MLOps</h3><p>Étant donné l’importance des données ainsi que des modèles d’IA et d’apprentissage automatique, les équipes DevOps doivent inclure des représentants des équipes DataOps et MLOps (ModelOps est un sous-ensemble de MLOps). Ce n’est qu’à ce moment-là que l’objectif de rapprocher la « production » et les « opérations » peut être atteint. À première vue, le remplacement peut sembler être la plus grande menace de l’IA, mais son premier acte sera plutôt de révéler et d’approfondir les fissures dans la collaboration.</p><h3>L’intelligence logicielle est plus importante que jamais</h3><p>Ne pas comprendre les systèmes d’application de manière globale avant de produire automatiquement le code en production sera désastreux. L’informatique d’entreprise est un mélange d’applications et d’actifs informatiques IA et non IA. De plus, l’« explicabilité » du code ne peut être obtenue que lorsque l’intelligence logicielle sur le code produit par GenAI est atteinte. En fin de compte, ce n’est pas l’exactitude fonctionnelle, mais l’adéquation architecturale qui compte le plus pour débloquer des améliorations de productivité. L’IA évolue rapidement, mais laisser la vitesse prendre trop de priorité ouvre la voie à l’échec.</p><h3>La conformité continue et la sécurité continue sont tout aussi importantes</h3><p>L’une des principales préoccupations des outils GenAI concerne les vulnérabilités que le code généré automatiquement peut introduire dans l’informatique des entreprises, ce qui est la principale raison pour laquelle le Congrès américain a interdit l’utilisation de Copilot. Il est important de procéder aux ajustements appropriés au niveau des modules pour la conformité et la sécurité, afin qu’ils soient conçus et livrés en continu, plutôt que d’être vérifiés et assurés périodiquement. La réglementation est connue pour être à l’origine de l’innovation, et les entreprises doivent délibérément anticiper les difficultés futures.</p><h3>Augmenter les portes de qualité dans votre pipeline CI/CD pour les assistants IA</h3><p>Les principes fondateurs de l'open source (transparence, inspection et adaptation) peuvent être étendus aux produits GenAI. L'« inspection » ne doit pas seulement couvrir la qualité, les performances, la sécurité et les aspects UX du code fourni par les outils, mais également l'adéquation architecturale au sein de l'informatique de l'entreprise.</p><h3>Mesurez le succès et soyez transparent sur vos lacunes</h3><p>L’impact de l’IA peut être flou, mais certains résultats doivent être mesurables pour justifier son adoption. L’élaboration d’indicateurs de performance clés spécifiques à l’IA peut contribuer à consolider le rôle du copilote au sein de l’équipe. Trouver les bons indicateurs à mesurer est de la plus haute importance et constitue en soi un défi de taille.</p><p>Dans l’analyse détaillée de l’impact de l’IA, il est également important d’accepter ouvertement les défauts. Le système est, bien sûr, imparfait par nature, et ces imperfections doivent être suivies et traitées. L’IA évoluant si rapidement, de nombreux problèmes seront probablement résolus à court terme. Notez les défauts et faites en sorte de les revoir à intervalles réguliers.</p><h3>L’inadéquation des compétences aura un impact sur les promesses de productivité</h3><p>Les outils ne sont efficaces que si ceux qui les utilisent le savent. Un développeur expérimenté peut démontrer un niveau de productivité supérieur à la moyenne avec un assistant IA, mais un développeur inexpérimenté peut rapidement créer plus de problèmes que de solutions. Former les communautés de développement et d'assurance qualité à maîtriser les outils et les directives de gouvernance du code et des tests nécessite d'ajuster le modèle opérationnel Agile.</p><p>N’oubliez pas que les équipes DevOps sont bien plus que des développeurs, les équipes DataOps bien plus que des ingénieurs de données et les équipes ModelOps bien plus que des data scientists. Les compétences interdisciplinaires des équipes DevOps vont considérablement évoluer lorsque l’IA fera partie de la conversation. À mesure que les frontières entre ces disciplines s’estomperont, ceux qui seront prêts à s’adapter se hisseront au sommet.</p><p>L’adaptation de la méthodologie aux défis perçus ne doit pas limiter les avantages potentiels que GenAI peut produire. S’il est utilisé correctement, GenAI peut contribuer à l’hyper-automatisation des tâches de développement et d’assurance qualité, à l’évaluation des options de conception grâce au prototypage rapide, à la simplification du processus de documentation, à la surveillance de l’environnement de production pour prévoir les goulots d’étranglement des performances, etc.</p><p>Si nous n’adaptons pas nos méthodes agiles pour répondre à ces nouvelles réalités et dégager de la valeur plus rapidement, le « time to market » et les avantages en termes de coûts associés seront mal perçus. Les changements dans la méthodologie agile sont inévitables, car GenAI et Agile offrent de réels avantages concurrentiels. Ne vous laissez pas aller.</p>"
CISA : une faille critique de Jenkins exploitée dans des attaques de ransomware,"<p>Une faille de sécurité critique dans le populaire serveur d’automatisation open source Jenkins figure sur la liste des vulnérabilités connues de la Cybersecurity and Infrastructure Security Agency (CISA) après avoir été exploitée dans un ransomware et d’autres attaques.</p><p>L'agence de cybersécurité la plus importante du gouvernement américain a ajouté le bug - identifié comme CVE-2024-23897 et avec un score de gravité CVSS de 9,8 sur 10 - à son catalogue de vulnérabilités exploitées connues, ce qui met en garde les agences fédérales contre la nécessité de sécuriser leurs serveurs Jenkins, bien que la CISA ait également averti toutes les organisations exploitant de tels serveurs de s'assurer qu'ils sont sécurisés.</p><p>La vulnérabilité dans l'interface de ligne de commande (CLI) de Jenkins est une faille de parcours de chemin causée par une faiblesse dans l'analyseur de commandes args4j, qui peut être exploitée par des acteurs malveillants pour obtenir l'exécution de code à distance (RCE) et pour lire des fichiers arbitraires sur le serveur Jenkins.</p><p>Le serveur Jenkins basé sur Java, maintenu par CloudBees et la communauté Jenkins, est utilisé par les développeurs dans leur intégration continue et leur développement continu (CI/CD) et automatise les étapes du cycle de vie du développement logiciel, y compris le développement et le déploiement. L'outil, soutenu par des sociétés telles qu'Amazon Web Services (AWS), GitHub et JFrog, compte plus d'un million d'utilisateurs.</p><h3>Une faille devient publique et est corrigée</h3><p>Yaniv Nizry, chercheur en vulnérabilité chez SonarSource, développeur de logiciels open source, a été le premier à signaler cette faille de sécurité en janvier, soulignant qu’avec une part de marché d’environ 44 %, « la popularité de Jenkins est évidente. Cela signifie que l’impact potentiel des vulnérabilités de sécurité de Jenkins est important ».</p><p>Un correctif a été publié en janvier avec Jenkins 2.442, LTS 2.426.3 en désactivant la fonctionnalité d'analyse de commandes, les responsables expliquant que Jenkins est livré avec une CLI intégrée pour accéder à Jenkins à partir d'un environnement de script ou de shell. Il utilise la bibliothèque args4j pour analyser les arguments et les options de commande sur le contrôleur Jenkins lors du traitement des commandes CLI.</p><p>« Cet analyseur de commandes possède une fonctionnalité qui remplace un caractère @ suivi d'un chemin de fichier dans un argument par le contenu du fichier (expandAtFiles) », ont écrit les responsables. « Cela permet aux attaquants de lire des fichiers arbitraires sur le système de fichiers du contrôleur Jenkins en utilisant l'encodage de caractères par défaut du processus du contrôleur Jenkins. »</p><h3>Les cybercriminels se lancent</h3><p>Des preuves de concept (POC) auraient commencé à émerger peu après la publication du correctif par Jenkins. Les chercheurs de Trend Micro ont signalé en mars qu'ils avaient observé plusieurs attaques exploitant la faille, 28 des 44 adresses IP sources des attaques provenant des Pays-Bas, les autres de pays comme Singapour et l'Allemagne. La plupart des cibles se trouvaient en Afrique du Sud.</p><p>Ils ont également constaté des cas où des exploits RCE étaient échangés.</p><p>D’autres chercheurs ont découvert des attaques plus récentes exploitant la vulnérabilité de Jenkins. En juillet, CloudSEK a signalé une attaque de la chaîne d’approvisionnement contre Born Group, une agence internationale de conseil et d’expérience client basée à New York, par le groupe de menaces IntelBroker, spécialisé dans les violations de données, l’extorsion et la vente d’accès à des systèmes compromis.</p><p>Les chercheurs de CloudSEK ont déclaré qu'IntelBroker avait exploité CVE-2024-23897 pour obtenir un accès initial via un serveur Jenkins vulnérable avant d'accéder au référentiel GitHub de Born Group.</p><h3>Attaque de ransomware en Inde</h3><p>Plus tôt ce mois-ci, les chercheurs du Juniper Threat Lab ont écrit sur une attaque de ransomware contre Brontoo Technology Solutions, une société de services et de conseil informatique en Inde qui collabore avec C-Edge Technologies, une coentreprise entre Tata Consultancy Services et la State Bank of India. Juniper et CloudSEK ont attribué l'attaque au groupe de ransomware RansomXXX, qui existe depuis 2018, opère depuis la Russie ou l'Europe de l'Est et cible les agences gouvernementales, les banques et les organismes de santé.</p><p>L’attaque a perturbé les paiements de détail dans les banques indiennes. Une fois encore, les acteurs malveillants ont obtenu un accès initial à l’environnement informatique de Brontoo via la vulnérabilité Jenkins.</p><p>« Cette vulnérabilité permet à un utilisateur non authentifié de lire les premières lignes de n’importe quel fichier du système de fichiers », ont écrit les chercheurs. « Elle existe parce que la fonction intégrée de l’analyseur de commandes n’a pas été désactivée par défaut. Si elle est exploitée avec succès, cette vulnérabilité peut entraîner la fuite de fichiers et de données sensibles, l’exécution potentielle de commandes et permettre une attaque par ransomware. »</p>"
Le succès d'AIOps nécessite des données de télémétrie Internet synthétiques,"<p>Toute forme d’intelligence artificielle (IA) n’est efficace que si les données utilisées pour la former sont exactes. Si les entreprises souhaitent appliquer efficacement l’IA aux opérations informatiques (ITOps), elles doivent collecter autant de données de télémétrie que possible.</p><p>Les équipes informatiques découvrent souvent que leur plateforme AIOps a été formée sur une base restreinte de données de télémétrie. Ces données peuvent avoir été collectées, par exemple, à partir d’une plateforme DevOps qui n’a pas une visibilité complète sur l’environnement informatique distribué dans lequel s’exécute leur application. En l’absence de données de télémétrie synthétiques collectées à partir d’une plateforme de surveillance des performances Internet (IPM) pour collecter ces données de télémétrie, il est tout simplement peu probable que les algorithmes d’apprentissage automatique qui sont au cœur de toute plateforme AIOps parviennent à faire émerger les meilleures recommandations pour optimiser les expériences applicatives.</p><p>Le défi réside dans la nature probabiliste de l’IA. La pertinence des recommandations qui en découlent est déterminée par la qualité des données qui ont été exposées au modèle d’IA. Les données réelles des utilisateurs, par exemple, peuvent être rares, voire inexistantes. Bien entendu, si les données de télémétrie n’ont jamais été partagées avec le modèle d’IA, il est extrêmement peu probable que les recommandations de l’IA améliorent l’expérience des applications.</p><h3>Gagner en visibilité grâce aux données synthétiques</h3><p>Les équipes informatiques doivent s’assurer que les données utilisées pour former le modèle d’IA reflètent les environnements de production dans lesquels les applications sont déployées. Sinon, quel que soit le niveau de développement d’un modèle d’IA, les données qui entrent sont toujours synonymes de données qui sortent. Avant qu’une équipe informatique n’adopte une plateforme AIOps, elle doit connaître la provenance des données du modèle d’IA sous-jacent. Si le pool de données de formation de l’IA est limité, les recommandations générées le seront également. Les équipes informatiques ne vont pas faire confiance aux plateformes AIOps qui leur conseillent de prendre des mesures spécifiques sur la base de données partielles ou incomplètes. Et elles ne devraient pas le faire.</p><p>Au lieu de cela, les équipes supposeront que chaque résultat doit être vérifié avant que l’étape suivante d’un processus ne soit autorisée. Après tout, la seule chose pire que de se tromper en matière d’informatique et d’IA est de se tromper à une échelle catastrophique. Bien entendu, continuer à gérer l’informatique de manière séquentielle va sans doute à l’encontre de l’objectif d’investir dans une plateforme AIOps censée gérer les tâches en parallèle.</p><p>Étant donné la dépendance des applications modernes aux services Internet, tout effort visant à appliquer l’IA à la gestion informatique sans inclure de données de télémétrie Internet synthétiques conduira à un résultat sous-optimal. En incluant ce type de télémétrie, les informations fournies aux équipes DevOps leur permettront de garantir que les indicateurs clés de performance (KPI) sont atteints et maintenus.</p><h3>Plusieurs modèles d'IA</h3><p>Il est peu probable qu’un seul modèle d’IA puisse tout contrôler. Dans de nombreux cas, les plateformes de réseau, de sécurité et de gestion des services informatiques (ITSM) auront déjà appliqué l’IA aux données de télémétrie qu’elles collectent en temps réel. Les résultats de ces modèles d’IA seront ensuite partagés avec les plateformes AIOps pour automatiser une série de tâches de bout en bout qui, auparavant, nécessitaient que les équipes informatiques orchestrent les flux de travail sur plusieurs îlots d’automatisation.</p><p>Les équipes DevOps doivent donc évaluer l’efficacité de ce qui sera bientôt un réseau de modèles d’IA. Il en existe tellement, chacun étant ou sera conçu pour automatiser des tâches spécifiques, comme l’analyse du trafic Internet pour identifier la source des goulots d’étranglement qui pourraient n’avoir qu’un impact intermittent sur une application. Armée de ces informations, la plateforme AIOps peut alors générer systématiquement des recommandations utiles auxquelles les équipes DevOps peuvent faire confiance. Elles peuvent ensuite laisser les outils appliquer automatiquement la suggestion sur la manière dont, par exemple, le trafic Internet doit être réacheminé pour maintenir les objectifs de niveau de service (SLO).</p><h3>Réaliser la promesse de l’IA</h3><p>À mesure que les plateformes AIOps s’améliorent, elles peuvent réduire considérablement la charge de travail que les équipes DevOps rencontrent régulièrement. Les équipes peuvent passer des semaines à essayer de déterminer la cause profonde d’un problème qui, une fois découvert, peut prendre quelques minutes à résoudre. Le défi est que la source du problème n’a souvent pas grand-chose à voir avec ce que l’équipe DevOps contrôle immédiatement, comme c’est le cas lorsque, par exemple, la latence créée par un service Internet a un impact négatif sur les performances de l’application. Cependant, ces informations devraient permettre aux équipes DevOps d’envoyer des demandes d’assistance qui identifient mieux la source exacte d’un problème de service Internet que leur fournisseur devrait être en mesure de résoudre plus rapidement. Tout aussi important, l’équipe DevOps peut passer aux problèmes sur lesquels elle a un contrôle plus direct.</p><p>Une grande partie du stress que subit une équipe DevOps provient du fait qu’elle ne connaît pas la véritable cause d’un problème qui, malgré tous ses efforts, continue de générer une série d’alertes en continu. L’AIOps promet de réduire ce stress en simplifiant d’abord la corrélation des causes, puis en automatisant la correction. Cette promesse ne sera toutefois jamais pleinement tenue si les données sur lesquelles repose l’entraînement du modèle d’IA ne fournissent pas suffisamment d’informations pour prendre une décision véritablement éclairée.</p>"
Sumo Logic élimine les frais d'ingestion pour les données du journal d'observabilité,"<p>Sumo Logic a annoncé cette semaine qu'il ne facturerait plus de frais pour l'ingestion de données de journaux dans sa plateforme d'observabilité afin d'encourager les équipes DevOps à appliquer l'analyse plus en profondeur.</p><p>Michael Cucchi, vice-président du marketing produit chez Sumo Logic, a déclaré que le plan de licences flexibles de Sumo Logic élimine ce qui équivaut à une taxe pour l'utilisation d'une plateforme d'observabilité. Les licences flexibles sont immédiatement disponibles pour les nouveaux clients et seront proposées aux clients Sumo Logic existants plus tard dans l'année. Il n'y a pas de frais mensuels cachés, de restrictions de fonctionnalités, de compromis de performances ou de limitations d'utilisateurs appliqués dans le cadre de ce plan.</p><p>Le plan de licence ne s'applique qu'aux données de log, mais les équipes DevOps pourront ingérer toutes les données structurées, semi-structurées et non structurées pour un coût inférieur que Sumo Logic s'efforce de stocker plus efficacement, a déclaré Cucchi. L'objectif global est de mettre à disposition une approche disruptive de la gestion des licences qui réduit le coût total de l'observabilité, a-t-il ajouté.</p><p>Le stockage des données de journalisation va devenir de plus en plus crucial à mesure que les entreprises commenceront à appliquer plusieurs types de modèles d’intelligence artificielle (IA) pour automatiser davantage les opérations informatiques, a-t-il ajouté. Ces modèles d’IA promettent de rendre les plateformes d’observabilité plus accessibles, car les algorithmes seront capables de faire apparaître les problèmes qui doivent être résolus. Historiquement, la valeur d’une plateforme d’observabilité a été liée à la compétence d’une équipe DevOps dans l’utilisation de son langage de requête.</p><p>Les équipes informatiques qui ont adopté des plateformes d’observabilité ont, à des degrés divers, limité la quantité de données qu’elles collectent et conservent pour éviter que les coûts de stockage n’explosent. Le défi est que, à mesure que les équipes DevOps déploient davantage d’applications basées sur des microservices, la quantité de données de journal générées a explosé. Déterminer la cause profonde d’un problème impliquant ces applications peut s’avérer problématique si les données de journal ne sont pas facilement disponibles. Les données de journal sont, après tout, l’unité atomique de l’observabilité, a noté Cucchi.</p><p>En général, les environnements informatiques deviennent trop complexes pour être gérés par des humains sans l’aide de plateformes d’observabilité complétées par des modèles d’IA, a noté Cucchi. Chaque environnement d’application est assez unique, il est donc essentiel que ces modèles d’IA soient exposés à autant de données que possible pour garantir des résultats optimaux.</p><p>Il n’est pas certain que les plateformes d’observabilité puissent un jour remplacer la plupart des outils de surveillance sur lesquels les équipes informatiques s’appuient aujourd’hui pour suivre un ensemble de mesures prédéfinies. Cependant, une chose est sûre : les plateformes d’observabilité offrent au moins l’occasion de rationaliser certains de ces outils. Le plus grand défi consiste à trouver des moyens de financer l’acquisition d’une plateforme d’observabilité en premier lieu, ce qui crée ensuite une opportunité de consolider ces outils par la suite.</p><p>Étant donné la complexité des plateformes informatiques hautement distribuées, ce n’est qu’une question de temps avant que la plupart des équipes informatiques ne disposent d’une certaine capacité d’observabilité. Bien entendu, l’observabilité a toujours été un principe fondamental de DevOps. Le problème est que les outils utilisés pour y parvenir sont des plateformes de surveillance héritées qui n’offrent pas le niveau de profondeur requis pour gérer avec succès les environnements d’application modernes.</p><p>Source de l'image : Joshua Sortino via Unsplash.</p>"
Techstrong Research PulseMeter : la mise en cache transforme les performances des applications,"<p>La demande d’accès instantané aux données et d’expériences numériques fluides n’a jamais été aussi forte. Le récent rapport PulseMeter de Techstrong Research, « Database Caching Hits the Mainstream », se penche sur l’adoption de la mise en cache des bases de données et son rôle essentiel dans l’amélioration des performances et de l’évolutivité des applications.</p><p>Alors que le volume et l'utilisation des données augmentent à un rythme sans précédent, les développeurs de logiciels et les professionnels des bases de données recherchent constamment des solutions innovantes pour améliorer la lecture/écriture afin d'offrir une efficacité et des performances optimales. La mise en cache des bases de données est passée de son statut de niche à celui de stratégie courante pour les organisations qui souhaitent accroître les performances des applications, relever les défis de l'évolutivité et améliorer la résilience des applications et des bases de données dans des conditions de fonctionnement fluctuantes.</p><p>Les entreprises numériques et les attentes des clients rendent les applications plus nécessaires pour fournir un accès aux données en temps réel ou quasi réel. Cette nécessité est particulièrement aiguë dans les applications mobiles et Web, les environnements de données distribués, l'architecture de microservices cloud-native et les cas d'utilisation à forte demande comme les jeux, qui nécessitent tous des niveaux de performance et de fiabilité des bases de données sans précédent.</p><p>Le rapport PulseMeter souligne que l’amélioration de la latence et des performances grâce à la mise en cache des bases de données peut considérablement accroître les revenus et favoriser des expériences numériques positives. À l’inverse, l’absence de mise en œuvre de ces améliorations peut faire la différence entre le succès et l’échec d’une organisation. La mise en cache des bases de données est passée d’une technique spécialisée pour des cas d’utilisation spécifiques à un aspect fondamental de l’optimisation des performances des bases de données et des applications. Elle est désormais considérée comme une technologie fiable et éprouvée qui répond aux défis croissants liés au volume de données et aux exigences analytiques des applications contemporaines.</p><p>Les données recueillies par Techstrong Research auprès de DevOps, de développeurs de logiciels, de professionnels des bases de données, d’ingénieurs SRE, d’ingénieurs de plateforme et d’autres parties prenantes révèlent des informations essentielles sur l’adoption et l’impact de la technologie de mise en cache. 64,8 % des répondants utilisent déjà la mise en cache, et 13,8 % supplémentaires évaluent son intégration dans leurs opérations. Cette adoption significative souligne le rôle essentiel de la mise en cache dans l’infrastructure informatique actuelle, avec des solutions de pointe comme Amazon ElastiCache, Redis et NGINX détenant collectivement plus de 40 % des parts de marché.</p><p>Parmi les technologies à l’origine de l’essor de la mise en cache des bases de données, Redis est bien connu pour sa polyvalence et ses performances. Disponible à la fois en version open source et commerciale, Redis est parfaitement adapté à un large éventail de cas d’utilisation, de la mise en cache de base aux structures de données avancées, aux opérations, aux analyses en temps réel et à la gestion des files d’attente. Ce qui distingue Redis, ce sont les améliorations de performances qu’il offre, permettant des applications en temps réel dans les secteurs des jeux, des services financiers, de la santé et bien d’autres. Le fort taux d’adoption de Redis, comme le souligne le rapport PulseMeter de Techstrong Research, souligne son rôle central dans l’acceptation et la mise en œuvre généralisées des technologies de mise en cache des bases de données.</p><p>L’étude identifie les performances et la fiabilité comme les principales considérations pour les entreprises qui explorent de nouvelles technologies de mise en cache. De plus, il existe une demande croissante d’expertise externe pour obtenir une conception et une mise en œuvre optimales de la mise en cache, ce qui reflète la complexité et les connaissances spécialisées requises pour naviguer efficacement dans ce domaine.</p><p>Le rapport PulseMeter de Techstrong Research souligne le rôle essentiel de la mise en cache des bases de données dans la prise en charge des applications en temps réel et de l’expérience numérique. Alors que les entreprises sont confrontées aux défis de la transformation numérique et à la croissance exponentielle des données, les technologies de mise en cache s’imposent comme des outils essentiels pour améliorer les performances des bases de données, garantir l’évolutivité et répondre aux exigences toujours croissantes des applications et services modernes. Ce passage d’une solution de niche à une nécessité grand public marque une évolution significative des stratégies de gestion des bases de données, soulignant la nécessité d’une innovation et d’une expertise continues dans l’application des technologies de mise en cache.</p><p>Remarque : Redis a sponsorisé le rapport PulseMeter de Techstrong Research, « La mise en cache des bases de données devient courante ».</p>"
JFrog étend ses efforts d'intégration MLOps via Qwak Alliance,"<p>JFrog a annoncé aujourd'hui l'intégration de sa plateforme DevSecOps avec une plateforme d'opérations d'apprentissage automatique gérées (MLOps) de Qwak pour faire progresser la collaboration entre les équipes créant et déployant plusieurs classes d'artefacts logiciels.</p><p>Cette alliance fait suite à une alliance similaire avec Amazon Sagemaker annoncée le mois dernier, qui intégrait également un service géré pour la création de modèles d'intelligence artificielle (IA) fournis par Amazon Web Services (AWS) avec la plateforme JFrog Software Supply Chain.</p><p>Les deux plates-formes d'opérations d'apprentissage automatique (MLOps) fournissent aux équipes de science des données une pile complète d'outils organisés nécessaires à la création de modèles d'IA à partir de zéro, plutôt qu'à la personnalisation de modèles d'IA déjà créés.</p><p>Gal Marder, vice-président exécutif de la stratégie chez JFrog, a déclaré que l'intégration avec la plateforme Qwak permet de gérer les artefacts logiciels créés par les équipes MLOps aux côtés du reste des artefacts logiciels qu'une équipe DevSecOps gère déjà. Cette approche permet également de détecter et de bloquer l'utilisation de modèles ML malveillants en plus de garantir que les modèles sont conformes aux politiques de l'entreprise et aux exigences réglementaires, a-t-il noté.</p><p>Comme il devient évident que davantage de modèles d'IA seront directement intégrés aux applications, la nécessité d'intégrer les flux de travail DevOps aux plateformes d'opérations d'apprentissage automatique (MLOps) utilisées pour créer des modèles d'IA devient de plus en plus prononcée, a déclaré Marder.</p><p>La plateforme JFrog Software Supply Chain peut être utilisée pour fournir aux scientifiques de données et aux développeurs une source unique de vérité pour gérer en toute sécurité les artefacts logiciels à l'aide d'un référentiel commun afin de favoriser une plus grande collaboration entre les équipes qui, dans la plupart des cas, déterminent encore la meilleure façon de collaborer, a-t-il ajouté.</p><p>Le défi est que les équipes de data science forment et déploient généralement des modèles d’IA tous les quelques mois, tandis que les équipes DevSecOps mettent souvent à jour les applications plusieurs fois par mois. Les équipes de data science et DevSecOps ont donc généralement des cultures distinctes, mais à long terme, les flux de travail DevOps et MLOps finiront par fusionner, a déclaré Marder.</p><p>La plupart des entreprises tentent encore de déterminer la meilleure façon d’exploiter l’IA à l’aide de leurs propres données. À terme, les modèles d’IA sont invoqués via des interfaces de programmation d’applications (API). Cependant, ce n’est désormais qu’une question de temps avant que davantage de modèles d’IA soient intégrés aux applications pour améliorer les performances globales. Le défi est que les modèles d’IA ne peuvent pas être mis à jour de la même manière que les autres artefacts logiciels sont corrigés. La gestion des versions des modèles d’IA nécessitera donc un ensemble de contrôles différent lorsqu’un modèle remplace un autre. JFrog, par exemple, a développé des fonctionnalités de gestion des versions qui peuvent être appliquées aux modèles d’IA dans le contexte d’un flux de travail DevOps.</p><p>Les plateformes MLOps ne manquent pas, les équipes DevOps doivent donc s’attendre à voir une vague d’alliances se former entre les fournisseurs de ces plateformes. Il est moins évident de savoir dans quelle mesure ces alliances pourraient conduire à des fusions et acquisitions entre les fournisseurs de ces plateformes.</p><p>D’une manière ou d’une autre, les modèles d’IA arrivent dans les workflows DevSecOps. La seule question à résoudre est de savoir comment gérer au mieux leur déploiement aux côtés de tous les autres types d’artefacts logiciels qui circulent déjà dans les pipelines DevOps existants.</p>"
Ce que l'évolution du matériel spécialisé pour l'IA et le ML signifie pour DevOps,"<p>L’adoption généralisée des technologies d’intelligence artificielle (IA) et d’apprentissage automatique (ML) accélère l’évolution du matériel informatique, essentiel pour automatiser les processus complexes et améliorer la précision de la prise de décision. Cette accélération est cruciale pour faire progresser l’informatique et le traitement des données, en particulier dans les différents segments des pipelines de données IA/ML.</p><p>Cette demande croissante incite un large éventail de fabricants de puces, des acteurs établis aux concurrents émergents, à innover et à jouer un rôle de premier plan dans le développement de solutions de traitement plus rapides et plus efficaces. L’objectif principal ? Concevoir des puces qui optimisent le transfert de données, améliorent la gestion de la mémoire et renforcent l’efficacité énergétique. Ces avancées sont loin d’être progressives ; elles sont essentielles pour répondre aux exigences croissantes des applications sophistiquées d’IA et de ML.</p><p>L’évolution du paysage matériel de DevOps nécessite une plus grande unification et une plus grande automatisation des différentes applications de l’infrastructure d’IA spécialisée. Avec la diversification des architectures de puces, la création de piles d’applications axées sur la portabilité, les performances et la facilité d’accès devient encore plus cruciale. La capacité à adopter de manière transparente plusieurs architectures sera essentielle pour trouver le bon équilibre entre les différentes capacités techniques des personnes qui souhaitent interagir avec la technologie.</p><p>Cet article explore les implications de ces avancées matérielles sur les processus DevOps au sein de l'IA et du ML. En outre, nous explorons les stratégies que les équipes DevOps peuvent envisager pour garantir que les applications restent efficaces et portables sur différentes architectures de puces.</p><p>Par le passé, le matériel à usage général, notamment les GPU et les CPU, était la base de nombreuses charges de travail. Cependant, un changement clair vers le matériel spécialisé est en cours dans l’intelligence artificielle et l’apprentissage automatique, où les exigences en matière de formation et d’inférence ont augmenté de manière exponentielle. Ces applications se heurtent souvent aux limitations de performances inhérentes au matériel traditionnel, en partie en raison des contraintes modernes de la loi de Moore. Par conséquent, il existe un besoin croissant de matériel capable de gérer des tâches d’IA spécifiques avec une efficacité et une rapidité accrues. Par exemple, dans certains scénarios d’apprentissage automatique, l’utilisation de matériel prenant en charge les calculs à virgule flottante en simple précision peut accélérer les processus sans avoir besoin de la précision fournie par les calculs en double précision.</p><p>Même si NVIDIA reste une force dominante sur le marché des puces d’IA, la concurrence s’intensifie et diverses entreprises proposent des alternatives innovantes. Et ce ne sont pas seulement les habituels Intel ou AMD qui sont à l’origine de cette concurrence. Google a également fait des progrès avec ses unités de traitement Tensor (TPU). Amazon a récemment annoncé Trainium2, une nouvelle puce d’IA conçue spécifiquement pour la formation des systèmes d’IA. Cette puce, qui devrait concurrencer Maia de Microsoft et les TPU de Google, souligne la tendance croissante des grandes entreprises technologiques à développer des puces d’IA personnalisées. Au-delà de ces géants, des startups telles que Cerebras, SambaNova Systems, Graphcore et Tenstorrent apportent de nouvelles solutions matérielles d’IA.</p><p>À mesure que le matériel spécialisé devient de plus en plus répandu, la communauté DevOps devra gérer de nouveaux défis, notamment la portabilité des performances. La portabilité des performances consiste à garantir que les applications fonctionnent efficacement et fonctionnent bien sur différentes architectures informatiques avec un minimum de modifications, voire aucune.</p><p>L’informatique cognitive (la catégorie plus large de l’IA et du ML) varie en complexité, en fonction des algorithmes, des modèles et des exigences uniques des créateurs en matière de fonctionnalités spécifiques au matériel. Si une version adaptée à l’architecture optimisera certainement les performances sur les plateformes respectives, elle complique le processus visant à garantir une expérience logicielle cohérente sur différents matériels.</p><p>Le défi de la conception d’un système consiste à optimiser l’environnement pour une efficacité maximale, en particulier lorsque la nature précise de la charge de travail est inconnue de ceux qui sont responsables de la conception et du support des systèmes.</p><p>Bien entendu, les pipelines d’intégration continue et de déploiement continu (CI/CD) doivent également faire l’objet de considérations importantes et connexes. Les subtilités des pipelines CI/CD sont amplifiées lorsque l’on recherche la portabilité des performances. La nécessité de valider les performances logicielles sur plusieurs configurations matérielles introduit une matrice de tests plus élaborée et peut allonger les cycles de déploiement, affectant directement les exigences de mise sur le marché. Soudain, les charges de travail franchissent désormais les limites traditionnelles de l’infrastructure informatique et des hautes performances/supercalculateurs autrefois définies par les technologies de microservices et de traitement par lots ; elles ne font désormais qu’un dans un pipeline CI/CD.</p><p>À mesure que les entreprises adoptent du matériel spécialisé, il existe un risque d’augmentation parallèle du nombre de spécialistes se concentrant uniquement sur un type de matériel ou sur un cas d’utilisation d’application. Si une telle expertise peut favoriser l’innovation et l’optimisation d’une plateforme particulière, elle crée également de la complexité et un risque de silos de connaissances et de complexités inutiles pour les équipes opérationnelles et les clients qui utilisent ces systèmes.</p><p>La portabilité des performances et les concepts étroitement liés, tels que les performances indépendantes du matériel et l'efficacité multiplateforme, sont de plus en plus importants pour les équipes DevOps. À mesure que le paysage technologique évolue, la question urgente devient : comment l'industrie et les équipes DevOps peuvent-elles gérer cette évolution en toute transparence ?</p><p>Les recherches et le développement en cours joueront sans aucun doute un rôle clé. Par exemple, le ministère américain de l’Énergie (DoE) étudie de nouvelles méthodologies pour soutenir son projet de calcul exascale. Il s’agit notamment d’affiner les bibliothèques de logiciels existantes, d’élaborer de nouveaux modèles de programmation et de développer de nouveaux outils qui pourraient éventuellement influencer les pratiques DevOps plus larges. D’autres chercheurs développent des couches d’abstraction logicielle, visant à simplifier l’adaptation de code générique à des configurations matérielles spécifiques.</p><p>Au-delà des nouveaux outils et méthodologies qui peuvent provenir des efforts actuels de R&D, il existe de nombreux outils et processus existants qui se prêtent à l’amélioration de la portabilité des performances, notamment :</p><li>Conteneurisation : les conteneurs encapsulent les applications et leurs dépendances de manière à garantir leur exécution cohérente dans différents environnements. Les outils open source comme SingularityCE avec compatibilité Open Container Initiative (OCI) peuvent aider à standardiser et à simplifier le déploiement sur différentes configurations matérielles, favorisant ainsi la portabilité des performances pour le calcul haute performance et la gestion traditionnelle de l'infrastructure informatique.</li><li>Analyse comparative et profilage : pour garantir la portabilité des performances, il est impératif de comprendre le comportement des logiciels sur différentes architectures. Les outils d’analyse comparative fournissent des mesures quantitatives des performances, tandis que les outils de profilage offrent des informations sur le comportement des logiciels, aidant ainsi les développeurs à identifier les goulots d’étranglement et les domaines nécessitant une optimisation.</li><li>Bibliothèques de portabilité de code : en plus des bibliothèques comme OpenCL qui permettent l'exécution de logiciels sur divers matériels, les avancées récentes dans les technologies de conteneurs complètent cette capacité. Par exemple, les améliorations apportées aux interfaces des périphériques de conteneur, telles que celles de SingularityCE, rationalisent l'intégration de ressources spécifiques au matériel. Ce développement aide les équipes DevOps à optimiser les logiciels pour divers matériels sans réécritures approfondies de la base de code, illustrant les outils prenant en charge la diversité matérielle et l'agilité logicielle dans tous les aspects de l'informatique cognitive.</li><p>Bien entendu, en plus de tous ces outils et stratégies, les méthodologies agiles resteront essentielles car elles privilégient le développement itératif, le feedback/l’amélioration continue et l’adaptabilité, autant d’éléments importants pour les configurations matérielles et logicielles en évolution rapide.</p><p>À l’heure où nous embrassons les nouvelles frontières de l’IA et du ML, le rôle des équipes DevOps dans la navigation dans un paysage matériel en constante évolution devient de plus en plus vital. Au cœur de ce parcours se trouve le formidable défi de la portabilité des applications, un défi qui nécessite une expertise technique et un changement stratégique vers l’adaptabilité. C’est là que les conteneurs apparaissent comme des outils indispensables qui garantissent une expérience et des performances applicatives cohérentes sur diverses plateformes et gèrent les subtilités de la portabilité.</p><p>De même, l’adoption de méthodologies agiles va au-delà de l’adhésion aux processus ; elle incarne un état d’esprit de flexibilité et de réactivité, essentiel dans les changements technologiques rapides. Ces approches, loin d’être des solutions temporaires, font partie intégrante d’une stratégie visant à libérer le potentiel de l’IA. Alors que les équipes DevOps continuent de relever ces défis, leur succès dépendra de leur adaptabilité et de leur volonté d’explorer et d’intégrer les technologies établies et émergentes, en particulier celles qui excellent en termes d’évolutivité et d’efficacité. Cette exploration et cette intégration proactives seront la clé de la survie et de la prospérité dans ce paysage technologique dynamique. Et pourtant, alors que nous avons évoqué ce problème concernant les applications scientifiques, l’impact se fait également sentir du côté AIOps, qui utilise du matériel spécifique à l’IA avec des pratiques AIOps. Bien que notre objectif ici était de discuter des complexités de la création et de l’administration de systèmes d’intelligence artificielle et d’apprentissage automatique, nous n’avons pas encore abordé la conversation sur la sécurité, en particulier avec l’informatique confidentielle, qui est un sujet pour une autre fois. Bon informatique.</p>"
Comment les équipes DevOps peuvent-elles utiliser les données de Customer Intelligence ?,"<p>L’intelligence client consiste à collecter et à analyser des données sur les comportements, les préférences et les besoins des clients. Elle aide les entreprises à mieux comprendre leurs clients et à adapter leurs produits et services pour répondre à leurs demandes. Les données sont collectées à partir de diverses sources, telles que les commentaires des clients, les analyses Web, les médias sociaux, l’historique des achats et les mesures d’utilisation des logiciels. Les informations tirées de l’intelligence client peuvent être utilisées pour éclairer la stratégie commerciale, le développement de produits et les efforts marketing.</p><p>L’objectif de la veille client est de comprendre en détail les besoins, les préférences et les habitudes des clients. Ces informations peuvent être utilisées pour prédire le comportement futur, améliorer le service client et stimuler les ventes. Par exemple, si une société de logiciels sait qu’un segment de clientèle particulier accède principalement à ses logiciels sur des appareils mobiles, l’organisation peut améliorer son support mobile ou créer des applications mobiles dédiées à ce segment de clientèle.</p><p>Une équipe DevOps peut exploiter les données recueillies à partir de l’intelligence client de plusieurs manières.</p><h3>Prioriser le développement des fonctionnalités en fonction des commentaires des clients</h3><p>Lors du développement de nouvelles fonctionnalités, il est utile d’analyser les commentaires des clients pour comprendre quelles fonctionnalités ils trouvent les plus utiles et les hiérarchiser pour le développement. Par exemple, si les commentaires suggèrent que les clients souhaitent une interface utilisateur plus intuitive, l’équipe peut donner la priorité aux améliorations de l’interface utilisateur.</p><p>En se concentrant sur les fonctionnalités que les clients trouvent les plus utiles, les équipes DevOps peuvent améliorer la convivialité et la fonctionnalité de leurs produits. Cela peut conduire à une plus grande satisfaction et fidélité des clients et, en fin de compte, à une augmentation des ventes. De plus, en donnant la priorité au développement de fonctionnalités en fonction des commentaires des clients, les équipes DevOps peuvent éviter de perdre du temps et des ressources sur des fonctionnalités que les clients n’apprécient pas.</p><h3>Adaptation de la conception UX et UI en fonction des préférences et des comportements des clients</h3><p>En analysant les données sur la façon dont les clients interagissent avec leurs produits, les équipes peuvent identifier les points faibles et les domaines à améliorer. Par exemple, si les données montrent que les clients ont des difficultés à naviguer sur le site Web, l'équipe peut repenser la navigation pour la rendre plus conviviale.</p><p>Alternativement, si les clients abandonnent leur panier à un certain moment du processus de paiement, l’équipe peut enquêter et apporter les améliorations nécessaires.</p><h3>Utilisation des données client pour piloter les fonctionnalités de personnalisation et de personnalisation</h3><p>La personnalisation consiste à adapter l'expérience client à l'utilisateur individuel, tandis que la customisation consiste à donner aux clients la possibilité d'adapter le produit ou le service à leurs besoins spécifiques.</p><p>Par exemple, en analysant les données sur l’historique de navigation et d’achat d’un client, une équipe pourrait développer une fonctionnalité qui recommande des produits en fonction du comportement passé du client. Ou, si un client achète fréquemment un certain type de produit, l’équipe pourrait créer une fonctionnalité qui lui permet de personnaliser ce produit à sa guise.</p><h3>Exploiter l'analyse prédictive pour prévoir les besoins et les tendances futurs des clients</h3><p>L'analyse prédictive consiste à utiliser des données historiques pour prédire des événements futurs. Dans le contexte de DevOps, cela peut signifier utiliser les données clients pour prévoir les besoins et les tendances futurs des clients.</p><p>Par exemple, si les données montrent un intérêt croissant pour les produits écologiques, l’équipe pourrait anticiper cette tendance et commencer à développer des produits plus respectueux de l’environnement. L’analyse prédictive peut également aider les équipes à identifier les problèmes potentiels avant qu’ils ne deviennent des problèmes, ce qui leur permet de les résoudre et d’améliorer l’expérience client de manière proactive.</p><p>Voici quelques bonnes pratiques que les équipes DevOps peuvent utiliser pour tirer le meilleur parti de leurs données d’intelligence client.</p><h3>Intégrez les commentaires des clients tôt et souvent</h3><p>Les commentaires des clients issus d’enquêtes, des réseaux sociaux et des interactions avec le service client fournissent des informations précieuses sur les besoins des clients, leurs difficultés et leurs préférences.</p><p>En intégrant ces commentaires dans le processus DevOps, les équipes peuvent mieux comprendre les besoins des clients et développer des produits ou des services qui répondent efficacement à ces besoins. L'intégration précoce des commentaires des clients permet aux équipes de s'adapter et d'apporter des modifications plus rapidement, économisant ainsi du temps et des ressources.</p><p>Il est également important de recueillir régulièrement des retours d’information. Les goûts et les préférences des clients évoluent en permanence, et leurs retours reflètent ces changements. L’intégration fréquente de renseignements sur les clients dans le processus de développement garantit que le travail de l’équipe DevOps reste en phase avec l’évolution du paysage client.</p><h3>Tirer parti des outils d'analyse</h3><p>Les outils d'analyse peuvent aider les équipes DevOps à analyser de grands volumes de données clients et à en extraire des informations exploitables. Ces informations peuvent ensuite être utilisées pour éclairer la prise de décision et stimuler l'innovation.</p><p>Certains outils excellent dans la visualisation des données, aidant les équipes à comprendre des ensembles de données complexes à l'aide de graphiques et de diagrammes. D'autres outils excellent dans l'analyse prédictive, aidant les équipes à prévoir les tendances futures en fonction des données historiques.</p><p>Le choix du bon outil d’analyse dépend des besoins spécifiques de l’équipe DevOps et de la nature des données de renseignement client dont elle dispose. L’essentiel est d’exploiter pleinement ses capacités pour extraire le plus de valeur possible des données de renseignement client.</p><h3>Assurer la qualité et la pertinence des données</h3><p>Toutes les données ne sont pas égales et si elles sont de mauvaise qualité ou non pertinentes, elles peuvent conduire à des décisions erronées et à un gaspillage de ressources.</p><p>La qualité des données fait référence à l’exactitude, à l’exhaustivité, à la cohérence et à la fiabilité des données. Les équipes DevOps doivent mettre en œuvre des contrôles de qualité des données rigoureux pour garantir que les données de renseignement client qu’elles utilisent sont de haute qualité.</p><p>La pertinence des données, quant à elle, fait référence à leur applicabilité à la tâche à accomplir. Toutes les données de veille client ne sont pas pertinentes pour tous les projets DevOps. Les équipes doivent sélectionner soigneusement les données les plus pertinentes pour leur projet spécifique et ignorer le reste.</p><h3>Équilibrer les données quantitatives et qualitatives</h3><p>Alors que les données quantitatives fournissent des chiffres précis et des faits concrets, les données qualitatives offrent des informations plus approfondies sur les attitudes, les perceptions et les comportements des clients.</p><p>Les données quantitatives peuvent aider les équipes DevOps à identifier les tendances, à mesurer les performances et à suivre les progrès au fil du temps. Cependant, elles ne parviennent souvent pas à expliquer pourquoi certaines tendances se produisent ou pourquoi les performances évoluent.</p><p>Cela peut également aider les équipes DevOps à comprendre les raisons qui se cachent derrière les chiffres. Cela peut fournir des informations sur les raisons pour lesquelles les clients se comportent comme ils le font et sur ce qu'ils pensent et ressentent vraiment à propos d'un produit ou d'un service. L'équilibre entre les données quantitatives et qualitatives donne aux équipes DevOps une vision plus globale du paysage client, leur permettant de prendre des décisions plus éclairées.</p><h3>Collaboration interfonctionnelle</h3><p>Les données d'intelligence client sont précieuses pour toutes les équipes de l'entreprise, pas seulement pour DevOps. En collaborant avec d'autres équipes, telles que le marketing, les ventes et le service client, l'équipe DevOps peut obtenir des informations et des perspectives supplémentaires.</p><p>La collaboration interfonctionnelle favorise également une culture de prise de décision basée sur les données dans toute l’organisation. Lorsque toutes les équipes utilisent les informations client pour éclairer leur travail, l’organisation dans son ensemble devient plus centrée sur le client, plus agile et plus innovante.</p><p>L’utilisation réussie des données de renseignement client dans le développement logiciel permet aux équipes de rester réactives aux besoins des clients et d’augmenter la valeur de leurs produits. Les équipes DevOps doivent intégrer les commentaires des clients tôt et souvent, exploiter les outils d’analyse, garantir la qualité et la pertinence des données, équilibrer les données quantitatives et qualitatives et favoriser la collaboration interfonctionnelle. En suivant ces bonnes pratiques, les équipes DevOps peuvent utiliser les données de renseignement client pour stimuler l’innovation et créer des produits et services qui trouvent un véritable écho auprès des clients.</p><p>Source de l'image : Josh Sortino via Unsplash</p>"
Guide DevOps sur le profilage Java,"<p>Le profilage Java est une technique utilisée pour comprendre le comportement détaillé d'une application Java. Il consiste à surveiller et à mesurer divers aspects de l'exécution d'un programme, tels que l'utilisation de la mémoire, l'utilisation du processeur, l'exécution des threads et la récupération de place.</p><p>Le profilage Java peut être utilisé à différentes étapes du cycle de vie du développement logiciel (SDLC). Au cours du développement, il peut aider à identifier les goulots d'étranglement et les points chauds de performances, qui peuvent ensuite être optimisés pour de meilleures performances. Dans la phase de test, le profilage peut être utilisé pour vérifier que l'application fonctionne comme prévu sous charge. Enfin, en production, le profilage peut être utilisé pour surveiller les performances de l'application et détecter les problèmes potentiels avant qu'ils n'affectent les utilisateurs.</p><p>Les outils de profilage Java fournissent des informations sur la machine virtuelle Java (JVM) et l'application qui y est exécutée. Ils permettent aux développeurs de surveiller l'exécution des threads, la création d'objets, la récupération de place et de nombreux autres aspects du fonctionnement de la JVM. En utilisant un profileur Java, les développeurs peuvent acquérir une compréhension approfondie des caractéristiques de performances de l'application et identifier les domaines potentiels d'optimisation.</p><h3>Impact sur l'efficacité et l'évolutivité des applications</h3><p>L'optimisation des performances joue un rôle essentiel dans DevOps en améliorant l'efficacité et l'évolutivité des applications. Les applications efficaces utilisent moins de ressources, ce qui réduit les coûts et permet d'avoir plus d'utilisateurs ou des charges de travail plus importantes. L'évolutivité est la capacité d'une application à gérer des charges de travail accrues sans diminution des performances. En identifiant et en éliminant les goulots d'étranglement des performances, les équipes DevOps peuvent s'assurer que les applications évoluent efficacement à mesure que la demande augmente.</p><h3>Relation entre performance et expérience utilisateur</h3><p>Les performances sont directement liées à l’expérience utilisateur. Une application lente ou peu réactive peut frustrer les utilisateurs et les amener à abandonner complètement l’application. En s’assurant que les applications fonctionnent bien, les équipes DevOps peuvent améliorer la satisfaction et la rétention des utilisateurs. De plus, en surveillant les performances des applications en production, les équipes DevOps peuvent identifier et résoudre de manière proactive les problèmes avant qu’ils n’affectent les utilisateurs.</p><h3>Gestion des coûts et optimisation des ressources</h3><p>L’optimisation des performances peut également contribuer à la gestion des coûts et à l’optimisation des ressources. En optimisant les performances des applications, les équipes DevOps peuvent réduire la quantité de ressources informatiques nécessaires à l’exécution de l’application. Cela peut se traduire par des économies de coûts importantes, en particulier dans les environnements cloud où les coûts sont directement liés à l’utilisation des ressources.</p><p>De plus, en comprenant les modèles d’utilisation des ressources de l’application, les équipes DevOps peuvent prendre des décisions plus éclairées concernant l’allocation des ressources et la planification des capacités. Cela peut aider à éviter le sur-provisionnement, qui conduit à un gaspillage de ressources, ou le sous-provisionnement, qui peut entraîner de mauvaises performances de l’application.</p><h3>Fuites de mémoire</h3><p>Les fuites de mémoire constituent un problème de performances courant dans les applications Java. Une fuite de mémoire se produit lorsqu'une application alloue continuellement de la mémoire mais ne parvient pas à la libérer lorsqu'elle n'est plus nécessaire. Au fil du temps, cela peut entraîner des exceptions OutOfMemoryError et provoquer le blocage de l'application.</p><p>Le profilage Java peut aider à identifier les fuites de mémoire en surveillant l’utilisation de la mémoire par l’application au fil du temps. Si l’utilisation de la mémoire augmente continuellement même lorsque l’application est inactive, cela peut indiquer une fuite de mémoire. Les outils de profilage peuvent également fournir des informations sur les objets qui consomment le plus de mémoire, ce qui peut aider à localiser la source de la fuite.</p><h3>Problèmes de conflit et de synchronisation des threads</h3><p>Les conflits de threads et les problèmes de synchronisation peuvent avoir un impact significatif sur les performances des applications Java. Les conflits de threads se produisent lorsque plusieurs threads tentent d'accéder simultanément à une ressource partagée, ce qui les oblige à attendre et entraîne une baisse des performances. Les problèmes de synchronisation, tels que les blocages et les livelocks, peuvent entraîner le blocage des threads, les empêchant de progresser.</p><p>Le profilage Java peut aider à détecter les conflits de threads et les problèmes de synchronisation en surveillant l'état et l'exécution des threads. Les outils de profilage peuvent afficher les threads en cours d'exécution, en attente ou bloqués et peuvent fournir une trace de pile de chaque thread, ce qui peut aider à identifier la cause des conflits ou des problèmes de synchronisation.</p><h3>Frais généraux de collecte des ordures</h3><p>La récupération de place est un aspect essentiel de la gestion de la mémoire de Java, mais elle peut également être une source importante de perte de performances. Pendant la récupération de place, la JVM interrompt l'exécution de l'application pour récupérer la mémoire des objets qui ne sont plus utilisés. Si la récupération de place se produit trop fréquemment ou prend trop de temps, cela peut entraîner des pauses de l'application et une baisse des performances.</p><p>Le profilage Java peut aider à comprendre et à optimiser le comportement du garbage collection. Les outils de profilage peuvent fournir des informations détaillées sur les événements de garbage collection, tels que leur fréquence, leur durée et la quantité de mémoire récupérée. Ces informations peuvent aider les développeurs à ajuster la configuration du garbage collector pour minimiser son impact sur les performances de l'application.</p><h3>Manque de ressources</h3><p>Le manque de ressources est un autre problème de performances courant dans les applications Java. Il se produit lorsqu'un système ou un processus ne parvient pas à obtenir un accès suffisant aux ressources, ce qui entraîne une baisse des performances ou une défaillance. Dans les applications Java, le manque de ressources peut être causé par des facteurs tels qu'une mémoire insuffisante, un processeur insuffisant, un espace disque insuffisant ou une bande passante réseau insuffisante.</p><p>Le profilage Java peut aider à détecter et à résoudre les problèmes de pénurie de ressources. En surveillant l'utilisation des ressources, les outils de profilage peuvent identifier le moment où les ressources deviennent rares et fournir des informations sur les facteurs contribuant à la pénurie de ressources. Ces informations peuvent aider les développeurs à optimiser l'utilisation des ressources et à garantir que l'application dispose de suffisamment de ressources pour fonctionner efficacement.</p><p>Les outils de profilage Java fournissent généralement les fonctionnalités suivantes :</p><li>Capacité à détecter les fuites de mémoire : les fuites de mémoire peuvent constituer un problème majeur dans les applications Java, car elles peuvent entraîner une erreur de manque de mémoire, perturbant le bon fonctionnement de l'application. Les outils de profilage peuvent aider à identifier ces fuites, permettant aux développeurs de les corriger avant qu'elles ne causent des problèmes plus graves.</li><li>Profilage du processeur : cela permet aux développeurs de voir combien de temps processeur chaque méthode de leur application consomme. En identifiant les méthodes qui consomment une quantité disproportionnée de temps processeur, les développeurs peuvent optimiser leur code pour améliorer ses performances.</li><li>Profilage détaillé des threads : cela peut aider les développeurs à identifier les problèmes de synchronisation, les blocages et autres problèmes potentiels dans les applications multithread. En fournissant ces informations essentielles, les outils de profilage peuvent aider les développeurs à créer des applications plus efficaces et plus performantes.</li><p>Voici quelques bonnes pratiques pour tirer le meilleur parti de vos efforts de profilage Java.</p><h3>Identifier les indicateurs clés de performance</h3><p>Tout d’abord, il est essentiel d’identifier les indicateurs de performance clés les plus pertinents pour votre application. Les indicateurs que vous choisissez dépendent de la nature de votre application et de ses exigences de performance. Parmi les indicateurs courants, citons le temps de réponse, l’utilisation du processeur, l’utilisation de la mémoire et l’activité de récupération de place. En identifiant ces indicateurs clés, vous pouvez utiliser vos outils de profilage pour les surveiller et identifier les problèmes potentiels.</p><h3>Intégration du profilage dans les pipelines CI/CD</h3><p>Une autre bonne pratique consiste à intégrer le profilage dans vos pipelines d’intégration continue/déploiement continu (CI/CD). Cela vous permet de détecter les problèmes de performances dès le début du processus de développement, avant qu’ils ne deviennent plus difficiles à résoudre. En profilant régulièrement votre application pendant le développement, vous pouvez vous assurer que les modifications que vous apportez n’affectent pas ses performances.</p><h3>Profilage dans des scénarios réels</h3><p>Bien qu’il puisse être utile de profiler votre application dans des conditions contrôlées, il est également important de le faire dans des scénarios réels. Cela signifie profiler votre application lorsqu’elle est sous charge, lorsqu’elle traite des données réelles et lorsqu’elle s’exécute sur les mêmes configurations matérielles et logicielles qu’en production. En procédant ainsi, vous obtiendrez une image beaucoup plus précise des performances de votre application dans le monde réel.</p><h3>Collaboration et partage des connaissances</h3><p>Enfin, le profilage Java ne doit pas être une activité solitaire. Il est important de partager vos découvertes et vos idées avec votre équipe et de collaborer pour trouver des solutions aux problèmes de performances que vous identifiez. Ce faisant, vous pouvez créer une culture de sensibilisation aux performances au sein de votre équipe et vous assurer que tout le monde travaille vers l’objectif commun de créer des applications Java efficaces et performantes.</p><p>Le profilage Java est un élément essentiel du processus de développement logiciel. En comprenant ses principales fonctionnalités et en suivant les meilleures pratiques, vous pouvez l’utiliser pour créer des applications efficaces, performantes et capables de répondre aux exigences du monde moderne. Ne sous-estimez donc pas la puissance du profilage : adoptez-le et regardez vos applications Java atteindre de nouveaux sommets de performances.</p>"
Dynatrace étend la portée et la portée des données de sa plateforme d'observabilité,"<p>Lors de son événement Perform 2024 aujourd'hui, Dynatrace a dévoilé un Dynatrace OpenPipeline qui permet d'appliquer des analyses à plusieurs types de sources de données en temps réel.</p><p>La société a également dévoilé une offre d'observabilité des données qui peut être utilisée pour vérifier la qualité et la lignée des données exposées au moteur d'intelligence artificielle (IA) Davis au cœur de la plateforme d'observabilité Dynatrace afin de réduire les faux positifs qui pourraient autrement générer une alerte, en plus de contribuer à réduire le volume de données qui pourraient devoir être stockées.</p><p>Enfin, Dynatrace a annoncé aujourd’hui qu’elle étendait la portée de sa plateforme d’observabilité aux grands modèles de langage (LLM) utilisés pour créer des plateformes d’IA génératives. Cette capacité permettra, par exemple, de simplifier le suivi de la consommation de jetons utilisés pour donner accès à ces modèles.</p><p>Steve Tack, vice-président senior des produits chez Dynatrace, a déclaré que Dynatrace OpenPipeline permettra aux organisations de rationaliser la collecte de données d'une manière qui leur permettra d'appliquer l'observabilité plus largement en appliquant des algorithmes de traitement de flux à des pétaoctets de données.</p><p>Prévue pour être disponible dans 90 jours, la fonctionnalité Dynatrace OpenPipeline permet aux équipes informatiques d'ingérer et d'acheminer les données d'observabilité, de sécurité et d'événements commerciaux à partir de n'importe quelle source et dans n'importe quel format au point d'ingestion, y compris les données non structurées qui sont automatiquement converties en un format exploitable. Ces données peuvent ensuite être enrichies pour permettre des analyses plus approfondies.</p><p>Cela offre également aux équipes informatiques davantage de contrôle sur les données qu'elles analysent, stockent ou excluent des analyses, ce qui contribue à réduire le coût total de l'observabilité, a noté Tack.</p><p>Enfin, cela offre aux équipes informatiques la possibilité d’appliquer des contrôles de sécurité et de confidentialité plus personnalisables à la manière dont ces données sont utilisées, a-t-il ajouté.</p><p>Dynatrace adopte une approche multimodale de l’IA qui englobe des modèles prédictifs, causaux et génératifs. Collectivement, cette approche simplifie l’identification de la cause profonde des problèmes, l’identification des anomalies susceptibles de perturber les services et la rationalisation de la gestion des incidents, par exemple en fournissant des résumés des événements dans un format en langage naturel.</p><p>On ne sait pas encore dans quelle mesure l’observabilité sera appliquée au-delà des données collectées pour gérer les flux de travail DevOps et d’autres processus informatiques, mais il est clair qu’il existe une forte corrélation entre les événements informatiques et les résultats commerciaux à mesure que les organisations deviennent plus dépendantes des logiciels. À mesure que l’IA devient de plus en plus utilisée, il devrait devenir plus simple d’appliquer l’analyse à un éventail beaucoup plus large de types de données pour faire apparaître la relation entre les événements informatiques et les processus commerciaux. En effet, Dynatrace simplifie l’application des meilleures pratiques d’ingénierie des données pour collecter, gérer et analyser ces données, a noté Tack.</p><p>En attendant, les équipes DevOps doivent revoir la manière dont leurs pipelines sont actuellement construits pour simplifier la capture de toutes les données pertinentes. Si les données de télémétrie collectées à partir de la plateforme DevOps sont essentielles pour garantir que le développement et la livraison des applications se déroulent le plus rapidement possible, ces données ne sont qu’un facteur dans une équation plus vaste. Le défi et l’opportunité à présent consistent à déterminer la meilleure façon d’appliquer l’IA pour corréler tout cela.</p>"
Prévisions technologiques pour les infrastructures en 2024,"<p>Ganesh Srinivasan, associé chez Venrock, est co-auteur de cet article.</p><p>2023 a été une année de montagnes russes sans précédent ; de la mort de l’étalement des piles de données modernes à la naissance de l’IA générative, nous ne sommes qu’au début d’une nouvelle ère dans « l’art du possible ». Nous vous garantissons que 2024 ne sera pas une déception.</p><p>À l’approche d’une nouvelle année, c’est le moment idéal pour examiner ce que nous prévoyons être les plus grands développements de l’année à venir. Voici ce qui, selon nous, va se passer en 2024 :</p><p>1. Le règne d’OpenAI remis en question</p><p>Avec les apprentissages émergents dans les architectures de réseaux neuronaux de base qui ont conduit au transformateur et à la domination d'OpenAI, il est probable que leur sortie imminente de GPT5 sera dépassée dans des tests de performance spécifiques par un nouvel entrant sur la base d'architectures plus efficaces, de capacités multimodales améliorées, d'une meilleure compréhension contextuelle du monde et d'un apprentissage par transfert amélioré. Ces nouveaux modèles seront construits sur la recherche émergente dans les réseaux spatiaux, les structures de graphes et les combinaisons de divers réseaux neuronaux qui conduiront à des capacités plus efficaces, polyvalentes et puissantes.</p><p>2. Apple : le nouveau leader de l'IA générative</p><p>L’un des acteurs les plus importants dans le domaine de l’IA générative commence tout juste à montrer ses cartes. 2024 sera l’année où Apple lancera son premier ensemble de fonctionnalités d’IA générative, libérant ainsi le véritable potentiel d’une architecture fermée d’IA en périphérie avec un accès complet à vos données personnelles – prouvant qu’Apple est en fait l’entreprise la plus importante dans la course à l’IA générative.</p><p>3. Construire en donnant la priorité au client</p><p>La dernière décennie a vu l’abandon des clients lourds au profit du rendu et du calcul côté serveur. Mais le monde revient au client. Les expériences mobiles devront fonctionner en mode hors ligne. Les expériences en temps réel nécessitent des transactions à très faible latence. L’exécution des LLM devra de plus en plus s’exécuter sur l’appareil pour augmenter les performances et réduire les coûts.</p><p>4. La mort de l’étalement des infrastructures de données</p><p>La croissance rapide des besoins en infrastructures de données des entreprises a conduit à une multiplication des solutions ponctuelles, des catalogues de données à la gouvernance des données, en passant par l'extraction inverse, la transformation, le chargement et les alternatives Airflow, jusqu'aux bases de données vectorielles et à un autre lakehouse. Le pendule reviendra vers des plateformes unifiées et moins de silos pour réduire le coût total de possession et les frais généraux d'exploitation d'ici 2024.</p><p>5. Approche de l'hiver de l'IA</p><p>En 2023, l’IA générative pourrait être qualifiée d’« art du possible », 2024 étant le véritable test pour voir si les prototypes se transforment en cas d’utilisation en production. Le pic du cycle de battage médiatique étant probablement atteint ici, 2024 connaîtra la phase de désillusion où les entreprises découvriront où l’IA générative peut créer un impact positif sur les marges et où les coûts l’emportent sur les avantages.</p><p>6. La menace de la désinformation</p><p>Si les modèles de diffusion d’images et de vidéos ont ouvert une nouvelle ère pour la création numérique et l’expression artistique, il ne fait aucun doute que leur côté obscur n’a pas encore fait de ravages. À l’approche d’une élection présidentielle, les modèles de diffusion en tant que machine de désinformation politique émergeront pour devenir la prochaine arme de désinformation majeure.</p><p>7. La percée de l’IA dans le monde réel</p><p>L’ère du « champ des rêves » pour l’IA sera révolue et 2024 marquera une avancée majeure pour les cas d’utilisation commerciale de l’IA, en particulier dans le monde physique. L’utilisation de l’IA pour les modalités du monde physique nous permettra de changer et d’interagir avec les machines et les appareils qui nous entourent, de la compréhension de la télémétrie des machines pour la détection des problèmes à la distillation d’informations à partir de milliers de flux de capteurs en temps réel.</p><p>8. S3 : La menace grandissante pour Snowflake</p><p>AWS S3 est sur le point de devenir le plus grand concurrent de Snowflake. Avec un écosystème d’outils en pleine croissance émergeant à mesure que les couches de calcul au-dessus de S3 tirent parti de leurs coûts ultra-faibles, de leur haute disponibilité, de la prise en charge de la zone de disponibilité et du nouveau service de latence à un chiffre en millisecondes, le « problème de coût de Snowflake » de tout le monde est sur le point de se voir offrir un nouvel ensemble d’alternatives attrayantes.</p><p>9. L'essor de Flink et de l'analyse opérationnelle</p><p>Si l’analyse commerciale a été à l’origine de la première vague d’évolution de l’infrastructure de données, l’analyse opérationnelle sera le moteur de la prochaine vague, avec Apache Flink au cœur de celle-ci. Les entreprises commenceront à adopter en masse des fonctionnalités en temps réel et en streaming pour commencer à utiliser les données afin de prendre des décisions opérationnelles. Cela créera une nouvelle valeur dans des domaines tels que la logistique, les transactions financières, la gestion des stocks et la détection des fraudes.</p><p>10. La pile d'IA du développeur</p><p>En 2024, l’impact le plus significatif de l’IA dans le monde numérique sera de débloquer la productivité des développeurs. De la génération de code au débogage en passant par la gestion des builds, l’IA ouvrira un nouveau niveau de productivité des développeurs que nous n’avons pas vu depuis l’avènement du cloud.</p><p>Si vous êtes d'accord, pas d'accord ou souhaitez partager vos propres prédictions pour 2024 avec nous, veuillez nous contacter sur X à @ethanjb et @gan3sh.</p>"
