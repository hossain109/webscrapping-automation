Title,Content
Optimisation des tests ETL pour une meilleure qualité et fiabilité des données,"[{'', '<p>Avez-vous déjà pris une décision commerciale sur la base de données inexactes ou incomplètes ? Dans le monde actuel axé sur les données, la qualité et la fiabilité de vos données peuvent faire la différence entre votre stratégie commerciale et la vôtre. Les entreprises s’appuyant de plus en plus sur les données pour guider leurs décisions, il est plus important que jamais de s’assurer que ces données sont exactes, cohérentes et fiables. C’est là qu’entrent en jeu les tests ETL (Extract, Transform, Load).</p>'}, {'', ""<p>Les processus ETL sont essentiels pour déplacer des données provenant de diverses sources vers un système unifié, mais sans tests rigoureux, même de petites erreurs dans ces processus peuvent entraîner des problèmes de qualité des données importants. Ce blog met en évidence les meilleures pratiques, les défis courants et les solutions innovantes pour optimiser les tests ETL, vous aidant à préserver l'intégrité de vos données et à prendre des décisions en toute confiance.</p>""}, {'', '<h3>Libérer la puissance des tests ETL</h3>'}, {'', ""<p>Dans le domaine de la gestion des données, les tests ETL constituent la pierre angulaire de l'assurance qualité des données. Leur importance ne peut être surestimée, car ils servent de gardien de l'intégrité des données tout au long du pipeline de données. Voici quelques raisons pour lesquelles les tests ETL sont essentiels :</p>""}, {'', ""<li>Validation de l'exactitude des données : vérifie méticuleusement que la transformation des données est exécutée correctement, garantissant que les données chargées dans les systèmes cibles sont exactes et fiables.</li>""}, {'', ""<li>Conformité aux règles métier : confirme que les données adhèrent aux règles et normes métier prédéfinies, préservant ainsi la cohérence au sein de l'organisation.</li>""}, {'', ""<li>Assurance de l'intégration des données : valide l'intégration transparente des données provenant de sources disparates, essentielle pour une veille économique et des analyses complètes.</li>""}, {'', '<li>Détection et prévention des erreurs : en identifiant les problèmes au début du pipeline de données, les tests ETL empêchent la propagation d’erreurs pouvant conduire à des décisions commerciales coûteuses.</li>'}, {'', '<li>Support de gouvernance des données : il joue un rôle essentiel dans le maintien des politiques de gouvernance des données en garantissant la qualité, la sécurité et la conformité des données tout au long du processus ETL.</li>'}, {'', '<p>« D’ici 2028, les outils basés sur GenAI seront capables d’écrire 70 % des tests logiciels, réduisant ainsi le besoin de tests manuels et améliorant la couverture des tests, la convivialité des logiciels et la qualité du code ». IDC.</p>'}, {'', '<h3>Défis pour atteindre la qualité des données</h3>'}, {'', ""<p>Bien que la recherche de données de haute qualité grâce aux tests ETL soit cruciale, les organisations se retrouvent souvent confrontées à un paysage complexe rempli d'obstacles. Ces défis découlent de la nature complexe des écosystèmes de données modernes, du rythme rapide des changements technologiques et des demandes toujours croissantes d'informations basées sur les données. Comprendre ces obstacles est la première étape vers l'élaboration de stratégies robustes pour les surmonter et atteindre le niveau de qualité des données souhaité. Les principaux défis des tests ETL auxquels les organisations sont confrontées sont les suivants :</p>""}, {'', '<li>Volume et diversité des données : les méthodes de test traditionnelles ont souvent du mal à gérer des téraoctets ou des pétaoctets de données, ce qui entraîne des cycles de test prolongés et des contraintes de ressources. De plus, les données se présentent sous différents formats structurés, semi-structurés et non structurés, chacun nécessitant des approches de traitement et de validation différentes.</li>'}, {'', ""<li>Transformations complexes : les transformations de données complexes sont difficiles à tester de manière approfondie, en particulier lorsqu'elles impliquent plusieurs règles métier et cas limites. Les transformations conditionnelles complexes créent une multitude de chemins de données possibles, ce qui rend difficile le test complet de tous les scénarios.</li>""}, {'', '<li>Contraintes de temps : la demande croissante de traitement des données en temps réel ou quasi réel exerce une pression sur les équipes de test pour valider la qualité des données à grande vitesse. Équilibrer le temps consacré au développement et aux tests conduit souvent à des compromis dans la couverture des tests.</li>'}, {'', ""<li>Différences d'environnement : les variations de puissance de traitement, de mémoire ou de stockage entre les environnements de test et de production peuvent masquer des problèmes de qualité des données liés aux performances. De même, les incohérences dans les versions ou les configurations logicielles entre les environnements peuvent entraîner un comportement inattendu en production.</li>""}, {'', ""<li>Manque de visibilité de bout en bout : il peut être extrêmement difficile de suivre les données via des processus ETL complexes pour identifier la cause profonde des problèmes de qualité. En outre, une surveillance inadéquate de l'ensemble du pipeline de données peut entraîner des angles morts où les problèmes de qualité des données passent inaperçus.</li>""}, {'', ""<li>Évolution du paysage des données : l'ajout fréquent de nouvelles sources de données nécessite des mises à jour constantes des processus ETL et des cas de test correspondants. L'évolution des réglementations en matière de confidentialité et de conformité des données nécessite des ajustements continus des procédures de traitement et de test des données.</li>""}, {'', '<h3>Stratégies pour obtenir des données de type production</h3>'}, {'', '<p>Pour garantir l’efficacité des tests ETL, il est essentiel de travailler avec des données qui ressemblent étroitement aux données de production. Voici quelques stratégies pour obtenir efficacement des données de type production\xa0:</p>'}, {'', ""<li>Sous-ensemble de données : créez des sous-ensembles représentatifs de données de production qui conservent les caractéristiques et les complexités de l'ensemble de données complet.</li>""}, {'', '<li>Masquage des données : mettez en œuvre des techniques de masquage des données robustes pour protéger les informations sensibles tout en préservant les propriétés statistiques des données.</li>'}, {'', '<li>Génération de données synthétiques : utilisez des algorithmes avancés pour générer des données synthétiques qui reflètent les modèles et les distributions des données de production.</li>'}, {'', '<li>Actualisation incrémentielle des données : mettez à jour les données de test avec de nouvelles données de production pour garantir la pertinence et capturer de nouveaux modèles de données.</li>'}, {'', '<li>Copies de données virtuelles : exploitez les technologies de virtualisation de bases de données pour créer des copies légères et à jour des données de production à des fins de test.</li>'}, {'', '<li>Profilage et analyse des données : effectuez un profilage approfondi des données pour comprendre les caractéristiques des données de production et les reproduire dans des environnements de test.</li>'}, {'', '<h3>Bonnes pratiques pour les tests ETL</h3>'}, {'', '<li>Établissez des objectifs de test clairs : définissez des objectifs spécifiques et mesurables pour chaque phase de test afin de garantir une couverture complète.</li>'}, {'', '<li>Implémenter le contrôle de version : utilisez des systèmes de contrôle de version pour suivre les modifications dans les processus ETL et les cas de test, facilitant ainsi le dépannage et les restaurations.</li>'}, {'', ""<li>Automatisez les tests répétitifs : exploitez les outils d'automatisation des tests pour exécuter des tests de routine, libérant ainsi des ressources pour des scénarios de test plus complexes.</li>""}, {'', ""<li>Priorisez les cas de test : concentrez-vous sur les éléments de données critiques et les zones à haut risque pour maximiser l'impact des efforts de test.</li>""}, {'', '<li>Mettre en œuvre des tests continus : intégrez les tests tout au long du cycle de développement pour découvrir et résoudre les problèmes le plus tôt possible.</li>'}, {'', '<li>Documentez minutieusement : conservez une documentation détaillée des cas de test, des résultats et de tout problème de qualité des données découvert pendant les tests.</li>'}, {'', '<li>Collaborer entre les équipes : Favorisez une collaboration étroite entre les ingénieurs de données, les testeurs et</li>'}, {'', '<h3>Types de tests ETL</h3>'}, {'', ""<p>Les tests ETL sont essentiels pour garantir l'exactitude et l'intégrité des données lors de leur transfert de leur source d'origine à leur destination. Ce processus implique une série de contrôles et de validations pour détecter les erreurs, les incohérences et d'autres problèmes lors des étapes d'extraction, de transformation et de chargement des données. Pour gérer efficacement cela, les tests ETL sont classés en différents types, chacun ciblant des aspects spécifiques du pipeline de données.</p>""}, {''}, {'', '<h3>Avantages des tests ETL automatisés</h3>'}, {'', ""<p>Les tests ETL automatisés sont devenus une véritable révolution pour les entreprises. Ils offrent une solution puissante aux défis liés à la garantie de la qualité des données à grande échelle. Alors que les entreprises sont confrontées à des volumes de données croissants, à des transformations complexes et à la nécessité d'obtenir des informations rapides, l'automatisation des tests se distingue par son efficacité et sa fiabilité. En exploitant des outils et des technologies de pointe, les tests ETL automatisés accélèrent non seulement le processus de test, mais améliorent également sa précision et son exhaustivité. Grâce à l'automatisation, les entreprises peuvent bénéficier des avantages suivants :</p>""}, {'', '<li>Couverture de test accrue : permet des tests plus complets sur une plus large gamme de scénarios et de variations de données.</li>'}, {'', '<li>Exécution plus rapide : les tests automatisés peuvent être exécutés rapidement et fréquemment, permettant une identification rapide des problèmes.</li>'}, {'', '<li>Cohérence et fiabilité : élimine les erreurs humaines et garantit une exécution cohérente des cas de test.</li>'}, {'', ""<li>Évolutivité : offre la possibilité d'augmenter ou de diminuer la capacité sans effort, ce qui est particulièrement avantageux pour gérer les charges de pointe ou les exigences de test fluctuantes.</li>""}, {'', ""<li>Efficacité et flexibilité améliorées : en automatisant les tâches répétitives, les testeurs peuvent se concentrer sur des activités de test plus complexes et à forte valeur ajoutée. Cela permet le déploiement et la gestion dynamiques des machines virtuelles, du stockage et des réseaux, accélérant ainsi le processus de test et facilitant l'itération et l'expérimentation rapides.</li>""}, {'', ""<li>Détection précoce des problèmes : des tests automatisés continus tout au long du processus de développement permettent d'identifier les problèmes plus tôt, réduisant ainsi le coût des correctifs.</li>""}, {'', '<li>Rapports améliorés : les outils de test automatisés fournissent souvent des rapports et des analyses détaillés, offrant des informations plus approfondies sur les résultats et les tendances des tests.</li>'}, {'', '<li>Maintenance plus facile : les tests automatisés bien conçus sont plus faciles à mettre à jour et à entretenir à mesure que les processus ETL évoluent.</li>'}, {'', ""<h3>Assurez l'avenir de votre stratégie de données avec un partenaire de test ETL compétent</h3>""}, {'', '<p>L’optimisation des tests ETL n’est pas seulement une nécessité technique ; c’est un impératif stratégique pour toute organisation qui s’appuie sur les données pour prendre des décisions. AgreeYa est l’une de ces options qui aident les organisations en proposant des stratégies de test robustes, en surmontant les défis courants et en tirant parti de l’automatisation.</p>'}]"
Naviguer dans les eaux agiles : pourquoi l'intégration de Copilot exige des ajustements méthodologiques,"[{'', ""<p>Le Congrès américain a récemment interdit à son personnel d’utiliser l’IA Copilot de Microsoft, un chatbot intégré à grand modèle de langage qui permet l’automatisation des produits Microsoft tels que Word, Excel, PowerPoint, Outlook et Teams, en invoquant des problèmes de sécurité. Et ils ne sont pas les seuls à penser ainsi, car de nombreux professionnels oscillent entre enthousiasme et crainte lorsqu’il est question de l’IA. En attendant, Microsoft a vanté avec assurance Copilot, promettant que la puissance de son IA réduira le travail quotidien de gestion d’une entreprise. L’entreprise est passée du simple discours sur l’IA à l’intégration de celle-ci dans toutes les couches de sa pile technologique. Son introduction récente de Copilot Runtime permet même aux développeurs d’utiliser l’IA dans leurs propres programmes, il n’est donc pas surprenant que la plupart des directeurs des systèmes d’information expérimentent Copilot. Ses promesses de productivité et l’élargissement du champ d’application et les économies de coûts qui en résultent pour un réinvestissement potentiel sont trop alléchantes pour être ignorées. Microsoft montre certainement l’exemple, et la seule question est de savoir comment les autres devraient suivre. Au cours de la dernière décennie, les organisations agiles ont surpassé les autres en prenant et en gérant les décisions plus rapidement. L'adoption de l'agilité dans le domaine informatique a entraîné des changements dans la manière dont les infrastructures, les applications, les données et les compétences sont produites, consommées ou les deux. Les éléments fondamentaux de l'agilité, tels que la collaboration, l'automatisation et les améliorations continues, sont les principales sources d'innovation pertinentes pour les applications, le développement et le déploiement.</p>""}, {'', '<p>Les événements récents ont perturbé la progression agile dans le domaine informatique. L’ère de flexibilité induite par la COVID-19 dans la manière, le moment et le lieu de travail remet en question le statu quo dans nos modes de collaboration. En outre, l’IA générative et les complexités associées à la gouvernance informatique ont également bouleversé la progression agile. Par conséquent, une économie incrémentale a été créée qui oblige chaque entreprise à prendre les opportunités et les défis plus au sérieux.</p>'}, {'', ""<p>L'agilité distribuée est déjà en pratique depuis un certain temps. Les réalités du travail à distance et hybride ne sont que des extensions de ce que nous avons déjà vu dans les équipes distribuées. Cependant, les promesses de productivité d'outils tels que Copilot sont nouvelles, donc supposer que les pratiques Agile actuelles fonctionneront avec les pratiques GenAI est une erreur. Alors que pouvons-nous faire ?</p>""}, {'', '<p>Voici quelques réflexions sur la manière d’intégrer la méthodologie agile dans le cadre de l’adoption de Copilot.</p>'}, {'', '<h3>Étendre DevOps pour inclure la représentation de DataOps et MLOps</h3>'}, {'', '<p>Étant donné l’importance des données ainsi que des modèles d’IA et d’apprentissage automatique, les équipes DevOps doivent inclure des représentants des équipes DataOps et MLOps (ModelOps est un sous-ensemble de MLOps). Ce n’est qu’à ce moment-là que l’objectif de rapprocher la « production » et les « opérations » peut être atteint. À première vue, le remplacement peut sembler être la plus grande menace de l’IA, mais son premier acte sera plutôt de révéler et d’approfondir les fissures dans la collaboration.</p>'}, {'', '<h3>L’intelligence logicielle est plus importante que jamais</h3>'}, {'', '<p>Ne pas comprendre les systèmes d’application de manière globale avant de produire automatiquement le code en production sera désastreux. L’informatique d’entreprise est un mélange d’applications et d’actifs informatiques IA et non IA. De plus, l’« explicabilité » du code ne peut être obtenue que lorsque l’intelligence logicielle sur le code produit par GenAI est atteinte. En fin de compte, ce n’est pas l’exactitude fonctionnelle, mais l’adéquation architecturale qui compte le plus pour débloquer des améliorations de productivité. L’IA évolue rapidement, mais laisser la vitesse prendre trop de priorité ouvre la voie à l’échec.</p>'}, {'', '<h3>La conformité continue et la sécurité continue sont tout aussi importantes</h3>'}, {'', '<p>L’une des principales préoccupations des outils GenAI concerne les vulnérabilités que le code généré automatiquement peut introduire dans l’informatique des entreprises, ce qui est la principale raison pour laquelle le Congrès américain a interdit l’utilisation de Copilot. Il est important de procéder aux ajustements appropriés au niveau des modules pour la conformité et la sécurité, afin qu’ils soient conçus et livrés en continu, plutôt que d’être vérifiés et assurés périodiquement. La réglementation est connue pour être à l’origine de l’innovation, et les entreprises doivent délibérément anticiper les difficultés futures.</p>'}, {'', '<h3>Augmenter les portes de qualité dans votre pipeline CI/CD pour les assistants IA</h3>'}, {'', ""<p>Les principes fondateurs de l'open source (transparence, inspection et adaptation) peuvent être étendus aux produits GenAI. L'« inspection » ne doit pas seulement couvrir les aspects qualité, performance, sécurité et UX du code fourni par les outils, mais également l'adéquation architecturale au sein de l'informatique de l'entreprise.</p>""}, {'', '<h3>Mesurez le succès et soyez transparent sur vos lacunes</h3>'}, {'', '<p>L’impact de l’IA peut être flou, mais certains résultats doivent être mesurables pour justifier son adoption. L’élaboration d’indicateurs de performance clés spécifiques à l’IA peut contribuer à consolider le rôle du copilote au sein de l’équipe. Trouver les bons indicateurs à mesurer est de la plus haute importance et constitue en soi un défi de taille.</p>'}, {'', '<p>Dans l’analyse détaillée de l’impact de l’IA, il est également important d’accepter ouvertement les défauts. Le système est, bien sûr, imparfait par nature, et ces imperfections doivent être suivies et corrigées. L’IA évoluant si rapidement, de nombreux problèmes seront probablement résolus à court terme. Notez les défauts et faites-en un examen régulier.</p>'}, {'', '<h3>L’inadéquation des compétences aura un impact sur les promesses de productivité</h3>'}, {'', ""<p>Les outils ne sont efficaces que si ceux qui les utilisent le savent. Un développeur expérimenté peut démontrer un niveau de productivité supérieur à la moyenne avec un assistant IA, mais un développeur inexpérimenté peut rapidement créer plus de problèmes que de solutions. Former les communautés de développement et d'assurance qualité à maîtriser les outils et les directives de gouvernance du code et des tests nécessite d'ajuster le modèle opérationnel Agile.</p>""}, {'', '<p>N’oubliez pas que les équipes DevOps sont bien plus que des développeurs, les équipes DataOps bien plus que des ingénieurs de données et les équipes ModelOps bien plus que des data scientists. Les compétences interdisciplinaires des équipes DevOps vont considérablement évoluer lorsque l’IA fera partie de la conversation. À mesure que les frontières entre ces disciplines s’estomperont, ceux qui seront prêts à s’adapter se hisseront au sommet.</p>'}, {'', '<p>L’adaptation de la méthodologie aux défis perçus ne doit pas limiter les avantages potentiels que GenAI peut produire. S’il est utilisé correctement, GenAI peut contribuer à l’hyper-automatisation des tâches de développement et d’assurance qualité, à l’évaluation des options de conception grâce au prototypage rapide, à la simplification du processus de documentation, à la surveillance de l’environnement de production pour prévoir les goulots d’étranglement des performances, etc.</p>'}, {'', '<p>Si nous n’adaptons pas nos méthodes agiles pour répondre à ces nouvelles réalités et dégager de la valeur plus rapidement, le « time to market » et les avantages en termes de coûts associés seront mal perçus. Les changements dans la méthodologie agile sont inévitables, car GenAI et Agile offrent de réels avantages concurrentiels. Ne vous laissez pas aller.</p>'}]"
CISA : une faille critique de Jenkins exploitée dans des attaques de ransomware,"[{'', '<p>Une faille de sécurité critique dans le populaire serveur d’automatisation open source Jenkins figure sur la liste des vulnérabilités connues de la Cybersecurity and Infrastructure Security Agency (CISA) après avoir été exploitée dans un ransomware et d’autres attaques.</p>'}, {'', ""<p>L'agence de cybersécurité la plus importante du gouvernement américain a ajouté le bug - identifié comme CVE-2024-23897 et avec un score de gravité CVSS de 9,8 sur 10 - à son catalogue de vulnérabilités exploitées connues, ce qui met en garde les agences fédérales contre la nécessité de sécuriser leurs serveurs Jenkins, bien que la CISA ait également averti toutes les organisations exploitant de tels serveurs de s'assurer qu'ils sont sécurisés.</p>""}, {'', ""<p>La vulnérabilité dans l'interface de ligne de commande (CLI) de Jenkins est une faille de parcours de chemin causée par une faiblesse dans l'analyseur de commandes args4j, qui peut être exploitée par des acteurs malveillants pour obtenir l'exécution de code à distance (RCE) et pour lire des fichiers arbitraires sur le serveur Jenkins.</p>""}, {'', ""<p>Le serveur Jenkins basé sur Java, maintenu par CloudBees et la communauté Jenkins, est utilisé par les développeurs dans leur intégration continue et leur développement continu (CI/CD) et automatise les étapes du cycle de vie du développement logiciel, y compris le développement et le déploiement. L'outil, soutenu par des sociétés telles qu'Amazon Web Services (AWS), GitHub et JFrog, compte plus d'un million d'utilisateurs.</p>""}, {'', '<h3>Une faille devient publique et est corrigée</h3>'}, {'', '<p>Yaniv Nizry, chercheur en vulnérabilité chez SonarSource, développeur de logiciels open source, a été le premier à signaler cette faille de sécurité en janvier, soulignant qu’avec une part de marché d’environ 44 %, « la popularité de Jenkins est évidente. Cela signifie que l’impact potentiel des vulnérabilités de sécurité de Jenkins est important ».</p>'}, {'', ""<p>Un correctif a été publié en janvier avec Jenkins 2.442, LTS 2.426.3 en désactivant la fonctionnalité d'analyse de commandes, les responsables expliquant que Jenkins est livré avec une CLI intégrée pour accéder à Jenkins à partir d'un environnement de script ou de shell. Il utilise la bibliothèque args4j pour analyser les arguments et les options de commande sur le contrôleur Jenkins lors du traitement des commandes CLI.</p>""}, {'', ""<p>« Cet analyseur de commandes possède une fonctionnalité qui remplace un caractère @ suivi d'un chemin de fichier dans un argument par le contenu du fichier (expandAtFiles) », ont écrit les responsables. « Cela permet aux attaquants de lire des fichiers arbitraires sur le système de fichiers du contrôleur Jenkins en utilisant l'encodage de caractères par défaut du processus du contrôleur Jenkins. »</p>""}, {'', '<h3>Les cybercriminels se lancent</h3>'}, {'', ""<p>Des preuves de concept (POC) auraient commencé à émerger peu après la publication du correctif par Jenkins. Les chercheurs de Trend Micro ont signalé en mars qu'ils avaient observé plusieurs attaques exploitant la faille, 28 des 44 adresses IP sources des attaques provenant des Pays-Bas, les autres de pays comme Singapour et l'Allemagne. La plupart des cibles se trouvaient en Afrique du Sud.</p>""}, {'', '<p>Ils ont également constaté des cas où des exploits RCE étaient échangés.</p>'}, {'', '<p>D’autres chercheurs ont découvert des attaques plus récentes exploitant la vulnérabilité de Jenkins. En juillet, CloudSEK a signalé une attaque de la chaîne d’approvisionnement contre Born Group, une agence internationale de conseil et d’expérience client basée à New York, par le groupe de menaces IntelBroker, spécialisé dans les violations de données, l’extorsion et la vente d’accès à des systèmes compromis.</p>'}, {'', ""<p>Les chercheurs de CloudSEK ont déclaré qu'IntelBroker avait exploité CVE-2024-23897 pour obtenir un accès initial via un serveur Jenkins vulnérable avant d'accéder au référentiel GitHub de Born Group.</p>""}, {'', '<h3>Attaque de ransomware en Inde</h3>'}, {'', ""<p>Plus tôt ce mois-ci, les chercheurs du Juniper Threat Lab ont écrit sur une attaque de ransomware contre Brontoo Technology Solutions, une société de services et de conseil informatique en Inde qui collabore avec C-Edge Technologies, une coentreprise entre Tata Consultancy Services et la State Bank of India. Juniper et CloudSEK ont attribué l'attaque au groupe de ransomware RansomXXX, qui existe depuis 2018, opère depuis la Russie ou l'Europe de l'Est et cible les agences gouvernementales, les banques et les organismes de santé.</p>""}, {'', '<p>L’attaque a perturbé les paiements de détail dans les banques indiennes. Une fois encore, les acteurs malveillants ont obtenu un accès initial à l’environnement informatique de Brontoo via la vulnérabilité Jenkins.</p>'}, {'', '<p>« Cette vulnérabilité permet à un utilisateur non authentifié de lire les premières lignes de n’importe quel fichier du système de fichiers », ont écrit les chercheurs. « Elle existe parce que la fonction intégrée de l’analyseur de commandes n’a pas été désactivée par défaut. Si elle est exploitée avec succès, cette vulnérabilité peut entraîner la fuite de fichiers et de données sensibles, l’exécution potentielle de commandes et permettre une attaque par ransomware. »</p>'}]"
Le succès d'AIOps nécessite des données de télémétrie Internet synthétiques,"[{'', '<p>Toute forme d’intelligence artificielle (IA) n’est efficace que si les données utilisées pour la former sont exactes. Si les entreprises souhaitent appliquer efficacement l’IA aux opérations informatiques (ITOps), elles doivent collecter autant de données de télémétrie que possible.</p>'}, {'', '<p>Les équipes informatiques découvrent souvent que leur plateforme AIOps a été formée sur une base restreinte de données de télémétrie. Ces données peuvent avoir été collectées, par exemple, à partir d’une plateforme DevOps qui n’a pas une visibilité complète sur l’environnement informatique distribué dans lequel s’exécute leur application. En l’absence de données de télémétrie synthétiques collectées à partir d’une plateforme de surveillance des performances Internet (IPM) pour collecter ces données de télémétrie, il est tout simplement peu probable que les algorithmes d’apprentissage automatique qui sont au cœur de toute plateforme AIOps parviennent à faire émerger les meilleures recommandations pour optimiser les expériences applicatives.</p>'}, {'', '<p>Le défi réside dans la nature probabiliste de l’IA. La pertinence des recommandations qui en découlent est déterminée par la qualité des données qui ont été exposées au modèle d’IA. Les données réelles des utilisateurs, par exemple, peuvent être rares, voire inexistantes. Bien entendu, si les données de télémétrie n’ont jamais été partagées avec le modèle d’IA, il est extrêmement peu probable que les recommandations de l’IA améliorent l’expérience des applications.</p>'}, {'', '<h3>Gagner en visibilité grâce aux données synthétiques</h3>'}, {'', '<p>Les équipes informatiques doivent s’assurer que les données utilisées pour former le modèle d’IA reflètent les environnements de production dans lesquels les applications sont déployées. Sinon, quel que soit le niveau de développement d’un modèle d’IA, les données qui entrent sont toujours synonymes de données qui sortent. Avant qu’une équipe informatique n’adopte une plateforme AIOps, elle doit connaître la provenance des données du modèle d’IA sous-jacent. Si le pool de données de formation de l’IA est limité, les recommandations générées le seront également. Les équipes informatiques ne vont pas faire confiance aux plateformes AIOps qui leur conseillent de prendre des mesures spécifiques sur la base de données partielles ou incomplètes. Et elles ne devraient pas le faire.</p>'}, {'', '<p>Au lieu de cela, les équipes supposeront que chaque résultat doit être vérifié avant que l’étape suivante d’un processus ne soit autorisée. Après tout, la seule chose pire que de se tromper en matière d’informatique et d’IA est de se tromper à une échelle catastrophique. Bien entendu, continuer à gérer l’informatique de manière séquentielle va sans doute à l’encontre de l’objectif d’investir dans une plateforme AIOps censée gérer les tâches en parallèle.</p>'}, {'', '<p>Étant donné la dépendance des applications modernes aux services Internet, tout effort visant à appliquer l’IA à la gestion informatique sans inclure de données de télémétrie Internet synthétiques conduira à un résultat sous-optimal. En incluant ce type de télémétrie, les informations fournies aux équipes DevOps leur permettront de garantir que les indicateurs clés de performance (KPI) sont atteints et maintenus.</p>'}, {""<h3>Plusieurs modèles d'IA</h3>"", ''}, {'', '<p>Il est peu probable qu’un seul modèle d’IA puisse tout contrôler. Dans de nombreux cas, les plateformes de réseau, de sécurité et de gestion des services informatiques (ITSM) auront déjà appliqué l’IA aux données de télémétrie qu’elles collectent en temps réel. Les résultats de ces modèles d’IA seront ensuite partagés avec les plateformes AIOps pour automatiser une série de tâches de bout en bout qui, auparavant, nécessitaient que les équipes informatiques orchestrent les flux de travail sur plusieurs îlots d’automatisation.</p>'}, {'', '<p>Les équipes DevOps doivent donc évaluer l’efficacité de ce qui sera bientôt un réseau de modèles d’IA. Il en existe tellement, chacun étant ou sera conçu pour automatiser des tâches spécifiques, comme l’analyse du trafic Internet pour identifier la source des goulots d’étranglement qui pourraient n’avoir qu’un impact intermittent sur une application. Armée de ces informations, la plateforme AIOps peut alors générer systématiquement des recommandations utiles auxquelles les équipes DevOps peuvent faire confiance. Elles peuvent ensuite laisser les outils appliquer automatiquement la suggestion sur la manière dont, par exemple, le trafic Internet doit être réacheminé pour maintenir les objectifs de niveau de service (SLO).</p>'}, {'<h3>Réaliser la promesse de l’IA</h3>', ''}, {'', '<p>À mesure que les plateformes AIOps s’améliorent, elles peuvent réduire considérablement la charge de travail que les équipes DevOps rencontrent régulièrement. Les équipes peuvent passer des semaines à essayer de déterminer la cause profonde d’un problème qui, une fois découvert, peut prendre quelques minutes à résoudre. Le défi est que la source du problème n’a souvent pas grand-chose à voir avec ce que l’équipe DevOps contrôle immédiatement, comme c’est le cas lorsque, par exemple, la latence créée par un service Internet a un impact négatif sur les performances de l’application. Cependant, ces informations devraient permettre aux équipes DevOps d’envoyer des demandes d’assistance qui identifient mieux la source exacte d’un problème de service Internet que leur fournisseur devrait être en mesure de résoudre plus rapidement. Tout aussi important, l’équipe DevOps peut passer aux problèmes sur lesquels elle a un contrôle plus direct.</p>'}, {'', '<p>Une grande partie du stress que subit une équipe DevOps provient du fait qu’elle ne connaît pas la véritable cause d’un problème qui, malgré tous ses efforts, continue de générer une série d’alertes en continu. L’AIOps promet de réduire ce stress en simplifiant d’abord la corrélation des causes, puis en automatisant la correction. Cette promesse ne sera toutefois jamais pleinement tenue si les données sur lesquelles repose l’entraînement du modèle d’IA ne fournissent pas suffisamment d’informations pour prendre une décision véritablement éclairée.</p>'}]"
Sumo Logic élimine les frais d'ingestion pour les données du journal d'observabilité,"[{'', ""<p>Sumo Logic a annoncé cette semaine qu'il ne facturerait plus de frais pour l'ingestion de données de journaux dans sa plateforme d'observabilité afin d'encourager les équipes DevOps à appliquer l'analyse plus en profondeur.</p>""}, {'', ""<p>Michael Cucchi, vice-président du marketing produit chez Sumo Logic, a déclaré que le plan de licences flexibles de Sumo Logic élimine ce qui équivaut à une taxe pour l'utilisation d'une plateforme d'observabilité. Les licences flexibles sont immédiatement disponibles pour les nouveaux clients et seront proposées aux clients Sumo Logic existants plus tard dans l'année. Il n'y a pas de frais mensuels cachés, de restrictions de fonctionnalités, de compromis de performances ou de limitations d'utilisateurs appliqués dans le cadre de ce plan.</p>""}, {'', ""<p>Le plan de licence ne s'applique qu'aux données de log, mais les équipes DevOps pourront ingérer toutes les données structurées, semi-structurées et non structurées pour un coût inférieur que Sumo Logic s'efforce de stocker plus efficacement, a déclaré Cucchi. L'objectif global est de mettre à disposition une approche disruptive de la gestion des licences qui réduit le coût total de l'observabilité, a-t-il ajouté.</p>""}, {'', '<p>Le stockage des données de journalisation va devenir de plus en plus crucial à mesure que les entreprises commenceront à appliquer plusieurs types de modèles d’intelligence artificielle (IA) pour automatiser davantage les opérations informatiques, a-t-il ajouté. Ces modèles d’IA promettent de rendre les plateformes d’observabilité plus accessibles, car les algorithmes seront capables de faire apparaître les problèmes qui doivent être résolus. Historiquement, la valeur d’une plateforme d’observabilité a été liée à la compétence d’une équipe DevOps dans l’utilisation de son langage de requête.</p>'}, {'', '<p>Les équipes informatiques qui ont adopté des plateformes d’observabilité ont, à des degrés divers, limité la quantité de données qu’elles collectent et conservent pour éviter que les coûts de stockage n’explosent. Le défi est que, à mesure que les équipes DevOps déploient davantage d’applications basées sur des microservices, la quantité de données de journal générées a explosé. Déterminer la cause profonde d’un problème impliquant ces applications peut s’avérer problématique si les données de journal ne sont pas facilement disponibles. Les données de journal sont, après tout, l’unité atomique de l’observabilité, a noté Cucchi.</p>'}, {'', '<p>En général, les environnements informatiques deviennent trop complexes pour être gérés par des humains sans l’aide de plateformes d’observabilité complétées par des modèles d’IA, a noté Cucchi. Chaque environnement d’application est assez unique, il est donc essentiel que ces modèles d’IA soient exposés à autant de données que possible pour garantir des résultats optimaux.</p>'}, {'', '<p>Il n’est pas certain que les plateformes d’observabilité puissent un jour remplacer la plupart des outils de surveillance sur lesquels les équipes informatiques s’appuient aujourd’hui pour suivre un ensemble de mesures prédéfinies. Cependant, une chose est sûre : les plateformes d’observabilité offrent au moins l’occasion de rationaliser certains de ces outils. Le plus grand défi consiste à trouver des moyens de financer l’acquisition d’une plateforme d’observabilité en premier lieu, ce qui crée ensuite une opportunité de consolider ces outils par la suite.</p>'}, {'', '<p>Étant donné la complexité des plateformes informatiques hautement distribuées, ce n’est qu’une question de temps avant que la plupart des équipes informatiques ne disposent d’une certaine capacité d’observabilité. Bien entendu, l’observabilité a toujours été un principe fondamental de DevOps. Le problème est que les outils utilisés pour y parvenir sont des plateformes de surveillance héritées qui n’offrent pas le niveau de profondeur requis pour gérer avec succès les environnements d’application modernes.</p>'}, {'', ""<p>Source de l'image\xa0: Joshua Sortino via Unsplash.</p>""}]"
Techstrong Research PulseMeter : la mise en cache transforme les performances des applications,"[{'', '<p>La demande d’accès instantané aux données et d’expériences numériques fluides n’a jamais été aussi forte. Le récent rapport PulseMeter de Techstrong Research, « Database Caching Hits the Mainstream », se penche sur l’adoption de la mise en cache des bases de données et son rôle essentiel dans l’amélioration des performances et de l’évolutivité des applications.</p>'}, {'', ""<p>Alors que le volume et l'utilisation des données augmentent à un rythme sans précédent, les développeurs de logiciels et les professionnels des bases de données recherchent constamment des solutions innovantes pour améliorer la lecture/écriture afin d'offrir une efficacité et des performances optimales. La mise en cache des bases de données est passée de son statut de niche à celui de stratégie courante pour les organisations qui souhaitent accroître les performances des applications, relever les défis de l'évolutivité et améliorer la résilience des applications et des bases de données dans des conditions de fonctionnement fluctuantes.</p>""}, {'', ""<p>Les entreprises numériques et les attentes des clients rendent les applications plus nécessaires pour fournir un accès aux données en temps réel ou quasi réel. Cette nécessité est particulièrement aiguë dans les applications mobiles et Web, les environnements de données distribués, l'architecture de microservices cloud-native et les cas d'utilisation à forte demande comme les jeux, qui nécessitent tous des niveaux de performance et de fiabilité des bases de données sans précédent.</p>""}, {'', '<p>Le rapport PulseMeter souligne que l’amélioration de la latence et des performances grâce à la mise en cache des bases de données peut considérablement accroître les revenus et favoriser des expériences numériques positives. À l’inverse, l’absence de mise en œuvre de ces améliorations peut faire la différence entre le succès et l’échec d’une organisation. La mise en cache des bases de données est passée d’une technique spécialisée pour des cas d’utilisation spécifiques à un aspect fondamental de l’optimisation des performances des bases de données et des applications. Elle est désormais considérée comme une technologie fiable et éprouvée qui répond aux défis croissants liés au volume de données et aux exigences analytiques des applications contemporaines.</p>'}, {''}, {'', '<p>Les données recueillies par Techstrong Research auprès de DevOps, de développeurs de logiciels, de professionnels des bases de données, d’ingénieurs SRE, d’ingénieurs de plateforme et d’autres parties prenantes révèlent des informations essentielles sur l’adoption et l’impact de la technologie de mise en cache. 64,8 % des répondants utilisent déjà la mise en cache, et 13,8 % supplémentaires évaluent son intégration dans leurs opérations. Cette adoption significative souligne le rôle essentiel de la mise en cache dans l’infrastructure informatique actuelle, avec des solutions de pointe comme Amazon ElastiCache, Redis et NGINX détenant collectivement plus de 40 % des parts de marché.</p>'}, {'', '<p>Parmi les technologies à l’origine de l’essor de la mise en cache des bases de données, Redis est bien connu pour sa polyvalence et ses performances. Disponible à la fois en version open source et commerciale, Redis est parfaitement adapté à un large éventail de cas d’utilisation, de la mise en cache de base aux structures de données avancées, aux opérations, aux analyses en temps réel et à la gestion des files d’attente. Ce qui distingue Redis, ce sont les améliorations de performances qu’il offre, permettant des applications en temps réel dans les jeux, les services financiers, la santé et d’autres secteurs. Le fort taux d’adoption de Redis, comme le souligne le rapport PulseMeter de Techstrong Research, souligne son rôle central dans l’acceptation et la mise en œuvre généralisées des technologies de mise en cache des bases de données.</p>'}, {'', '<p>L’étude identifie les performances et la fiabilité comme les principales considérations pour les entreprises qui explorent de nouvelles technologies de mise en cache. De plus, il existe une demande croissante d’expertise externe pour obtenir une conception et une mise en œuvre optimales de la mise en cache, ce qui reflète la complexité et les connaissances spécialisées requises pour naviguer efficacement dans ce domaine.</p>'}, {'', '<p>Le rapport PulseMeter de Techstrong Research souligne le rôle essentiel de la mise en cache des bases de données dans la prise en charge des applications en temps réel et de l’expérience numérique. Alors que les entreprises sont confrontées aux défis de la transformation numérique et à la croissance exponentielle des données, les technologies de mise en cache s’imposent comme des outils essentiels pour améliorer les performances des bases de données, garantir l’évolutivité et répondre aux exigences toujours croissantes des applications et services modernes. Ce passage d’une solution de niche à une nécessité grand public marque une évolution significative des stratégies de gestion des bases de données, soulignant la nécessité d’une innovation et d’une expertise continues dans l’application des technologies de mise en cache.</p>'}, {'', '<p>Remarque\xa0: Redis a sponsorisé le rapport PulseMeter de Techstrong Research, «\xa0La mise en cache des bases de données devient courante\xa0».</p>'}]"
JFrog étend ses efforts d'intégration MLOps via Qwak Alliance,"[{'', ""<p>JFrog a annoncé aujourd'hui l'intégration de sa plateforme DevSecOps avec une plateforme d'opérations d'apprentissage automatique gérées (MLOps) de Qwak pour faire progresser la collaboration entre les équipes créant et déployant plusieurs classes d'artefacts logiciels.</p>""}, {'', ""<p>Cette alliance fait suite à une alliance similaire avec Amazon Sagemaker annoncée le mois dernier, qui intégrait également un service géré pour la création de modèles d'intelligence artificielle (IA) fournis par Amazon Web Services (AWS) avec la plateforme JFrog Software Supply Chain.</p>""}, {'', ""<p>Les deux plates-formes d'opérations d'apprentissage automatique (MLOps) fournissent aux équipes de science des données une pile complète d'outils organisés nécessaires à la création de modèles d'IA à partir de zéro, plutôt qu'à la personnalisation de modèles d'IA déjà créés.</p>""}, {'', ""<p>Gal Marder, vice-président exécutif de la stratégie chez JFrog, a déclaré que l'intégration avec la plateforme Qwak permet de gérer les artefacts logiciels créés par les équipes MLOps aux côtés du reste des artefacts logiciels qu'une équipe DevSecOps gère déjà. Cette approche permet également de détecter et de bloquer l'utilisation de modèles ML malveillants en plus de garantir que les modèles sont conformes aux politiques de l'entreprise et aux exigences réglementaires, a-t-il noté.</p>""}, {'', ""<p>Comme il devient évident que davantage de modèles d'IA seront directement intégrés aux applications, la nécessité d'intégrer les flux de travail DevOps aux plateformes d'opérations d'apprentissage automatique (MLOps) utilisées pour créer des modèles d'IA devient de plus en plus prononcée, a déclaré Marder.</p>""}, {'', ""<p>La plateforme JFrog Software Supply Chain peut être utilisée pour fournir aux scientifiques de données et aux développeurs une source unique de vérité pour gérer en toute sécurité les artefacts logiciels à l'aide d'un référentiel commun afin de favoriser une plus grande collaboration entre les équipes qui, dans la plupart des cas, déterminent encore la meilleure façon de collaborer, a-t-il ajouté.</p>""}, {'', '<p>Le défi est que les équipes de data science forment et déploient généralement des modèles d’IA tous les quelques mois, tandis que les équipes DevSecOps mettent souvent à jour les applications plusieurs fois par mois. Les équipes de data science et DevSecOps ont donc généralement des cultures distinctes, mais à long terme, les flux de travail DevOps et MLOps finiront par fusionner, a déclaré Marder.</p>'}, {''}, {'', '<p>La plupart des entreprises tentent encore de déterminer la meilleure façon d’exploiter l’IA à l’aide de leurs propres données. À terme, les modèles d’IA sont invoqués via des interfaces de programmation d’applications (API). Cependant, ce n’est désormais qu’une question de temps avant que davantage de modèles d’IA soient intégrés aux applications pour améliorer les performances globales. Le défi est que les modèles d’IA ne peuvent pas être mis à jour de la même manière que les autres artefacts logiciels sont corrigés. La gestion des versions des modèles d’IA nécessitera donc un ensemble de contrôles différent lorsqu’un modèle remplace un autre. JFrog, par exemple, a développé des fonctionnalités de gestion des versions qui peuvent être appliquées aux modèles d’IA dans le contexte d’un flux de travail DevOps.</p>'}, {'', '<p>Les plateformes MLOps ne manquent pas, les équipes DevOps doivent donc s’attendre à voir une vague d’alliances se former entre les fournisseurs de ces plateformes. Il est moins évident de savoir dans quelle mesure ces alliances pourraient conduire à des fusions et acquisitions entre les fournisseurs de ces plateformes.</p>'}, {'', '<p>D’une manière ou d’une autre, les modèles d’IA arrivent dans les workflows DevSecOps. La seule question à résoudre est de savoir comment gérer au mieux leur déploiement aux côtés de tous les autres types d’artefacts logiciels qui circulent déjà dans les pipelines DevOps existants.</p>'}]"
Ce que l'évolution du matériel spécialisé pour l'IA et le ML signifie pour DevOps,"[{'', '<p>L’adoption généralisée des technologies d’intelligence artificielle (IA) et d’apprentissage automatique (ML) accélère l’évolution du matériel informatique, essentiel pour automatiser les processus complexes et améliorer la précision de la prise de décision. Cette accélération est cruciale pour faire progresser l’informatique et le traitement des données, en particulier dans les différents segments des pipelines de données IA/ML.</p>'}, {'', '<p>Cette demande croissante incite un large éventail de fabricants de puces, des acteurs établis aux concurrents émergents, à innover et à jouer un rôle de premier plan dans le développement de solutions de traitement plus rapides et plus efficaces. L’objectif principal ? Concevoir des puces qui optimisent le transfert de données, améliorent la gestion de la mémoire et renforcent l’efficacité énergétique. Ces avancées sont loin d’être progressives ; elles sont essentielles pour répondre aux exigences croissantes des applications sophistiquées d’IA et de ML.</p>'}, {'', '<p>L’évolution du paysage matériel de DevOps nécessite une plus grande unification et une plus grande automatisation des différentes applications de l’infrastructure d’IA spécialisée. Avec la diversification des architectures de puces, la création de piles d’applications axées sur la portabilité, les performances et la facilité d’accès devient encore plus cruciale. La capacité à adopter de manière transparente plusieurs architectures sera essentielle pour trouver le bon équilibre entre les différentes capacités techniques des personnes qui souhaitent interagir avec la technologie.</p>'}, {'', ""<p>Cet article explore les implications de ces avancées matérielles sur les processus DevOps au sein de l'IA et du ML. En outre, nous explorons les stratégies que les équipes DevOps peuvent envisager pour garantir que les applications restent efficaces et portables sur différentes architectures de puces.</p>""}, {'', '<p>Par le passé, le matériel à usage général, notamment les GPU et les CPU, était la base de nombreuses charges de travail. Cependant, un changement clair vers le matériel spécialisé est en cours dans l’intelligence artificielle et l’apprentissage automatique, où les exigences en matière de formation et d’inférence ont augmenté de manière exponentielle. Ces applications se heurtent souvent aux limitations de performances inhérentes au matériel traditionnel, en partie en raison des contraintes modernes de la loi de Moore. Par conséquent, il existe un besoin croissant de matériel capable de gérer des tâches d’IA spécifiques avec une efficacité et une rapidité accrues. Par exemple, dans certains scénarios d’apprentissage automatique, l’utilisation de matériel prenant en charge les calculs à virgule flottante en simple précision peut accélérer les processus sans avoir besoin de la précision fournie par les calculs en double précision.</p>'}, {'', '<p>Même si NVIDIA reste une force dominante sur le marché des puces d’IA, la concurrence s’intensifie et diverses entreprises proposent des alternatives innovantes. Et ce ne sont pas seulement les habituels Intel ou AMD qui sont à l’origine de cette concurrence. Google a également fait des progrès avec ses unités de traitement Tensor (TPU). Amazon a récemment annoncé Trainium2, une nouvelle puce d’IA conçue spécifiquement pour la formation des systèmes d’IA. Cette puce, qui devrait concurrencer Maia de Microsoft et les TPU de Google, souligne la tendance croissante des grandes entreprises technologiques à développer des puces d’IA personnalisées. Au-delà de ces géants, des startups telles que Cerebras, SambaNova Systems, Graphcore et Tenstorrent apportent de nouvelles solutions matérielles d’IA.</p>'}, {'', '<p>À mesure que le matériel spécialisé devient de plus en plus répandu, la communauté DevOps devra gérer de nouveaux défis, notamment la portabilité des performances. La portabilité des performances consiste à garantir que les applications fonctionnent efficacement et fonctionnent bien sur différentes architectures informatiques avec un minimum de modifications, voire aucune.</p>'}, {'', '<p>L’informatique cognitive (la catégorie plus large de l’IA et du ML) varie en complexité, en fonction des algorithmes, des modèles et des exigences uniques des créateurs en matière de fonctionnalités spécifiques au matériel. Si une version adaptée à l’architecture optimisera certainement les performances sur les plateformes respectives, elle complique le processus visant à garantir une expérience logicielle cohérente sur différents matériels.</p>'}, {'', '<p>Le défi de la conception d’un système consiste à optimiser l’environnement pour une efficacité maximale, en particulier lorsque la nature précise de la charge de travail est inconnue de ceux qui sont responsables de la conception et du support des systèmes.</p>'}, {'', '<p>Bien entendu, les pipelines d’intégration continue et de déploiement continu (CI/CD) doivent également faire l’objet de considérations importantes et connexes. Les subtilités des pipelines CI/CD sont amplifiées lorsque l’on recherche la portabilité des performances. La nécessité de valider les performances logicielles sur plusieurs configurations matérielles introduit une matrice de tests plus élaborée et peut allonger les cycles de déploiement, affectant directement les exigences de mise sur le marché. Soudain, les charges de travail franchissent désormais les limites traditionnelles de l’infrastructure informatique et des hautes performances/supercalculateurs autrefois définies par les technologies de microservices et de traitement par lots ; elles ne font désormais qu’un dans un pipeline CI/CD.</p>'}, {'', '<p>À mesure que les entreprises adoptent du matériel spécialisé, il existe un risque d’augmentation parallèle du nombre de spécialistes se concentrant uniquement sur un type de matériel ou sur un cas d’utilisation d’application. Si une telle expertise peut favoriser l’innovation et l’optimisation d’une plateforme particulière, elle crée également de la complexité et un risque de silos de connaissances et de complexités inutiles pour les équipes opérationnelles et les clients qui utilisent ces systèmes.</p>'}, {'', ""<p>La portabilité des performances et les concepts étroitement liés, tels que les performances indépendantes du matériel et l'efficacité multiplateforme, sont de plus en plus importants pour les équipes DevOps. À mesure que le paysage technologique évolue, la question urgente devient : comment l'industrie et les équipes DevOps peuvent-elles gérer cette évolution en toute transparence ?</p>""}, {'', '<p>Les recherches et le développement en cours joueront sans aucun doute un rôle clé. Par exemple, le ministère américain de l’Énergie (DoE) étudie de nouvelles méthodologies pour soutenir son projet de calcul exascale. Il s’agit notamment d’affiner les bibliothèques de logiciels existantes, d’élaborer de nouveaux modèles de programmation et de développer de nouveaux outils qui pourraient éventuellement influencer les pratiques DevOps plus larges. D’autres chercheurs développent des couches d’abstraction logicielle, visant à simplifier l’adaptation de code générique à des configurations matérielles spécifiques.</p>'}, {'', '<p>Au-delà des nouveaux outils et méthodologies qui peuvent provenir des efforts actuels de R&D, il existe de nombreux outils et processus existants qui se prêtent à l’amélioration de la portabilité des performances, notamment :</p>'}, {'', ""<li>Conteneurisation : les conteneurs encapsulent les applications et leurs dépendances de manière à garantir leur exécution cohérente dans différents environnements. Les outils open source comme SingularityCE avec compatibilité Open Container Initiative (OCI) peuvent aider à standardiser et à simplifier le déploiement sur différentes configurations matérielles, favorisant ainsi la portabilité des performances pour le calcul haute performance et la gestion traditionnelle de l'infrastructure informatique.</li>""}, {'', '<li>Analyse comparative et profilage : pour garantir la portabilité des performances, il est impératif de comprendre le comportement des logiciels sur différentes architectures. Les outils d’analyse comparative fournissent des mesures quantitatives des performances, tandis que les outils de profilage offrent des informations sur le comportement des logiciels, aidant ainsi les développeurs à identifier les goulots d’étranglement et les domaines nécessitant une optimisation.</li>'}, {'', ""<li>Bibliothèques de portabilité de code : en plus des bibliothèques comme OpenCL qui permettent l'exécution de logiciels sur divers matériels, les avancées récentes dans les technologies de conteneurs complètent cette capacité. Par exemple, les améliorations apportées aux interfaces des périphériques de conteneur, telles que celles de SingularityCE, rationalisent l'intégration de ressources spécifiques au matériel. Ce développement aide les équipes DevOps à optimiser les logiciels pour divers matériels sans réécritures approfondies de la base de code, illustrant les outils prenant en charge la diversité matérielle et l'agilité logicielle dans tous les aspects de l'informatique cognitive.</li>""}, {'', '<p>Bien entendu, en plus de tous ces outils et stratégies, les méthodologies agiles resteront essentielles car elles privilégient le développement itératif, le feedback/l’amélioration continue et l’adaptabilité, autant d’éléments importants pour les configurations matérielles et logicielles en évolution rapide.</p>'}, {'', '<p>À l’heure où nous embrassons les nouvelles frontières de l’IA et du ML, le rôle des équipes DevOps dans la navigation dans un paysage matériel en constante évolution devient de plus en plus vital. Au cœur de ce parcours se trouve le formidable défi de la portabilité des applications, un défi qui nécessite une expertise technique et un changement stratégique vers l’adaptabilité. C’est là que les conteneurs apparaissent comme des outils indispensables qui garantissent une expérience et des performances applicatives cohérentes sur diverses plateformes et gèrent les subtilités de la portabilité.</p>'}, {'', '<p>De même, l’adoption de méthodologies agiles va au-delà de l’adhésion aux processus ; elle incarne un état d’esprit de flexibilité et de réactivité, essentiel dans les changements technologiques rapides. Ces approches, loin d’être des solutions temporaires, font partie intégrante d’une stratégie visant à libérer le potentiel de l’IA. Alors que les équipes DevOps continuent de relever ces défis, leur succès dépendra de leur adaptabilité et de leur volonté d’explorer et d’intégrer les technologies établies et émergentes, en particulier celles qui excellent en termes d’évolutivité et d’efficacité. Cette exploration et cette intégration proactives seront la clé de la survie et de la prospérité dans ce paysage technologique dynamique. Et pourtant, alors que nous avons évoqué ce problème concernant les applications scientifiques, l’impact se fait également sentir du côté AIOps, qui utilise du matériel spécifique à l’IA avec des pratiques AIOps. Bien que notre objectif ici était de discuter des complexités de la création et de l’administration de systèmes d’intelligence artificielle et d’apprentissage automatique, nous n’avons pas encore abordé la conversation sur la sécurité, en particulier avec l’informatique confidentielle, qui est un sujet pour une autre fois. Bon informatique.</p>'}]"
Comment les équipes DevOps peuvent-elles utiliser les données de Customer Intelligence ?,"[{'', '<p>L’intelligence client consiste à collecter et à analyser des données sur les comportements, les préférences et les besoins des clients. Elle aide les entreprises à mieux comprendre leurs clients et à adapter leurs produits et services pour répondre à leurs demandes. Les données sont collectées à partir de diverses sources, telles que les commentaires des clients, les analyses Web, les médias sociaux, l’historique des achats et les mesures d’utilisation des logiciels. Les informations tirées de l’intelligence client peuvent être utilisées pour éclairer la stratégie commerciale, le développement de produits et les efforts marketing.</p>'}, {'', '<p>L’objectif de la veille client est de comprendre en détail les besoins, les préférences et les habitudes des clients. Ces informations peuvent être utilisées pour prédire le comportement futur, améliorer le service client et stimuler les ventes. Par exemple, si une société de logiciels sait qu’un segment de clientèle particulier accède principalement à ses logiciels sur des appareils mobiles, l’organisation peut améliorer son support mobile ou créer des applications mobiles dédiées à ce segment de clientèle.</p>'}, {'', '<p>Une équipe DevOps peut exploiter les données recueillies à partir de l’intelligence client de plusieurs manières.</p>'}, {'', '<h3>Prioriser le développement des fonctionnalités en fonction des commentaires des clients</h3>'}, {'', '<p>Lors du développement de nouvelles fonctionnalités, il est utile d’analyser les commentaires des clients pour comprendre quelles fonctionnalités ils trouvent les plus utiles et les hiérarchiser pour le développement. Par exemple, si les commentaires suggèrent que les clients souhaitent une interface utilisateur plus intuitive, l’équipe peut donner la priorité aux améliorations de l’interface utilisateur.</p>'}, {'', '<p>En se concentrant sur les fonctionnalités que les clients trouvent les plus utiles, les équipes DevOps peuvent améliorer la convivialité et la fonctionnalité de leurs produits. Cela peut conduire à une plus grande satisfaction et fidélité des clients et, en fin de compte, à une augmentation des ventes. De plus, en donnant la priorité au développement de fonctionnalités en fonction des commentaires des clients, les équipes DevOps peuvent éviter de perdre du temps et des ressources sur des fonctionnalités que les clients n’apprécient pas.</p>'}, {'', '<h3>Adaptation de la conception UX et UI en fonction des préférences et des comportements des clients</h3>'}, {'', ""<p>En analysant les données sur la façon dont les clients interagissent avec leurs produits, les équipes peuvent identifier les points faibles et les domaines à améliorer. Par exemple, si les données montrent que les clients ont des difficultés à naviguer sur le site Web, l'équipe peut repenser la navigation pour la rendre plus conviviale.</p>""}, {'', '<p>Alternativement, si les clients abandonnent leur panier à un certain moment du processus de paiement, l’équipe peut enquêter et apporter les améliorations nécessaires.</p>'}, {'', '<h3>Utilisation des données client pour piloter les fonctionnalités de personnalisation et de personnalisation</h3>'}, {'', ""<p>La personnalisation consiste à adapter l'expérience client à l'utilisateur individuel, tandis que la customisation consiste à donner aux clients la possibilité d'adapter le produit ou le service à leurs besoins spécifiques.</p>""}, {'', '<p>Par exemple, en analysant les données sur l’historique de navigation et d’achat d’un client, une équipe pourrait développer une fonctionnalité qui recommande des produits en fonction du comportement passé du client. Ou, si un client achète fréquemment un certain type de produit, l’équipe pourrait créer une fonctionnalité qui lui permet de personnaliser ce produit à sa guise.</p>'}, {'', ""<h3>Exploiter l'analyse prédictive pour prévoir les besoins et les tendances futurs des clients</h3>""}, {'', ""<p>L'analyse prédictive consiste à utiliser des données historiques pour prédire des événements futurs. Dans le contexte de DevOps, cela peut signifier utiliser les données clients pour prévoir les besoins et les tendances futurs des clients.</p>""}, {'', '<p>Par exemple, si les données montrent un intérêt croissant pour les produits écologiques, l’équipe pourrait anticiper cette tendance et commencer à développer des produits plus respectueux de l’environnement. L’analyse prédictive peut également aider les équipes à identifier les problèmes potentiels avant qu’ils ne deviennent des problèmes, ce qui leur permet de les résoudre et d’améliorer l’expérience client de manière proactive.</p>'}, {'', '<p>Voici quelques bonnes pratiques que les équipes DevOps peuvent utiliser pour tirer le meilleur parti de leurs données d’intelligence client.</p>'}, {'<h3>Intégrez les commentaires des clients tôt et souvent</h3>', ''}, {'', '<p>Les commentaires des clients issus d’enquêtes, des réseaux sociaux et des interactions avec le service client fournissent des informations précieuses sur les besoins des clients, leurs difficultés et leurs préférences.</p>'}, {'', ""<p>En intégrant ces commentaires dans le processus DevOps, les équipes peuvent mieux comprendre les besoins des clients et développer des produits ou des services qui répondent efficacement à ces besoins. L'intégration précoce des commentaires des clients permet aux équipes de s'adapter et d'apporter des modifications plus rapidement, économisant ainsi du temps et des ressources.</p>""}, {'', '<p>Il est également important de recueillir régulièrement des retours d’information. Les goûts et les préférences des clients évoluent en permanence, et leurs retours reflètent ces changements. L’intégration fréquente de renseignements sur les clients dans le processus de développement garantit que le travail de l’équipe DevOps reste en phase avec l’évolution du paysage client.</p>'}, {""<h3>Tirer parti des outils d'analyse</h3>"", ''}, {'', ""<p>Les outils d'analyse peuvent aider les équipes DevOps à analyser de grands volumes de données clients et à en extraire des informations exploitables. Ces informations peuvent ensuite être utilisées pour éclairer la prise de décision et stimuler l'innovation.</p>""}, {'', ""<p>Certains outils excellent dans la visualisation des données, aidant les équipes à comprendre des ensembles de données complexes à l'aide de graphiques et de diagrammes. D'autres outils excellent dans l'analyse prédictive, aidant les équipes à prévoir les tendances futures en fonction des données historiques.</p>""}, {'', '<p>Le choix du bon outil d’analyse dépend des besoins spécifiques de l’équipe DevOps et de la nature des données de renseignement client dont elle dispose. L’essentiel est d’exploiter pleinement ses capacités pour extraire le plus de valeur possible des données de renseignement client.</p>'}, {'', '<h3>Assurer la qualité et la pertinence des données</h3>'}, {'', '<p>Toutes les données ne sont pas égales et si elles sont de mauvaise qualité ou non pertinentes, elles peuvent conduire à des décisions erronées et à un gaspillage de ressources.</p>'}, {'', '<p>La qualité des données fait référence à l’exactitude, à l’exhaustivité, à la cohérence et à la fiabilité des données. Les équipes DevOps doivent mettre en œuvre des contrôles de qualité des données rigoureux pour garantir que les données de renseignement client qu’elles utilisent sont de haute qualité.</p>'}, {'', '<p>La pertinence des données, quant à elle, fait référence à leur applicabilité à la tâche à accomplir. Toutes les données de veille client ne sont pas pertinentes pour tous les projets DevOps. Les équipes doivent sélectionner soigneusement les données les plus pertinentes pour leur projet spécifique et ignorer le reste.</p>'}, {'', '<h3>Équilibrer les données quantitatives et qualitatives</h3>'}, {'', '<p>Alors que les données quantitatives fournissent des chiffres précis et des faits concrets, les données qualitatives offrent des informations plus approfondies sur les attitudes, les perceptions et les comportements des clients.</p>'}, {'', '<p>Les données quantitatives peuvent aider les équipes DevOps à identifier les tendances, à mesurer les performances et à suivre les progrès au fil du temps. Cependant, elles ne parviennent souvent pas à expliquer pourquoi certaines tendances se produisent ou pourquoi les performances évoluent.</p>'}, {'', ""<p>Cela peut également aider les équipes DevOps à comprendre les raisons qui se cachent derrière les chiffres. Cela peut fournir des informations sur les raisons pour lesquelles les clients se comportent comme ils le font et sur ce qu'ils pensent et ressentent vraiment à propos d'un produit ou d'un service. L'équilibre entre les données quantitatives et qualitatives donne aux équipes DevOps une vision plus globale du paysage client, leur permettant de prendre des décisions plus éclairées.</p>""}, {'', '<h3>Collaboration interfonctionnelle</h3>'}, {'', ""<p>Les données d'intelligence client sont précieuses pour toutes les équipes de l'entreprise, pas seulement pour DevOps. En collaborant avec d'autres équipes, telles que le marketing, les ventes et le service client, l'équipe DevOps peut obtenir des informations et des perspectives supplémentaires.</p>""}, {'', '<p>La collaboration interfonctionnelle favorise également une culture de prise de décision basée sur les données dans toute l’organisation. Lorsque toutes les équipes utilisent les informations client pour éclairer leur travail, l’organisation dans son ensemble devient plus centrée sur le client, plus agile et plus innovante.</p>'}, {'', '<p>L’utilisation réussie des données de renseignement client dans le développement logiciel permet aux équipes de rester réactives aux besoins des clients et d’augmenter la valeur de leurs produits. Les équipes DevOps doivent intégrer les commentaires des clients tôt et souvent, exploiter les outils d’analyse, garantir la qualité et la pertinence des données, équilibrer les données quantitatives et qualitatives et favoriser la collaboration interfonctionnelle. En suivant ces bonnes pratiques, les équipes DevOps peuvent utiliser les données de renseignement client pour stimuler l’innovation et créer des produits et services qui trouvent un véritable écho auprès des clients.</p>'}, {'', ""<p>Source de l'image\xa0: Josh Sortino via Unsplash</p>""}]"
Guide DevOps sur le profilage Java,"[{'', ""<p>Le profilage Java est une technique utilisée pour comprendre le comportement détaillé d'une application Java. Il consiste à surveiller et à mesurer divers aspects de l'exécution d'un programme, tels que l'utilisation de la mémoire, l'utilisation du processeur, l'exécution des threads et la récupération de place.</p>""}, {'', ""<p>Le profilage Java peut être utilisé à différentes étapes du cycle de vie du développement logiciel (SDLC). Au cours du développement, il peut aider à identifier les goulots d'étranglement et les points chauds de performances, qui peuvent ensuite être optimisés pour de meilleures performances. Dans la phase de test, le profilage peut être utilisé pour vérifier que l'application fonctionne comme prévu sous charge. Enfin, en production, le profilage peut être utilisé pour surveiller les performances de l'application et détecter les problèmes potentiels avant qu'ils n'affectent les utilisateurs.</p>""}, {'', ""<p>Les outils de profilage Java fournissent des informations sur la machine virtuelle Java (JVM) et l'application qui y est exécutée. Ils permettent aux développeurs de surveiller l'exécution des threads, la création d'objets, la récupération de place et de nombreux autres aspects du fonctionnement de la JVM. En utilisant un profileur Java, les développeurs peuvent acquérir une compréhension approfondie des caractéristiques de performances de l'application et identifier les domaines potentiels d'optimisation.</p>""}, {""<h3>Impact sur l'efficacité et l'évolutivité des applications</h3>"", ''}, {'', ""<p>L'optimisation des performances joue un rôle essentiel dans DevOps en améliorant l'efficacité et l'évolutivité des applications. Les applications efficaces utilisent moins de ressources, ce qui réduit les coûts et permet d'avoir plus d'utilisateurs ou des charges de travail plus importantes. L'évolutivité est la capacité d'une application à gérer des charges de travail accrues sans diminution des performances. En identifiant et en éliminant les goulots d'étranglement des performances, les équipes DevOps peuvent s'assurer que les applications évoluent efficacement à mesure que la demande augmente.</p>""}, {'', '<h3>Relation entre performance et expérience utilisateur</h3>'}, {'', '<p>Les performances sont directement liées à l’expérience utilisateur. Une application lente ou peu réactive peut frustrer les utilisateurs et les amener à abandonner complètement l’application. En s’assurant que les applications fonctionnent bien, les équipes DevOps peuvent améliorer la satisfaction et la rétention des utilisateurs. De plus, en surveillant les performances des applications en production, les équipes DevOps peuvent identifier et résoudre de manière proactive les problèmes avant qu’ils n’affectent les utilisateurs.</p>'}, {'', '<h3>Gestion des coûts et optimisation des ressources</h3>'}, {'', '<p>L’optimisation des performances peut également contribuer à la gestion des coûts et à l’optimisation des ressources. En optimisant les performances des applications, les équipes DevOps peuvent réduire la quantité de ressources informatiques nécessaires à l’exécution de l’application. Cela peut se traduire par des économies de coûts importantes, en particulier dans les environnements cloud où les coûts sont directement liés à l’utilisation des ressources.</p>'}, {'', '<p>De plus, en comprenant les modèles d’utilisation des ressources de l’application, les équipes DevOps peuvent prendre des décisions plus éclairées concernant l’allocation des ressources et la planification des capacités. Cela peut aider à éviter le sur-provisionnement, qui conduit à un gaspillage de ressources, ou le sous-provisionnement, qui peut entraîner de mauvaises performances de l’application.</p>'}, {'', '<h3>Fuites de mémoire</h3>'}, {'', ""<p>Les fuites de mémoire constituent un problème de performances courant dans les applications Java. Une fuite de mémoire se produit lorsqu'une application alloue continuellement de la mémoire mais ne parvient pas à la libérer lorsqu'elle n'est plus nécessaire. Au fil du temps, cela peut entraîner des exceptions OutOfMemoryError et provoquer le blocage de l'application.</p>""}, {'', '<p>Le profilage Java peut aider à identifier les fuites de mémoire en surveillant l’utilisation de la mémoire par l’application au fil du temps. Si l’utilisation de la mémoire augmente continuellement même lorsque l’application est inactive, cela peut indiquer une fuite de mémoire. Les outils de profilage peuvent également fournir des informations sur les objets qui consomment le plus de mémoire, ce qui peut aider à localiser la source de la fuite.</p>'}, {'', '<h3>Problèmes de conflit et de synchronisation des threads</h3>'}, {'', ""<p>Les conflits de threads et les problèmes de synchronisation peuvent avoir un impact significatif sur les performances des applications Java. Les conflits de threads se produisent lorsque plusieurs threads tentent d'accéder simultanément à une ressource partagée, ce qui les oblige à attendre et entraîne une baisse des performances. Les problèmes de synchronisation, tels que les blocages et les livelocks, peuvent entraîner le blocage des threads, les empêchant de progresser.</p>""}, {'', ""<p>Le profilage Java peut aider à détecter les conflits de threads et les problèmes de synchronisation en surveillant l'état et l'exécution des threads. Les outils de profilage peuvent afficher les threads en cours d'exécution, en attente ou bloqués et peuvent fournir une trace de pile de chaque thread, ce qui peut aider à identifier la cause des conflits ou des problèmes de synchronisation.</p>""}, {'', '<h3>Frais généraux de collecte des ordures</h3>'}, {'', ""<p>La récupération de place est un aspect essentiel de la gestion de la mémoire de Java, mais elle peut également être une source importante de perte de performances. Pendant la récupération de place, la JVM interrompt l'exécution de l'application pour récupérer la mémoire des objets qui ne sont plus utilisés. Si la récupération de place se produit trop fréquemment ou prend trop de temps, cela peut entraîner des pauses de l'application et une baisse des performances.</p>""}, {'', ""<p>Le profilage Java peut aider à comprendre et à optimiser le comportement du garbage collection. Les outils de profilage peuvent fournir des informations détaillées sur les événements de garbage collection, tels que leur fréquence, leur durée et la quantité de mémoire récupérée. Ces informations peuvent aider les développeurs à ajuster la configuration du garbage collector pour minimiser son impact sur les performances de l'application.</p>""}, {'', '<h3>Manque de ressources</h3>'}, {'', ""<p>Le manque de ressources est un autre problème de performances courant dans les applications Java. Il se produit lorsqu'un système ou un processus ne parvient pas à obtenir un accès suffisant aux ressources, ce qui entraîne une baisse des performances ou une défaillance. Dans les applications Java, le manque de ressources peut être causé par des facteurs tels qu'une mémoire insuffisante, un processeur insuffisant, un espace disque insuffisant ou une bande passante réseau insuffisante.</p>""}, {'', ""<p>Le profilage Java peut aider à détecter et à résoudre les problèmes de pénurie de ressources. En surveillant l'utilisation des ressources, les outils de profilage peuvent identifier le moment où les ressources deviennent rares et fournir des informations sur les facteurs contribuant à la pénurie de ressources. Ces informations peuvent aider les développeurs à optimiser l'utilisation des ressources et à garantir que l'application dispose de suffisamment de ressources pour fonctionner efficacement.</p>""}, {'', '<p>Les outils de profilage Java fournissent généralement les fonctionnalités suivantes\xa0:</p>'}, {'', ""<li>Capacité à détecter les fuites de mémoire : les fuites de mémoire peuvent constituer un problème majeur dans les applications Java, car elles peuvent entraîner une erreur de manque de mémoire, perturbant le bon fonctionnement de l'application. Les outils de profilage peuvent aider à identifier ces fuites, permettant aux développeurs de les corriger avant qu'elles ne causent des problèmes plus graves.</li>""}, {'', '<li>Profilage du processeur : cela permet aux développeurs de voir combien de temps processeur chaque méthode de leur application consomme. En identifiant les méthodes qui consomment une quantité disproportionnée de temps processeur, les développeurs peuvent optimiser leur code pour améliorer ses performances.</li>'}, {'', '<li>Profilage détaillé des threads : cela peut aider les développeurs à identifier les problèmes de synchronisation, les blocages et autres problèmes potentiels dans les applications multithread. En fournissant ces informations essentielles, les outils de profilage peuvent aider les développeurs à créer des applications plus efficaces et plus performantes.</li>'}, {'', '<p>Voici quelques bonnes pratiques pour tirer le meilleur parti de vos efforts de profilage Java.</p>'}, {'', '<h3>Identifier les indicateurs clés de performance</h3>'}, {'', '<p>Tout d’abord, il est essentiel d’identifier les indicateurs de performance clés les plus pertinents pour votre application. Les indicateurs que vous choisissez dépendent de la nature de votre application et de ses exigences de performance. Parmi les indicateurs courants, citons le temps de réponse, l’utilisation du processeur, l’utilisation de la mémoire et l’activité de récupération de place. En identifiant ces indicateurs clés, vous pouvez utiliser vos outils de profilage pour les surveiller et identifier les problèmes potentiels.</p>'}, {'', '<h3>Intégration du profilage dans les pipelines CI/CD</h3>'}, {'', '<p>Une autre bonne pratique consiste à intégrer le profilage dans vos pipelines d’intégration continue/déploiement continu (CI/CD). Cela vous permet de détecter les problèmes de performances dès le début du processus de développement, avant qu’ils ne deviennent plus difficiles à résoudre. En profilant régulièrement votre application pendant le développement, vous pouvez vous assurer que les modifications que vous apportez n’affectent pas ses performances.</p>'}, {'', '<h3>Profilage dans des scénarios réels</h3>'}, {'', '<p>Bien qu’il puisse être utile de profiler votre application dans des conditions contrôlées, il est également important de le faire dans des scénarios réels. Cela signifie profiler votre application lorsqu’elle est sous charge, lorsqu’elle traite des données réelles et lorsqu’elle s’exécute sur les mêmes configurations matérielles et logicielles qu’en production. En procédant ainsi, vous obtiendrez une image beaucoup plus précise des performances de votre application dans le monde réel.</p>'}, {'<h3>Collaboration et partage des connaissances</h3>', ''}, {'', '<p>Enfin, le profilage Java ne doit pas être une activité solitaire. Il est important de partager vos découvertes et vos idées avec votre équipe et de collaborer pour trouver des solutions aux problèmes de performances que vous identifiez. Ce faisant, vous pouvez créer une culture de sensibilisation aux performances au sein de votre équipe et vous assurer que tout le monde travaille vers l’objectif commun de créer des applications Java efficaces et performantes.</p>'}, {'', '<p>Le profilage Java est un élément essentiel du processus de développement logiciel. En comprenant ses principales fonctionnalités et en suivant les meilleures pratiques, vous pouvez l’utiliser pour créer des applications efficaces, performantes et capables de répondre aux exigences du monde moderne. Ne sous-estimez donc pas la puissance du profilage : adoptez-le et regardez vos applications Java atteindre de nouveaux sommets de performances.</p>'}]"
Dynatrace étend la portée et la portée des données de sa plateforme d'observabilité,"[{'', ""<p>Lors de son événement Perform 2024 aujourd'hui, Dynatrace a dévoilé un Dynatrace OpenPipeline qui permet d'appliquer des analyses à plusieurs types de sources de données en temps réel.</p>""}, {'', ""<p>La société a également dévoilé une offre d'observabilité des données qui peut être utilisée pour vérifier la qualité et la lignée des données exposées au moteur d'intelligence artificielle (IA) Davis au cœur de la plateforme d'observabilité Dynatrace afin de réduire les faux positifs qui pourraient autrement générer une alerte, en plus de contribuer à réduire le volume de données qui pourraient devoir être stockées.</p>""}, {'', '<p>Enfin, Dynatrace a annoncé aujourd’hui qu’elle étendait la portée de sa plateforme d’observabilité aux grands modèles de langage (LLM) utilisés pour créer des plateformes d’IA génératives. Cette capacité permettra, par exemple, de simplifier le suivi de la consommation de jetons utilisés pour donner accès à ces modèles.</p>'}, {'', ""<p>Steve Tack, vice-président senior des produits chez Dynatrace, a déclaré que Dynatrace OpenPipeline permettra aux organisations de rationaliser la collecte de données d'une manière qui leur permettra d'appliquer l'observabilité plus largement en appliquant des algorithmes de traitement de flux à des pétaoctets de données.</p>""}, {'', ""<p>Prévue pour être disponible dans 90 jours, la fonctionnalité Dynatrace OpenPipeline permet aux équipes informatiques d'ingérer et d'acheminer les données d'observabilité, de sécurité et d'événements commerciaux à partir de n'importe quelle source et dans n'importe quel format au point d'ingestion, y compris les données non structurées qui sont automatiquement converties en un format exploitable. Ces données peuvent ensuite être enrichies pour permettre des analyses plus approfondies.</p>""}, {'', ""<p>Cela offre également aux équipes informatiques davantage de contrôle sur les données qu'elles analysent, stockent ou excluent des analyses, ce qui contribue à réduire le coût total de l'observabilité, a noté Tack.</p>""}, {'', '<p>Enfin, cela offre aux équipes informatiques la possibilité d’appliquer des contrôles de sécurité et de confidentialité plus personnalisables à la manière dont ces données sont utilisées, a-t-il ajouté.</p>'}, {'', '<p>Dynatrace adopte une approche multimodale de l’IA qui englobe des modèles prédictifs, causaux et génératifs. Collectivement, cette approche simplifie l’identification de la cause profonde des problèmes, l’identification des anomalies susceptibles de perturber les services et la rationalisation de la gestion des incidents, par exemple en fournissant des résumés des événements dans un format en langage naturel.</p>'}, {'', '<p>On ne sait pas encore dans quelle mesure l’observabilité sera appliquée au-delà des données collectées pour gérer les flux de travail DevOps et d’autres processus informatiques, mais il est clair qu’il existe une forte corrélation entre les événements informatiques et les résultats commerciaux à mesure que les organisations deviennent plus dépendantes des logiciels. À mesure que l’IA devient de plus en plus utilisée, il devrait devenir plus simple d’appliquer l’analyse à un éventail beaucoup plus large de types de données pour faire apparaître la relation entre les événements informatiques et les processus commerciaux. En effet, Dynatrace simplifie l’application des meilleures pratiques d’ingénierie des données pour collecter, gérer et analyser ces données, a noté Tack.</p>'}, {'', '<p>En attendant, les équipes DevOps doivent revoir la manière dont leurs pipelines sont actuellement construits pour simplifier la capture de toutes les données pertinentes. Si les données de télémétrie collectées à partir de la plateforme DevOps sont essentielles pour garantir que le développement et la livraison des applications se déroulent le plus rapidement possible, ces données ne sont qu’un facteur dans une équation plus vaste. Le défi et l’opportunité à présent consistent à déterminer la meilleure façon d’appliquer l’IA pour corréler tout cela.</p>'}]"
Prévisions technologiques pour les infrastructures en 2024,"[{'', '<p>Ganesh Srinivasan, associé chez Venrock, est co-auteur de cet article.</p>'}, {'', '<p>2023 a été une année de montagnes russes sans précédent ; de la mort de l’étalement des piles de données modernes à la naissance de l’IA générative, nous ne sommes qu’au début d’une nouvelle ère dans « l’art du possible ». Nous vous garantissons que 2024 ne sera pas une déception.</p>'}, {'', '<p>À l’approche d’une nouvelle année, c’est le moment idéal pour examiner ce que nous prévoyons être les plus grands développements de l’année à venir. Voici ce qui, selon nous, va se passer en 2024 :</p>'}, {'', '<p>1. Le règne d’OpenAI remis en question</p>'}, {'', ""<p>Avec les apprentissages émergents dans les architectures de réseaux neuronaux de base qui ont conduit au transformateur et à la domination d'OpenAI, il est probable que leur sortie imminente de GPT5 sera dépassée dans des tests de performance spécifiques par un nouvel entrant sur la base d'architectures plus efficaces, de capacités multimodales améliorées, d'une meilleure compréhension contextuelle du monde et d'un apprentissage par transfert amélioré. Ces nouveaux modèles seront construits sur la recherche émergente dans les réseaux spatiaux, les structures de graphes et les combinaisons de divers réseaux neuronaux qui conduiront à des capacités plus efficaces, polyvalentes et puissantes.</p>""}, {'', ""<p>2. Apple : le nouveau leader de l'IA générative</p>""}, {'', '<p>L’un des acteurs les plus importants dans le domaine de l’IA générative commence tout juste à montrer ses cartes. 2024 sera l’année où Apple lancera son premier ensemble de fonctionnalités d’IA générative, libérant ainsi le véritable potentiel d’une architecture fermée d’IA en périphérie avec un accès complet à vos données personnelles – prouvant qu’Apple est en fait l’entreprise la plus importante dans la course à l’IA générative.</p>'}, {'', '<p>3. Construire en donnant la priorité au client</p>'}, {'', '<p>La dernière décennie a vu l’abandon des clients lourds au profit du rendu et du calcul côté serveur. Mais le monde revient au client. Les expériences mobiles devront fonctionner en mode hors ligne. Les expériences en temps réel nécessitent des transactions à très faible latence. L’exécution des LLM devra de plus en plus s’exécuter sur l’appareil pour augmenter les performances et réduire les coûts.</p>'}, {'', '<p>4. La mort de l’étalement des infrastructures de données</p>'}, {'', ""<p>La croissance rapide des besoins en infrastructures de données des entreprises a conduit à une multiplication des solutions ponctuelles, des catalogues de données à la gouvernance des données, en passant par l'extraction inverse, la transformation, le chargement et les alternatives Airflow, jusqu'aux bases de données vectorielles et à un autre lakehouse. Le pendule reviendra vers des plateformes unifiées et moins de silos pour réduire le coût total de possession et les frais généraux d'exploitation d'ici 2024.</p>""}, {'', ""<p>5. Approche de l'hiver de l'IA</p>""}, {'', '<p>En 2023, l’IA générative pourrait être qualifiée d’« art du possible », 2024 étant le véritable test pour voir si les prototypes se transforment en cas d’utilisation en production. Le pic du cycle de battage médiatique étant probablement atteint ici, 2024 connaîtra la phase de désillusion où les entreprises découvriront où l’IA générative peut créer un impact positif sur les marges et où les coûts l’emportent sur les avantages.</p>'}, {'', '<p>6. La menace de la désinformation</p>'}, {'', '<p>Si les modèles de diffusion d’images et de vidéos ont ouvert une nouvelle ère pour la création numérique et l’expression artistique, il ne fait aucun doute que leur côté obscur n’a pas encore fait de ravages. À l’approche d’une élection présidentielle, les modèles de diffusion en tant que machine de désinformation politique émergeront pour devenir la prochaine arme de désinformation majeure.</p>'}, {'', '<p>7. La percée de l’IA dans le monde réel</p>'}, {'', '<p>L’ère du « champ des rêves » pour l’IA sera révolue et 2024 marquera une avancée majeure pour les cas d’utilisation commerciale de l’IA, en particulier dans le monde physique. L’utilisation de l’IA pour les modalités du monde physique nous permettra de changer et d’interagir avec les machines et les appareils qui nous entourent, de la compréhension de la télémétrie des machines pour la détection des problèmes à la distillation d’informations à partir de milliers de flux de capteurs en temps réel.</p>'}, {'', '<p>8. S3 : La menace grandissante pour Snowflake</p>'}, {'', '<p>AWS S3 est sur le point de devenir le plus grand concurrent de Snowflake. Avec un écosystème d’outils en pleine croissance émergeant à mesure que les couches de calcul au-dessus de S3 tirent parti de leurs coûts ultra-faibles, de leur haute disponibilité, de la prise en charge de la zone de disponibilité et du nouveau service de latence à un chiffre en millisecondes, le « problème de coût de Snowflake » de tout le monde est sur le point de se voir offrir un nouvel ensemble d’alternatives attrayantes.</p>'}, {'', ""<p>9. L'essor de Flink et de l'analyse opérationnelle</p>""}, {'', '<p>Si l’analyse commerciale a été à l’origine de la première vague d’évolution de l’infrastructure de données, l’analyse opérationnelle sera le moteur de la prochaine vague, avec Apache Flink au cœur de celle-ci. Les entreprises commenceront à adopter en masse des fonctionnalités en temps réel et en streaming pour commencer à utiliser les données afin de prendre des décisions opérationnelles. Cela créera une nouvelle valeur dans des domaines tels que la logistique, les transactions financières, la gestion des stocks et la détection des fraudes.</p>'}, {'', ""<p>10. La pile d'IA du développeur</p>""}, {'', '<p>En 2024, l’impact le plus significatif de l’IA dans le monde numérique sera de débloquer la productivité des développeurs. De la génération de code au débogage en passant par la gestion des builds, l’IA ouvrira un nouveau niveau de productivité des développeurs que nous n’avons pas vu depuis l’avènement du cloud.</p>'}, {'', ""<p>Si vous êtes d'accord, pas d'accord ou souhaitez partager vos propres prédictions pour 2024 avec nous, veuillez nous contacter sur X à @ethanjb et @gan3sh.</p>""}]"
Naviguer dans le labyrinthe du cloud hybride : surmonter les obstacles à l'adoption,"[{'', ""<h3>Types d'organisations adaptées aux solutions de cloud hybride</h3>""}, {'', ""<h3>Protocoles d'intégration standardisés</h3>""}, {'<h3>Stratégies de sécurité dynamiques</h3>', ''}, {'', '<h3>Solutions pour exploiter les avantages du cloud hybride</h3>'}, {""<h3>L'évolution du paysage des architectures de cloud hybride</h3>"", ''}]"
Naviguer dans l'écosystème multi-cloud,"[{'', ""<p>L'infrastructure multicloud, une approche dans laquelle les organisations utilisent les services de plusieurs fournisseurs de cloud, gagne rapidement du terrain dans le monde de la technologie.</p>""}, {'', '<p>Dans cet article, nous allons décortiquer le concept d’infrastructure multicloud, en examinant comment il permet aux entreprises de diversifier leurs écosystèmes cloud pour des performances améliorées, une plus grande flexibilité et une meilleure gestion des risques.</p>'}, {""<h3>Comprendre l'écosystème multi-cloud</h3>"", ''}, {'', '<p>Les environnements multi-cloud représentent une approche stratégique du cloud computing où les entreprises utilisent les services de plusieurs fournisseurs de cloud pour répondre à divers besoins opérationnels.</p>'}, {'', ""<p>Ce concept diffère du cloud hybride, qui combine généralement des infrastructures de cloud privé et public, dans le but d'offrir un équilibre entre les deux dans un seul environnement unifié.</p>""}, {'', '<p>En revanche, une stratégie multicloud implique l’utilisation de services cloud distincts – par exemple, un fournisseur pour le stockage, un autre pour la puissance de calcul et un autre pour l’analyse – sans nécessairement les intégrer dans un système cohérent. Cette approche permet aux organisations de tirer parti des atouts et des offres uniques de divers fournisseurs de cloud, optimisant ainsi leur infrastructure informatique pour plus d’efficacité, de rentabilité et de performances.</p>'}, {'', '<h3>Les avantages d’une approche multi-cloud</h3>'}, {'', '<p>L’adoption d’une approche multicloud apporte plusieurs avantages clés, les plus importants étant une résilience et une flexibilité améliorées et l’évitement du verrouillage des fournisseurs.</p>'}, {'', '<p>En répartissant les ressources et les charges de travail sur plusieurs environnements cloud, les entreprises peuvent atténuer les risques associés au recours à un seul fournisseur cloud, comme les pannes de service ou la perte de données. Cette diversité garantit un fonctionnement continu et de solides capacités de reprise après sinistre.</p>'}, {'', ""<p>De plus, une stratégie multi-cloud permet aux entreprises de choisir les meilleurs services proposés par chaque fournisseur de cloud, en adaptant leur infrastructure informatique à des besoins spécifiques et à des exigences opérationnelles. Cette approche sur mesure permet aux entreprises d'optimiser à la fois les performances et les coûts. Différents fournisseurs peuvent proposer des tarifs compétitifs pour des services spécifiques, ce qui permet aux entreprises de choisir les solutions les plus rentables en fonction de leurs besoins.</p>""}, {'', ""<p>De plus, en évitant le blocage des fournisseurs, les entreprises conservent la liberté de changer de fournisseur en fonction de l'évolution des besoins ou de l'arrivée de nouveaux services plus avantageux. Cette utilisation stratégique de plusieurs clouds conduit finalement à une infrastructure informatique plus efficace, plus agile et plus rentable.</p>""}, {'', '<h3>Principaux défis du multi-cloud</h3>'}, {'', ""<p>La gestion multicloud s'accompagne de son lot de défis, notamment en termes de complexité, de sécurité des données et de conformité. Les équipes DevOps sont souvent confrontées à la complexité de la gestion de services cloud disparates, chacun doté de fonctionnalités et d'interfaces uniques, ce qui rend l'intégration transparente et le fonctionnement cohérent une tâche exigeante.</p>""}, {'', '<p>La sécurité des données est une autre préoccupation majeure, car la protection des informations sensibles sur plusieurs plateformes nécessite des mesures de sécurité robustes et adaptables. En outre, la conformité aux différentes réglementations dans différents environnements cloud ajoute un niveau de complexité supplémentaire.</p>'}, {'', '<p>Ces défis nécessitent que les équipes DevOps soient techniquement compétentes et expertes en planification stratégique et en coordination pour gérer efficacement un écosystème multicloud.</p>'}, {'<h3>Bonnes pratiques en matière de déploiement et de gestion multi-cloud</h3>', ''}, {'', ""<p>Lors de la gestion et du déploiement au sein d'un écosystème multi-cloud, les équipes DevOps peuvent grandement bénéficier des cinq meilleures pratiques suivantes\xa0:</p>""}, {'', ""<p>1. Sélection stratégique des services cloud : évaluez et choisissez soigneusement les services cloud en fonction des exigences spécifiques de la charge de travail, des mesures de performance et de la rentabilité pour garantir un déploiement optimal. 2. Surveillance et gestion centralisées : utilisez des outils centralisés pour surveiller et gérer différents environnements cloud afin de maintenir la cohérence et le contrôle et de rationaliser les opérations. 3. Posture de sécurité cohérente : appliquez des politiques et des pratiques de sécurité uniformes sur toutes les plateformes cloud pour protéger l'intégrité des données et vous conformer aux normes réglementaires. 4. Automatisation : automatisez les tâches répétitives et routinières sur différents clouds pour une utilisation efficace des ressources, une réduction des erreurs et des cycles de déploiement plus rapides. 5. Gestion et optimisation des coûts : surveillez et gérez en permanence les dépenses cloud sur toutes les plateformes pour éviter les dépassements de coûts, garantir une utilisation efficace des ressources et suivre les meilleures pratiques d'optimisation des coûts cloud.</p>""}, {'', ""<h3>Élaboration d'une stratégie multi-cloud : une perspective DevOps</h3>""}, {'', '<p>Du point de vue DevOps, la création d’une stratégie multicloud complète implique d’aligner l’infrastructure cloud sur les objectifs commerciaux et les principes fondamentaux de DevOps.</p>'}, {'', ""<p>La première étape consiste à définir clairement les objectifs de l'entreprise et à identifier comment les différents environnements cloud peuvent les prendre en charge. Il convient ensuite de sélectionner les services cloud qui correspondent le mieux aux besoins spécifiques de chaque charge de travail, en tenant compte de facteurs tels que les performances, les coûts et les exigences de conformité. L'intégration des processus d'intégration continue et de déploiement continu (CI/CD) est essentielle dans une configuration multicloud.</p>""}, {'', ""<p>Cette intégration garantit que les logiciels peuvent être développés, testés et déployés rapidement et de manière fiable sur différentes plateformes cloud. La stratégie doit également inclure la mise en œuvre d'outils d'automatisation et de surveillance robustes pour maintenir la cohérence et l'efficacité dans l'ensemble du cloud.</p>""}, {'', '<p>En se concentrant sur ces éléments, une équipe DevOps peut développer une stratégie multi-cloud qui non seulement s’aligne sur les objectifs de l’organisation, mais adhère également à l’éthique agile et efficace des méthodologies DevOps, garantissant un fonctionnement et une évolutivité transparents dans l’environnement multi-cloud.</p>'}, {'<h3>Pour conclure</h3>', ''}, {'', '<p>En conclusion, une bonne maîtrise de l’écosystème multi-cloud est essentielle à la réussite de DevOps, car il offre une flexibilité, une évolutivité et une résilience inégalées dans la gestion de diverses ressources cloud. Les professionnels DevOps sont encouragés à adopter des stratégies multi-cloud car elles améliorent l’agilité opérationnelle et ouvrent des perspectives d’innovation et d’amélioration de la prestation de services.</p>'}]"
Mise en œuvre de l'analyse des données dans des environnements multi-cloud,"[{'', ""<p>L'analyse des données multicloud est plus complexe et exige une approche nuancée pour garantir des opérations fluides et des informations précises. La coordination des analyses entre plusieurs fournisseurs de cloud pose divers problèmes, nécessitant une réflexion approfondie sur la stratégie de mise en œuvre. De la gestion des coûts à la variabilité des performances et à l'orchestration des données, nous explorerons chaque défi en détail et fournirons des informations exploitables sur la manière de surmonter efficacement ces obstacles, garantissant une infrastructure d'analyse de données à toute épreuve dans les solutions multicloud.</p>""}, {'', '<h3>Interopérabilité et normes</h3>'}, {'', ""<p>Les problèmes d'interopérabilité se posent dans l'analyse de données multicloud en raison des différences entre les API et les normes des différents fournisseurs de cloud. Le manque d'uniformité des formats de données et des protocoles de communication entrave la collaboration et l'échange de données. Pour y remédier, les organisations doivent privilégier les services cloud conformes aux normes ouvertes et plaider en faveur d'initiatives d'interopérabilité à l'échelle du secteur. En outre, créez une couche d'abstraction ou un middleware qui fait office de passerelle entre les différents fournisseurs de cloud. Cette couche d'abstraction permet de normaliser les interactions, de faciliter l'échange de données fluide et de réduire les dépendances vis-à-vis des API spécifiques des fournisseurs. Ainsi, vous améliorez la flexibilité de votre environnement multicloud, facilitant l'intégration et le basculement entre différentes plateformes cloud.</p>""}, {'', '<h3>Verrouillage du fournisseur</h3>'}, {'', '<p>Les entreprises risquent de devenir trop dépendantes d’un seul fournisseur, ce qui limite leur flexibilité et rend la migration complexe. L’utilisation de fonctionnalités ou d’outils spécialisés d’un fournisseur peut être confrontée à des difficultés lors de la transition vers un autre, ce qui limite la capacité à choisir des solutions plus rentables ou plus adaptées et peut entraîner une augmentation des coûts de changement. Pour atténuer ce problème, il convient d’adopter une stratégie multicloud, privilégiant les technologies standardisées et les API ouvertes, permettant aux entreprises de répartir les charges de travail entre plusieurs fournisseurs de cloud et d’éviter d’être liées à l’écosystème d’un fournisseur spécifique, améliorant ainsi la flexibilité et la prise de décision stratégique. Concevez des architectures qui exploitent les couches d’abstraction et évitent un couplage étroit avec des services spécifiques au fournisseur. Cette approche garantit la flexibilité, permettant à votre entreprise de choisir et de basculer entre les fournisseurs de cloud en fonction des performances, des coûts et des considérations stratégiques tout en minimisant le risque de dépendance vis-à-vis d’un fournisseur.</p>'}, {'', '<h3>Gestion des coûts</h3>'}, {'', '<p>Chaque fournisseur a des modèles de tarification et des mécanismes de facturation uniques, ce qui rend difficile pour les organisations d’estimer et de comparer avec précision les coûts. La nature dynamique des charges de travail d’analyse de données, associée aux dépenses de transfert de données entre les clouds, contribue à la complexité de la budgétisation. Pour y faire face, implémentez un outil de gestion centralisée des coûts du cloud pour surveiller les dépenses entre les fournisseurs. De plus, examinez et optimisez régulièrement l’utilisation des ressources pour identifier les instances sous-utilisées ou sur-approvisionnées. Vous pouvez exploiter les instances réservées ou repérer les instances pour un calcul rentable et implémenter l’automatisation pour la mise à l’échelle en fonction de la demande, en veillant à ce que les ressources soient allouées efficacement. Enfin, négociez les prix avec les fournisseurs de cloud et étudiez les options de remise pour garantir des conditions favorables pour les engagements à long terme.</p>'}, {'', '<h3>Variabilité des performances</h3>'}, {'', ""<p>Les fournisseurs disposant d'une infrastructure, de configurations réseau et de niveaux de service uniques, des niveaux de performances incohérents peuvent survenir et avoir un impact sur la fiabilité et la prévisibilité des résultats d'analyse. Pour atténuer ces conséquences, effectuez des tests de performances approfondis dans différents environnements cloud, optimisez les requêtes pour des plates-formes spécifiques et envisagez la répartition de la charge de travail en fonction des points forts des fournisseurs. L'utilisation d'outils de surveillance des performances permet de suivre et d'analyser les variations, ce qui permet d'effectuer des ajustements en temps opportun. Le choix des fournisseurs de cloud en fonction d'exigences de performances spécifiques et l'établissement d'accords de niveau de service (SLA) de performance atténuent également l'impact de la variabilité des performances dans les analyses de données multicloud.</p>""}, {'', '<h3>Orchestration des ressources</h3>'}, {'', ""<p>En raison des différences entre les outils d'orchestration, les API et les mécanismes de gestion des ressources utilisés par les fournisseurs de cloud, des problèmes de compatibilité se produisent souvent. L'hétérogénéité rend complexe la garantie d'un déploiement, d'une mise à l'échelle et d'une gestion transparents des charges de travail d'analyse. Les organisations peuvent envisager d'adopter des outils et des cadres d'orchestration indépendants du cloud pour faire abstraction des différences sous-jacentes entre les fournisseurs de cloud, ce qui permet une gestion cohérente des ressources. L'adoption des pratiques d'infrastructure en tant que code (IaC) et de l'automatisation rationalise également l'orchestration des ressources, ce qui permet aux organisations de déployer et de gérer les charges de travail d'analyse plus efficacement dans divers environnements cloud.</p>""}, {'', '<h3>Mouvement et latence des données</h3>'}, {'', '<p>Les architectures de réseau variées, les distances géographiques et les différents mécanismes de transfert de données contribuent à accroître la latence et à poser des problèmes de performances potentiels. Le transfert d’ensembles de données importants entre clouds entraîne des coûts supplémentaires et peut entraîner des inefficacités opérationnelles. Pour éviter cela, les organisations doivent optimiser les processus de transfert de données en exploitant des protocoles de transfert efficaces, en envisageant l’informatique de pointe pour la proximité des sources de données et en répartissant stratégiquement les charges de travail en fonction de l’emplacement des données. La mise en œuvre de stratégies de mise en cache et l’utilisation de réseaux de diffusion de contenu (CDN) peuvent contribuer à atténuer les problèmes de latence, garantissant ainsi un environnement d’analyse de données multicloud plus réactif et plus rentable.</p>'}, {'', '<h3>En note de bas de page</h3>'}, {'', '<p>En conclusion, la mise en œuvre de l’analyse de données dans des environnements multi-cloud présente un ensemble de défis uniques. L’adoption de normes ouvertes, l’adoption d’outils indépendants du cloud et la promotion d’une culture d’amélioration continue sont des éléments essentiels pour les surmonter. Une surveillance continue, des mises à jour régulières des cadres de gouvernance et une prise de décision stratégique alignée sur les spécificités et les objectifs de votre entreprise sont essentielles pour tirer le meilleur parti de l’analyse de données multi-cloud. Vous pouvez également bénéficier de conseils en cloud computing pour obtenir une stratégie sur mesure auprès d’experts.</p>'}]"
Exécuter MongoDB sur AWS : un guide pratique,"[{'', ""<p>MongoDB est un programme de base de données orienté document disponible en source. MongoDB est une solution de base de données NoSQL qui utilise des documents de type JSON avec des schémas facultatifs. De nombreuses organisations choisissent d'exécuter MongoDB dans le cloud Amazon Web Services (AWS) pour améliorer l'évolutivité et la fiabilité de leur déploiement MongoDB.</p>""}, {'', ""<p>AWS propose une gamme de services pouvant être intégrés à MongoDB, notamment une puissance de calcul et de multiples options de stockage. L'exécution de MongoDB sur AWS peut être effectuée de différentes manières\xa0:</p>""}, {'', '<li>Vous pouvez installer et gérer manuellement MongoDB sur une instance AWS.</li>'}, {'', '<li>Vous pouvez utiliser MongoDB Atlas, une base de données en tant que service entièrement gérée par les créateurs de MongoDB, qui prend automatiquement en charge vos opérations de base de données.</li>'}, {'', '<p>Dans cet article, nous aborderons les avantages de l’exécution de MongoDB sur AWS et montrerons comment démarrer avec MongoDB Atlas sur AWS.</p>'}, {'', '<h3>Flexibilité des infrastructures</h3>'}, {'', ""<p>Avec MongoDB sur AWS, vous pouvez choisir l'instance qui correspond le mieux à votre charge de travail. Par exemple, si vous avez besoin de plus de puissance de calcul, vous pouvez sélectionner une instance optimisée pour le calcul. Si votre charge de travail nécessite plus de mémoire, vous pouvez opter pour une instance optimisée pour la mémoire. Cette flexibilité s'étend également au stockage, AWS proposant des options de stockage SSD et HDD pour votre base de données MongoDB.</p>""}, {'', '<h3>Évolutivité</h3>'}, {'', ""<p>Que vous ayez affaire à une petite application ou à une opération à grande échelle, AWS vous permet d'augmenter ou de réduire rapidement vos capacités pour gérer les changements d'exigences ou les pics de popularité. La fonction de mise à l'échelle automatique d'AWS, combinée aux capacités de partitionnement de MongoDB, vous offre une solution de base de données qui peut évoluer avec votre entreprise.</p>""}, {'', '<h3>Sécurité intégrée</h3>'}, {'', ""<p>L'exécution de MongoDB sur AWS offre également des mesures de sécurité robustes. AWS fournit diverses fonctionnalités de sécurité, telles qu'AWS Identity and Access Management (IAM), qui vous permet de définir les autorisations et les politiques des utilisateurs. Cela signifie que vous pouvez avoir un contrôle précis sur qui peut accéder à vos bases de données MongoDB et sur les actions qu'ils peuvent effectuer.</p>""}, {'', '<h3>Sauvegarde et récupération après sinistre</h3>'}, {'', ""<p>Enfin, la combinaison de MongoDB et d'AWS offre des options robustes pour la sauvegarde et la reprise après sinistre. AWS propose Amazon S3 pour le stockage d'objets, connu pour sa durabilité et bien adapté aux charges de travail NoSQL. Vous pouvez également utiliser AWS Snapshots pour créer des sauvegardes de vos bases de données MongoDB. En cas de sinistre, vous pouvez rapidement lancer davantage de ressources dans AWS pour assurer la continuité des activités.</p>""}, {'', ""<p>MongoDB Atlas est un service de base de données entièrement géré basé sur le cloud proposé par MongoDB Inc. Il fournit une suite intégrée de fonctionnalités avancées telles que les sauvegardes automatisées, la surveillance, la mise à l'échelle et la sécurité pour exécuter MongoDB dans le cloud.</p>""}, {'', ""<p>MongoDB Atlas gère les aspects opérationnels de l'exécution de MongoDB, éliminant ainsi le besoin d'interventions manuelles. Voici quelques-unes des principales caractéristiques de MongoDB Atlas\xa0:</p>""}, {'', ""<li>Service géré\xa0: MongoDB Atlas décharge l'utilisateur des tâches d'installation, de configuration, de maintenance et de mise à l'échelle, ce qui en fait une option incontournable pour ceux qui souhaitent se concentrer davantage sur les opérations de base de données et moins sur l'administration.</li>""}, {'', ""<li>Indépendant de la plateforme : bien que MongoDB Atlas fonctionne parfaitement sur AWS, il est également disponible sur d'autres plateformes cloud, notamment Google Cloud Platform (GCP) et Microsoft Azure. Les utilisateurs peuvent choisir leur fournisseur de cloud et leur région préférés.</li>""}, {'', ""<li>Sécurité : MongoDB Atlas est livré avec des mesures de sécurité intégrées, notamment un cryptage de bout en bout, un peering VPC et des mécanismes d'authentification de niveau entreprise.</li>""}, {'', ""<li>Sauvegardes automatisées : Atlas fournit des sauvegardes continues avec récupération à un instant donné. Cela signifie que vous pouvez restaurer vos données à n'importe quelle seconde du passé sans temps d'arrêt significatif.</li>""}, {'', ""<li>Informations sur les performances\xa0: MongoDB Atlas offre une surveillance et des alertes en temps réel. Les utilisateurs peuvent obtenir des informations sur les requêtes, l'utilisation des ressources et d'autres mesures directement à partir du tableau de bord Atlas.</li>""}, {'', ""<p>Voici les étapes de haut niveau nécessaires à la migration d'une base de données MongoDB auto-hébergée vers MongoDB Atlas sur AWS Cloud. Pour plus de détails, consultez les instructions prescriptives d'Amazon.</p>""}, {'', '<h3>Première étape : découverte et évaluation</h3>'}, {'', '<p>La première phase consiste à comprendre votre configuration MongoDB actuelle et à préparer la migration. Commencez par évaluer votre environnement MongoDB existant, notamment la taille de la base de données, la configuration et les mesures de performances. Cette évaluation vous aidera à déterminer la taille et la configuration appropriées pour votre cluster MongoDB Atlas sur AWS. Il est également essentiel d’analyser les dépendances de votre application sur la base de données pour comprendre l’impact de la migration sur votre application.</p>'}, {'', ""<p>Au cours de cette phase, vous devez également identifier les éventuels défis ou limitations qui pourraient survenir pendant la migration. Il peut s'agir de problèmes de compatibilité, de problèmes d'intégrité des données ou d'implications en termes de performances. En abordant ces problèmes en amont, vous pouvez atténuer les risques et planifier une migration plus fluide.</p>""}, {'<h3>Deuxième étape\xa0: configurer la sécurité et la conformité</h3>', ''}, {'', ""<p>La sécurité et la conformité sont primordiales dans tout processus de migration. Commencez par configurer les paramètres réseau dans MongoDB Atlas pour garantir une connectivité sécurisée. Cela implique généralement la configuration d'un peering VPC ou d'AWS Direct Connect entre votre environnement AWS et MongoDB Atlas.</p>""}, {'', '<p>Ensuite, concentrez-vous sur la configuration des fonctionnalités de sécurité dans MongoDB Atlas. Celles-ci incluent la configuration du chiffrement au repos et en transit, la gestion des identités et des accès, ainsi que les fonctionnalités d’audit et de surveillance. Assurez-vous que ces paramètres sont conformes aux politiques de sécurité et aux exigences de conformité de votre organisation.</p>'}, {'', '<p>Il est également essentiel de comprendre le modèle de responsabilité partagée dans AWS et MongoDB Atlas. Tandis que MongoDB Atlas gère la sécurité du cloud, vous êtes responsable de la sécurité dans le cloud, ce qui inclut la sécurisation de vos données et le contrôle des accès.</p>'}, {'', '<h3>Troisième étape : migrer les données</h3>'}, {'', ""<p>Une fois les préparatifs terminés, vous pouvez commencer à migrer vos données vers MongoDB Atlas. MongoDB Atlas propose un service de migration en direct, qui vous permet de migrer vos données avec un temps d'arrêt minimal. Pour utiliser ce service, vous devrez fournir des détails sur votre environnement MongoDB source et lancer la migration via l'interface MongoDB Atlas.</p>""}, {'', '<p>Surveillez de près le processus de migration pour vous assurer que les données sont transférées avec précision et efficacité. Il est conseillé d’effectuer des contrôles de validation pendant et après la migration pour garantir l’intégrité des données. Soyez également prêt à résoudre rapidement tout problème qui pourrait survenir au cours de cette phase.</p>'}, {'', '<h3>Étape 4\xa0: Configurer l’intégration opérationnelle</h3>'}, {'', '<p>Une fois vos données migrées, l’étape suivante consiste à intégrer MongoDB Atlas à vos processus opérationnels. Cela implique de mettre à jour la configuration de votre application pour qu’elle pointe vers le nouveau cluster MongoDB Atlas. Vous devrez mettre à jour les chaînes de connexion à la base de données de votre application et tester minutieusement l’application pour vous assurer qu’elle interagit correctement avec MongoDB Atlas.</p>'}, {'', ""<p>De plus, configurez la surveillance et les alertes dans MongoDB Atlas pour suivre les performances et l'état de votre base de données. MongoDB Atlas fournit divers outils et mesures qui peuvent vous aider à surveiller efficacement votre base de données.</p>""}, {'', ""<p>Enfin, pensez à mettre en œuvre les modifications nécessaires à vos procédures de sauvegarde et de récupération. MongoDB Atlas propose des solutions de sauvegarde automatisées, mais vous devez les aligner sur vos politiques de protection des données et tester votre processus de sauvegarde et de récupération pour vous assurer qu'il répond à vos exigences.</p>""}, {'', '<p>En conclusion, l’exécution de MongoDB sur AWS offre une combinaison de flexibilité, d’évolutivité, de sécurité et d’options robustes de sauvegarde et de reprise après sinistre. En tirant parti de l’infrastructure diversifiée d’AWS, vous pouvez personnaliser votre environnement MongoDB pour répondre à des exigences de charge de travail spécifiques, qu’il s’agisse d’exigences centrées sur le calcul, la mémoire ou le stockage.</p>'}, {'', ""<p>Le processus de migration vers MongoDB Atlas sur AWS implique la découverte et l'évaluation, la configuration de la sécurité et de la conformité, la migration des données et l'intégration opérationnelle. En suivant ce processus, les organisations peuvent migrer et optimiser efficacement leurs déploiements MongoDB dans le cloud AWS, en profitant des avantages d'un environnement de base de données entièrement géré, évolutif et sécurisé.</p>""}]"
Comment les supergraphes contribuent à consolider l'accès aux données,"[{'', ""<p>GraphQL, un langage de requête pour les API, est devenu un outil de plus en plus courant pour le développement de nouveaux outils basés sur le Web. GraphQL peut aider à rationaliser l'accès aux données en fournissant toutes les ressources dont un développeur a besoin dans une seule requête. Par rapport à l'approche REST traditionnelle de l'accès aux données basées sur le Web, GraphQL dispose d'un schéma plus standardisé et plus navigable et d'une expérience de développement sans doute meilleure.</p>""}, {'', '<p>Mais GraphQL ne se contente pas d’améliorer la convivialité des développeurs pour un modèle de données unique. S’il est utilisé pour regrouper diverses sources de données et API dans un graphique unifié, GraphQL pourrait devenir « un schéma unique pour les gouverner toutes » pour les entreprises. Certains qualifient cette architecture de supergraphe.</p>'}, {'', '<p>J’ai récemment rencontré Tanmai Gopal, PDG de Hasura, pour explorer le concept de supergraphe et voir comment il peut contribuer à rendre les données plus accessibles aux entreprises. Ci-dessous, nous examinerons l’expansion du concept de supergraphe et les avantages du déploiement d’une telle stratégie.</p>'}, {'', '<p>Dans les grandes architectures logicielles, les équipes produit doivent avoir accès à diverses sources de données issues de différents domaines. « Il existe un besoin d’expériences intégratives, qui deviennent de plus en plus importantes pour l’utilisateur final », explique Gopal. Par exemple, une application de commerce électronique peut être composée de données de catalogue de produits, de fonctionnalités de connexion, de données de paiement, d’informations d’expédition et d’autres informations nécessaires.</p>'}, {'', '<p>Cependant, il existe des obstacles à l’extraction de données provenant de ces différents domaines. Différents domaines sont souvent conçus sous forme de microservices, pris en charge par une équipe dédiée et utilisant un portail de développement et un schéma d’API sur mesure pour externaliser les services, a déclaré Gopal. Les frictions liées à l’intégration des API et l’absence d’un registre commun de services pourraient ralentir le développement.</p>'}, {'', '<p>Non seulement les microservices qui exposent des points de terminaison discrets posent des problèmes, mais les frontières entre interne et externe sont floues, ce qui nécessite une approche zero-trust. Et si nous commençons à considérer les données comme un produit, explique Gopal, nous devons accéder à plusieurs domaines de manière sécurisée, cohérente et évolutive.</p>'}, {'', ""<p>Un supergraphe peut résoudre certaines de ces difficultés, a expliqué Gopal. En exposant des graphes provenant de différents services de domaine, vous pouvez automatiquement unifier les données dans un supergraphe, auquel différentes équipes peuvent accéder. Et l'avantage de l'utilisation de GraphQL est que l'acte de documentation est l'acte de construction de l'API, ce qui signifie que la description et le schéma sont plus standardisés.</p>""}, {'', ""<p>En regroupant des services disparates dans un registre centralisé, vous pourriez travailler sur plusieurs domaines de manière systématique mais fédérée, a déclaré Gopal. L'implémentation d'une couche ici pourrait également être un bon domaine pour appliquer une identité ou une authentification basée sur les rôles.</p>""}, {'', '<p>Andrew Carlson, architecte principal chez Apollo GraphQL, a également proposé d’utiliser GraphQL pour centraliser le contrôle d’accès aux données. « Lorsqu’il est utilisé comme couche pour agréger et orchestrer les API existantes », a déclaré Carlson. « C’est un emplacement idéal dans notre architecture pour centraliser le contrôle d’accès et l’autorisation jusqu’au niveau du terrain, offrant une observabilité au niveau du terrain pour savoir quels clients demandent quelles données. »</p>'}, {'', '<p>Gopal a partagé une poignée d’autres avantages spécifiques liés à l’utilisation de GraphQL dans ce contexte\xa0:</p>'}, {'', ""<li>Vous aide à écrire moins de code\xa0: comme GraphQL vous permet d'extraire le champ de votre choix, l'alignement sur GraphQL en tant qu'interface commune pourrait réduire le nombre de requêtes requises pour intégrer les données. De plus, certains avantages liés à l'expérience du développeur, tels que la saisie semi-automatique, pourraient améliorer la découverte des schémas et présenter des avantages exponentiels lorsque les sources sont entrelacées dans un graphique combiné.</li>""}, {'', '<li>Augmente l’agilité pour le nouveau développement : la nécessité de comprendre rapidement le schéma global et son apparence devient particulièrement pertinente, notamment pour les applications d’IA, a déclaré Gopal. L’utilisation d’un schéma de supergraphes apporte un gain d’agilité considérable, a-t-il ajouté, notamment en permettant aux équipes de développement de logiciels d’extraire le contexte dont elles ont besoin pour mettre rapidement les choses en mouvement.</li>'}, {'', '<li>Améliore la découverte des données et des services : les équipes de développement logiciel sont confrontées à une prolifération d’outils et le nombre d’API utilisées au sein d’une organisation ne cesse d’augmenter. Un supergraphe peut permettre un maillage de données traversable, ce qui facilite grandement la découverte et, par conséquent, la capacité d’innovation. « L’avantage de la capacité à comprendre et à utiliser des données provenant de différents domaines est énorme », a déclaré Gopal.</li>'}, {'', ""<li>Unifie les équipes de différents domaines : un autre avantage potentiel du concept de supergraphe est l'amélioration de la collaboration entre les services. Aider les équipes à travailler ensemble pourrait améliorer la création de rapports et réduire les risques. Ce nouveau modèle de fonctionnement partagé permet de faire le ménage lors de la mise à jour du code et du schéma, a déclaré Gopal.</li>""}, {'', '<p>L’idée de regrouper des sources et des formats de données disparates dans une couche standard et unifiée est un concept séduisant. « L’impact en termes d’agilité est énorme », a déclaré Gopal. Surtout, ajoute-t-il, si vous avez la possibilité d’automatiser la création d’un supergraphe autour de différents styles de bases de données existantes.</p>'}, {'', '<p>Cependant, l’investissement dans un supergraphe peut prendre du temps. Les avantages se débloquent au fil du temps, mais vous aurez besoin d’un champion interne pour le défendre au départ et le concrétiser, prévient-il. En outre, les entreprises peuvent hésiter à se lancer à fond dans GraphQL en raison d’investissements antérieurs dans d’autres types de technologies.</p>'}, {'', '<p>Bien que GraphQL gagne en popularité, divers styles d’API sont encore couramment utilisés et continueront d’être pris en charge pendant un certain temps. Par exemple, une étude réalisée en 2023 par la société de gestion et de test d’API Postman a révélé que l’utilisation de GraphQL avait éclipsé le format d’API principal de SOAP. Pourtant, REST reste le style d’API dominant, utilisé par 86 % des répondants, suivi par Webhooks (36 %), GraphQL (29 %) et SOAP (26 %).</p>'}, {'', ""<p>L'étape suivante consiste donc à rendre le supergraphe indépendant de GraphQL, a déclaré Gopal. Au lieu de cela, il devrait pouvoir fonctionner avec différents protocoles puisque la plupart des entreprises utilisent plusieurs styles.</p>""}]"
KubeCon 2023 : Télémétrie et gestion des données,"[{'', ""<p>Orateur 1\xa0: C'est Techstrong TV.</p>""}, {'', ""<p>Alan Shimel : Salut à tous. Nous sommes de retour à KubeCon et c'est très animé. C'est un espace immense avec beaucoup de monde qui se promène, alors nous espérons que vous aimerez ça. C'est un peu différent de ce que nous faisons habituellement. Habituellement, les gens ne peuvent pas voir l'arrière-plan parce que les caméras sont orientées dans cette direction, mais nous avons généralement le fond typique que vous voyez sur certains de ces autres stands. Mais nous voulions donner aux gens une idée de ce que nous sommes en vie ici.</p>""}, {'', '<p>Tucker Callaway : Nous sommes en plein dedans.</p>'}, {'', '<p>Alan Shimel : Nous sommes en plein milieu de cette période. C’est un peu comme si vous regardiez un match de foot le dimanche soir. C’est donc ce que nous recherchions. Mais quoi qu’il en soit, laissez-moi vous présenter Tucker Callaway. Tucker est le PDG d’une entreprise appelée Mezmo. Certains d’entre vous en ont peut-être entendu parler, je crois que c’était Log DNA.</p>'}, {'', '<p>Tucker Callaway : Avant. Ouais.</p>'}, {'', ""<p>Alan Shimel : Et ils ont changé de cap. C'était il y a environ quatre ans ?</p>""}, {'', '<p>Tucker Callaway : Il y a environ deux ans maintenant.</p>'}, {'', '<p>Alan Shimel : Il y a deux ans.</p>'}, {'', ""<p>Tucker Callaway : Je suis ici depuis quatre ans, mais le grand changement a eu lieu il y a deux ans, j'ai changé de nom.</p>""}, {'', ""<p>Alan Shimel : Rebaptisé Mezmo et avec une mission légèrement différente, disons. Et puis j'ai eu l'occasion de retrouver Tucker à RSA la dernière fois, je crois que c'était en avril dernier.</p>""}, {'', '<p>Tucker Callaway : Avril. Ouais.</p>'}, {'', ""<p>Alan Shimel : Cette année, c'est en mai. Je pense que c'était à peu près à cette époque l'année dernière. Et pour ceux d'entre vous qui veulent voir cette interview, elle est en fait disponible sur Techstrong TV. Si vous recherchez RSA 2023, elle y est. Mais alors Tucker, avant d'aborder ce sujet, revenons en arrière. Mezmo. Tout le monde ici n'est pas au courant. Écoutez, tout le monde ici ne connaissait pas non plus Log DNA, alors ne vous inquiétez pas.</p>""}, {'', ""<p>Tucker Callaway : C'est vrai. Ouais.</p>""}, {'', '<p>Alan Shimel : Commençons par là. Parlons de la mission.</p>'}, {'', '<p>Tucker Callaway : Oui, la mission de Mezmo est essentiellement d’aider les gens avec leurs données de télémétrie. Nous avons donc constaté qu’il y a en fait trois problèmes principaux dans le domaine des données de télémétrie. Il y a trop de données de télémétrie. Elles ne sont pas au bon format. Elles ne sont pas au bon endroit. C’était donc en quelque sorte l’un de nos principes fondateurs pour résoudre ce problème.</p>'}, {'', '<p>Alan Shimel : Oui, je pense ça.</p>'}, {'', '<p>Tucker Callaway : Résoudre le problème.</p>'}, {'', '<p>Alan Shimel : Ces trois-là étaient solides comme un roc.</p>'}, {'', '<p>Tucker Callaway : Difficile de débattre.</p>'}, {'', '<p>Alan Shimel : Difficile de débattre de la question. C’est la bonne façon de le dire. Et puis, très franchement, ces trois problèmes se compliquent mutuellement.</p>'}, {'', '<p>Tucker Callaway : Ils le font.</p>'}, {'', '<p>Alan Shimel : C’est comme si vous aviez trop de données. C’est un gros problème, mais nous avons trop de données que nous ne savons pas comment classer, ce qui aggrave exponentiellement le problème. Donc, ce sont…</p>'}, {'', '<p>Tucker Callaway : Des problèmes de composition. Aucun doute là-dessus.</p>'}, {'', '<p>Alan Shimel : Aucun doute là-dessus. Donc, lorsque nous étions chez RSA, vous aviez l’impression d’être sur le point de prendre conscience de quelque chose, n’est-ce pas ? Vous alliez aider les clients. Pourquoi ne le faites-vous pas ? Eh bien, vous racontez l’histoire mieux que moi.</p>'}, {'', '<p>Tucker Callaway : Comme nous l’avons évoqué plus tôt, nous venions de lancer cette nouvelle fonctionnalité de produit, le pipeline de télémétrie chez RSA. Nous avons depuis lors de nombreux clients formidables et une expérience formidable. Lorsque nous réfléchissons à la valeur fondamentale de ce qu’il apporte, nous pensons qu’il aide les gens à gérer les coûts, à obtenir de meilleures informations et à faire respecter la conformité. Ce qui nous a surpris au cours des derniers mois, c’est à quel point les gens comprennent peu leurs données.</p>'}, {'', '<p>En fait, nous avons consacré beaucoup de temps et d’efforts en tant qu’industrie à maîtriser nos applications et notre infrastructure, mais nous n’avons pas encore vraiment maîtrisé nos données de télémétrie, et c’est là que se situe la grande opportunité que nous voyons. Mais pour obtenir la valeur que j’ai décrite, les gens n’avaient pas besoin de mieux comprendre leurs données. Et cela a donc été une grande révélation pour nous.</p>'}, {'', '<p>Nous venons de lancer notre fonctionnalité de profilage des données qui aide les utilisateurs à catégoriser et à comprendre les données de base, à gérer les schémas de leurs données, ce qui constitue une base très solide pour les principes d’exploitation des données appliqués aux données de télémétrie. Ensuite, après avoir résolu ce problème, nous avons découvert que les utilisateurs comprenaient désormais leurs données et qu’ils étaient capables de les optimiser parce qu’ils les comprenaient. Ils ont également réalisé que le contexte et les données de télémétrie évoluent rapidement. Par exemple, vous ne souhaitez pas stocker toutes ces données, car vous n’en avez pas besoin jusqu’à ce que vous en ayez vraiment besoin.</p>'}, {'', '<p>Alan Shimel : Je vais en prendre un peu, ils veulent stocker les données parce que les gens sont des thésauriseurs de données.</p>'}, {'', ""<p>Tucker Callaway : L'accumulateur de données. Ouais.</p>""}, {'', '<p>Alan Shimel : C’est quand ils doivent payer la facture pour stocker toutes ces données qu’ils se disent : « Attendez une seconde, je n’ai pas besoin de toutes ces données. Pas à ce prix-là. »</p>'}, {'', '<p>Tucker Callaway : Il faut que quelque chose cède quelque part.</p>'}, {'', '<p>Alan Shimel : Exactement.</p>'}, {'', '<p>Tucker Callaway : Nous avons donc réalisé que si nous prenons en compte cette valeur, nous pensons que comprendre, optimiser et répondre, nous avons dit que l’étape de compréhension et d’optimisation de la réponse était très importante pour nous. Ainsi, lorsqu’un nouveau contexte arrive, le pipeline peut modifier son comportement et ses optimisations pour tenir compte du flux actuel de données et des besoins actuels. Ainsi, si vous réduisez l’échantillonnage à 20 % et que vous voyez un événement, vous voyez les performances, vous pouvez réhydrater, retraiter, modifier dynamiquement la réactivité du pipeline et faire parvenir toutes les données au bon endroit pour les bonnes personnes, ce qui permet de résoudre le problème des coûts, mais vous donne également la visibilité complète dont vous avez besoin.</p>'}, {'', '<p>Alan Shimel : Si l’on devait réaliser une étude de cas dans une école de commerce sur ce sujet, quelqu’un qui n’est pas issu de ce secteur dirait : « Hé, le simple fait que vous ayez découvert que la plupart de ces organisations ne savent même pas quelle quantité de données elles possèdent serait suffisant pour construire une entreprise autour de cela. »</p>'}, {'', '<p>Tucker Callaway : Oui, j’aimerais le penser.</p>'}, {'', '<p>Alan Shimel : Non, c’est logique. C’est logique, mais ce n’est pas le cas, c’est le seuil. Parce que je pense que vous avez mis le doigt dessus, Tucker, c’est qu’une fois qu’ils ont reconnu cela, ils ont une toute nouvelle perspective de réalisation : « Mon Dieu, j’ai toutes ces données, maintenant je veux commencer à les comprendre. Waouh. Je vois des choses que je n’aurais jamais pensé voir. Peut-être que je pensais intuitivement que c’était le cas, mais j’ai compris maintenant. »</p>'}, {'', '<p>Tucker Callaway : Nous aimons penser que c’est fascinant, n’est-ce pas ?</p>'}, {'', ""<p>Alan Shimel : Ok, j'adore. Je suis fasciné par Mezmo.</p>""}, {'', ""<p>Tucker Callaway : Je ne pouvais pas ne pas prendre la photo, mais c'était juste-</p>""}, {'', ""<p>Alan Shimel : Ouais, c'était ton coup.</p>""}, {'', ""<p>Tucker Callaway : C'est le dérivé du nom.</p>""}, {'', '<p>Alan Shimel : Je me sens utilisé. Je me sens utilisé.</p>'}, {'', ""<p>Tucker Callaway : Je m'y suis remis un peu.</p>""}, {'', '<p>Alan Shimel : Vous êtes fasciné et vous vous dites : « Oh mon Dieu, que puis-je faire avec ça ? » Et c’est vraiment devenu votre métier. Et je suppose que c’est là que vous en êtes maintenant.</p>'}, {'', '<p>Tucker Callaway : La prochaine étape pour nous est clairement celle-là. Oui, et je pense qu’il y a deux façons de procéder. Il y a la question : « Que dois-je en faire ? Aidez-moi, dites-moi quoi faire. Donnez-moi les meilleures pratiques de tous vos clients, que dois-je faire ? » Nous sortirons donc probablement plus tard au printemps quelque chose qui vous permettra de simuler à quoi ces données pourraient ressembler une fois l’optimisation appliquée, afin que vous puissiez commencer à prendre ces décisions plus efficacement.</p>'}, {'', '<p>Mais la deuxième phase de cette démarche ne sera pas non plus suffisante pour certaines organisations qui souhaitent considérer leurs données comme un avantage stratégique. Elles vont donc chercher des moyens d’agir et d’extraire davantage d’informations de ces données. Nous envisageons cela à travers des recettes. Nous vous proposerons les offres standard, mais vous aurez ensuite la possibilité d’y accéder et de les modifier sous forme de code et de faire ce que vous devez faire pour tirer davantage de valeur de ces données. Si vous êtes prêt pour des mouvements de yoga avancés dans l’espace des données.</p>'}, {'', ""<p>Alan Shimel : Je l'ai, je l'ai, je l'ai. J'ai une formation en sécurité.</p>""}, {'', '<p>Tucker Callaway : Ouais.</p>'}, {'', '<p>Alan Shimel : J’ai lancé une entreprise de sécurité en 2001, et c’était juste au moment où nous passions de l’IDS à l’IPS. Donc de la détection d’intrusion à la prévention d’intrusion. Même chose avec la gestion des vulnérabilités. Nous sommes passés de la simple recherche de vulnérabilités à l’application de correctifs, de mesures correctives, pas toujours. Qui ne voudrait pas de cela ? C’est évident, n’est-ce pas ? Ne me dites pas que je suis attaqué. Bloquez l’attaque. Au moment où vous me le dites, l’attaque a eu lieu. Ne me dites pas simplement que vous avez trouvé une vulnérabilité, assurez-vous que je ne suis pas exploité. Mais une chose amusante que j’ai apprise est que beaucoup de gens disent : « Allez-y doucement. Je ne peux pas me le permettre, car parfois la vulnérabilité, l’attaque ou les données sur lesquelles vous voulez que j’agisse peuvent affecter quelque chose que je considère comme plus précieux. »</p>'}, {'', ""<p>Tucker Callaway : Oui, c'est vrai.</p>""}, {'', '<p>Alan Shimel : Et j’aime y aller doucement quand il s’agit de laisser un programme, une application ou un produit faire des choses. Donnez-moi juste mon menu et je déciderai de ce que je veux faire. Bien sûr, c’était avant l’avènement de l’IA, du ML et de beaucoup d’automatisations. Et très franchement, la vitesse des affaires était probablement un peu plus lente à l’époque. Voyez-vous un stade où les gens vont vouloir cela ? Donnez-moi juste les meilleures pratiques et je déciderai quand, comment et où je veux les mettre en œuvre. Ou bien allons-nous directement à « Hé mec, fais en sorte que cela se réalise pour moi ».</p>'}, {'', '<p>Tucker Callaway : Je pense qu’il y a une étape à franchir lorsque nous pensons à la gestion des données, à la télémétrie, à la gestion des données en général. Le mot qui est le plus important pour nous est la confiance, n’est-ce pas ? C’est une chose d’optimiser les données, mais il faut avoir confiance que les données sont constamment optimisées et traitées de la bonne manière pour tous les différents besoins que vous avez décrits.</p>'}, {'', '<p>Je pense donc que la première étape consiste à faire une suggestion avec un humain dans la boucle. Vous voulez appuyer sur ce bouton pour dire «\xa0go\xa0», puis lorsque vous établissez la confiance dans ces données et que vous savez que les algorithmes fonctionnent, vous les laissez aller. Probablement avec quelques garde-fous sur le côté, je vais les laisser aller, mais je suis [inaudible 00:09:52] juste au cas où. Et ensuite vous commencerez à l’optimiser de plus en plus. Et je pense que naturellement, presque comme une plate-forme de données en temps réel, le système d’exploitation commencera à évoluer.</p>'}, {'', '<p>Alan Shimel : La clé ici est la confiance.</p>'}, {'', '<p>Tucker Callaway : La clé est la confiance.</p>'}, {'', '<p>Alan Shimel : Pour instaurer la confiance envers les autres. En tant que PDG, comment instaurer cette confiance ?</p>'}, {'', '<p>Tucker Callaway : Eh bien, la confiance est toujours une question difficile.</p>'}, {'', '<p>Alan Shimel : Je suis intéressé, dites-moi.</p>'}, {'', '<p>Tucker Callaway : Oui. Je pense que comme pour tout le reste dans la vie, la confiance est instaurée par une prestation cohérente. C’est pourquoi je pense que l’intervention humaine dans la boucle est importante. Elle doit être durable. Cela ne peut pas être un moment précis, cela ne peut pas être la toute première mise en œuvre. C’est quelque chose que vous gagnez au fil du temps, où les gens peuvent compter sur vous ou sur les données ou sur la façon dont vous traitez les données pour vous amener au résultat souhaité. Ce n’est donc vraiment qu’en fournissant des résultats au client qu’il fera confiance aux systèmes et à toutes ces choses.</p>'}, {'', '<p>Alan Shimel : Cela a du sens ?</p>'}, {'', '<p>Tucker Callaway : Ouais.</p>'}, {'', '<p>Alan Shimel : Très bien. Je dois faire un peu de ménage.</p>'}, {'', '<p>Tucker Callaway : Ok, faisons-le.</p>'}, {'', '<p>Alan Shimel : Les gens qui veulent en savoir plus sur Mezmo.</p>'}, {'', '<p>Tucker Callaway : Rendez-vous sur le site Web. Nous avons une offre pour profiler les données de n’importe qui gratuitement dès maintenant.</p>'}, {'', '<p>Alan Shimel : Vraiment ?</p>'}, {'', '<p>Tucker Callaway : Oui. Venez sur mezmo.com ou arrêtez-vous à notre stand.</p>'}, {'', '<p>Alan Shimel : MEZM-O.com.</p>'}, {'', '<p>Tucker Callaway : M-E-Z-M-O.com</p>'}, {'', '<p>Alan Shimel : Là-bas, dans le monde de la télévision ou si vous entendez un coupon.</p>'}, {'', ""<p>Tucker Callaway : Venez au stand. J'ai oublié le numéro, mais il est là-bas.</p>""}, {'', '<p>Alan Shimel : Vous savez quoi, si vous regardez sur le « je ne l’ai pas ». Si vous regardez au dos de votre carte d’identité, vous pouvez effectivement chercher des choses comme ça.</p>'}, {'', ""<p>Tucker Callaway : Oui, c'est là-bas. Si vous vous êtes connecté au wifi ici à KubeCon, c'est Mezmo Data.</p>""}, {'', '<p>Alan Shimel : J’ai remarqué ça.</p>'}, {'', ""<p>Tucker Callaway : C'est le wifi.</p>""}, {'', '<p>Alan Shimel : Je me suis dit : « Waouh, quelle chose cool. » Ouais.</p>'}, {'', '<p>Tucker Callaway : Vous nous avez vus. Venez nous voir. Le profilage de vos données vous donnera un aperçu et une compréhension de cela et vous parlera des prochaines étapes.</p>'}, {'', '<p>Alan Shimel : Très bien, allez voir ça sur Mezmo. Tucker, merci.</p>'}, {'', ""<p>Tucker Callaway : Merci de m'avoir invité.</p>""}, {'', '<p>Alan Shimel : En fait, on se voit. Serez-vous présent chez AWS ?</p>'}, {'', '<p>Tucker Callaway : Oui, nous serons là. Oui, je serai là. Oui.</p>'}, {'', '<p>Alan Shimel : Nous allons faire des vidéos, mais pas sur le salon. Ce sera un peu plus calme dans une suite privée. Et puis, bien sûr, vous serez à la RSA.</p>'}, {'', '<p>Tucker Callaway : Nous serons à RSA.</p>'}, {'', '<p>Alan Shimel : Nous serons là aussi.</p>'}, {'', '<p>Tucker Callaway : Nous ne serons peut-être pas présents sur le salon AWS simplement à cause de-</p>'}, {'', ""<p>Alan Shimel : C'est un peu fou, non ? Un peu cher.</p>""}, {'', '<p>Tucker Callaway : Ouais.</p>'}, {'', '<p>Alan Shimel : Oui, je vous entends. C’est pour ça que nous serons au studio.</p>'}, {'', '<p>Tucker Callaway : On se voit là-bas.</p>'}, {'', '<p>Alan Shimel : Viens, je vais prendre un café.</p>'}, {'', '<p>Tucker Callaway : Cela semble bien.</p>'}, {'', '<p>Alan Shimel : Mais RSA sera présent à Broadcast Alley et nous ferons notre truc DevSecOps lundi. Et le genre de folie RSA habituelle.</p>'}, {'', ""<p>Tucker Callaway : J'ai hâte d'y être.</p>""}, {'', '<p>Alan Shimel : Absolument.</p>'}, {'', ""<p>Tucker Callaway : D'accord.</p>""}, {'', '<p>Alan Shimel\xa0: Consultez Mezmo.com.</p>'}, {'', '<p>Tucker Callaway : Merci à tous.</p>'}, {'', ""<p>Alan Shimel : Tucker Callaway ici sur Tech Drunk TV. Nous allons faire une pause. Je suppose qu'il est presque l'heure du déjeuner ici, mais il y a encore du monde.</p>""}, {'', '<p>Tucker Callaway : Presque.</p>'}, {'', '<p>Alan Shimel : Restez à l’écoute. Nous sommes là toute la journée. Nous reviendrons. Au revoir.</p>'}]"
L'observabilité des données et son importance : tout ce que vous devez savoir,"[{'', '<p>Dans le monde actuel, où la technologie est omniprésente, les entreprises s’appuient sur des volumes de données presque incompréhensibles pour dicter leurs opérations et leurs décisions commerciales. Les entreprises se concentrent sur la création de multiples référentiels et pipelines de données pour traiter, stocker, gérer et utiliser les données provenant de diverses sources. Compte tenu de la taille et de la complexité croissantes de l’environnement de données d’entreprise, il devient de plus en plus difficile de garantir l’exactitude et l’exhaustivité des données. Alors, comment comprendre simultanément les performances des données sur l’ensemble de l’infrastructure informatique ? La réponse est l’observabilité des données.</p>'}, {""<h3>Qu'est-ce que l'observabilité des données ?</h3>"", ''}, {'', ""<p>L'observabilité des données est un processus qui vise à vous alerter sur la fiabilité et la santé de vos données tout en fournissant les informations et les analyses nécessaires pour identifier et résoudre les problèmes avant qu'ils n'affectent l'ensemble de votre organisation.</p>""}, {'', ""<p>La surveillance des opérations dans la base de données sert de mécanisme de défense proactif contre les menaces potentielles de sécurité. Cela garantit des données précises, complètes, sécurisées et précieuses et élimine les temps d'arrêt des données. Plongeons plus en détail dans l'infrastructure d'observabilité et explorons certaines des meilleures plateformes d'observabilité des données qui peuvent vous aider à optimiser et à sécuriser vos opérations de données.</p>""}, {'', ""<h3>Cadre d'observabilité des données</h3>""}, {'', '<p>Nous savons que l’observabilité repose sur trois piliers : les journaux, les traces et les métriques.</p>'}, {'', '<p>Cependant, l’observabilité des données repose sur cinq piliers qui, ensemble, fournissent des informations clés sur la fiabilité et la qualité de vos données.</p>'}, {'', '<p>L’intégration de chaque pilier peut vous aider à élaborer une stratégie d’observabilité efficace.</p>'}, {'', ""<p>1. Récence : également appelée « fraîcheur », la récence consiste à confirmer si les données sont à jour. Elle analyse également les éventuels écarts temporels inhabituels dans les tableaux de données. Cela permet d'éviter les problèmes de rapidité dans les pipelines de données.</p>""}, {'', '<p>2. Volume : Le volume consiste à vérifier si la quantité de données contenues dans la base de données correspond aux seuils prévus. Cela permet de garantir que les ensembles de données sont complets.</p>'}, {'', '<p>3. Distribution : cette mesure permet de mesurer la qualité des données au niveau du champ et de confirmer si les valeurs des données se situent dans les plages attendues. Des fluctuations inattendues dans les modèles de distribution indiquent un problème de données.</p>'}, {'', ""<p>4. Schéma : il s'agit de surveiller et d'auditer les modifications apportées aux tables de données et à l'organisation des données pour rechercher des signes de données endommagées. Les schémas sont extrêmement importants, car les modifications de la structure des données sources sont souvent la cause des temps d'arrêt des données.</p>""}, {'', '<p>5. Lineage : Lineage collecte les métadonnées et fournit une image complète du paysage de données de votre organisation, y compris les sources en amont, en aval et les équipes qui peuvent accéder aux données à chaque étape. Ce processus aide les équipes de données à résoudre les problèmes liés aux ruptures de données.</p>'}, {'', ""<h3>Avantages de l'observabilité des données</h3>""}, {'', '<p>L’observabilité des données offre une meilleure visibilité sur l’état interne de votre système informatique, notamment son comportement, ses performances et ses interactions avec d’autres systèmes. Cela peut être bénéfique de plusieurs manières\xa0:● Facilite l’analyse des causes profondes\xa0: grâce à une visibilité et une surveillance des données de bout en bout sur une infrastructure informatique multicouche, l’observabilité des données permet aux équipes de données de repérer rapidement les problèmes dans les ensembles de données avec moins d’efforts. Cela augmente également les chances d’identifier de nouveaux problèmes, quelle que soit leur origine.</p>'}, {'', ""<p>● Délai moyen de détection et de résolution plus rapide : comme l'observabilité des données surveille une large gamme de résultats, les équipes de données peuvent trier et déboguer activement leurs systèmes. En fournissant des informations précieuses sur la manière dont les données interagissent et se déplacent au sein de l'architecture informatique, l'observabilité des données aide les équipes à repérer les problèmes qu'elles ignoraient, ce qui se traduit par un délai moyen de détection (MTTD) et un délai moyen de résolution (MTTR) plus rapides.</p>""}, {'', ""<p>● Automatisation de la gestion de la sécurité : l'observabilité des données offre non seulement une visibilité en temps réel sur la posture de sécurité, mais facilite également l'automatisation de certaines parties du processus de tri. Cela permet de détecter instantanément les problèmes d'intégrité des données ou les temps d'arrêt des données.</p>""}, {""<h3>Défis d'observabilité des données</h3>"", ''}, {'', ""<p>En fonction de leur architecture informatique existante, les organisations peuvent être confrontées aux défis suivants en matière d'observabilité des données :● Silos de données : en raison de la présence de plusieurs agents, d'outils de surveillance des silos et de sources de données disparates, il devient difficile de comprendre les interdépendances entre les applications, les canaux numériques et les différents clouds.</p>""}, {'', ""<p>● Intégration à l'écosystème de données complet : pour que l'observabilité des données fonctionne, l'outil doit disposer d'informations sur l'ensemble du pipeline de données et sur les serveurs, bases de données, logiciels et applications concernés. Cependant, certaines organisations peuvent trouver difficile de connecter tous les systèmes à une plateforme d'observabilité des données.</p>""}, {'', ""<p>● Instrumentation et configuration manuelles : les outils d'observabilité des données visent à normaliser les données de télémétrie et les directives de journalisation afin de corréler efficacement les informations. Cependant, comme les grandes organisations gèrent plusieurs sources de données (des centaines, voire des milliers), les données de ces sources peuvent avoir des normes différentes. Cela nécessite un effort manuel pour normaliser les données.</p>""}, {'', '<h3>Observabilité des données vs. Gouvernance des données</h3>'}, {'', '<p>Dans le contexte actuel de digitalisation, où la sécurité des données est une préoccupation mondiale, la gouvernance des données fait l’objet d’une attention croissante. La gouvernance des données permet de définir les politiques et les procédures nécessaires pour contrôler la manière dont une organisation collecte, analyse, stocke, partage et utilise ses données.</p>'}, {'', ""<p>Un programme de gouvernance des données solide garantit la disponibilité, l'intégrité, la facilité d'utilisation et la sécurité des données. Il élimine les problèmes d'intégration des données, les silos de données et la mauvaise qualité des données, résolvant ainsi les défis de l'observabilité des données. Les ensembles de données étant désormais évolutifs avec davantage de tables, davantage de sources de données et davantage de complexité, les ingénieurs et développeurs de données sont sous pression pour répondre aux exigences de cohérence, de disponibilité et de sécurité des données. Tout temps d'arrêt peut entraîner un gaspillage de ressources et de temps tout en détériorant la confiance dans la prise de décision.</p>""}, {'', ""<p>L'observabilité des données, ainsi que la gouvernance des données, aident les organisations à gérer les problèmes de sécurité et de qualité des données de manière simplifiée.</p>""}, {'', '<h3>Observabilité des données et qualité des données</h3>'}, {'', ""<p>La qualité des données mesure l'exhaustivité et l'exactitude des ensembles de données afin de déterminer s'ils peuvent être utilisés dans des applications analytiques et opérationnelles. L'observabilité des données, quant à elle, permet aux organisations de détecter et de résoudre les problèmes dans le pipeline de données de manière efficace et rapide.</p>""}, {'', ""<p>Pour une gestion efficace des données, une entreprise doit prendre en compte ces deux éléments. Auparavant, la vérification et le nettoyage manuels des ensembles de données permettaient de déterminer la qualité des données. Mais aujourd'hui, bon nombre de ces tâches ont été automatisées grâce aux piles de données modernes.</p>""}, {'', '<p>En conséquence, l’accent est passé de la qualité des données à l’observabilité des données. Grâce à l’observabilité des données, les entreprises peuvent surveiller et résoudre efficacement les problèmes de leur pipeline de données. Sans cela, elles risquent de s’appuyer sur des données incomplètes ou inexactes pour prendre des décisions. Cela peut entraîner des erreurs coûteuses.</p>'}, {'', ""<h3>Trouver les bons outils d'observabilité des données</h3>""}, {'', ""<p>L'observabilité des données est sans aucun doute une caractéristique essentielle de toute entreprise qui utilise des données. Cependant, tous les outils d'observabilité des données ne sont pas aussi bénéfiques pour votre entreprise. Recherchez les caractéristiques suivantes lors du choix d'une plateforme d'observabilité des données : ● Compatibilité : l'outil doit être compatible avec vos lacs de données, bases de données et solutions de stockage cloud.</p>""}, {'', ""<p>● Autonome : la technologie autonome réagit aux stimuli sans aucune intervention humaine. Cela est essentiel dans un outil d'observabilité des données car cela permet de détecter rapidement les anomalies et de répondre instantanément aux alertes.</p>""}, {'', ""<p>● Rapide : un outil d'observabilité des données doit vous aider à identifier les erreurs le plus tôt possible. La meilleure plateforme surveille en permanence l'état des données depuis leur ajout à l'écosystème jusqu'à la fin de leur cycle de vie. La détection précoce des erreurs permet d'éviter les problèmes avant qu'ils ne deviennent critiques.</p>""}, {'', '<p>● Sophistiqué : les meilleures plateformes exploitent l’apprentissage automatique (ML) et l’intelligence artificielle (IA) pour identifier les problèmes difficiles à trouver.</p>'}, {'', ""<p>● Autres fonctionnalités : L'outil doit être capable de collecter, d'échantillonner, d'examiner et de traiter des données de télémétrie provenant de plusieurs sources de données. Il doit servir de référentiel de données centralisé, offrir des services complets de surveillance des données et permettre la visualisation des données.</p>""}, {'', '<p>En fin de compte, la bonne plateforme d’observabilité des données dépend des besoins d’ingénierie d’observabilité de votre organisation et de son architecture informatique unique.</p>'}, {'', ""<h3>Principaux outils d'observabilité des données</h3>""}, {'', ""<p>Les plateformes d'observabilité des données sont encore une catégorie de produits en plein essor. La bonne nouvelle est que plus d'une demi-douzaine d'entreprises spécialisées dans l'observabilité des données proposent désormais des outils commerciaux dotés d'excellentes fonctionnalités. Il s'agit notamment des éléments suivants :</p>""}, {'', ""<h3>Fournisseurs d'observabilité, aperçu, avantages et inconvénients</h3>""}, {'', '<p>Monte Carlo Data, dont le siège social se trouve à San Francisco, est le créateur du premier outil d’observabilité des données de bout en bout du secteur.</p>'}, {'', ""<p>● Fournit des capacités complètes d'observabilité des données.● Offre un niveau élevé de fonctionnalités, notamment des alertes automatisées, des catalogues de données, etc.● Prend en charge une configuration entièrement automatisée.</p>""}, {'', ""<p>● Des volumes de données élevés peuvent entraîner des problèmes d'interface utilisateur.● D'énormes quantités de variables restreintes par différentes contraintes peuvent entraîner une inefficacité informatique.</p>""}, {'', ""<p>Bigeye est une plateforme d'observabilité des données de pointe qui permet aux équipes d'améliorer, de mesurer et de communiquer des données de qualité de manière claire et rapide à n'importe quelle échelle.</p>""}, {'', ""<p>● Une interface facile à utiliser qui facilite la configuration des données tout en garantissant la cohérence et la précision.● Fournit de puissantes capacités d'intégration d'API.● Comprend un tableau de bord polyvalent avec suivi et surveillance en temps réel des mesures de qualité des données par plusieurs personnes.</p>""}, {'', ""<p>● Cela peut être coûteux pour les petites organisations.● L'outil peut parfois planter, nécessitant des améliorations de performances.</p>""}, {'', ""<p>Acceldata propose des outils pour Hadoop, les services cloud et les entreprises qui incluent la fiabilité des données de bout en bout, la surveillance du pipeline de données et l'observabilité des données multicouches.</p>""}, {'', '<p>● Fournit des contrôles de fiabilité des données entièrement automatisés.● Offre une interface glisser-déposer pour analyser les pipelines de données sur plusieurs couches et plates-formes.</p>'}, {'', ""<p>● La configuration initiale peut être complexe et difficile.● L'ajout et la suppression de nœuds nécessitent une intervention humaine.</p>""}, {'', '<p>Databand est une société IBM qui offre des capacités proactives pour identifier et résoudre les problèmes de données dès les premières étapes du cycle de développement.</p>'}, {'', ""<p>● Offre une visibilité sur l'ensemble de la pile. Cela signifie que vous pouvez obtenir une vue d'ensemble de toutes les tâches de données du début à la fin. ● Offre des DataOps standardisés et une lignée de données de bout en bout, garantissant la fiabilité et l'exactitude des données.</p>""}, {'', '<p>● Le programme nécessite une quantité importante d’espace, ce qui rend son installation sur le système de l’utilisateur difficile. ● Il nécessite des mises à jour logicielles constantes, ce qui le rend beaucoup plus lourd.</p>'}, {'', ""<p>Datafold possède une capacité unique à détecter, enquêter et hiérarchiser de manière proactive les erreurs de qualité des données avant qu'elles n'affectent la production.</p>""}, {'', '<p>● Vous permet de transformer les requêtes SQL en alertes intelligentes, vous tenant informé de tout problème pouvant survenir.● Automatise les tests de régression en intégrant le processus CI via GitLab et GitHub.</p>'}, {'', ""<p>● Options d'intégration limitées.● Ne fournit aucun support pour l'analyse des données et la science des données.</p>""}, {'', '<h3>Réflexions finales</h3>'}, {'', '<p>L’observabilité des données est l’épine dorsale de la capacité des ingénieurs de données à être agiles avec leurs produits. Si vous souhaitez moderniser vos pratiques de gestion des données et améliorer la qualité de vos données, l’observabilité des données est la voie à suivre. Sans elle, votre équipe de données ne peut pas compter sur ses outils et son infrastructure, car les problèmes ne peuvent pas être détectés efficacement et rapidement.</p>'}]"
Observabilité : la tour de guet centrale dont votre informatique a besoin pour tout voir,"[{'', '<p>Si vous devez garder quelque chose en sécurité, il est naturel de vouloir le garder sous clé. Cependant, même garder quelque chose dans un coffre-fort, loin des regards indiscrets et des mauvaises intentions, peut ne pas suffire à vous offrir la tranquillité d’esprit. Après tout, comment pouvez-vous avoir la tranquillité d’esprit de savoir que votre objet est en sécurité si vous ne vérifiez pas régulièrement qu’il n’a pas été verrouillé, cambriolé ou cassé ?</p>'}, {'', '<p>Maintenant, multipliez par dix le volume et la valeur des éléments surveillés, et vous aurez une idée des inefficacités et des angoisses quotidiennes auxquelles les professionnels de l’informatique sont confrontés.</p>'}, {'', '<p>Au lieu de bijoux, ces équipes protègent, gèrent et entretiennent quelque chose de bien plus précieux : les données qui sous-tendent tout, des transactions commerciales aux performances du système, en passant par la détection des menaces et la prestation de services, et bien plus encore. Pour compliquer encore les choses, les écosystèmes numériques d’aujourd’hui sont plus complexes, distribués et disparates que jamais.</p>'}, {'', '<p>Les bases de données sont essentielles à la gestion d’un service informatique performant. Il est temps de mettre un terme à l’idée largement répandue dans le secteur selon laquelle elles sont une boîte noire insondable. Les bases de données représentent en effet le composant le plus difficile à observer, à ajuster, à gérer et à faire évoluer des écosystèmes informatiques. Mais la base de données n’est pas une boule magique en termes de processus internes : elle est un outil indispensable pour gérer les processus internes. Nous n’avons plus besoin de secouer le jouet mystérieux et d’accepter l’une des six réponses qu’il est censé fournir.</p>'}, {'', '<p>Le fait est que les spécialistes des bases de données et les équipes informatiques ont besoin d’une vision claire de la télémétrie des performances des bases de données s’ils veulent espérer maintenir la santé, la stabilité et l’évolutivité de leurs services.</p>'}, {""<h3>Outils d'observabilité</h3>"", ''}, {'', ""<p>C'est là que les outils d'observabilité peuvent jouer un rôle transformateur. S'inspirant de la tour de guet panoptique omnisciente, l'observabilité élimine les recoins sombres du donjon de la base de données en offrant une vue complète de l'ensemble des piles technologiques cloud natives, sur site et hybrides.</p>""}, {'', ""<p>Le mot panoptique dérive du mot grec panoptes, qui signifie « tout voir ». Le modèle panoptique comprend une tour de garde centrale éclairée au centre d'une pièce circulaire, ce qui permet des lignes de vue à 360 degrés qui permettent aux gardes d'observer chaque cellule environnante à partir d'un seul point d'observation centralisé.</p>""}, {'', '<p>Le philosophe anglais Jeremy Bentham n’avait aucun moyen d’anticiper les défis auxquels sont confrontés les professionnels des bases de données d’aujourd’hui lorsqu’il a imaginé le panoptique dans les années 1700. Ce concept allait poser les bases d’un système pénitentiaire conçu autour d’un principe fondamental : permettre au nombre minimum de gardiens de surveiller efficacement le nombre maximum de détenus. Le système de Bentham fonctionne également comme une stratégie pour garantir la santé et les performances des bases de données critiques de votre organisation.</p>'}, {'', ""<p>Le panoptique a été conçu pour que chaque cellule puisse être surveillée à partir d'un point central, ce qui garantit non seulement une sécurité plus efficace et plus simple, mais aussi une préservation des ressources en termes de temps, de main-d'œuvre et d'efforts. Considérez vos équipes ITOps, base de données, DataOps et DevOps comme les gardiens de vos performances informatiques. La surveillance traditionnelle peut être comparée à la patrouille d'une prison traditionnelle : des rangées de cellules donnant sur un couloir qui est patrouillé selon un calendrier ou utilisé pour repérer et traiter les mauvais comportements dès qu'ils sont remarqués.</p>""}, {'', '<p>Comment peuvent-ils être sûrs de concentrer leurs efforts au bon endroit ? Ou être sûrs que le bloc cellulaire A ne déclenchera pas d’évasion alors qu’ils interrompent une dispute dans le bloc cellulaire B ?</p>'}, {'', '<p>En bref : ils ne peuvent pas. Il y a de fortes chances que l’équipe soit en train de courir d’une panne à l’autre dans un mode de lutte contre les incendies permanent, soit qu’elle s’appuie sur des contrôles ponctuels aléatoires pour quelque chose d’aussi vital que la santé de nos systèmes informatiques.</p>'}, {'', ""<p>L'observabilité, par opposition à la surveillance, place les équipes dans cette tour de guet panoptique centrale, leur permettant non seulement de tout voir, mais aussi de le voir dans le contexte d'une vue d'ensemble. Cette visibilité complète et cette observation permanente permettent aux équipes informatiques d'identifier les problèmes critiques au fur et à mesure qu'ils surviennent, même ceux causés par des dépendances complexes entre la base de données, le système d'exploitation, le sous-système de stockage et le réseau.</p>""}, {'', '<h3>Problèmes de performances de la base de données</h3>'}, {'', '<p>En outre, les problèmes de performances des bases de données sont le point de départ de goulots d’étranglement coûteux ou de pannes graves qui peuvent entraver la capacité de votre entreprise à être compétitive ou à se développer. Sans surveillance et observabilité complètes et précises des bases de données, les opérations informatiques ont du mal à déterminer avec précision la cause profonde des problèmes de performances d’une application. Cela augmente le risque de temps d’arrêt, de perte de données et de mauvaise expérience client.</p>'}, {'', '<p>La découverte de la cause profonde des problèmes de performances est une tâche critique pour le système, que vous dirigiez simplement votre entreprise, que vous déployiez un nouveau code ou que vous évoluiez dans le cadre de vos opérations. L’observabilité offre également aux équipes la rare opportunité d’anticiper les problèmes de performances, car vous pouvez voir les anomalies de performances avant une interruption de service plutôt que de vous fier à une correction en temps réel.</p>'}, {'', '<p>Avec les méthodes traditionnelles, les équipes DevOps et informatiques doivent analyser manuellement les données qui leur sont présentées, les corréler au problème et localiser l’erreur avant de pouvoir enfin commencer à la résoudre. En revanche, l’observabilité collecte des données pour fournir des informations sur ce qui ne fonctionne pas comme prévu et pourquoi. Cela permet aux équipes d’adopter une approche proactive pour résoudre les temps d’arrêt et les goulots d’étranglement et les prévenir de manière proactive.</p>'}, {'', '<p>Dans le monde du logiciel, cela se traduit directement par plus de temps consacré à ce qui compte : développer une nouvelle valeur commerciale au sein de vos applications et infrastructures, alimenter l’innovation et dépasser les attentes des clients.</p>'}, {'', '<p>Étant donné la complexité des bases de données modernes, qui est devenue et continue de devenir telle, équiper votre entreprise des bonnes solutions d’observabilité peut améliorer l’efficacité et les performances et éviter à vos équipes de se retrouver dans un travail fastidieux, fastidieux et sujet aux erreurs.</p>'}]"
Couchbase ajoute une base de données en colonnes à l'environnement DBaaS,"[{'', ""<p>Lors de la conférence AWS re:Invent 2023 qui s'est tenue aujourd'hui, Couchbase, Inc. a annoncé avoir ajouté une base de données en colonnes basée sur le format de fichier JSON à son portefeuille de bases de données en tant que service (DBaaS) pour permettre aux organisations de créer des applications d'analyse en temps réel.</p>""}, {'', '<p>Scott Anderson, vice-président senior de la gestion des produits et des opérations commerciales chez Couchbase, a déclaré que la base de données en colonnes Capella est conçue pour être intégrée à la base de données de documents basée sur JSON de la société via un protocole de changement de base de données (DCP) que Couchbase a intégré dans son environnement DBaaS.</p>'}, {'', ""<p>Déployé sur les services cloud d'Amazon Web Services (AWS), l'objectif global est de réduire le niveau de friction que les équipes informatiques rencontreraient autrement lors de l'intégration d'une base de données de documents avec une base de données en colonnes optimisée pour traiter les analyses à l'aide de colonnes plutôt que des lignes généralement associées à une base de données relationnelle.</p>""}, {'', ""<p>Cette approche donne aux équipes informatiques la possibilité d'utiliser une architecture de base de données sans schéma qui offre une alternative à l'utilisation de bases de données disparates qui devraient être déployées puis intégrées à l'aide de processus d'extraction, de transformation et de chargement (ETL) pour déplacer les données d'une base de données à l'autre, a noté Anderson.</p>""}, {'', ""<p>Capella columnar utilise également le même langage SQL++ que la base de données de documents de l'entreprise pour rationaliser les requêtes en plus de prendre en charge Capella iQ, un copilote basé sur l'intelligence artificielle générative (IA) qui permet de créer des requêtes en langage naturel.</p>""}, {'', ""<p>De plus, Capella columnar fournit des connecteurs pour déplacer des données depuis Amazon DynamoDB, Amazon DocumentDB, Amazon Relational Database Service (Amazon RDS) et d'autres sources de données déployées sur le cloud AWS.</p>""}, {''}, {'', ""<p>À plus long terme, Couchbase prévoit également d'ajouter des fonctionnalités vectorielles à son environnement DBaaS pour simplifier l'extension sécurisée des modèles d'IA génératifs, a déclaré Anderson.</p>""}, {'', ""<p>En général, les bases de données documentaires ont été largement adoptées car elles offrent aux développeurs une alternative aux plates-formes de bases de données existantes qu'ils peuvent télécharger et déployer eux-mêmes. La plate-forme DBaaS de Capella simplifie encore davantage ce processus via un service géré par lequel Couchbase assume la responsabilité opérationnelle de la gestion des bases de données. La base de données en colonnes étend cette capacité en utilisant une base de données en colonnes qui peut piloter les capacités d'analyse pour offrir une expérience d'application plus personnalisée, a noté Anderson.</p>""}, {'', '<p>On ne sait pas encore dans quelle mesure les entreprises finiront par s’appuyer sur des plateformes de type « as-a-service », mais depuis le début de la pandémie de COVID-19, le recours à cette approche de consommation des ressources informatiques s’est considérablement accru. En conséquence, la manière dont les équipes informatiques internes sont organisées évolue, les tâches de niveau inférieur étant soit automatisées, soit gérées par une équipe informatique externe qui travaille pour un fournisseur.</p>'}, {'', ""<p>Quelle que soit la manière dont les bases de données sont gérées, il reste nécessaire d'intégrer les processus d'exploitation des données (DataOps) utilisés pour les gérer aux flux de travail DevOps utilisés pour créer des applications modernes. Chaque organisation devra déterminer dans quelle mesure il est économiquement judicieux pour elle de gérer ces données ou de faire appel à un fournisseur de services externe. Le défi lorsqu'on fait appel à un fournisseur de services externe est, comme toujours, d'intégrer ce service dans un environnement informatique existant.</p>""}]"
Cinq excellentes opportunités d'emploi DevOps,"[{'', '<p>DevOps.com fournit désormais un rapport hebdomadaire sur les emplois DevOps à travers lequel les opportunités pour les professionnels DevOps seront mises en évidence pour mieux servir notre public.</p>'}, {'', '<p>Notre objectif en ces temps économiques difficiles est de permettre aux professionnels DevOps de faire progresser plus facilement leur carrière.</p>'}, {'', '<p>Bien entendu, le vivier de talents DevOps disponibles est encore relativement limité, donc lorsqu’un professionnel DevOps assume un nouveau rôle, cela a tendance à créer une opportunité pour les autres.</p>'}, {'', '<p>Les cinq offres d’emploi partagées cette semaine sont sélectionnées en fonction de l’entreprise qui cherche à embaucher, du segment industriel vertical et, naturellement, de l’échelle salariale proposée.</p>'}, {'', '<p>Nous nous engageons également à fournir des informations supplémentaires sur l’état du marché du travail DevOps. En attendant, voici quelques informations à prendre en compte :</p>'}, {'', '<p>CareerBuilder.com</p>'}, {'', '<p>ZoomSan Jose, CalifornieResponsable de programme technique - DevOps134\xa0300\xa0$</p>'}, {'', '<p>LinkedIn</p>'}, {'', '<p>GrammarlySpringdale, Caroline du SudIngénieur logiciel - infrastructure cloud123 000 $ à 253 000 $</p>'}, {'', '<p>Dés.com</p>'}, {'', '<p>Amtex EnterprisesIrving, TexasIngénieur DevOps en automatisation120\xa0000\xa0$</p>'}, {'', '<p>SimplyHired.com</p>'}, {'', '<p>KUBRATempe, ArizonaIngénieur DevOps senior115\xa0000 à 146\xa0000\xa0$</p>'}, {'', '<p>Indeed.com</p>'}, {'', '<p>McDonald’s CorpChicago, IllinoisIngénieur DevOps101\xa0000 à 127\xa0000 $</p>'}]"
Redgate automatise l'extraction des données de test à partir des bases de données de production,"[{'', ""<p>Redgate a ajouté aujourd'hui un outil Redgate Test Data Management (TDM) qui automatise l'extraction de données de test masquées à partir de bases de données exécutées dans des environnements de production.</p>""}, {'', ""<p>David Gummer, directeur des produits chez Redgate, a déclaré que cette capacité élimine les tâches manuelles qui empêchaient auparavant les tests ou entraînaient l'utilisation par inadvertance d'informations personnelles identifiables (PPI) pour tester les applications.</p>""}, {'', ""<p>Redgate TDM classe et masque automatiquement les données résidant dans les bases de données SQL Server, PostgreSQL, MySQL ou Oracle avant de les utiliser pour tester une application. Il produit ensuite une copie ou un clone de ces données qui représente une fraction de la taille de l'original, a déclaré Gummer.</p>""}, {'', '<p>Cette approche crée un processus reproductible pour les équipes DevOps qui peut résister à un audit réglementaire des processus de développement d’applications, a-t-il ajouté. En fait, avec l’attention accrue portée aux problèmes de chaîne d’approvisionnement en logiciels, de plus en plus d’auditeurs examinent spécifiquement la manière dont les données sont sécurisées pendant le processus de développement d’applications.</p>'}, {'', ""<p>Conçu pour être invoqué via une interface de ligne de commande (CLI), dans un environnement de développement intégré ou via une interface graphique, Redgate TDM réduit les frictions qui se produisent actuellement lorsque les équipes DevOps demandent des données de test aux administrateurs de base de données (DBA), a déclaré Gummer. De nombreux administrateurs de base de données sont soit trop occupés pour répondre à ces demandes, soit réticents à donner accès à des données sensibles qui conduiraient l'organisation à enfreindre une obligation de conformité.</p>""}, {'', '<p>Les développeurs ont alors souvent recours à des données anonymes qui ne reflètent pas réellement l’environnement dans lequel leur application sera finalement déployée.</p>'}, {''}, {'', ""<p>La quantité de données dont les développeurs ont besoin pour créer des applications ne fera qu'augmenter. En l'absence de moyen efficace d'automatiser le processus d'accès à ces données, il faut finalement plus de temps pour créer et déployer des applications stables. Dans de nombreux cas, les applications ne sont tout simplement pas testées aussi minutieusement qu'elles devraient l'être avant d'être déployées dans un environnement de production, ce qui entraîne des retours en arrière qui pourraient autrement être évités.</p>""}, {'', '<p>Les progrès de l’intelligence artificielle (IA) facilitent la création de scripts de test, mais une grande partie du processus de configuration des tests repose encore sur des processus manuels qui augmentent le niveau global de travail.</p>'}, {'', '<p>À l’heure où de plus en plus d’entreprises tentent d’accroître la productivité de leurs développeurs, l’automatisation de tâches relativement banales telles que l’extraction de données de test peut avoir un impact considérable. Les développeurs veulent consacrer le plus de temps possible à l’écriture de code. Tout le reste est une tâche qui les empêche d’atteindre cet objectif. Bien qu’il existe un désir général de responsabiliser les développeurs, cela ne signifie pas nécessairement qu’ils souhaitent que la charge cognitive associée à la création d’applications augmente. En fait, l’une des raisons pour lesquelles l’ingénierie de plateforme est devenue une méthodologie de gestion des flux de travail DevOps à grande échelle est de réduire le niveau actuel de charge cognitive que subissent les développeurs aujourd’hui.</p>'}, {'', ""<p>Bien entendu, les équipes DevOps sont, par nature, impitoyablement déterminées à automatiser autant de processus que possible. Le problème est que les goulots d'étranglement dans ces processus conspirent pour ralentir la vitesse globale à laquelle les applications peuvent être créées et déployées malgré les meilleures intentions de toutes les parties impliquées.</p>""}]"
Grafana Labs acquiert Asserts.ai pour amener l'IA au service de l'observabilité,"[{'', ""<p>Lors de son événement ObservabilityCON, Grafana Labs a annoncé aujourd'hui l'acquisition d'Asserts.ai pour automatiser les configurations et la personnalisation des tableaux de bord.</p>""}, {'', '<p>En outre, l’entreprise présente un aperçu de la possibilité d’appliquer l’intelligence artificielle (IA) à la gestion des incidents pour simplifier la recherche de la cause profonde d’un problème. Sift est un assistant de diagnostic dans Grafana Cloud qui analyse automatiquement les métriques, les journaux et les données de traçage, tandis que Grafana Incident est un outil d’IA génératif qui résume les chronologies des incidents en un seul clic, crée des métadonnées pour les tableaux de bord et simplifie l’écriture des requêtes PromQL.</p>'}, {'', ""<p>Grafana Labs met également à disposition un module d'observabilité des applications pour Grafana Cloud afin de fournir une vue plus holistique des environnements informatiques.</p>""}, {''}, {'', '<p>Enfin, Grafana Beyla, un projet d’auto-instrumentation open source qui utilise le filtrage de paquets Berkeley étendu (eBPF), est désormais également disponible. Cet outil permet aux équipes DevOps de collecter des données de télémétrie pour un environnement informatique à partir d’un environnement sandbox exécuté dans le micro-noyau d’un système d’exploitation. Cette approche simplifie l’instrumentation automatique d’un environnement informatique, mais il existe des cas où les équipes DevOps géreront des applications complexes qui nécessiteront toujours qu’elles collectent des données de télémétrie via l’espace utilisateur d’une application.</p>'}, {'', '<p>Richi Hartmann, directeur de la communauté de Grafana Labs, a déclaré que ces fonctionnalités supplémentaires simplifieront l’application de l’observabilité dans des environnements informatiques de plus en plus complexes. Par exemple, les technologies d’IA développées par Assert.ai permettront aux équipes DevOps de commencer à envoyer des données à Grafana Labs qui permettront au service cloud d’identifier les applications et l’infrastructure utilisées. Les modèles d’IA pourront alors générer automatiquement un tableau de bord personnalisé pour cet environnement que les équipes DevOps pourront étendre à leur guise, a déclaré Hartmann.</p>'}, {'', '<p>En général, les algorithmes d’apprentissage automatique et l’IA générative commencent à être plus largement appliqués à l’observabilité. L’objectif ultime est d’identifier automatiquement les problèmes de manière à réduire la charge cognitive nécessaire à la gestion d’environnements informatiques complexes tout en facilitant le lancement de requêtes permettant d’identifier les goulots d’étranglement susceptibles d’avoir un impact négatif sur les performances et la disponibilité des applications.</p>'}, {'', '<p>On ne sait pas exactement dans quelle mesure les outils d’observabilité pourraient éliminer le besoin d’outils de surveillance qui suivent des mesures prédéfinies, mais la plupart des équipes DevOps utiliseront probablement un mélange des deux dans un avenir proche.</p>'}, {'', '<p>Dans le même temps, les environnements informatiques deviennent de plus en plus complexes, car divers types d’applications cloud natives sont déployés aux côtés d’applications monolithiques existantes qui sont continuellement mises à jour. Le défi est que la taille globale des équipes DevOps ne s’agrandit pas, d’où un besoin accru d’outils pour rationaliser la gestion des flux de travail DevOps.</p>'}, {'', '<p>L’IA jouera naturellement un rôle plus important pour permettre aux organisations d’atteindre cet objectif, mais il est peu probable qu’elle remplace le besoin d’ingénieurs DevOps, a déclaré Hartmann.</p>'}, {'', '<p>À l’inverse, de nombreuses équipes DevOps se tourneront naturellement vers des organisations qui mettent à leur disposition les outils dont elles ont besoin pour réussir. Aujourd’hui, un nombre bien trop important de tâches manuelles entraînent une augmentation du turnover au fur et à mesure que les équipes DevOps s’épuisent. Les organisations qui souhaitent embaucher et conserver les meilleurs ingénieurs DevOps devront investir dans l’IA.</p>'}, {'', '<p>Bien entendu, DevOps a toujours eu pour objectif d’automatiser impitoyablement autant de tâches manuelles que possible. L’IA n’est que la dernière d’une série d’avancées qui, au fil du temps, continuent de rendre DevOps plus accessible aux professionnels de l’informatique de tous niveaux.</p>'}]"
