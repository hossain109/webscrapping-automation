Title,Content
L'ingénierie des plateformes gagne du terrain chez les constructeurs de systèmes embarqués,"[{'', ""<p>Une enquête menée auprès de 317 décideurs et influenceurs responsables de l'application de l'ingénierie de plate-forme au développement de logiciels personnalisés destinés à fonctionner sur des systèmes embarqués identifie l'amélioration de la rentabilité (43 %) comme le principal avantage commercial de l'adoption d'une méthodologie de gestion des flux de travail DevOps à grande échelle.</p>""}, {'', ""<p>Réalisée par Forrester Consulting pour le compte de Qt Group, un fournisseur d'un framework de développement logiciel, l'enquête révèle que d'autres facteurs d'adoption incluent des délais de mise sur le marché plus rapides (35 %), des taux de défauts réduits (25 %) et une meilleure réutilisabilité du code.</p>""}, {'', '<p>Les principaux avantages commerciaux identifiés comprennent une expérience client finale améliorée grâce à un logiciel fiable de meilleure qualité (68 %), une conformité accrue aux normes du secteur et à la sécurité (56 %), une identité de marque améliorée (57 %) et une efficacité du flux de travail (54 %).</p>'}, {'', ""<p>Près de la moitié (45 %) des répondants ont également noté que l'ingénierie de la plateforme contribue à améliorer les pratiques DevOps. Plus de la moitié (52 %) préfèrent une approche dédiée et de pointe en matière d'outillage, contre 35 % qui privilégient les fonctionnalités tout-en-un de bout en bout.</p>""}, {'', '<p>Sans surprise, de nombreuses entreprises sont confrontées à des défis en matière d’ingénierie de plateformes. Près de la moitié des répondants (49 %) ont également du mal à trouver un équilibre entre la réutilisabilité de composants standardisés de haute qualité et la nécessité d’adapter les plateformes à divers cas d’utilisation. Plus de la moitié (51 %) ont également déclaré qu’il restait difficile de créer des logiciels pour plusieurs appareils, systèmes d’exploitation, matériels et formats.</p>'}, {'', '<p>En outre, la moitié des répondants (50 %) ont déclaré que la pénurie de talents constituait le principal obstacle à l’ingénierie des plateformes, suivie de près par l’intégration des plateformes existantes (49 %).</p>'}, {'', ""<p>Au total, 43 % des entreprises ont du mal à maintenir des capacités de libre-service dans la plupart des cas d'utilisation, tandis que 41 % ont déclaré que la collaboration interfonctionnelle est difficile à mettre en place dans les phases de conception, de développement, de test et de déploiement. Un peu plus d'un tiers (34 %) ont également identifié la résistance culturelle des équipes produit comme un problème.</p>""}, {'', '<p>Enfin, 44 % ont noté que les systèmes embarqués manquent de processus d’interface utilisateur unifiés pour l’accessibilité et l’inclusivité.</p>'}, {'', '<p>Malgré ces problèmes, 93 % ont déclaré que la direction de leur entreprise soutenait les stratégies d’ingénierie de plateforme de l’organisation.</p>'}, {'', ""<p>Maurice Kalinowski, responsable de la gestion technique des produits pour le groupe Qt, a déclaré que l'application de l'ingénierie de plateforme au développement de logiciels pour systèmes embarqués est un défi en raison de la grande diversité des environnements eux-mêmes. Près des deux tiers (63 %) des personnes interrogées ont déclaré que les logiciels qu'elles ont créés sont créés à l'aide de solutions personnalisées et ad hoc. Les organisations qui adoptent l'ingénierie de plateforme doivent adopter une approche plus holistique de la création de logiciels, a déclaré M. Kalinowski.</p>""}, {'', '<p>Sinon, le niveau de dette technique que les développeurs de logiciels personnalisés accumuleront au fil du temps deviendra tout simplement trop écrasant, a-t-il ajouté.</p>'}, {'', '<p>Parallèlement, des réglementations telles que le Cyber \u200b\u200bResilience Act adopté par l’Union européenne rendent les organisations plus responsables de la qualité des logiciels qu’elles déploient, a noté M. Kalinowski. Il est donc urgent d’appliquer les meilleures pratiques de développement logiciel à une époque où les cas d’utilisation impliquant des logiciels pour systèmes embarqués ne cessent de se multiplier, a-t-il déclaré.</p>'}, {'', '<p>On ne sait pas exactement combien d’entreprises adoptent l’ingénierie de plateforme, mais à mesure que le nombre de plateformes sur lesquelles les logiciels sont déployés continue d’augmenter, les approches traditionnelles de création de logiciels personnalisés sont revues. Le défi, bien sûr, est de convaincre les développeurs de logiciels d’adopter une méthodologie qui, à un certain niveau, peut les obliger à limiter leurs options d’outils et de plateformes au nom du bien commun.</p>'}]"
Lancez-vous : passez au mode multijoueur pour dynamiser votre plateforme interne,"[{'', '<p>Comment les organisations peuvent-elles accroître la valeur de leurs plateformes internes sans élargir massivement leurs équipes de plateformes ?</p>'}, {'', '<p>Nous célébrons souvent le héros ou l’héroïne qui crée seul quelque chose d’ingénieux – l’innovateur non-conformiste. Cependant, la réalité nous montre constamment que la plupart des innovations sont le résultat d’un travail d’équipe. Cela est également vrai pour la conception d’une plateforme de développement interne. La plateforme interne peut offrir une valeur exceptionnelle. Cependant, pour atteindre son plein potentiel, il est logique de s’éloigner du développement de plateformes en mode solo et de passer en mode multijoueur pour dynamiser votre plateforme interne. Mais qu’est-ce que cela signifie exactement ?</p>'}, {'', '<p>Si de nombreuses organisations se lancent à fond dans la création d’une plateforme interne, elles n’exploitent pas toujours toutes les ressources et l’expertise dont elles disposent en interne. Au lieu de cela, il y a souvent plusieurs équipes cloisonnées qui construisent des plateformes qui résolvent leurs problèmes, mais elles ne communiquent pas avec d’autres équipes qui pourraient utiliser et améliorer la plateforme qu’elles construisent. S’il ne s’agit pas de plusieurs équipes avec des plateformes concurrentes ou redondantes, les organisations font parfois reposer la responsabilité du développement d’une plateforme sur les équipes de plateforme. Mais ces approches répondent-elles aux difficultés auxquelles les organisations sont confrontées et aux défis qu’elles cherchent à résoudre en adoptant une plateforme interne ?</p>'}, {'', ""<h3>Un bref historique : de DevOps à l'ingénierie de plateforme</h3>""}, {'', '<p>Par le passé, les développeurs passaient beaucoup de temps à attendre que les opérateurs leur fournissent des services ou des outils. Cela entraînait des frictions, des retards et de la frustration. Les développeurs et les opérateurs étaient isolés les uns des autres et n’avaient aucun moyen de briser les silos qui les empêchaient d’être productifs. Cela a donné naissance à DevOps, qui vise à donner aux équipes les moyens de tout créer et de tout gérer elles-mêmes.</p>'}, {'', '<p>Le DevOps fonctionne – et fonctionne bien – jusqu’à ce que les organisations évoluent et que des problèmes doubles apparaissent. Tout d’abord, les équipes DevOps finissent par se débattre avec des problèmes répétitifs et redondants, et ensuite, la charge de travail DevOps augmente et s’accélère à mesure que l’entreprise évolue. Cela signifie que les équipes DevOps ont non seulement beaucoup plus de travail à faire au quotidien, mais doivent également gérer la création et l’exécution de tout, de l’infrastructure au réseau en passant par le CI/CD et l’observabilité. Dans le même temps, les équipes DevOps doivent gérer une multitude de processus et de préoccupations internes, tels que la sécurité et la conformité. Naturellement, la surcharge cognitive est inévitable.</p>'}, {'', ""<h3>Tuer le dragon DevOps : découvrez l'ingénierie de plateforme</h3>""}, {'', '<p>Pour résoudre les défis croissants de DevOps, l’ingénierie de plateforme a émergé avec pour objectif principal d’offrir une meilleure expérience aux développeurs. Que signifie l’expérience des développeurs dans ce contexte ? Cela signifie réduire la charge cognitive des développeurs et leur permettre de se concentrer sur leur travail principal : apporter de la valeur aux clients. Mais pour y parvenir, il serait essentiel de réduire les charges de travail cognitives et littérales, et de rationaliser le travail des équipes DevOps. C’est là que l’ingénierie de plateforme et la formation d’équipes de plateforme jouent un rôle crucial.</p>'}, {'', '<p>Cette évolution n’a cependant pas complètement résolu le problème. La charge cognitive a simplement été transférée aux équipes de plateformes, chargées de fournir une plateforme de développement interne personnalisée qui réponde aux besoins des développeurs et offre des outils et des services à la demande. Pourtant, les équipes de plateformes finissent par prendre en charge un ensemble d’outils et de services déjà vaste et en plein essor et ressentent la pression de devoir fournir une expérience utilisateur agréable aux développeurs. L’ensemble du secteur fait écho à l’importance de l’expérience et des flux des développeurs lorsqu’ils créent leurs logiciels. Et si c’est vrai, comment pouvons-nous tirer le meilleur parti de ces plateformes internes sans épuiser toute l’équipe ou augmenter massivement ses effectifs ?</p>'}, {'', '<h3>Jouer en équipe</h3>'}, {'', '<p>Pour obtenir une valeur plus importante et potentiellement plus rapide avec la plateforme interne, commencez par réfléchir à la manière dont les équipes sont organisées, puis déterminez leurs rôles et responsabilités. D’après l’expérience des développeurs, des équipes DevOps et des équipes de plateforme, il est clair qu’aucun individu ou équipe ne doit (ou ne peut) tout gérer. Dans l’ouvrage phare Team Topologies, quatre types d’équipes sont décrits : équipe alignée sur le flux (équipe d’application), équipe d’activation, équipe de sous-systèmes complexes et équipe de plateforme.</p>'}, {''}, {'', ""<p>Lorsque l'idée de plateforme interne commence à s'effondrer, les équipes alignées sur les flux ont trop de charge cognitive et de bricolage dans le cadre de leur travail. Dans le même temps, avec l'ingénierie de plateforme, une grande partie du travail préparatoire et de la maintenance est transférée aux équipes de plateforme, ce qui alourdit leur charge de travail.</p>""}, {'', '<p>En comprenant ces types d’équipes et leur dynamique, nous pouvons commencer à nous demander : comment pouvons-nous partager la charge et même impliquer d’autres équipes dans le travail de construction de la plateforme interne ? Comment pouvons-nous répartir la responsabilité de manière plus équitable et de manière à offrir une valeur maximale ?</p>'}, {'', '<h3>Jouer sur la plateforme avec une approche multijoueur</h3>'}, {'', '<p>C’est là que l’intérêt d’une approche multijoueur pour la création de plateformes se fait sentir. Si les développeurs et les équipes de plateformes détiennent une grande partie de l’expertise nécessaire pour créer une plateforme interne efficace, la responsabilité de créer une plateforme utile capable de susciter une adhésion et une adoption quasi universelles n’incombe pas entièrement à l’un ou l’autre. Les autres équipes de l’organisation peuvent avoir une grande influence sur le développement de la plateforme en y apportant leurs propres compétences, expertise et connaissances, ce qui profite en fin de compte à l’organisation.</p>'}, {'', '<h3>Prêt, joueur 1</h3>'}, {'', '<p>Un exemple d’injection de compétences et d’expertise spécifiques dans une plateforme est celui où plusieurs équipes d’application ont besoin d’accéder à une base de données. Dans un monde idéal, les développeurs auraient un accès à la demande, par exemple à une base de données Postgres en tant que service. Ils pourraient en fournir une à chaque fois qu’ils en ont besoin. De nombreuses organisations disposent en interne d’une équipe spécialisée dans les bases de données qui sait exactement comment fonctionne Postgres, comment il doit être configuré et comment les équipes alignées sur les flux doivent l’utiliser.</p>'}, {'', '<p>Il n’y a aucune raison pour que les développeurs ou les équipes de plateforme d’une entreprise soient obligés de posséder des connaissances approfondies en matière de bases de données lorsqu’une équipe de sous-systèmes composée d’experts en la matière peut facilement proposer cette option de base de données en tant que service au sein de la plateforme. En déchargeant cette charge, les équipes d’application peuvent obtenir ce dont elles ont besoin de la plateforme, et les équipes de plateforme peuvent facilement utiliser l’option de base de données en tant que service comme élément de base au sein de la plateforme, qu’elles peuvent regrouper avec d’autres services, le tout en s’appuyant sur l’expertise de l’équipe de base de données.</p>'}, {'<h3>Prêt Joueur Deux</h3>', ''}, {'', '<p>Un deuxième exemple concerne une exigence spécifique plutôt qu’un outil spécifique. Les processus de sécurité, par exemple, sont des exigences qui doivent être intégrées à de nombreuses charges de travail de livraison de logiciels. Mais il est probable que les ressources de sécurité ne soient pas suffisantes pour les intégrer aux équipes chargées des applications et des plateformes. Au lieu de cela, l’approche multi-acteurs permet aux équipes de sécurité de sauver la mise en définissant des processus et en les intégrant à la plateforme interne.</p>'}, {'', '<p>Encore une fois, les équipes de développement et de plateforme ne sont pas obligées de comprendre tous les tenants et aboutissants de la sécurité, mais peuvent suivre les étapes et les garde-fous définis par l’équipe de sécurité au fur et à mesure de leur intégration dans les charges de travail de la plateforme.</p>'}, {'', '<h3>Tout le monde peut contribuer à rendre la plateforme interne plus puissante</h3>'}, {'', '<p>L’utilisation de compétences et de connaissances spécialisées provenant de toute l’organisation et leur intégration dans la plateforme interne de manière simple à utiliser constituent une stratégie efficace pour réduire la charge cognitive au sein des équipes.</p>'}, {'', '<p>Cette approche, parfois appelée « sourcing interne » ou « démocratisation de la plateforme », s’est révélée très fructueuse. L’essentiel est de tirer le meilleur parti des talents, des compétences et des connaissances dont dispose votre organisation. Cela permet non seulement de soulager la charge cognitive et d’éviter de devoir constituer une équipe massive ou de submerger une petite équipe avec trop d’informations, mais cela ouvre également la voie à l’adhésion et à l’appropriation de la plateforme à l’échelle de l’organisation.</p>'}, {'', ""<h3>Comment démarrer avec l'approche multijoueur</h3>""}, {'', ""<p>L'ingénierie des plateformes étant considérée comme une pratique sociotechnique, il est important d'impliquer à la fois les personnes et la technologie. Considérez à nouveau les topologies d'équipe et les trois modes d'interaction décrits dans le livre : la collaboration, le x-as-a-service et la facilitation.</p>""}, {''}, {'', '<h3>Collaboration : Jouons ensemble</h3>'}, {'', '<p>Pour comprendre à quoi pourrait ressembler une plateforme interne multijoueur, les équipes doivent se concentrer sur la collaboration. Elles doivent se comprendre, développer l’empathie et s’aligner. La collaboration est le meilleur type d’interaction pour comprendre les exigences, telles que les besoins des équipes d’application, la façon dont elles aimeraient utiliser la plateforme et si et comment les équipes d’application souhaiteraient contribuer aux outils et services qu’elles ont elles-mêmes développés dans la plateforme.</p>'}, {'', ""<p>L'équipe de la plateforme doit également collaborer avec les autres équipes, en particulier les spécialistes qui fournissent des outils et des services spécifiques. Cela permet de mieux comprendre ce que sont ces services, comment ils peuvent être produits et intégrés à la plateforme, ainsi que ce que ces équipes doivent consommer des autres équipes. Pour faciliter ce travail, une équipe de facilitation de courte durée axée sur la facilitation dans toute l'organisation peut être utile pour cartographier les connexions et les collaborations.</p>""}, {'', '<h3>X-as-a-Service : là où le mode multijoueur prend vie</h3>'}, {'', ""<p>Le mode multijoueur prend vie lorsque les équipes de l'organisation sont habilitées à apporter leur expertise à la plateforme. L'équipe de la plateforme peut le permettre. L'objectif final est de faire en sorte que la collaboration mène à une expertise qui se transforme en outils et services à la demande X-as-a-service.</p>""}, {'', ""<p>En utilisant l'exemple précédent, nous pouvons voir que la base de données Postgres est proposée en tant que service au sein de la plateforme et qu'elle bénéficie de l'expertise en sécurité apportée par l'équipe de sécurité intégrée à la plateforme. Les consommateurs de la plateforme interagissent désormais avec la plateforme en mode x-as-a-service, ce qui signifie qu'ils obtiennent l'outil dont ils ont besoin à la demande avec une sécurité renforcée déjà intégrée.</p>""}, {''}, {'', '<p>D’un point de vue technique également, l’équipe de la plateforme doit tenir compte de la facilité avec laquelle les autres équipes peuvent contribuer à la plateforme. S’agit-il d’une boîte noire que personne ne comprend ou est-il facile pour les autres équipes de contribuer ? Quel est ce processus de contribution, non seulement sous forme d’interaction ponctuelle en amont, mais également de manière continue pour garantir que la plateforme reste à jour et adaptée à ses objectifs ? Ce processus est-il compris et documenté ?</p>'}, {'', '<p>C’est l’un des domaines dans lesquels une infrastructure de plateforme peut prendre en charge ce modèle multijoueur et même permettre le mode multijoueur prêt à l’emploi. Si votre plateforme est constituée de blocs de construction composables fournis et maintenus par des équipes d’experts, elle permettra à l’équipe de plateforme d’utiliser ces blocs de construction pour composer des routes pavées ou des chemins d’or. Ceux-ci peuvent ensuite être utilisés par les équipes d’application pour une expérience de développement encore plus simple et plus fluide. En fin de compte, la plateforme interne finit par être supérieure à la somme de ses parties et devient un moteur d’innovation.</p>'}, {'', ""<h3>Le jeu n'est pas terminé : passez en mode multijoueur pour obtenir plus de jetons</h3>""}, {'', ""<p>Le passage au mode multijoueur est au cœur de la volonté de permettre une meilleure expérience aux développeurs et de réduire la charge cognitive des équipes de développement et de plateforme. Et même si ces équipes sont celles qui en bénéficient le plus en apparence, une plateforme interne inspirée du multijoueur peut bénéficier de la participation de l'ensemble de l'organisation.</p>""}, {'', '<p>En passant du mode solo au mode multijoueur, chacun peut devenir un héros de plateforme interne.</p>'}, {''}]"
PDG de CrowdStrike : 97 % des systèmes Windows sont à nouveau en ligne après une panne,"[{'', ""<p>Selon le directeur général de l'entreprise de cybersécurité, plus de 97 % des postes de travail Windows qui ont été soudainement perturbés il y a une semaine par une mise à jour logicielle problématique de CrowdStrike sont de nouveau en ligne, mais les conséquences de la panne mondiale continueront de se faire sentir.</p>""}, {'', '<p>Dans un message sur LinkedIn, le PDG de CrowdStrike, George Kurtz, a écrit que les efforts de récupération du fournisseur ont été aidés par « le développement de techniques de récupération automatique et par la mobilisation de toutes nos ressources pour soutenir nos clients ».</p>'}, {'', '<p>Microsoft estime qu’au total, 8,5 millions de PC et autres appareils Windows – soit moins de 1 % de toutes les machines Windows – ont été mis hors service et ont affiché l’écran bleu de la mort (BSOD) après l’envoi de la mise à jour défectueuse aux capteurs Falcon de CrowdStrike. Les appareils Linux et Mac n’ont pas été affectés par la mise à jour.</p>'}, {'', '<p>Aux quelques utilisateurs qui tentent encore de se reconnecter, Kurtz a écrit que l’entreprise « ne se reposera pas tant que nous n’aurons pas complètement récupéré. Chez CrowdStrike, notre mission est de gagner votre confiance en protégeant vos opérations. Je suis profondément désolé pour la perturbation que cette panne a causée et je présente mes excuses personnelles à toutes les personnes touchées ».</p>'}, {'', ""<p>Dans un rapport préliminaire post-incident (PIR) du 24 juillet, Kurtz a écrit que son outil Content Validator, qui est un élément clé d'un processus de test de mise à jour en plusieurs étapes, a validé une instance de modèle qui avait un contenu problématique qui a déclenché la panne de Windows pour être déployée en production. Le validateur de contenu aurait dû détecter le problème, a-t-il écrit.</p>""}, {'', '<p>CrowdStrike a publié un correctif et le fournisseur et Microsoft ont travaillé pour remettre les systèmes en marche, mais certains appareils ont mis plus de temps à revenir en ligne.</p>'}, {'', '<h3>Un problème persistant</h3>'}, {'', '<p>Les perturbations se poursuivent de plusieurs manières. Parmi les secteurs les plus touchés figurent les services financiers, les soins de santé, les services d’urgence et les compagnies aériennes. La plupart des compagnies aériennes ont pu se rétablir en quelques jours, mais Delta est celle qui a mis le plus de temps à revenir à la normale. Le 22 juillet, quatre jours après le crash, Delta a annulé 1 160 vols, selon USA Today, et ce nombre a régulièrement diminué, tombant à 47 le 25 juillet.</p>'}, {'', '<p>Cependant, Delta – la plus grande compagnie aérienne du monde – devra désormais faire face à une enquête du ministère américain des Transports. Le secrétaire aux Transports Pete Buttigieg a déclaré dans un message sur X (anciennement Twitter) qu’il voulait « s’assurer que la compagnie aérienne respecte la loi et prenne soin de ses passagers pendant les perturbations généralisées qui se poursuivent. Tous les passagers des compagnies aériennes ont le droit d’être traités équitablement, et je veillerai à ce que ce droit soit respecté. »</p>'}, {'', ""<p>Delta a déclaré dans un communiqué qu'elle essayait de répondre aux besoins des passagers dont les vols ont été annulés ou retardés, notamment en payant les frais engagés par les clients qui ont payé pour d'autres moyens de transport. Elle a également déclaré que la compagnie aérienne continuerait à offrir des bons de repas, des transports terrestres et des chambres d'hôtel aux passagers « dont le voyage a été perturbé par des vols annulés ou considérablement retardés ».</p>""}, {'<h3>De lourdes pertes, mais peu d’aide de la part des assurances</h3>', ''}, {'', '<p>Les clients touchés par la situation devront également s’orienter vers l’assurance cyber. Parametrix, dont l’activité comprend les services d’assurance cloud, a déclaré dans un rapport que les entreprises américaines du Fortune 500 – dont environ un quart ont été touchées – subiront des pertes pouvant atteindre 5,4 milliards de dollars. Cela dit, les polices d’assurance cyber ne couvriront probablement que 10 à 20 % de ce coût « en raison des importantes rétentions de risque de nombreuses entreprises et des limites de police faibles par rapport aux pertes potentielles dues aux pannes », a écrit Parametrix.</p>'}, {'', '<p>La panne « nous en dit plus sur les moyens par lesquels les assureurs et les réassureurs peuvent diversifier leurs portefeuilles de cyber-risques afin de minimiser les impacts potentiels du cyber-risque systémique », a déclaré Jonatan Hatzor, cofondateur et PDG de Parametrix. « Cependant, notre analyse ne montre pas l’ensemble de la situation en matière de diversification. Un assureur cyber axé sur les très grandes entreprises subira certainement une perte CrowdStrike bien plus importante par rapport aux primes qu’un assureur ayant un portefeuille important de PME. »</p>'}, {'', '<p>Les maux de tête ne sont pas réservés aux utilisateurs. Le Congrès souhaite que Kurtz, de CrowdStrike, témoigne lors d’une audience, affirmant dans une lettre que « les Américains méritent de savoir en détail comment cet incident s’est produit et les mesures d’atténuation prises par CrowdStrike ».</p>'}, {'', '<h3>Les cartes-cadeaux de 10 $ ne servent à rien</h3>'}, {'', ""<p>Un autre casse-tête a été la carte-cadeau Uber Eats de 10 $ envoyée aux membres du personnel et aux partenaires qui ont aidé à résoudre le problème et à remettre en marche les systèmes concernés. CrowdStrike leur aurait dit dans un e-mail que l'entreprise comprenait que la panne avait entraîné un surcroît de travail et des maux de tête. Certains n'ont pas été impressionnés.</p>""}, {'', '<p>Sur Reddit, une personne a écrit : « Il aurait été préférable de ne rien donner aux gens, mais maintenant Crowdstrike suggère que les heures que les gens ont investies pour corriger leur erreur ne valent que 10 $ ? »</p>'}, {'', ""<p>« Est-ce que c'est pour vous dire que nous vous avons fait enregistrer plus de 40 heures de réparations d'urgence, alors voilà, vous avez moins d'un demi-déjeuner à notre charge ? Sérieusement, c'est insultant », a écrit un autre.</p>""}, {'', '<p>Une personne a écrit : « Je prendrai tout ce que je pourrai obtenir, je suppose. Puis-je avoir un chapeau aussi ? Je veux le porter avec mon t-shirt Solarwinds. Je me drape dans mes horreurs. »</p>'}, {'', ""<h3>De plus en plus d'escrocs se lancent dans la mêlée</h3>""}, {'', ""<p>Le chaos créé par le crash a donné aux cybercriminels l'occasion de lancer des opérations de phishing et une myriade d'autres escroqueries dans l'espoir de voler de l'argent ou des informations aux victimes en se faisant passer pour du personnel de support technique de CrowdStrike ou d'autres sociétés ou en promettant des solutions au problème.</p>""}, {'', '<p>Les chercheurs de CrowdStrike ont identifié une campagne dans laquelle un acteur malveillant utilise un domaine de phishing comprenant le nom de l’entreprise et Office 365 pour se faire passer pour CrowdStrike et diffuser des fichiers ZIP et RAR malveillants contenant un chargeur Microsoft Installer. Si la cible ouvre le fichier, le chargeur exécute Lumma Stealer, un malware de vol d’informations, contenant CypherIt, un chargeur fortement obscurci utilisé pour entraver l’analyse statique.</p>'}, {'', ""<p>Lumma Stealer est utilisé pour collecter des données à partir des navigateurs, telles que les identifiants, les cookies et les extensions de navigateur. Les chercheurs ont ajouté que, d'après l'horodatage de la création, l'acteur de la menace a très probablement créé l'échantillon le lendemain de l'identification de la mise à jour du contenu problématique et du déploiement du correctif.</p>""}]"
CrowdStrike : un logiciel de test défectueux a provoqué un crash mondial de Windows,"[{'', '<p>Un bug dans un outil de test a validé de manière incorrecte une mise à jour de contenu du logiciel Falcon de CrowdStrike, provoquant l’envoi d’un « contenu problématique » dans un fichier à des millions d’appareils Windows et déclenchant une panne mondiale des systèmes informatiques dont les ramifications se font encore sentir.</p>'}, {'', ""<p>Dans un rapport préliminaire publié mercredi, la société de cybersécurité a déclaré que son outil Content Validator, qui fait partie d'un processus en plusieurs étapes pour fournir des mises à jour de configuration de contenu au capteur Falcon, contenait deux instances de modèle fournissant du contenu à réponse rapide. L'une des instances contenait le contenu problématique, mais sur la base des résultats des contrôles effectués plus tôt dans le processus, « ces instances ont été déployées en production », a écrit CrowdStrike.</p>""}, {'', ""<p>Le résultat fut que lorsque le capteur reçut le contenu du fichier de canal 291 et le chargea dans l'interpréteur de contenu, cela provoqua une lecture de mémoire hors limites qui déclencha une exception inattendue qui « ne pouvait pas être gérée correctement, entraînant un crash du système d'exploitation Windows » et l'écran bleu de la mort (BSOD).</p>""}, {'', ""<p>Selon Microsoft, 8,5 millions de systèmes Windows dans le monde ont été touchés par la panne, qui a touché de nombreux secteurs comme le tourisme, la santé et les services financiers et a entraîné l'immobilisation d'avions, le report d'opérations chirurgicales et le retard des opérations bancaires. Certaines entreprises sont encore en phase de rétablissement, Delta Airlines continuant d'annuler des vols cinq jours plus tard.</p>""}, {'', '<p>La panne a également fait chuter le cours de l’action de CrowdStrike et a donné aux cybercriminels l’occasion de lancer des escroqueries en exploitant le crash.</p>'}, {'', ""<p>En outre, le Congrès exerce désormais un contrôle strict sur cette affaire : les responsables du Comité de la sécurité intérieure et du Sous-comité de la cybersécurité et de la protection des infrastructures de la Chambre des représentants ont envoyé cette semaine une lettre donnant au PDG de CrowdStrike, George Kurtz, jusqu'à mercredi pour planifier une audience avec le sous-comité.</p>""}, {'', '<p>« Bien que nous apprécions la réponse de CrowdStrike et sa coordination avec les parties prenantes, nous ne pouvons ignorer l’ampleur de cet incident, que certains ont qualifié de la plus grande panne informatique de l’histoire », ont écrit Mark Green (R-TN), président du Comité de la sécurité intérieure, et Andrew Garbarino (R-NY), président du sous-comité. « Sachant que les Américains ressentiront sans aucun doute les conséquences durables et réelles de cet incident, ils méritent de savoir en détail comment cet incident s’est produit et les mesures d’atténuation prises par CrowdStrike. »</p>'}, {'', ""<p>Le rapport PIR de CrowdStrike fournit une analyse détaillée des problèmes survenus le 19 juillet, indiquant que les hôtes Windows exécutant la version 7.11 du capteur de CrowdStrike qui étaient en ligne pendant une période de 90 minutes ce matin-là ont reçu la mise à jour du problème. Les systèmes qui se sont connectés après ou qui n'étaient pas connectés pendant cette période n'ont pas été affectés. De plus, les systèmes Mac et Linux n'ont pas été affectés.</p>""}, {'', ""<p>CrowdStrike envoie des mises à jour de configuration du contenu de sécurité à ses capteurs de deux manières, notamment via le contenu du capteur qui est livré directement avec le capteur. Ce contenu n'est pas mis à jour de manière dynamique à partir du cloud et comprend des types de modèles, qui ont des champs prédéfinis écrits dans le code que les ingénieurs de détection des menaces peuvent utiliser dans le contenu de réponse rapide. Les types de modèles « passent par un processus d'assurance qualité approfondi, qui comprend des tests automatisés, des tests manuels, des étapes de validation et de déploiement », a écrit le fournisseur.</p>""}, {'', ""<p>Le contenu de réponse rapide est utilisé pour répondre rapidement aux changements dans le paysage des menaces et peut être mis à jour de manière dynamique en dehors du capteur Falcon. Le contenu est fourni sous forme d'instances de modèle qui correspondent à des comportements spécifiques que le capteur peut voir, détecter ou empêcher et qui disposent d'un ensemble de champs correspondant au comportement souhaité. Trois systèmes clés sont utilisés pour tester et déployer le contenu de réponse rapide, notamment le système de configuration de contenu, qui crée des instances de modèle et inclut le validateur de contenu.</p>""}, {'', '<p>Les deux autres sont l’interpréteur de contenu sur le capteur, qui permet au moteur de détection du capteur de détecter et de prévenir les activités malveillantes.</p>'}, {'', ""<p>CrowdStrike a indiqué qu'en février, il a introduit un nouveau type de modèle InterProcessCommunication (IPC) pour détecter de nouvelles techniques d'attaque. Le mois suivant, le type de modèle IPC a été soumis à un test de résistance dans un environnement de test comprenant des systèmes d'exploitation et des charges de travail. Après avoir réussi le test de résistance et avoir été validé pour l'utilisation, l'instance de modèle IPC et trois autres ont été déployées en avril et ont fonctionné comme prévu en production.</p>""}, {'', '<p>Deux instances de modèle IPC supplémentaires ont été déployées le 19 juillet – y compris celle avec le contenu problématique – sur la base des tests effectués avant le déploiement initial du type de modèle en mars, des vérifications effectuées dans le validateur de contenu et du fait que les instances de modèle IPC précédentes ont été déployées sans déclencher de problèmes dans les systèmes Windows, a écrit CrowdStrike.</p>'}, {'', ""<p>En réponse, le fournisseur a déclaré qu'il améliorait les tests de contenu de réponse rapide en utilisant des types de tests supplémentaires, notamment des tests de développement local et des tests de mise à jour et de restauration de contenu, en ajoutant davantage de contrôles de validation à Content Validator pour le contenu de réponse rapide et en améliorant la gestion des erreurs existante dans Content Interpreter.</p>""}, {'', ""<p>De nouveaux contrôles ont également été introduits dans le déploiement du contenu de réponse rapide, notamment le déploiement progressif des mises à jour, l'amélioration de la surveillance et l'octroi aux utilisateurs d'un meilleur contrôle sur le moment et le lieu de déploiement des mises à jour.</p>""}]"
Donner plus de pouvoir aux utilisateurs du libre-service à l'ère numérique,"[{'', '<p>Nous vivons dans un monde de plus en plus axé sur le libre-service, où les natifs du numérique sont devenus la génération dominante sur le lieu de travail. La plupart des individus d’aujourd’hui sont non seulement plus à l’aise avec le libre-service, mais ils le choisissent aussi activement. Par conséquent, la création et l’évolution d’un portail libre-service intuitif mais sécurisé pour les utilisateurs qui consomment des ressources informatiques sont devenues une tâche essentielle pour de nombreuses organisations.</p>'}, {'', '<p>Les portails peuvent prendre des années à être mis en place par les équipes de plateformes, et la maintenance reste permanente. Pourtant, s’ils sont bien conçus, ils peuvent alléger considérablement la charge des services informatiques pour les utilisateurs finaux en améliorant l’expérience globale et en réduisant la charge cognitive, et pour les équipes de plateforme en optimisant l’efficacité opérationnelle et en réduisant le nombre de tickets ouverts. Par conséquent, ils accélèrent les processus métier et placent l’autonomie et le contrôle entre les mains d’un personnel plus heureux.</p>'}, {'', ""<p>L'informatique en libre-service consiste à centraliser et à donner accès à des éléments importants tels que les applications, l'automatisation, le déploiement, la documentation, le cloud, la sécurité, FinOps et GreenOps de manière simple et complète. Pour utiliser une métaphore, il ne s'agit pas de construire un Boeing 747 surdimensionné doté de toutes les fonctionnalités nécessaires pour une escapade rapide en ville. Bien au contraire. Il s'agit de créer un planeur simplifié doté de la capacité nécessaire pour gérer les tâches opérationnelles intensives dont il a besoin.</p>""}, {'', '<p>Mais, bien sûr, le libre-service présente des avantages et des inconvénients, et il est essentiel de trouver un équilibre pour donner plus d’autonomie aux utilisateurs à l’ère numérique.</p>'}, {'', '<h3>Gestion du Shadow IT</h3>'}, {'', ""<p>De nombreuses organisations ont déjà connu un niveau croissant de libre-service non autorisé. Par conséquent, l'informatique fantôme, c'est-à-dire l'utilisation de matériel ou de logiciel informatique par une personne ou un service sans l'accord ou la connaissance du service informatique, est devenue une préoccupation majeure pour les DSI. Même si elle n'est pas totalement mauvaise, elle est loin d'être idéale.</p>""}, {'', '<p>Le Shadow IT est un signe clair de la demande de libre-service, mais il met également en évidence un défi que l’informatique seule ne peut pas relever : les personnes. Sur les infrastructures traditionnelles, les entreprises commencent déjà à développer des interfaces sur des virtualisations car elles ont identifié la nécessité de fournir des garde-fous en libre-service aux utilisateurs. Cependant, lorsqu’il s’agit de gérer des déploiements multi-cloud à grande échelle, avec des considérations de sécurité, de gouvernance, de FinOps et de GreenOps, cela devient un défi.</p>'}, {'', '<p>Les gens sont imprévisibles, ils peuvent avoir des habitudes bien ancrées et/ou être prêts à expérimenter. Ils peuvent utiliser un nouveau service pendant un certain temps, puis migrer vers un autre sans trop se soucier de la sécurité ou de la confidentialité. Ils peuvent déployer un nouveau service pour prouver le concept et oublier de le supprimer. Ils peuvent s’attacher à des processus ou à des systèmes qu’ils connaissent bien. Il est bien beau de déclarer son intention de créer un portail en libre-service, mais rien ne garantit que les gens se précipiteront pour l’utiliser.</p>'}, {'', '<p>En partant du principe que tout nouveau portail en libre-service dispose de toutes les fonctionnalités et caractéristiques nécessaires pour répondre aux demandes existantes des personnes qui l’utiliseront, il existe (en gros) deux options en matière de gestion du changement. Forcer les gens à faire la nouvelle chose ou rendre la nouvelle chose plus facile à faire que l’ancienne. Attention, l’une de ces options a tendance à être plus efficace que l’autre.</p>'}, {'', '<p>Il est donc essentiel de fournir un support complet aux utilisateurs lorsqu’ils naviguent sur un portail en libre-service. Les organisations doivent fournir des guides clairs et accessibles, organiser des sessions de formation informatives et établir un système de support fiable. Lorsque les développeurs se sentent responsabilisés et bien soutenus, ils ne se contenteront pas d’utiliser le portail, ils l’intégreront comme un élément essentiel de leur boîte à outils.</p>'}, {'', '<p>À long terme, cela contribuera à fidéliser et à retenir les meilleurs talents. C’est également essentiel pour créer la meilleure expérience possible pour les développeurs et aider l’organisation et les employés à se concentrer sur ce qui compte vraiment : améliorer et maintenir l’automatisation et l’infrastructure d’un côté et développer leurs applications de l’autre.</p>'}, {'', ""<h3>Maximiser l'expérience utilisateur</h3>""}, {'', '<p>L’expérience du développeur, ou DevX (comme je l’ai déjà écrit), est d’une importance primordiale pour l’adoption réussie de tout portail en libre-service.</p>'}, {'', ""<p>Cela peut paraître évident, mais lors de la création d'un portail, une organisation doit tenir compte des personnes qui l'utiliseront, plutôt que de créer quelque chose qui oblige les gens à travailler différemment. Un portail en libre-service bien conçu se caractérise par sa capacité à être utilisé sans avoir besoin d'une assistance extérieure, avec à la fois la sophistication adaptée à une communauté de développeurs qualifiés et l'intuitivité pour encourager l'engagement avec un groupe d'utilisateurs moins avertis techniquement. Un portail réussi est donc simple, intuitif et enrichi de fonctionnalités pour améliorer les flux de travail des développeurs.</p>""}, {'', ""<p>En ce qui concerne le flux de travail, le portail doit s'intégrer de manière transparente aux outils et systèmes existants. L'exploitation de tout en tant que code constitue une base efficace pour assurer une intégration fluide dans tous les systèmes, au-delà du simple portail en libre-service, avec un minimum d'erreurs ou de retards. L'intégration de fonctionnalités d'automatisation et d'orchestration peut rationaliser les opérations, libérant du temps sur les tâches répétitives et banales pour améliorer DevX, permettant ainsi de se concentrer davantage sur l'innovation et la résolution de problèmes.</p>""}, {'', '<h3>Liberté vs contrôle</h3>'}, {'', ""<p>En fin de compte, les portails doivent trouver le juste équilibre entre liberté et contrôle, ce qui peut être obtenu en garantissant la flexibilité grâce à un contrôle d'accès basé sur les rôles. Accorder aux utilisateurs finaux la liberté de déployer dans un cadre sécurisé d'autorisations prédéfinies crée un environnement propice à l'innovation dans un environnement solidement protégé. Cela signifie que les utilisateurs peuvent explorer, expérimenter et innover sans se soucier des limites de sécurité ou des obstacles inutiles.</p>""}, {'', '<p>Mais bien sûr, comme pour tout projet, les entreprises ne peuvent pas se permettre de construire quelque chose et de considérer que le travail est terminé. La mesure du succès est un processus continu. Des indicateurs tels que la fréquence d’accès au portail, qui utilise quoi, quels catalogues de services sont utilisés et comment l’utilisation du portail doit être suivie, ainsi que d’autres données pertinentes, aideront à identifier les domaines nécessitant des améliorations.</p>'}, {'', '<p>Il est également important de garder à l’esprit qu’il s’agit d’un travail collaboratif entre l’équipe de la plateforme et les utilisateurs finaux. Et en matière de technologie, il y a toujours place à l’amélioration. Par exemple, les avancées récentes en matière d’IA/ML pourraient bientôt être exploitées pour analyser des ensembles de données jusqu’alors inaccessibles et générer des prises de décision plus intelligentes et plus rapides.</p>'}, {'', '<p>Un portail en libre-service bien conçu doit offrir une expérience qui démystifie et démocratise l’accès aux services et aux données et améliore le processus du cycle de vie du projet entre les équipes. Pour y parvenir, ils doivent simplifier la convivialité, s’efforcer d’aider les développeurs à exploiter efficacement ses fonctionnalités, assurer l’alignement avec les flux de travail existants, améliorer ses fonctionnalités et évaluer régulièrement son impact sur la productivité. Cela permettra de garantir la survie et l’épanouissement des utilisateurs en libre-service à l’ère numérique.</p>'}]"
Comparaison des approches : centre d'excellence versus ingénierie de plateforme,"[{'', ""<p>Gérer l'infrastructure tout en innovant rapidement et en gardant les développeurs satisfaits est un combat constant pour de nombreux leaders technologiques, dont moi-même. Pour rationaliser certains de ces efforts, deux approches importantes ont émergé ces dernières années, les centres d'excellence (CoE) et l'ingénierie de plateforme. Bien que toutes deux visent à accroître la productivité et la collaboration, elles diffèrent considérablement dans leurs principes fondamentaux et leurs méthodes de mise en œuvre.</p>""}, {'', '<p>Examinons les aspects clés de chaque approche, en examinant leurs forces et leurs faiblesses pour vous aider à déterminer celle qui pourrait être la meilleure pour votre organisation.</p>'}, {'', ""<h3>Centres d'excellence</h3>""}, {'', '<p>L’approche du centre d’excellence repose sur l’idée de créer une équipe centralisée d’experts possédant des connaissances et des compétences spécialisées dans un domaine particulier. En général, un centre d’excellence est créé pour répondre officiellement au déficit de connaissances spécifique d’une organisation. Cette équipe agit comme un centre de connaissances, de meilleures pratiques et de conseils pour l’ensemble de l’organisation.</p>'}, {'', '<p>Parmi les avantages du CoE :</p>'}, {'', ""<p>● Plus d'expertise et de partage des connaissances : un centre d'excellence rassemble des experts en la matière qui fournissent des conseils, un mentorat et une formation aux autres équipes. Cela garantit le partage des connaissances au sein de l'organisation, ce qui conduit à une efficacité et une normalisation améliorées.</p>""}, {'', ""<p>● Meilleur contrôle de la qualité et gouvernance : un CoE établit des normes, des cadres et des processus de gouvernance pour garantir la cohérence et la qualité des projets. En d'autres termes, tout le monde est sur la même longueur d'onde. Cela permet de réduire les erreurs, d'améliorer la qualité du code et de minimiser la dette technique.</p>""}, {'', '<p>Cependant, un centre d’excellence présente également des inconvénients potentiels en raison de la centralisation des prises de décision, d’une évolutivité et d’une adaptabilité limitées. Il crée une dépendance vis-à-vis d’un petit groupe d’experts.</p>'}, {'', '<p>Plusieurs experts du secteur tentent de faire évoluer la discussion d’une approche de « centre d’excellence » vers une approche de « centre d’activation ». Lors de notre récent panel sur les tendances des API, l’expert James Higginbotham a déclaré : « Nous devons faire beaucoup pour permettre l’automatisation de nos développeurs avant même de parler d’une plateforme. Déplaçons la conversation vers l’activation des API en nous penchant sur les centres d’activation. »</p>'}, {'<h3>Ingénierie de la plateforme</h3>', ''}, {'', ""<p>À mesure que les logiciels évoluent et deviennent plus complexes, DevOps permet de mettre en place une infrastructure utilisable sur laquelle les développeurs peuvent s'appuyer. Et ce qui a évolué à partir de ces discussions est le monde de l'ingénierie des plateformes.</p>""}, {'', ""<p>L'ingénierie de plateforme se concentre sur la création d'une plateforme technologique robuste et évolutive qui sert de base à diverses équipes et projets au sein d'une organisation, l'objectif principal étant d'améliorer l'expérience des développeurs à tous les niveaux. L'approche PE permet souvent de relever les défis de DevOps, notamment l'augmentation de l'automatisation et la gestion d'équipes cloisonnées à grande échelle. Les autres avantages d'une approche PE sont les suivants\xa0:</p>""}, {'', ""<p>● Une approche axée sur l'infrastructure et les outils : l'ingénierie de plateforme met l'accent sur le développement et la maintenance d'un écosystème d'infrastructures et d'outils partagé. Cela permet aux équipes de tirer parti de composants, de services et de cadres prédéfinis, ce qui réduit la duplication des efforts et accélère les cycles de développement.</p>""}, {'', ""<p>● Amélioration du libre-service et de l'autonomie : les fonctionnalités de libre-service permettent aux équipes de déployer, de faire évoluer et de gérer leurs applications de manière indépendante. Cela réduit les dépendances vis-à-vis des équipes spécialisées et permet une mise sur le marché plus rapide.</p>""}, {'', '<p>● Flexibilité et adaptabilité : cette approche permet aux équipes d’expérimenter, d’itérer et d’innover sans être contraintes par des processus ou des technologies rigides.</p>'}, {'', ""<p>En ce qui concerne les faiblesses, les pratiques PE nécessitent souvent un investissement initial et des coûts de mise en place importants, que les petites organisations et les équipes n'ont peut-être pas le budget ou les ressources nécessaires pour prendre en charge. Elles nécessitent également une maintenance et des mises à jour continues.</p>""}, {'', '<p>Le plus gros obstacle est souvent la résistance potentielle au changement des équipes existantes. En fin de compte, le changement culturel peut être le plus grand obstacle à surmonter, et il peut falloir des mois pour faire adopter complètement une nouvelle approche à une équipe.</p>'}, {'', '<h3>Alors, que dois-je choisir ?</h3>'}, {'', '<p>Si un centre d’excellence peut centraliser l’expertise et le partage des connaissances, une approche d’ingénierie de plateforme peut donner aux équipes des capacités de libre-service et de flexibilité. Une approche hybride peut être la meilleure solution. Utilisez un centre d’excellence pour stimuler l’innovation, proposer des formations et développer les meilleures pratiques. Et pendant qu’ils se concentrent sur cela, optimisez l’ingénierie de plateforme pour garantir que ces pratiques sont évolutives et intégrées au processus de développement, en fournissant les outils et les environnements nécessaires pour satisfaire vos développeurs.</p>'}, {'', '<p>Quand un centre d’excellence est pertinentSupposons que vous ayez besoin que votre équipe se concentre et développe une expertise dans un domaine spécifique ou que votre équipe se concentre sur l’innovation et l’adoption de nouvelles technologies plutôt que sur la définition d’aspects fondamentaux. Dans ces scénarios, un centre d’excellence vous serait plus utile. De même, si vos projets nécessitent des connaissances spécialisées et un accompagnement rapproché, un centre d’excellence serait la solution la plus judicieuse.</p>'}, {'', ""<p>Lorsque l'ingénierie de plate-forme a du sensLes grandes entreprises ou les équipes composées de plusieurs groupes de développement travaillant vers des objectifs similaires tireront le plus grand bénéfice de cette approche. Si votre plus grand besoin est de standardiser les processus de développement et de déploiement avant tout, l'ingénierie de plate-forme vous y aidera. Les entreprises qui disposent du temps et des ressources nécessaires pour investir dans la création et la maintenance d'une plate-forme robuste prospéreront avec une approche d'ingénierie de plate-forme.</p>""}, {'', ""<p>Quelle que soit l'approche, les deux visent à rationaliser l'infrastructure d'innovation et à garantir le bonheur de vos équipes d'ingénieurs. Ces deux problèmes peuvent être résolus par une simple règle d'or : toujours écouter vos développeurs.</p>""}, {'', '<p>Votre plus grand atout est toujours votre équipe, et son plus grand atout est toujours un leadership qui soutient ces personnes en leur fournissant les ressources dont elles ont besoin. Il ne s’agit donc pas d’un bras de fer et d’un choix entre le Centre d’excellence et le PE, mais plutôt de savoir lequel aide votre entreprise à créer des logiciels innovants. Je vous laisse avec l’étape la plus importante que vous pouvez franchir vers l’une ou l’autre approche, et cela commence par une chose : une conversation avec vos équipes.</p>'}]"
Ingénierie de plateforme : l'évolution vers DevOps-as-a-Service,"[{'', ""<p>Dans le paysage en constante évolution du développement logiciel et des opérations informatiques, de nouvelles méthodologies et de nouveaux cadres apparaissent fréquemment pour répondre aux défis d'efficacité, d'évolutivité et de fiabilité. Parmi ceux-ci, l'ingénierie de plateforme et DevOps ont suscité une attention particulière.</p>""}, {'', ""<p>Dans mon précédent blog, Mesurer la valeur de DevOps en tant que service, j'ai déclaré : « En termes généraux, DaaS correspond aux capacités DevOps mises à la disposition des utilisateurs de ces capacités via des portails et des API. »</p>""}, {'', '<p>Ce blog soutiendra que l’ingénierie de plate-forme est essentiellement une évolution de DevOps vers un modèle plus structuré et orienté services, offrant une approche standardisée et évolutive pour la mise en œuvre des pratiques DevOps dans les organisations.</p>'}, {'', '<h3>Comprendre DevOps</h3>'}, {'', ""<p>Principes DevOps : DevOps est une approche culturelle et méthodologique qui met l'accent sur la collaboration entre les équipes de développement et d'exploitation. Ses principes fondamentaux sont les suivants :</p>""}, {'', ""<p>Collaboration et communication : briser les silos entre le développement et les opérations pour favoriser une culture de collaboration continue.Intégration continue et livraison continue (CI/CD) : automatisation des processus d'intégration et de déploiement pour garantir une livraison rapide et fiable des logiciels.Infrastructure en tant que code (IaC) : gestion et provisionnement de l'infrastructure informatique via du code plutôt que des processus manuels.Surveillance et observabilité : surveillance continue des applications et de l'infrastructure pour garantir les performances, la fiabilité et la disponibilité.</p>""}, {'', '<p>Défis liés à la mise en œuvre de DevOps : Bien que les avantages de DevOps soient évidents, de nombreuses organisations ont du mal à le mettre en œuvre. Les défis courants incluent :</p>'}, {'', ""<p>Intégration d'outils : l'intégration de divers outils et technologies nécessaires au CI/CD, à la surveillance et à l'IaC peut être complexe et prendre du temps. Lacunes en matière de compétences : les organisations manquent souvent des compétences et de l'expertise nécessaires pour mettre en œuvre et gérer efficacement les pratiques DevOps. Changement culturel : faire évoluer la culture organisationnelle pour adopter les principes DevOps peut être difficile et lent.</p>""}, {'', ""<h3>L'essor de l'ingénierie de plateforme</h3>""}, {'', ""<p>Qu'est-ce que l'ingénierie de plateforme ? L'ingénierie de plateforme se concentre sur la création et la maintenance de plateformes intégrées qui fournissent les outils, les environnements et les services nécessaires pour prendre en charge l'ensemble du cycle de vie du développement logiciel. Ces plateformes sont conçues pour rationaliser le développement, les tests, le déploiement et les opérations en offrant un ensemble cohérent et standardisé de services.</p>""}, {'', ""<p>Composants clés de l'ingénierie de plate-forme\xa0:</p>""}, {'', '<p>Interfaces en libre-service : fournir aux développeurs un accès facile aux ressources nécessaires, telles que les environnements, les bases de données et les pipelines CI/CD.</p>'}, {'', '<p>Automatisation : mise en œuvre de l’automatisation pour gérer l’infrastructure, les déploiements et la surveillance, réduisant ainsi les efforts manuels et les erreurs.</p>'}, {'', '<p>Normalisation : création d’environnements et de processus standardisés pour garantir la cohérence et la fiabilité des projets et des équipes.</p>'}, {'', '<p>Évolutivité : concevoir des plateformes qui s’adaptent aux besoins de l’organisation, prenant en charge plusieurs équipes et divers projets.</p>'}, {'', '<h3>Ingénierie de plateforme en tant que DevOps-as-a-Service</h3>'}, {'', ""<p>Définition de DevOps en tant que service : DevOps en tant que service (DaaS) peut être compris comme une approche orientée services pour fournir des fonctionnalités DevOps. Il s'agit de fournir un ensemble d'outils et de pratiques standardisés et évolutifs sous forme de service géré, permettant aux organisations de tirer parti des principes DevOps sans avoir à créer et à maintenir elles-mêmes l'infrastructure sous-jacente.</p>""}, {'', ""<p>Similitudes entre l'ingénierie de plateforme et DevOps-as-a-Service\xa0:</p>""}, {'', ""<p>1. Standardisation et automatisation : l'ingénierie de plateforme et le DaaS mettent tous deux l'accent sur la standardisation et l'automatisation. En fournissant des outils et des environnements standardisés, l'ingénierie de plateforme simplifie les processus d'intégration et de déploiement, de la même manière que le DaaS fournit un ensemble unifié de fonctionnalités DevOps en tant que service.</p>""}, {'', ""<p>2. Capacités en libre-service : l'ingénierie de plateforme fournit des interfaces en libre-service qui permettent aux développeurs d'accéder aux ressources dont ils ont besoin sans attendre les équipes d'exploitation. Cela correspond au modèle DaaS, où les services sont facilement accessibles aux utilisateurs via un portail en libre-service.</p>""}, {'', '<p>3. Concentrez-vous sur l’efficacité et l’évolutivité : les deux approches visent à améliorer l’efficacité et l’évolutivité. L’ingénierie de plateforme crée des plateformes évolutives qui peuvent prendre en charge plusieurs équipes et projets, tandis que DaaS offre des capacités DevOps évolutives qui peuvent évoluer en fonction des besoins de l’organisation.</p>'}, {'', ""<p>4. Réduction de la complexité et des lacunes en matière de compétences : en proposant une plateforme gérée avec des outils intégrés, l'ingénierie de plateforme réduit la complexité de la mise en œuvre des pratiques DevOps et comble les lacunes en matière de compétences au sein des organisations. De même, DaaS fournit une solution clé en main qui simplifie l'adoption et la gestion de DevOps.</p>""}, {'', ""<p>5. Collaboration et intégration améliorées : l'ingénierie de plateforme favorise la collaboration en fournissant une plateforme unifiée qui intègre divers outils et services, favorisant ainsi une communication et une coordination fluides au sein de l'équipe. Cela reflète la nature collaborative du DaaS, qui intègre et orchestre les outils DevOps pour soutenir la livraison et les opérations continues.</p>""}, {'', ""<p>Avantages de l'ingénierie de plateforme en tant que DevOps en tant que service1. Adoption accélérée : l'ingénierie de plateforme accélère l'adoption des pratiques DevOps en fournissant une plateforme prête à l'emploi qui intègre les meilleures pratiques et outils. Les organisations peuvent rapidement intégrer des équipes et commencer à exploiter les capacités DevOps sans installation ni configuration approfondies.</p>""}, {'', ""<p>2. Cohérence et fiabilité : les plateformes standardisées garantissent la cohérence et la fiabilité des projets et des équipes. Cela réduit le risque d'erreurs et de divergences résultant d'intégrations d'outils ad hoc et de processus manuels.</p>""}, {'', ""<p>3. Concentrez-vous sur les compétences de base : en déchargeant la responsabilité de la création et de la maintenance de l'infrastructure DevOps sur une équipe de plateforme centralisée, les organisations peuvent permettre aux équipes de développement et d'exploitation de se concentrer sur leurs compétences de base : développer et fournir des logiciels de haute qualité.</p>""}, {'', ""<p>4. Expérience de développement améliorée : les interfaces en libre-service et les processus automatisés améliorent l'expérience du développeur en offrant un accès rapide et facile aux ressources nécessaires. Cela réduit les frictions et accélère les cycles de développement.</p>""}, {'', '<p>5. Évolutivité et flexibilité : les plateformes évolutives peuvent s’adapter aux besoins croissants des organisations, en prenant en charge des projets plus vastes et plus complexes. Cette flexibilité garantit que la plateforme peut évoluer en fonction des besoins de l’organisation.</p>'}, {'', '<h3>Défis et considérations</h3>'}, {'', ""<p>1. Investissement initial : la création d'une plateforme complète nécessite un investissement initial en temps et en ressources. Les organisations doivent planifier et allouer soigneusement les ressources pour garantir une mise en œuvre réussie.</p>""}, {'', ""<p>2. Amélioration continue : les plateformes doivent être continuellement améliorées et mises à jour pour intégrer de nouveaux outils, technologies et meilleures pratiques. Cela nécessite un effort et un engagement continus de la part de l'équipe de la plateforme.</p>""}, {'', '<p>3. Équilibrer la standardisation et la flexibilité : si la standardisation est bénéfique, il est essentiel de trouver un équilibre entre standardisation et flexibilité. Les plateformes doivent être adaptables pour répondre aux divers besoins des différentes équipes et des différents projets.</p>'}, {'', '<h3>Conclusion</h3>'}, {'', ""<p>L'ingénierie de plateforme représente l'évolution de DevOps vers un modèle plus structuré et orienté services, incarnant efficacement les principes de DevOps en tant que service. En fournissant des plateformes standardisées et évolutives avec des outils et des services intégrés, l'ingénierie de plateforme simplifie l'adoption et la gestion des pratiques DevOps, en répondant aux défis courants tels que l'intégration des outils, les lacunes en matière de compétences et le changement culturel.</p>""}, {'', ""<p>Alors que les entreprises cherchent des moyens d'améliorer l'efficacité, l'évolutivité et la fiabilité de leur développement logiciel et de leurs opérations informatiques, l'ingénierie de plateforme en tant que DevOps-as-a-service offre une solution convaincante. En accélérant l'adoption, en garantissant la cohérence et en améliorant l'expérience des développeurs, l'ingénierie de plateforme permet aux entreprises de tirer pleinement parti des avantages de DevOps et de favoriser l'innovation et l'amélioration continues.</p>""}, {'', '<p>En conclusion, l’ingénierie de plateforme ne remplace pas DevOps mais constitue une évolution qui offre une approche structurée et évolutive pour fournir des fonctionnalités DevOps en tant que service. Cette perspective met en évidence la nature complémentaire de l’ingénierie de plateforme et de DevOps, soulignant l’importance de réussir la transformation numérique.</p>'}]"
Shreds.AI lance une plateforme LLM destinée à l'ingénierie logicielle,"[{'', ""<p>Shreds.AI a dévoilé aujourd'hui une plateforme d'intelligence artificielle (IA) générative du même nom basée sur un grand modèle de langage (LLM) qu'elle a formé pour automatiser spécifiquement les tâches d'ingénierie logicielle. Disponible en version bêta, la plateforme Shreds.AI peut attribuer des tâches à huit autres LLM en invoquant les interfaces de programmation d'applications (API) qu'elles exposent.</p>""}, {'', ""<p>Selon Soufiane Amar, PDG de Shreds.AI, plutôt que de produire de petites quantités de code, la plateforme a été formée pour créer les dizaines de milliers de lignes et de fichiers de code nécessaires pour piloter des flux de travail d'ingénierie logicielle complexes. La plateforme Shreds.AI orchestrera également avec précision l'intégration de divers composants logiciels pour créer une application, car elle a été formée à l'aide d'outils que les développeurs utilisent régulièrement, a-t-il ajouté.</p>""}, {'', ""<p>Un développeur saisit une description simple en langage naturel du logiciel qu'il souhaite que Shreds.AI crée, et la plateforme génère ensuite des diagrammes d'architecture et le code des fonctionnalités indépendantes et isolées appelées shreds. Les équipes DevOps n'ont plus qu'à valider le code avant de l'utiliser, un processus que Shreds.AI simplifie grâce à un réseau de développeurs indépendants que les organisations peuvent engager pour examiner le code, a déclaré Amar.</p>""}, {'', '<p>Shreds.AI est une méta-IA, dans la mesure où, en plus de générer du code et de raisonner sur plusieurs processus, elle est capable de classer les LLM tiers en fonction de leur capacité à effectuer des tâches spécifiques, a-t-il noté.</p>'}, {''}, {'', '<p>La plateforme Shreds.AI est déjà testée par le conglomérat automobile Stellantis et le Réseau de Transport d’Électricité (RTE), le gestionnaire du réseau de transport d’électricité français. Shreds.AI estime qu’une application qui aurait pu coûter auparavant 1 million de dollars peut désormais être créée pour moins de 30 000 dollars. L’entreprise affirme que Shreds.AI réduit le délai de mise sur le marché des logiciels, ainsi que la taille des équipes et les coûts, de plus de 80 % par rapport aux méthodes de développement de logiciels traditionnelles. En permettant la maintenance automatique, elle résout également le problème de l’obsolescence des logiciels. Elle augmente la durée de vie des logiciels de plus de 60 %, par exemple en facilitant la conversion du langage de programmation utilisé pour créer cette application en un autre langage que davantage de développeurs connaissent.</p>'}, {'', '<p>L’intégration de l’IA dans les workflows DevOps est encore trop récente et le code généré par ces plateformes doit encore être géré. Le défi est que, grâce à l’essor de l’IA, on s’attend à ce que la quantité de logiciels créés et déployés au cours des deux prochaines années dépasse la quantité de logiciels déployés au cours des deux dernières décennies. La seule façon de suivre ce rythme de développement sera d’appliquer également l’IA à la gestion des workflows DevOps.</p>'}, {'', '<p>Dans l’espoir d’éliminer autant de tâches que possible, les équipes DevOps doivent identifier les processus manuels qu’elles effectuent régulièrement aujourd’hui, en vue d’y appliquer l’IA demain. Après tout, l’objectif initial de l’adoption de DevOps était d’automatiser impitoyablement autant de processus d’ingénierie logicielle que possible, afin de permettre la création et le déploiement d’un plus grand nombre d’applications le plus rapidement possible.</p>'}]"
Intégration de Catchpoint dans les pipelines CI/CD,"[{'', '<p>La proposition de valeur de Catchpoint est simple à comprendre. Elle surveille les performances des sites Web et des applications au-delà de votre propre infrastructure pour fournir une « vue d’ensemble de l’utilisateur ».</p>'}, {'', ""<p>Je connais Catchpoint depuis plusieurs années, car nous en avons déjà parlé ici sur DevOps.com et sur d'autres sites Techstrong. Le deuxième jour du #AppDev Tech Field Day à Santa Clara, le PDG de Catchpoint, Mehdi Daoudi, a présenté l'histoire de Catchpoint, expliquant la différence entre la gestion des performances des applications (APM) classique et ce que fait Catchpoint, qu'il appelle Internet Performance Monitoring (IPM).</p>""}, {'', '<p>Je n’ai pas pu m’empêcher de repenser à l’époque des dot-com. Plusieurs de mes amis – Rajat Bhargava, Tom Higley et Neil Robertson – ont lancé une entreprise appelée Service Metrics. Là encore, le principe était simple : comprendre les performances d’un site Web à partir de différents endroits sur Internet.</p>'}, {'', '<p>Un événement étrange s’est produit avec Service Metrics. Moins de 18 mois après la création de l’entreprise – avant même que le produit ne soit complètement prêt – elle a été rachetée par Exodus Communications, l’un des plus grands fournisseurs de centres de données de l’époque. Dans la folie typique des dot-com, les actions d’Exodus ont été divisées plusieurs fois et ont explosé entre le moment où l’accord a été annoncé et celui où il a été conclu. Ce qui était une acquisition de 280 millions de dollars est devenue une transaction de plus d’un milliard de dollars au moment de la clôture. Telle était la vie à l’époque des dot-com.</p>'}, {'', '<p>C’était il y a plus de 25 ans, mais le principe de base – surveiller les performances d’Internet – est similaire aux objectifs actuels de Catchpoint. Bien sûr, le monde et le réseau d’aujourd’hui sont beaucoup plus complexes. Mais au final, nous voulons tous savoir comment nos sites Web et nos applications fonctionnent pour les utilisateurs finaux.</p>'}, {'', '<p>L’intérêt de Catchpoint réside dans le fait qu’il a atteint un niveau de maturité tel que les services de surveillance peuvent être transmis directement au pipeline CI/CD via des boucles de rétroaction. En utilisant des outils standard du secteur tels qu’Open Telemetry, Catchpoint fonctionne également avec des outils. Ces intégrations et cette ingénierie basée sur des normes permettent à Catchpoint de fonctionner parfaitement avec le reste des outils d’une organisation, de contribuer aux objectifs de niveau de service (SLO) et de tirer parti des investissements informatiques existants.</p>'}, {'', '<p>Ce qui m’a vraiment enthousiasmé, c’est la réponse de Mehdi à ma question sur l’ajout de réponses automatisées aux données surveillées par Catchpoint. L’IA peut vraiment changer la donne. L’entreprise dispose d’environ 15 ans de données de surveillance, ce qui constituerait un filon-mère pour former une IA afin qu’elle puisse ajouter des réponses automatisées et exploitables à ce que Catchpoint considère comme des problèmes.</p>'}, {'', '<p>Daoudi envisage un avenir dans lequel un SRE peut se réveiller, consulter un tableau de bord « sonar » de Catchpoint et recevoir un rapport complet sur les performances sous-optimales observées par Catchpoint et les mesures prises pour y remédier. Et bien sûr, dans ce scénario, les correctifs et les résultats ont été intégrés à la version suivante du code et des tests pour le vérifier avant le déploiement étaient déjà préparés.</p>'}, {'', ""<p>Utopique ? Peut-être. Mais c'est le genre de chose dont tous les professionnels du réseau que je connais ne pouvaient que rêver à l'époque des métriques de service.</p>""}, {'', '<p>Au fait, j’ai adoré ma première expérience en tant que délégué du Tech Field Day et j’attends avec impatience la prochaine. Ce sera peut-être en novembre, à Salt Lake City, autour de la conférence Kubecon North America.</p>'}]"
"Exploration des plateformes Low/No-Code, GenAI, Copilots et Générateurs de code","[{'', '<p>L’émergence des plateformes low/no-code remet en question les notions traditionnelles d’expertise en codage. L’époque où le codage était un ensemble de compétences exclusives réservées à quelques personnes averties est révolue. Les plateformes low/no-code ont démocratisé le développement de logiciels. Elles permettent aux personnes issues de milieux non informatiques ou techniques de traduire leurs idées commerciales en applications sans avoir besoin de maîtriser des langages de programmation complexes.</p>'}, {'', ""<p>Ces plateformes low/no-code, avec leurs interfaces glisser-déposer conviviales et leurs composants pré-intégrés, ont simplifié le développement d'applications. Imaginez créer une application pour maison intelligente en quelques clics et glisser-déposer : les plateformes low/no-code rendent cela possible.</p>""}, {'', '<p>Ajoutez à cela l’intelligence artificielle générative (Gen AI) et les nouvelles pratiques de développement changent considérablement le domaine.</p>'}, {'', ""<h3>L'impact de la génération d'IA sur les plateformes Low-Code</h3>""}, {'', ""<p>Les plateformes low-code devraient continuer à évoluer, devenant plus intelligentes, plus adaptables et plus intégrées au cycle de vie du développement logiciel. On assistera à une évolution vers des plateformes qui utilisent l'IA de génération pour comprendre l'intention des utilisateurs, automatiser les tâches de routine et générer des extraits de code complexes.</p>""}, {'', ""<p>Gartner prédit que, d'ici 2026, plus de 80 % des entreprises utiliseront les API et modèles GenAI ou déploieront des applications compatibles GenAI dans des environnements de production, contre moins de 5 % début 2023.</p>""}, {'', '<p>La combinaison du low-code et de l’IA générative permettra aux développeurs d’automatiser les tâches de routine et de se concentrer sur la logique et la créativité de haut niveau. L’IA générative agit comme un multiplicateur qui peut accélérer et optimiser les opérations de développement. McKinsey estime que l’IA générative pourrait ajouter entre 2,6 et 4,4 billions de dollars par an à l’économie mondiale d’ici 2040.</p>'}, {'', '<h3>Un autre outil : les copilotes IA et les générateurs de code</h3>'}, {'', ""<p>Une autre application prometteuse des grands modèles de langage (LLM) de l'IA est celle des assistants basés sur l'IA. C'est là qu'interviennent les copilotes et les générateurs de code, des assistants IA qui simplifient le codage.</p>""}, {'', '<p>Alors que les plateformes low/no-code ont transformé le développement d’applications, les copilotes et les générateurs de code ajoutent une couche supplémentaire d’efficacité.</p>'}, {'', '<p>Les copilotes IA utilisent une interface de chat pour simplifier le codage via une interface de chat, où les développeurs articulent leurs exigences de code dans des invites en langage naturel, et le copilote génère le code en conséquence. Ces assistants fournissent une assistance en temps réel, en proposant des suggestions contextuelles et des extraits de code, et ils peuvent anticiper les bugs potentiels. De plus, ils peuvent expliquer les blocs de code, générer des tests unitaires et proposer des correctifs de bugs, ce qui améliore le processus de codage. Les copilotes IA sont comme des assistants de confiance, prêts à vous aider dans des tâches de codage spécifiques ou à générer du code complexe - et ils le font rapidement.</p>'}, {'', '<h3>La grande question</h3>'}, {'', '<p>Avec l’essor des plateformes low/no-code, les compétences requises dans le secteur technologique vont évoluer. L’époque où la maîtrise de plusieurs langages de programmation était indispensable est révolue.</p>'}, {'', '<p>Les outils low/no-code occupant une place centrale, cela signifie-t-il que le codage devient obsolète ?</p>'}, {'', '<p>Un grand non.</p>'}, {'', '<p>Si les connaissances de base en programmation restent précieuses, elles ne constituent plus la seule condition requise pour créer des applications innovantes. Grâce aux plateformes low/no-code qui démocratisent le développement d’applications, des personnes d’horizons divers peuvent exploiter la technologie pour créer des solutions efficaces.</p>'}, {'', '<p>GenAI, les copilotes et les générateurs de code rendent-ils les plateformes low/no-code obsolètes\xa0?</p>'}, {'', '<p>Encore une fois, non.</p>'}, {'', '<p>Si GenAI, copilots et générateurs de code excellent dans certaines tâches spécifiques, les plateformes low/no code offrent une solution holistique, combinant conception d’interface visuelle, intégration de bases de données et bien plus encore. Ce sont les couteaux suisses du développement, répondant à un large éventail de besoins au-delà de la simple génération de code.</p>'}, {'', '<p>L’IA de génération, les copilotes et les générateurs de code ne sont pas là pour remplacer les plateformes low/no-code ou les pratiques de codage traditionnelles. Au contraire, ils sont positionnés pour les compléter et les améliorer. La fusion de ces technologies est très prometteuse pour simplifier le développement de logiciels, accroître l’efficacité et ouvrir de nouvelles voies d’innovation.</p>'}, {'', '<h3>Le dernier mot</h3>'}, {'', '<p>Grâce à de meilleures suggestions de code et à des capacités de test automatisées, les plateformes low/no-code offrent des solutions de haute qualité qui répondent aux exigences des utilisateurs férus de technologie d’aujourd’hui.</p>'}, {'', '<p>Grâce à la combinaison passionnante des plateformes low/no-code et de GenAI, l’avenir du développement logiciel est prometteur. Si ces avancées peuvent bouleverser le codage traditionnel, elles offrent également des opportunités d’innovation et de créativité.</p>'}, {'', '<p>Il est temps pour nous d’accepter ce changement et de construire un avenir meilleur, une ligne de code (ou son absence) à la fois.</p>'}]"
Comment migrer une plateforme d'observabilité vers l'open source,"[{'', ""<p>Il est difficile d'obtenir une observabilité complète des applications d'entreprise. Si des services tiers comme New Relic ou Datadog facilitent l'observabilité de bout en bout, à mesure que votre application devient plus complexe, les données de télémétrie le deviennent également, ce qui entraîne une augmentation des coûts. La migration vers une pile open source vous permet de contrôler les données de télémétrie et de réduire les coûts d'observabilité, malgré les défis liés aux engagements des fournisseurs de services existants.</p>""}, {'', ""<p>La migration vers des solutions open source est simple avec une architecture et des piles technologiques simples. Cependant, la transition de l'ensemble de la pile d'observabilité vers l'open source exige une planification méticuleuse, des évaluations d'outils/frameworks, des tests, une analyse des risques et une communication entre les équipes, en particulier compte tenu du nombre de microservices, de la forte dépendance au cloud et de la diversité des langages et frameworks à prendre en compte.</p>""}, {'', ""<p>Une plateforme d'observabilité cloud absorbe généralement 20 à 30 % des dépenses globales d'infrastructure, mais dans certains cas, elle peut atteindre 50 à 60 %. Si vos dépenses d'observabilité dépassent 50 %, la transition vers des solutions open source serait une option judicieuse pour atténuer les coûts de l'infrastructure technologique.</p>""}, {'', '<p>Depuis deux ans, je travaille à la migration de la plateforme d’observabilité d’un fournisseur de solutions propriétaire vers une pile open source. Plongeons-nous dans les étapes clés nécessaires à une telle migration.</p>'}, {'', '<h3>Finaliser les données et systèmes clés de télémétrie</h3>'}, {'', ""<p>Les plateformes d'observabilité gérées offrent des informations complètes sur l'état du système, ce qui nécessite un volume important de données de télémétrie de haute qualité. Par défaut, les agents chargés de capturer et de transmettre les données de télémétrie sont configurés pour recueillir autant d'informations que possible, ce qui facilite la création de tableaux de bord et de rapports complets. Cependant, cette collecte de données extensive contribue au coût global.</p>""}, {'', '<p>Sélectionnez les systèmes que vous devez réellement surveiller. Une application d’entreprise typique comprend des bases de données, des bases de données de mise en cache, des orchestrateurs de conteneurs et de nombreux services cloud. Les ingénieurs surveillent souvent plus de services que nécessaire, ce qui entraîne des complexités et des coûts inutiles. Il suffit souvent de rationaliser le périmètre de surveillance aux systèmes essentiels.</p>'}, {'', '<p>D’après mon expérience, la quantité de données de télémétrie requise est nettement inférieure à celle que les services tiers récupèrent généralement. Vous pouvez facilement diviser ces collectes de données en deux catégories\xa0: les données indispensables et les données utiles. Les données «\xa0indispensables\xa0» correspondent aux quatre signaux d’or bien établis\xa0:</p>'}, {'', '<li>Latence : le temps nécessaire pour répondre à une requête</li>'}, {'', ""<li>Trafic : le volume de requêtes qu'un système traite actuellement</li>""}, {'', ""<li>Le taux d'erreur : le nombre de requêtes qui échouent ou renvoient des réponses inattendues</li>""}, {'', '<li>Saturation des ressources : le pourcentage de ressources disponibles consommées</li>'}, {'', ""<p>Ces signaux fondamentaux servent de base aux tableaux de bord et aux alertes clés. Les plateformes d'observabilité construites sur ces signaux couvrent 80 % des cas d'utilisation typiques.</p>""}, {'', '<p>Il est essentiel de comprendre les exigences en matière de métadonnées de chaque signal. L’étendue des métadonnées capturées a un impact direct sur la complexité et le coût de la plateforme d’observabilité. Par exemple, lors de la surveillance de la latence d’un service à l’autre, tenez compte de la nécessité d’adresses IP de service, d’ID d’instance ou d’informations d’en-tête.</p>'}, {'', '<h3>Sélectionnez la pile concernée</h3>'}, {'', ""<p>Les plateformes d'observabilité gérées telles que New Relic et Datadog offrent une surveillance complète de divers composants d'entreprise. Cependant, lorsque vous migrez vers une pile open source, vous devez évaluer et intégrer les piles d'outils pour répondre à diverses exigences de surveillance.</p>""}, {'', ""<p>Un aspect important à prendre en compte est la mise à l'échelle : comment gérer l'énorme quantité de données générées chaque minute sur tous ces systèmes. Concentrez-vous sur deux fronts : la sélection de la pile pour le stockage et le traitement des données de télémétrie (journaux, métriques et traces) et la conception de méthodes pour capturer et transmettre les données de télémétrie à partir de divers systèmes.</p>""}, {'', '<h4>Pile pour les bûches</h4>'}, {'', ""<p>Vous avez besoin d'une pile capable de traiter et de stocker de manière efficace et économique le volume considérable de journaux générés par le système. Par le passé, j'utilisais la pile ELK pour stocker et rechercher des journaux, mais il s'agit d'une solution générique et non conçue spécifiquement pour les journaux.</p>""}, {'', '<p>Je recommande Loki de Grafana pour sa gestion efficace des gros volumes de journaux et LogQL, un langage proche de PromQL. Si vous connaissez PromQL, la navigation dans des données de journaux volumineuses avec LogQL est simple et intuitive.</p>'}, {'', '<h4>Pile pour les métriques</h4>'}, {'', ""<p>Prometheus est une base de données populaire pour le stockage de données de mesures de séries chronologiques. Cependant, elle présente des limitations liées à la mise à l'échelle ; elle ne peut pas être mise à l'échelle horizontalement. Des alternatives comme Thanos, Mimir de Grafana et VictoriaMetrics offrent de meilleures solutions prêtes à l'emploi pour la mise à l'échelle horizontale.</p>""}, {'', '<p>J’ai effectué quelques recherches et j’ai finalement choisi Mimir de Grafana pour stocker les métriques dans un projet de migration similaire. Cette décision était basée sur la capacité de stockage à distance de Mimir, ainsi que sur son architecture évolutive et hautement disponible.</p>'}, {'', '<h3>Pile de traçage</h3>'}, {'', '<p>Le traçage distribué est indispensable dans une architecture de microservices, pour identifier facilement les goulots d’étranglement en termes de latence et pour résoudre les bugs liés aux performances. Tempo de Grafana pourrait être une bonne option pour plusieurs raisons :</p>'}, {'', ""<li>Il peut s'intégrer de manière transparente en tant que backend pour le tableau de bord de Grafana. La consolidation des tableaux de bord dans Grafana permet d'éviter de naviguer entre plusieurs applications. Changer de plateforme uniquement pour le traçage tout en utilisant Grafana pour d'autres aspects de l'observabilité ne serait pas pratique.</li>""}, {'', ""<li>Tempo offre une expérience similaire à TraceQL lors de l'interaction avec la base de données, simplifiant le processus d'apprentissage et réduisant considérablement la courbe d'apprentissage.</li>""}, {'', '<li>Tempo est compatible avec les protocoles de traçage open source les plus répandus, tels que Zipkin et Jaeger. Si une équipe utilise déjà ces protocoles, la transition vers Tempo de Grafana se fera plus facilement.</li>'}, {'', ""<p>Vous devez installer un agent sur le système pour capturer et transmettre les données de télémétrie. Ces étapes varient en fonction du système surveillé. Les configurations d'applications d'entreprise classiques impliquent la surveillance des intégrations cloud, la surveillance des processus, des hôtes d'infrastructure, des clusters Kubernetes et la surveillance des applications. Parmi les possibilités :</p>""}, {'', '<li>Intégrations cloud : cela inclut la surveillance des services cloud tels que SQS, SNS, EMR et EC2.</li>'}, {'', ""<li>Surveillance des processus : cela implique la surveillance des processus exécutés sur des machines bare-metal. Avec l'avènement de la dockerisation et de Kubernetes, il existe désormais une manière standardisée de démarrer l'application. Dans le passé, il n'y avait pas de mécanisme fixe. Par exemple, pour exécuter une application Java, vous pouviez utiliser une application Java, la commande « java -jar », Tomcat ou OS systemctl. Dans le cas de node, il pourrait s'agir de npm ou de PM2. Chaque équipe ou service peut avoir sa propre façon de démarrer le processus.</li>""}, {'', ""<li>Hôte d'infrastructure\xa0: cela implique de surveiller la machine elle-même pour des mesures telles que l'utilisation du processeur, la mémoire, les E/S de disque et les E/S réseau ou de vérifier si la machine est hors ligne.</li>""}, {'', '<li>Surveillance Kubernetes\xa0: vous devez régulièrement surveiller un cluster Kubernetes, par exemple dans les cas où Kubernetes ne parvient pas à planifier un pod en raison de ressources insuffisantes.</li>'}, {'', ""<li>Surveillance des applications : cette surveillance se concentre sur la supervision des services créés par l'équipe. Chaque service peut différer dans son approche de développement et son choix de pile technologique, mais du point de vue de l'observabilité, ils sont généralement traités de la même manière.</li>""}, {'', ""<li>Surveillance du navigateur et des appareils mobiles\xa0: ces mesures garantissent des performances et une expérience utilisateur optimales sur différentes plateformes. La surveillance du navigateur comprend le suivi des temps de chargement des pages, des performances de rendu, des erreurs JavaScript et de l'utilisation des ressources. Pour les appareils mobiles, elle comprend la surveillance des plantages d'applications, de la latence, de l'utilisation de la batterie, des requêtes réseau et des mesures spécifiques à l'appareil.</li>""}, {'', ""<p>Avant l'avènement d'OpenTelemetry, il n'existait aucune méthode standardisée de surveillance des applications. OpenTelemetry est né de la fusion de deux projets antérieurs, OpenTracing et OpenCensus. Il s'agit d'un cadre indépendant des fournisseurs et des outils permettant d'instrumenter des applications ou des systèmes indépendamment de la langue, de l'infrastructure ou de l'environnement d'exécution. OpenTelemetry représente un effort communautaire important, et sa popularité et sa stabilité ne cessent de croître.</p>""}, {'', ""<h3>Valider la pile d'observabilité sur l'architecture d'application</h3>""}, {'', ""<p>Les applications varient en termes d'architecture et de stades de développement. Pour répondre à cette diversité, vous avez besoin de plusieurs preuves de concept (POC) sur différents systèmes, tels que l'infrastructure, le service cloud et les applications back-end et front-end.</p>""}, {'', ""<p>La migration d'une plateforme d'observabilité d'une pile technologique à une autre est simple lorsque l'architecture de l'application est monolithique. Cependant, lorsque l'architecture sous-jacente est constituée de microservices, cela devient difficile.</p>""}, {'', ""<p>Bien que l'architecture des microservices offre une flexibilité dans le choix de différentes piles technologiques pour la création de services, la conception d'une solution standard pour la capture de données de télémétrie devient un défi en raison de la diversité des piles technologiques impliquées.</p>""}, {'', '<p>Il est nécessaire de réaliser des POC pour différentes combinaisons technologiques afin de créer un outil en libre-service que les équipes individuelles peuvent suivre pour migrer facilement leurs services. Par exemple, si votre service utilise Java 11, Spring Boot 3.x.x et PostgreSQL, ces POC peuvent fournir des étapes standardisées pour permettre la surveillance des applications.</p>'}, {'', '<h3>Migrer les composants principaux</h3>'}, {'', '<p>La mise en place de la plateforme d’observabilité implique la configuration de la pile pour les métriques, les journaux et les traces, ainsi que l’installation des agents nécessaires. L’un des avantages de la migration de la plateforme d’observabilité est qu’il n’est pas nécessaire de transférer les anciennes données de télémétrie. Pendant la phase de test, vous disposerez d’une période de chevauchement pour accumuler suffisamment de données utiles pour créer des tableaux de bord et des alertes spécifiques.</p>'}, {'', ""<p>La migration des alertes et des tableaux de bord vers le nouveau système est essentielle, même lorsque la migration des données n'est pas nécessaire. En effet, les modifications apportées aux techniques sous-jacentes de capture des mesures peuvent modifier les noms des mesures et, par la suite, toutes les expressions de requête utilisées dans les alertes et les tableaux de bord. Certaines mesures peuvent ne pas être disponibles dans certains scénarios, en particulier les mesures dérivées qui peuvent être recréées à partir de mesures sous-jacentes.</p>""}, {'', ""<p>Bien que la migration manuelle des expressions de requête soit une option, un processus manuel est sujet à des erreurs et prend du temps pour un grand nombre d'alertes. J'ai connu une situation similaire avec des numéros d'alerte allant de 100 à 400.</p>""}, {'', '<p>Pour simplifier la migration, nous avons développé un script Node.js qui convertit par programmation les expressions de requête de New Relic en expressions Prometheus. Le script a effectué les étapes de haut niveau suivantes\xa0:</p>'}, {'', ""<li>Connecté au serveur API New Relic et récupéré toutes les alertes configurées pour l'intégration cloud</li>""}, {'', ""<li>Conversion des expressions d'alerte New Relic en expressions d'alerte PromQL</li>""}, {'', ""<li>J'ai écrit toutes les expressions d'alerte Prometheus dans le fichier YAML</li>""}, {'', '<p>La mise en œuvre de cette technique nous a permis de migrer avec succès toutes les alertes et les tableaux de bord en quatre ou cinq jours, un processus qui nécessiterait normalement un mois de travail manuel. De plus, les scripts ont considérablement réduit le risque d’erreur humaine.</p>'}, {'', '<h3>Tester la migration</h3>'}, {'', '<p>Vérifiez chaque alerte et chaque tableau de bord. Exécutez à la fois les anciennes et les nouvelles plateformes pendant les tests. Le fonctionnement en parallèle permet de tester en profondeur les métriques.</p>'}, {'', ""<p>Cependant, l'exécution simultanée des deux systèmes peut entraîner une baisse des performances en raison de l'envoi de données de télémétrie par les services à deux emplacements. De plus, il peut y avoir une légère augmentation des coûts jusqu'à la transition complète.</p>""}, {'', '<p>L’exécution des deux systèmes en parallèle fournit une configuration idéale pour garantir que les tableaux de bord et les alertes migrés fonctionnent correctement.</p>'}, {'', ""<p>Il est simple de tester les alertes déclenchées au cours de ce processus, mais évaluer celles qui ne se sont pas encore déclenchées constitue un défi. Dans le cadre d'un projet, j'ai réduit la valeur seuil de chaque alerte et testé ses fonctionnalités. Environ 90 % des alertes générées par des scripts personnalisés ont fonctionné de manière transparente, et seulement 10 % ont nécessité quelques ajustements manuels.</p>""}, {""<h3>Migrer d'autres composants associés</h3>"", ''}, {'', ""<p>D'autres systèmes dépendent des données générées par la plateforme d'observabilité. Par exemple :</p>""}, {'', ""<li>Systèmes de notification comme le courrier électronique, le canal Slack, etc., qui alertent en cas d'incident</li>""}, {'', '<li>Les outils de gestion des incidents comme PagerDuty offrent un moyen simplifié de gérer les incidents.</li>'}, {'', ""<p>Ces systèmes s'appuient sur la charge utile de l'alerte pour fonctionner correctement. Leurs ressources doivent être mises à jour, car un changement dans la plateforme d'observabilité entraîne un changement dans la charge utile de l'alerte. Mettez à jour les modèles des systèmes de notification pour vous assurer qu'ils s'intègrent parfaitement à la nouvelle charge utile de l'alerte.</p>""}, {'', '<p>De même, les outils de gestion des incidents comme PagerDuty nécessitent des modifications des règles de routage, des politiques d’escalade et de la planification. Heureusement, des outils de migration open source prêts à l’emploi sont disponibles pour faciliter la migration de PagerDuty vers OnCall de Grafana. Cependant, dans d’autres solutions, vous n’aurez peut-être pas accès à des outils de migration prêts à l’emploi. Dans de tels cas, une migration manuelle ou la rédaction de scripts peuvent être nécessaires.</p>'}, {'', '<h3>Calendrier de la migration</h3>'}, {'', ""<p>La planification d'une migration implique la finalisation des données et des systèmes de télémétrie clés, la sélection des piles pertinentes et la validation de la pile d'observabilité sur l'architecture de l'application. Cela prend généralement environ trois semaines.</p>""}, {'', ""<p>La durée du projet dépend de facteurs tels que le nombre d'équipes impliquées et la quantité de services actifs. Par exemple, une migration vers un environnement open source avec 100 microservices et la participation de 10 équipes différentes peut prendre quatre mois.</p>""}, {'', '<h3>Migration terminée\xa0!</h3>'}, {'', '<p>La transition de votre plateforme d’observabilité vers une pile open source offre une voie prometteuse pour réduire les coûts et renforcer le contrôle des données de télémétrie. Cependant, cette migration exige une planification et une exécution méticuleuses, englobant des étapes essentielles telles que la priorisation des fonctionnalités, la sélection de la pile, les POC, la migration des composants principaux, les tests et la migration des systèmes associés. Malgré les défis posés par la diversité des architectures et des technologies, une approche systématique, une évaluation complète et une collaboration entre les équipes peuvent faciliter un processus de migration fluide et garantir son succès.</p>'}]"
Une brève histoire de DevOps et le lien avec les environnements de développement cloud,"[{'', '<p>L’histoire de DevOps mérite d’être lue, et « The Phoenix Project », qui se décrit lui-même comme « un roman sur l’informatique et DevOps », est souvent cité comme un incontournable. Pourtant, pour les praticiens comme moi, un compte rendu plus pratique est « The DevOps Handbook » (du même auteur, Gene Kim, et d’autres), qui raconte certains des moments marquants de l’évolution de l’ingénierie logicielle et les illustre avec une technologie qui fournit de bonnes références sur la mise en œuvre. Ce livre décrit comment reproduire la transformation expliquée dans le projet Phoenix et fournit des études de cas.</p>'}, {'', ""<p>Dans ce bref article, j'utiliserai mes notes sur ce livre pour décrire une histoire concise de DevOps, ajouter mon expérience et mon opinion personnelles et établir un lien avec les environnements de développement cloud (CDE), c'est-à-dire la catégorie technologique au centre de la plateforme que nous avons développée dans mon entreprise, Strong Network.</p>""}, {'', '<p>En particulier, j’explique comment un environnement de développement cloud finalise l’effort de mise en ligne complète de DevOps. Cette notion d’exhaustivité, c’est-à-dire la façon dont les CDE marquent la fin de la mise en ligne du développement logiciel, est ma principale contribution à ce bref article. Pour clarifier le lien entre DevOps et les CDE, examinons d’abord la chaîne d’événements et les contributions techniques qui ont conduit à la principale méthodologie actuelle de livraison de logiciels.</p>'}, {'', '<h3>Le manifeste Agile</h3>'}, {'', '<p>La création du Manifeste Agile en 2001 énonce des valeurs et des principes en réponse à des méthodologies de développement de logiciels plus lourdes comme Waterfall et le Rational Unified Process (RUP).</p>'}, {'', '<p>L’un des principes fondamentaux du manifeste souligne l’importance de livrer des logiciels fonctionnels à intervalles réguliers, de quelques semaines à quelques mois, avec une préférence pour des délais plus courts. L’influence du mouvement Agile s’est accrue en 2008 lors de la conférence Agile de Toronto, où Andrew Shafer a suggéré d’appliquer les principes Agile à l’infrastructure informatique plutôt qu’au code d’application.</p>'}, {'', '<p>Cette idée a été renforcée par une présentation en 2009 à la conférence Velocity, où un article de Flickr a démontré l’impressionnant exploit de « 10 déploiements par jour » grâce à la collaboration Dev et Ops. Inspiré par ces développements, Patrick Debois a organisé les premiers DevOps Days en Belgique, inventant ainsi le terme « DevOps ». Cela a marqué une étape importante dans l’évolution du développement logiciel et des pratiques opérationnelles, en combinant l’adaptabilité rapide d’Agile avec une approche plus inclusive de l’ensemble de l’infrastructure informatique.</p>'}, {'', '<h3>Les trois voies de DevOps et les principes du flow</h3>'}, {'', '<p>Tous les concepts évoqués jusqu’à présent sont regroupés dans les « Trois voies de DevOps », qui sont les principes fondamentaux qui guident les pratiques et les processus de DevOps. En bref, ces principes se concentrent sur :</p>'}, {'', ""<p>1. Améliorer le flux de travail (première voie) en éliminant les goulots d'étranglement, en réduisant la taille des lots et en accélérant le flux de travail du développement à la production. 2. Amplifier les boucles de rétroaction (deuxième voie) en recevant rapidement et avec précision des informations sur les problèmes ou les inefficacités du système. 3. Favoriser une culture d'apprentissage et d'expérimentation continus (troisième voie) en encourageant une culture d'apprentissage, d'expérimentation et de prise de risques continus.</p>""}, {'', '<p>En suivant l’exemple du lean manufacturing et d’Agile, il est facile de comprendre ce qui a conduit à la définition des trois principes ci-dessus. J’aborde chacun de ces principes plus en détail dans d’autres articles. Pour la discussion actuelle sur la façon dont l’histoire de DevOps se connecte aux environnements de développement cloud, nous devons examiner la première voie, le principe de flux, pour comprendre le lien de cause à effet.</p>'}, {'', '<p>Le chapitre neuf du manuel DevOps explique que le contrôle des versions et la conteneurisation sont essentiels à la mise en œuvre des flux DevOps et à l’établissement d’un processus de développement fiable et cohérent.</p>'}, {'', ""<p>Premièrement, l'intégration de tous les artefacts de production dans le contrôle de version sert de source unique de vérité, permettant la recréation de l'ensemble de l'environnement de production de manière reproductible et documentée. Cela garantit que les environnements de développement de code de type production peuvent être générés automatiquement et entièrement autogérés sans nécessiter d'intervention manuelle de la part des opérations.</p>""}, {'', '<p>L’importance de cette approche devient évidente au moment de la publication, qui est souvent le premier moment où le comportement d’une application est observé dans un environnement de production, avec des ensembles de données de charge et de production réalistes. Pour réduire la probabilité de problèmes, les développeurs sont encouragés à exploiter des environnements de production sur leurs postes de travail, créés à la demande et auto-entretenus via des mécanismes tels que des images virtuelles ou des conteneurs, en utilisant des outils comme Vagrant ou Docker. Placer ces environnements sous contrôle de version permet de recréer l’intégralité des processus de pré-production et de build.</p>'}, {'', '<h3>Des postes de travail de développement à une plate-forme CDE</h3>'}, {'', '<p>La notion de libre-service est déjà mise en avant dans ce livre de 2016 comme un élément clé du principe de flux. En repensant au TPS, la notion de libre-service vise à permettre aux employés de Toyota d’être plus efficaces dans le contrôle du processus de développement. Grâce à la technologie de 2016, cette notion est concrétisée en téléchargeant des environnements sur les postes de travail des développeurs à partir d’un registre (un portail en ligne) qui fournit des environnements préconfigurés de type production.</p>'}, {'', '<p>Grâce à cette opération, les développeurs (1) copient en effet les fichiers contenant des informations sur l’infrastructure sur leurs machines, (2) y ajoutent du code source et (3) construisent l’application en utilisant les ressources informatiques de leur poste de travail.</p>'}, {'', ""<p>Une fois que l'application fonctionne correctement, le code source est envoyé (poussé) vers un référentiel de code central, et l'application est créée et déployée à l'aide de ressources en ligne ou basées sur le cloud.</p>""}, {'', '<p>Les trois étapes énumérées ci-dessus sont en réalité les seules opérations en plus de la création de code source à l’aide d’un IDE « local ». En d’autres termes, elles utilisent les ressources de stockage et de calcul physiques des postes de travail. Toutes les autres opérations DevOps sont effectuées à l’aide d’applications Web utilisées en tant que service par le développement (même lorsque ces applications sont auto-hébergées par l’organisation). L’objectif des environnements de développement cloud est également de déplacer ces opérations en ligne.</p>'}, {'', '<p>Pour ce faire, la plateforme CDE fournit essentiellement les services de base suivants :</p>'}, {'', ""<p>● Gérer les environnements de développement en ligne, sous forme de conteneurs ou de machines virtuelles, de sorte que les développeurs puissent y accéder entièrement créés et configurés, en remplacement de l'étape (1) ci-dessus.● Fournir un mécanisme de création de code source directement en ligne, par exemple à l'intérieur de l'environnement de développement à l'aide d'un IDE ou d'un terminal, en remplacement de l'étape (2).● Fournir un moyen d'exécuter des commandes de construction à l'intérieur de l'environnement de développement (via l'IDE ou le terminal), en remplacement de l'étape (3).</p>""}, {'', ""<p>Notez que le remplacement de l'étape (2) peut être effectué de plusieurs manières : par exemple, l'IDE peut être basé sur un navigateur (dans un IDE cloud), ou un IDE installé localement peut implémenter un moyen de créer à distance le code dans l'environnement distant. Il est possible d'utiliser un éditeur de texte en console via un terminal tel que vim.</p>""}, {'', '<p>Je ne peux pas conclure cette discussion sans prendre une mesure récursive pour repenser le flux de travail du développeur. Souvent, plusieurs environnements conteneurisés sont utilisés pour les tests locaux, en particulier en combinaison avec l’environnement de développement conteneurisé principal. Par conséquent, les plates-formes IDE cloud doivent fournir la possibilité d’exécuter des environnements conteneurisés à l’intérieur de l’environnement de développement cloud (lui-même un environnement conteneurisé). Si cela devient un peu compliqué, ne vous inquiétez pas. Nous avons atteint la fin de la discussion et nous pouvons passer à la conclusion.</p>'}, {'', ""<h3>Quels sont les bénéfices de l'utilisation des environnements de développement cloud dans DevOps ?</h3>""}, {'', '<p>Une bonne façon de conclure cette discussion est de résumer les avantages du déplacement des environnements de développement des postes de travail des développeurs en ligne. Par exemple, l’utilisation de CDE pour DevOps entraîne les avantages suivants :</p>'}, {'', ""<p>● Flux de travail rationalisé : les CDE améliorent le flux de travail en supprimant les données du poste de travail du développeur et en découplant le matériel du processus de développement. Cela garantit que l'environnement de développement est cohérent et non limité par des contraintes matérielles locales. ● Intégration du contrôle de version : avec les CDE, le contrôle de version devient plus robuste car il peut uniformiser la définition de l'environnement et tous les outils attachés au flux de travail, ce qui conduit à un processus de développement standardisé dans toute l'organisation. ● Environnements en libre-service : l'aspect libre-service est amélioré en centralisant la production, la maintenance et l'évolution des environnements en fonction des activités de développement distribuées. Cela permet aux développeurs d'accéder rapidement à leurs environnements et de les gérer sans avoir besoin de travaux manuels d'exploitation.● Cohérence entre les équipes : l'utilisation de CDE conduit à des environnements et des outils uniformes, ce qui ouvre la possibilité d'harmoniser l'ensemble du flux de travail de développement, facilitant l'intégration de nouveaux membres de l'équipe et garantissant que tous les développeurs utilisent les mêmes configurations et outils.● Réduction de la dépendance aux ressources locales : la migration de la consommation de ressources informatiques du matériel local vers des ressources cloud centralisées et partagées allège la charge sur les machines locales et conduit à une utilisation plus efficace des ressources organisationnelles et à des économies de coûts potentielles.● Amélioration de la collaboration : l'accès omniprésent aux environnements de développement, sécurisé par des mesures de sécurité intégrées dans les mécanismes d'accès, permet aux organisations de répondre aux besoins d'un groupe diversifié de développeurs, y compris les travailleurs internes, externes et temporaires, favorisant la collaboration entre différentes équipes et zones géographiques.● Évolutivité et flexibilité : les CDE offrent des ressources cloud évolutives qui peuvent être ajustées aux exigences du projet, facilitant la gestion de plusieurs environnements conteneurisés pour les tests et le développement, prenant ainsi en charge la nature distribuée des équipes de développement de logiciels modernes.● Sécurité et observabilité améliorées : la centralisation des environnements de développement dans le cloud renforce la sécurité et offre une observabilité immédiate en raison de leur nature en ligne, permettant une surveillance et une gestion en temps réel des activités de développement.</p>""}, {'', ""<p>En intégrant ces aspects, les CDE deviennent une solution complète pour le développement de logiciels modernes. Ils s'alignent sur les principes de DevOps pour améliorer le flux, le feedback et l'apprentissage continu, tout en répondant aux besoins pratiques des équipes de développement en matière de sécurité, d'accessibilité et d'efficacité des ressources.</p>""}, {'', '<h3>Dissiper quelques mythes sur DevOps</h3>'}, {'', '<p>Pour commencer, « The DevOps Handbook » vise à dissiper les idées reçues sur DevOps et sur la manière dont il est applicable dans divers environnements professionnels, y compris la livraison de produits non logiciels. Ce dernier point a été l’un de mes premiers moments d’éclaircissement personnel lors de la lecture du livre sur l’applicabilité générale de la méthode. Cela m’a conduit à suggérer à un ami qui créait une entreprise de meubles de maison conçus à l’échelle mondiale mais fabriqués localement que les designers publient leurs modèles via une plateforme pour que les meubles soient fabriqués par des ateliers de menuiserie locaux (par rapport à l’adresse de livraison) et que son processus de fabrication corresponde parfaitement à un canevas DevOps.</p>'}, {'', '<p>De plus, le livre réfute l’idée selon laquelle DevOps signifierait éliminer les opérations informatiques. Au contraire, il souligne l’importance de la collaboration et de l’intégration entre les équipes de développement et d’exploitation. Sur ce point, il me semble également que la mention de DevOps est toujours fortement liée aux opérations informatiques de création et de déploiement de logiciels plutôt qu’aux phases de conception et de codage de ce processus. En d’autres termes, certains responsables ne parviennent pas à reconnaître que les développeurs « font effectivement du DevOps » de la même manière que les opérateurs informatiques. L’ingénierie de plateforme vise à réduire encore davantage l’écart entre les développeurs et les opérations informatiques en permettant aux développeurs (et à d’autres rôles) d’effectuer des opérations informatiques complexes dans un environnement en libre-service.</p>'}, {'', '<p>Enfin, l’idée selon laquelle DevOps se résume uniquement à une « infrastructure en tant que code » et à l’automatisation est également remise en question dans le livre. Si l’automatisation est un élément essentiel de nombreux modèles DevOps, le livre souligne que DevOps nécessite également un changement culturel et une architecture qui prend en charge des objectifs partagés dans l’ensemble du flux de valeur informatique.</p>'}, {'', ""<p>Le RUP était la principale méthodologie d'ingénierie logicielle qu'IBM utilisait pour structurer son processus de développement et celui de ses clients. À l'époque, je faisais partie de la division IBM Research chez T.J. Watson, à New York, dans les années 2000. Nous avons vendu le RUP à tous les secteurs, y compris aux constructeurs automobiles (une belle boucle de rétroaction), lorsqu'ils ont commencé à intégrer des gigaoctets de logiciels dans les voitures.</p>""}, {'', ""<p>Chez IBM Research, nous avons même fait évoluer le RUP vers une méthodologie adaptée aux besoins du développement de produits à base de logiciels du point de vue du système, appelée développement de systèmes pilotés par les modèles. C'est important car les logiciels utilisés dans la fabrication doivent communiquer et fonctionner avec du matériel construit indépendamment. J'ai enseigné ce concept au Japon à l'université Keio (avec le professeur Nishimura) de 2006 jusqu'au milieu des années 2010.</p>""}]"
Créer une plateforme d'observabilité open source,"[{'', ""<p>Les entreprises sont soumises à une pression constante pour garantir la disponibilité de leur infrastructure informatique et de leurs applications tout au long de l'année. La complexité des architectures modernes (conteneurs, cloud hybride, SOA, microservices, etc.) ne cesse de croître, générant d'énormes volumes de journaux ingérables. Nous avons besoin d'outils intelligents de gestion des performances des applications (APM) et d'observabilité pour atteindre l'excellence de la production et atteindre les objectifs de disponibilité et de disponibilité. Il s'agit notamment d'analyser l'état des applications, les performances et l'expérience utilisateur. L'adoption de techniques d'apprentissage automatique pour identifier les anomalies et les modèles de comportement permettra de détecter rapidement la cause profonde et de respecter les accords de niveau de service (SLA) des clients.</p>""}, {'', '<p>Le marché des outils d’APM et d’observabilité est sans aucun doute en plein essor. Ces outils ingèrent plusieurs flux de données de télémétrie et constituent de puissantes plateformes d’analyse fournissant des informations essentielles sur la santé des applications et des infrastructures, y compris les performances du système. Les équipes de développement de logiciels qui adoptent l’observabilité sont bien mieux équipées pour publier leur code d’application de manière itérative. Selon une étude de « MarketsandMarkets », la taille du marché des outils et plateformes d’observabilité devrait passer de 2,4 milliards de dollars en 2023 à plus de 4 milliards de dollars d’ici 2028, à un taux de croissance annuel composé (TCAC) de 11,7 %.</p>'}, {""<h3>Qu'est-ce que l'observabilité ?</h3>"", ''}, {'', ""<p>L'observabilité est la capacité à collecter des données sur les applications distribuées, l'infrastructure et la communication entre ses composants et services internes et externes, permettant aux équipes de déboguer leurs systèmes avec diligence. Elle permet aux équipes d'ingénierie de fiabilité du site (SRE), d'ingénierie logicielle et d'exploitation d'analyser l'impact sur le client et de trier une panne de service. L'observabilité et la surveillance sont parfois utilisées de manière interchangeable. L'observabilité (proactive) rend les données accessibles et vous permet de poser n'importe quelle question sur le système pour savoir plus en profondeur comment le code se comporte. La surveillance (réactive) est la tâche de collecte et d'affichage de ces données et la capacité de déterminer l'état général du système. L'observabilité peut être divisée en trois piliers clés : les journaux, les traces et les métriques, qui sont essentiels pour l'observabilité SRE.</p>""}, {'', ""<p>• Les journaux nous aident à diagnostiquer les problèmes et à nous dire pourquoi ils se sont produits.• Les traces nous aident à isoler les problèmes et à nous dire où ils se sont produits.• Les métriques nous aident à détecter les problèmes et à nous dire ce qui s'est passé.</p>""}, {'', '<h3>Outils, capacités et défis du marché</h3>'}, {'', '<p>Le quadrant magique de Gartner pour l’APM et l’observabilité a identifié plus de 20 produits de fournisseurs offrant des capacités d’APM et d’observabilité, y compris des déploiements auto-hébergés, gérés par le fournisseur ou SaaS. Ces produits offrent de nombreuses fonctionnalités, notamment des mesures de performances des applications, la surveillance et les alertes sur les événements, la traçabilité, la détection des anomalies et des vulnérabilités, etc.</p>'}, {'', ""<p>Une application métier d'entreprise comprend des applications développées en interne (telles que .NET, Java, Python, SQL, NoSQL DB, etc.), des produits tiers/prêts à l'emploi (tels que Salesforce, HubSpot, etc.) et des intégrations (telles que Stripe, PayPal, etc.). Les applications développées en interne sont hébergées dans un centre de données sur site ou par des fournisseurs de cloud comme AWS, GCP ou Azure. Les produits prêts à l'emploi sont basés sur SaaS ou intégrés via des API. Il existe des applications hautement distribuées couvrant des dizaines et des centaines de nœuds, de services et d'instances.</p>""}, {'', ""<p>• Trop d'outils : les applications d'entreprise utilisent divers outils pour surveiller l'état et les performances des applications (tels que New Relic, Data Dog, etc.), la journalisation des erreurs (comme Splunk) et les outils fournis par les fournisseurs de cloud (tels que CloudWatch). Ces produits se chevauchent en termes de fonctionnalités et la maintenance et la gestion de ces outils (approvisionnement, courbe d'apprentissage, etc.) peuvent s'avérer fastidieuses.</p>""}, {'', ""<p>• Volume de données imprévisible : imaginez le volume de données d'observabilité (journaux, traces, métriques) collectées en fonction du trafic applicatif, de l'utilisation, de la dépendance aux produits externes, etc. La quantité de stockage de données requise pour consolider ces flux de données peut rapidement devenir incontrôlable.</p>""}, {'', ""<p>• La tarification est complexe : ces produits de fournisseurs proposent également différents modèles de tarification, tels que la facturation par hôte (comme Splunk, Data Dog, Dynatrace), par utilisateur (comme New Relic), par ingestion (comme SumoLogic, AppDynamics). La complexité des modèles de tarification rend difficile la comparaison du coût total de possession (TCO) entre les fournisseurs et la détermination de l'outil adapté à vos besoins et à votre budget.</p>""}, {'', '<h3>Pourquoi choisir une plateforme d’observabilité open source\xa0?</h3>'}, {'', ""<p>Les outils d'observabilité open source visent à fournir une approche standard et indépendante du fournisseur pour l'ingestion, la transformation et l'envoi de données à un backend d'observabilité. Les outils d'observabilité open source peuvent être une alternative pour économiser sur les coûts de licence et consolider plusieurs outils APM avec l'outil qui correspond à vos besoins et à votre budget.</p>""}, {'', '<p>Cependant, la maintenance de systèmes open source peut nécessiter des efforts de mise en place et de maintenance et augmentera vos coûts opérationnels initiaux. Mais à long terme, vous économiserez sur les frais de licence et éviterez la dépendance vis-à-vis des fournisseurs et les accords contractuels.</p>'}, {'', ""<p>Gartner prédit que, d'ici 2025, 70 % des nouvelles applications cloud natives utiliseront une instrumentation open source plutôt que des agents spécifiques aux fournisseurs pour une meilleure interopérabilité, et 70 % des nouvelles applications cloud natives adopteront OpenTelemetry pour l'observabilité plutôt que des agents spécifiques aux fournisseurs et des kits de développement logiciel (SDK).</p>""}, {'', ""<h3>Observabilité à grande échelle grâce à l'écosystème Open Source</h3>""}, {'', ""<p>Le paysage open source de l'observabilité est assez dynamique. Il existe plusieurs outils open source de la Cloud Native Computing Foundation (CNCF) pour l'observabilité et la surveillance. Cet article se concentrera principalement sur le framework OpenTelemetry et la pile technologique LGTM.</p>""}, {'', '<p>OpenTelemetry\xa0: le défi «\xa0trop d’outils\xa0» décrit ci-dessus pose un nouveau problème dans la collecte de données de télémétrie. Chaque fournisseur d’outils possède ses propres API, SDK, agents et collecteurs pour les journaux, les métriques et les traces. Nous avons besoin d’une collecte de télémétrie unifiée utilisant le framework OpenTelemetry pour créer et gérer les données de télémétrie telles que les journaux, les traces et les métriques.</p>'}, {'', ""<p>Le projet OTEL, sous l'égide de la CNCF, propose un ensemble unifié d'API, de SDK et d'outils indépendants des fournisseurs pour générer et collecter des données de télémétrie et les exporter vers divers outils d'analyse. Vous disposez d'une API et d'un SDK par langage de programmation pour extraire les données d'observabilité de votre application, d'un collecteur standard, d'un protocole de transmission (OTLP) et bien plus encore.</p>""}, {'', ""<p>LGTM\xa0: l'observabilité et la surveillance open source les plus populaires sont mises en œuvre à l'aide de la pile technologique LGTM.</p>""}, {'', ""<p>Dans la pile LGTM, nous exploitons\xa0: • Loki pour l'agrégation des journaux • Tableaux de bord Grafana pour les visualisations de télémétrie • Tempo (ou Jaeger) pour l'agrégation des traces • Prometheus géré pour l'agrégation des métriques</p>""}, {'', '<h3>Conclusion</h3>'}, {'', '<p>L’observabilité consiste à avoir une visibilité complète sur vos systèmes et à associer les indicateurs commerciaux aux données techniques. La surveillance consiste à comprendre si les choses fonctionnent correctement, et l’AIOps consiste à tirer un sens de cette visibilité. L’observabilité et la surveillance sont essentielles pour garantir le bon fonctionnement des applications et respecter les SLA des clients. En conclusion, en investissant dans des frameworks OTel open source et des outils LGTM, les équipes SRE peuvent surveiller efficacement leurs applications et obtenir des informations sur le comportement du système et les problèmes potentiels. Ces outils offrent une rentabilité et une personnalisation adaptées à des exigences spécifiques. Ils favorisent la neutralité des fournisseurs, ce qui peut être essentiel pour éviter la dépendance vis-à-vis d’un fournisseur.</p>'}]"
Une étude révèle des gains substantiels en matière d'ingénierie des plateformes,"[{'', '<p>Une enquête mondiale menée auprès de 500 professionnels du développement et du déploiement d’applications révèle que même si 43 % d’entre eux travaillent pour des organisations qui disposent d’une équipe de plateforme depuis trois à cinq ans, il ne semble pas y avoir beaucoup de cohérence quant à l’endroit où cette équipe rend compte au sein d’une organisation.</p>'}, {'', ""<p>L'enquête, menée par Puppet by Perforce, un fournisseur d'un cadre d'automatisation, a révélé que parmi les organisations qui disposent d'équipes de plateforme, plus de la moitié (58 %) opèrent dans le contexte d'une organisation DevOps ou de gestion d'infrastructure plus vaste, contre 40 % qui ont leur propre direction dédiée.</p>""}, {'', ""<p>David Sandilands, architecte principal des solutions pour Puppet by Perforce, a déclaré que l'enquête montre clairement que l'ingénierie de plateforme, en général, n'est pas un concept nouveau, mais il est clair que de plus en plus d'organisations tentent de réduire le niveau de travail actuellement associé à la création et au déploiement d'applications.</p>""}, {'', '<p>Peu importe où se trouvent les équipes de plateformes au sein d’une organisation, près des deux tiers des répondants (65 %) ont déclaré qu’elles étaient importantes pour leur organisation et qu’elles recevraient un financement continu. L’objectif principal de la formation de ces équipes est d’augmenter la productivité (58 %) et d’automatiser les processus standardisés (51 %), selon l’enquête. Les principaux objectifs des équipes d’ingénierie de plateformes sont de résoudre les problèmes (30 %), d’appliquer les processus de sécurité (27 %) et d’accélérer les transitions vers des environnements informatiques natifs du cloud (26 %). 70 % des répondants ont déclaré que la sécurité était intégrée à leurs plateformes dès le départ.</p>'}, {'', ""<p>Cela suggère qu'en plus d'assumer la responsabilité des flux de travail DevOps traditionnels, de nombreuses équipes de plateformes sont désormais également invitées à garantir que les meilleures pratiques DevSecOps sont suivies à un moment où les exigences de sécurité des applications deviennent plus strictes, a noté Sandilands.</p>""}, {'', '<p>Plus des trois quarts (76 %) ont également indiqué avoir déployé deux ou plusieurs portails libre-service, et 27 % en avoir déployé cinq ou plus.</p>'}, {'', ""<p>Cependant, seulement 22 % ont déclaré avoir déployé des applications sur Kubernetes dans un environnement de production, et un peu moins de la moitié (46 %) ont déclaré qu'ils n'avaient pas actuellement l'intention de déployer une plate-forme informatique cloud native, qui est largement considérée comme difficile à déployer et à maintenir.</p>""}, {'', '<p>Dans l’ensemble, l’enquête montre clairement que l’objectif principal est de garantir que les plateformes continuent d’évoluer au rythme des besoins des développeurs, a déclaré Sandilands. Les équipes de plateformes ne cherchent pas nécessairement à dicter les outils à utiliser ; elles trouvent plutôt des moyens d’automatiser les processus qui augmentent le niveau de friction auquel les développeurs sont confrontés, a-t-il ajouté.</p>'}, {'', '<p>À plus long terme, les progrès de l’intelligence artificielle (IA) permettront d’atteindre plus facilement cet objectif en utilisant des interfaces en langage naturel que les équipes DevOps peuvent invoquer plus facilement, a noté Sandilands.</p>'}, {'', '<p>Chaque organisation devra décider elle-même si l’ingénierie de plateforme est une méthodologie judicieuse pour gérer DevOps à grande échelle, mais à mesure que le développement et le déploiement des applications continuent d’évoluer, des approches plus nuancées seront nécessaires. En général, les organisations tentent de trouver un équilibre entre l’autonomisation des développeurs et la nécessité de rationaliser les processus back-end pour augmenter la productivité tout en simplifiant la conformité et en améliorant la sécurité.</p>'}, {'', '<p>Le défi, bien sûr, est d’inciter les développeurs à adhérer à ce concept plutôt que d’utiliser leur expertise pour résister aux équipes de plateforme qui pourraient, de leur point de vue, limiter leurs prérogatives d’une manière qui, à terme, étouffe l’innovation.</p>'}]"
L'enquête révèle un manque de progrès significatif en matière d'observabilité,"[{'', '<p>Une enquête mondiale menée auprès de 500 professionnels de l’informatique suggère que les organisations ne progressent pas beaucoup dans leur capacité à observer véritablement les environnements applicatifs, d’autant plus qu’ils deviennent chaque jour plus complexes.</p>'}, {'', ""<p>L'enquête, menée par Logz.io, un fournisseur d'une plateforme d'observabilité, a révélé que seulement un répondant sur dix a déclaré disposer d'une observabilité complète de ses environnements d'application.</p>""}, {'', '<p>Asaf Yigal, directeur technique de Logz.io, a déclaré que même si de plus en plus d’équipes DevOps collectent des logs, des mesures et des traces, la plupart d’entre elles n’ont pas encore déterminé comment exploiter toutes les données collectées. Au fur et à mesure que les données sont ingérées, elles ne doivent pas seulement être stockées ; elles doivent également être corrélées aux différents services qui composent une application, a-t-il noté.</p>'}, {'', ""<p>Plus les entreprises déploient des applications cloud natives dans des environnements de production, plus ce problème devient urgent. Tous les microservices qui composent ces applications génèrent désormais une quantité massive de données de télémétrie qui génèrent plus d'alertes que jamais.</p>""}, {'', '<p>Sans surprise, les plus grands défis rencontrés par les organisations lors de la gestion des clusters Kubernetes dans des environnements de production sont la surveillance/le dépannage (40 %), suivis de près par la sécurité (37 %) et la mise en réseau (33 %).</p>'}, {'', '<p>De nombreuses entreprises ne disposent tout simplement pas des compétences nécessaires pour gérer des applications cloud natives. Près de la moitié des répondants (48 %) ont spécifiquement cité le manque de connaissances comme le plus grand défi qu’ils ont rencontré lorsqu’ils ont essayé d’observer ces types d’applications. Du point de vue de la gestion informatique, la plupart des microservices utilisent tous le même modèle de base, il est donc difficile pour les équipes DevOps d’identifier les microservices susceptibles d’avoir le plus grand impact sur les objectifs de niveau de service (SLO) et les accords de niveau de service (SLA) en cas de perturbation, a noté Yigal.</p>'}, {'', '<p>En l’absence de capacité à déterminer la cause réelle d’un problème, les alertes se multiplient et la fatigue s’accroît, ce qui finit par entraîner des niveaux d’épuisement plus élevés au sein de l’équipe DevOps, a noté Yigal. En fait, 82 % des personnes interrogées ont déclaré que leur délai moyen de résolution (MTTR) lors des incidents de production était supérieur à une heure.</p>'}, {'', ""<p>L'enquête a également révélé que plus de la moitié des répondants (52 %) travaillaient pour des organisations qui tentaient simultanément de maîtriser les coûts de surveillance. Plus des trois quarts (76 %) des répondants ont également indiqué que les outils OpenTelemetry (OTEL) ou centrés sur OTEL étaient au moins quelque peu importants pour leur stratégie globale d'observabilité.</p>""}, {'', '<p>En outre, 87 % des personnes interrogées ont déclaré que leur organisation utilise déjà une certaine forme d’ingénierie de plateforme pour gérer les flux de travail DevOps à grande échelle.</p>'}, {'', '<p>On ne sait pas exactement à quelle vitesse les entreprises adoptent les outils et les plateformes d’observabilité, mais trop d’équipes DevOps n’ont pas la visibilité nécessaire pour identifier la cause profonde d’un problème. Par conséquent, des variantes d’un même problème continuent souvent de se manifester parce que les efforts de correction précédents n’ont tout simplement pas été assez approfondis pour résoudre le problème principal.</p>'}, {'', '<p>Bien sûr, il se peut qu’un jour les algorithmes d’apprentissage automatique, ainsi que d’autres formes d’intelligence artificielle (IA), permettent de faire apparaître plus facilement ces problèmes. Le défi, en attendant, est de poser dès aujourd’hui les bases de l’observabilité pour donner accès aux données qui seront nécessaires à l’entraînement de ces modèles d’IA.</p>'}]"
Backstage est-il le portail de développement interne qui vous convient ?,"[{'', '<p>Il n’y a jamais eu de meilleur moment pour devenir développeur de logiciels. Il existe un langage et un cadre pour résoudre pratiquement tous les défis auxquels nous sommes confrontés. De nouveaux outils, modèles d’architecture et méthodologies tels que DevOps et Agile ont changé notre façon de travailler, d’architecturer nos applications et de nous organiser. Cela a permis aux équipes d’être plus agiles que jamais. Mais tout cela a un coût caché : une complexité tentaculaire.</p>'}, {'', '<p>L’étalement est un réseau complexe d’outils, de langages et de cadres disparates utilisés dans les projets d’une organisation. Si cette diversité permet de trouver des solutions spécialisées adaptées à des problèmes spécifiques, elle crée un labyrinthe d’interdépendances et d’exigences de compétences variées. Cette complexité oblige souvent les développeurs à naviguer en permanence dans un labyrinthe d’outils, ce qui entraîne des inefficacités et une courbe d’apprentissage abrupte pour les nouveaux membres de l’équipe. À mesure que les organisations adoptent davantage d’outils et de technologies, la compréhension cohérente de l’ensemble de l’écosystème logiciel diminue. Il existe deux domaines en particulier où cette difficulté se fait sentir de manière aiguë.</p>'}, {'', ""<p>Le premier domaine touché par la prolifération est la capacité à trouver des connaissances essentielles sur les systèmes, qui se fragmentent entre les outils et les ressources à mesure que la prolifération s'accroît. Cette fragmentation allonge les délais de réponse aux incidents et complique la montée en puissance des ingénieurs nouvellement embauchés. La fragmentation des connaissances complique également considérablement la tâche des entreprises qui souhaitent mesurer et gérer les initiatives visant à atteindre des normes élevées de sécurité et de fiabilité, ainsi que le suivi des migrations vers de nouvelles architectures telles que les conteneurs ou les clouds.</p>""}, {'', '<p>L’étalement urbain a également un impact sur les tâches fréquemment exécutées par les développeurs, comme la gestion des « opérations du deuxième jour » comme la modification des variables d’environnement, l’ajout de secrets ou la modification d’un nombre de répliques, ou la création d’un élément nouveau comme un microservice ou un environnement. Compte tenu de la complexité requise pour créer et exploiter une infrastructure, de nombreuses organisations ont investi dans la création d’automatisations via des systèmes d’infrastructure en tant que code et CI/CD. Cependant, ces automatisations sont souvent difficiles à découvrir et à utiliser pour les développeurs. En conséquence, les développeurs déposent souvent des tickets auprès des équipes opérationnelles pour effectuer ces tâches, ce qui crée un goulot d’étranglement du processus qui ralentit la vitesse de développement tout en surchargeant les équipes opérationnelles de tâches répétitives et banales. C’est ce que l’on appelle communément TicketOps.</p>'}, {'', '<p>Tout cela a rendu l’expérience des développeurs pire que jamais. Les développeurs signalent des niveaux élevés de surcharge cognitive et d’épuisement professionnel, ce qui peut entraîner une attrition indésirable des développeurs et une baisse de la productivité globale. Mais il n’est pas nécessaire que cela se passe ainsi !</p>'}, {'', '<h3>Présentation de Backstage</h3>'}, {'', '<p>Aux débuts du cloud, seule une poignée d’entreprises aux valorisations élevées pouvaient se permettre de résoudre ces problèmes en consacrant 50 à 100 ingénieurs à la création d’outils internes conçus pour répondre aux défis de l’étalement urbain. Tous les autres devaient se contenter d’expériences de développement médiocres ou de solutions partielles telles que des feuilles de calcul obsolètes, des processus manuels et des informations souvent désynchronisées avec le système sous-jacent.</p>'}, {'', '<p>Alors que la plupart des entreprises commençaient à avoir du mal à supporter ces difficultés, Spotify a fait don à la Cloud Native Computing Foundation des outils qu’elle avait créés pour y faire face en interne. En tant que projet gratuit, personnalisable et open source axé sur ces problèmes, Backstage a rapidement attiré l’attention des entreprises du monde entier.</p>'}, {'', '<p>Plus de 2\xa0000 entreprises ont déjà essayé Backstage, ce qui a fait du projet un succès phénoménal en promouvant une vision de la manière dont les portails de développeurs internes peuvent atténuer les problèmes liés à la prolifération. Cependant, un nombre surprenant d’entreprises ont du mal à adopter Backstage. Dans une interview avec le responsable de l’ingénierie de Backstage, il a été révélé que de nombreuses entreprises ayant déployé Backstage n’ont atteint qu’un taux d’adoption d’environ 10 %, ce qui équivaut généralement à une preuve de concept. Découvrez pourquoi les entreprises ont eu du mal à réussir et décidez si Backstage est fait pour vous.</p>'}, {'', '<h3>Déterminer si Backstage est fait pour vous</h3>'}, {'', '<p>Voici quelques questions clés pour vous aider à guider votre décision.</p>'}, {'', ""<p>Êtes-vous prêt à créer un nouveau produit ? Backstage est un framework permettant de créer un portail de développement interne, mais il ne s'agit pas d'un outil ou d'une solution standard. De nombreux utilisateurs n'ont pas compris ce que cela signifie, du moins au début.</p>""}, {'', ""<p>En tant que framework pour créer un outil, Backstage nécessite des ressources produit et d'ingénierie dédiées pour la configuration, la personnalisation et la maintenance. Il n'est pas rare que les organisations aient besoin de quatre à six ressources ou plus entièrement dédiées pour réussir avec Backstage. Les ingénieurs que vous consacrez à la création de ce framework dans un produit devront être compétents en TypeScript et React, et disposés à travailler avec une communauté et un produit open source.</p>""}, {'', ""<p>La mise en route de Backstage nécessite plus de 70 étapes. Ce n'est pas intuitif et peut nécessiter des mois de travail avant de pouvoir démontrer la valeur de la solution. Certaines étapes et les engagements de temps associés sont évidents, comme le déploiement de votre instance de Backstage. D'autres étapes peuvent ne pas être évidentes pour les nouveaux utilisateurs.</p>""}, {'', ""<p>Par exemple, Backstage s'est toujours appuyé sur des plugins open source pour permettre aux organisations de connecter leurs outils. La nature open source de ces plugins a été un argument de vente clé de Backstage. Cependant, la qualité et la profondeur de ces plugins sont inégales et peuvent nécessiter une personnalisation supplémentaire pour activer des fonctionnalités de base comme la recherche. Dans un autre exemple, l'adaptation du modèle de données Backstage à votre organisation est si difficile que de nombreux utilisateurs abandonnent tout espoir et travaillent avec le modèle de données par défaut. La mise à niveau des versions et des plugins et l'effort requis pour maintenir les données de Backstage synchronisées avec votre système contribuent à la charge de ressources d'ingénierie nécessaire pour réussir.</p>""}, {'', '<p>L’avantage de Backstage est la personnalisation infinie que vous pouvez réaliser grâce à sa base de code open source. Vous pouvez modifier tout ce que vous voulez si vous l’écrivez et le maintenez. Cela comprend l’ajout de sources de données personnalisées, la personnalisation du modèle de données, l’extension des fonctionnalités des plugins et l’adaptation de l’interface utilisateur à l’identité de votre marque d’entreprise. Là encore, le compromis est le temps et la complexité par rapport à vos alternatives. Par exemple, configure8 et Port offrent la flexibilité d’adapter le modèle de données du portail aux contours de votre organisation et de traiter toute source de données personnalisée à utiliser dans l’ensemble du portail. Plusieurs fournisseurs peuvent prendre en charge des logos et des schémas de couleurs personnalisés dans l’interface utilisateur du portail (ou vous pouvez créer une interface utilisateur plus personnalisée sur la base de leurs API).</p>'}, {'', '<p>Est-ce que cela prend en charge vos scénarios d’intérêt ? Les portails de développeurs internes en tant que catégorie de produits ont bien évolué au-delà de leurs racines de catalogue de microservices. Si vous envisagez Backstage, il est important de vous assurer qu’il offre des fonctionnalités qui répondent aux besoins des parties prenantes de votre entreprise.</p>'}, {'', '<p>Par exemple, de nombreuses organisations exploitent une fonctionnalité appelée Scorecards pour définir et mesurer la conformité des équipes aux normes de fiabilité, de sécurité et d’architecture. Backstage open source propose une version de cette fonctionnalité, mais elle n’est pas intuitive et présente un niveau de difficulté assez élevé (c’est-à-dire écrire du code, écrire des collecteurs de données, etc.). La communauté des fournisseurs commerciaux propose généralement des Scorecards. Les adoptants doivent tenir compte de l’étendue et de la profondeur des informations qui peuvent être vérifiées, de la facilité de création de nouvelles cartes de pointage et de la qualité du flux de travail et de l’interface utilisateur du développeur lors de l’évaluation des Scorecards.</p>'}, {'', ""<p>Dans un autre exemple, Backstage permet aux utilisateurs de créer de nouveaux services via le package open source Cookiecutter. Pourquoi s'appuyer sur une solution de génération de code au lieu d'une approche ouverte qui vous permet d'utiliser le meilleur outil pour le travail ? Il existe un solide paysage de solutions disponibles pour générer du code et orchestrer des flux de travail. Disposer d'un moyen flexible pour les intégrer directement réduit la complexité.</p>""}, {'', ""<p>Backstage propose également une fonctionnalité qui permet aux développeurs de voir les coûts cloud du code qu'ils exploitent. En y regardant de plus près, les utilisateurs constatent que ce module ne permet que des agrégations de coûts de haut niveau sur une période de temps fixe. Pour vraiment fournir aux développeurs les connaissances et les fonctionnalités dont ils ont besoin pour améliorer les coûts, envisagez des fournisseurs à code source fermé qui offrent des informations et des actions plus granulaires, telles que l'exploration au-delà des coûts agrégés au niveau du microservice pour voir les facteurs de coût par ressource, les facteurs de coût d'une ressource individuelle, les paramètres de configuration et le rayon d'explosion et intégrez des actions en libre-service afin que les développeurs puissent apporter des modifications en toute sécurité lorsque cela est approprié et autorisé.</p>""}, {'', ""<p>Le catalogue des ressources et des environnements est un autre domaine à prendre en compte. Est-il nécessaire que les environnements et les ressources qui les alimentent soient automatiquement mappés à chaque service ? Voulez-vous un catalogue centralisé de vos ressources sur les clouds et les centres de données que vous pouvez trier et filtrer à l'aide de différentes clés ? Backstage peut prendre en charge certains de ces scénarios, mais pas sans un travail important et, à l'heure actuelle, la connexion à ces données est fragile et manuelle par rapport à automatique. Cela contraste avec certains fournisseurs de sources fermées, tels que configure8, qui dispose de fonctionnalités étendues.</p>""}, {'', '<p>Dans une section précédente, nous avons évoqué l’importance de la flexibilité du modèle de données. Il s’agit d’un critère d’évaluation important pour les organisations qui souhaitent avoir la possibilité d’intégrer et de présenter n’importe quelle source de données, de personnaliser le schéma du catalogue et enfin d’insérer des clés personnalisées pour le filtrage et l’analyse des résultats (c’est-à-dire, cette ressource touche-t-elle des informations personnellement identifiables ? Montrez-moi les résultats liés à un secteur d’activité, etc.). Assurez-vous de comprendre si vous êtes prêt à réussir à adapter le modèle de données de Backstage, qui a contrarié de nombreux adoptants, et si tout fournisseur commercial que vous évaluez peut facilement prendre en charge les personnalisations.</p>'}, {'', '<p>Un autre élément à prendre en compte est la documentation technique de Backstage, et si vous souhaitez adopter cette documentation pour l’ensemble de votre entreprise ou permettre aux équipes d’utiliser les meilleures solutions. Alors que Backstage vous enferme dans la documentation technique (ce qui peut également rendre une migration future vers une nouvelle solution de portail plus difficile une fois que le comportement de l’utilisateur final s’est enraciné), les fournisseurs commerciaux prennent généralement en charge des approches ouvertes et flexibles qui permettent l’intégration avec n’importe quelle solution de documentation.</p>'}, {'', '<p>Assurez-vous également que Backstage répond à vos besoins en matière de sécurité des informations. Par exemple, la plupart des organisations souhaitent un modèle de contrôle d’accès basé sur les rôles robuste afin que les développeurs ne voient que les services, les tableaux de bord et les actions autorisés dans l’interface utilisateur, via la recherche et l’API. Backstage n’offre pas cette fonctionnalité dans l’édition open source. Il existe un framework pour travailler avec les autorisations incluses dans Backstage, mais pour en tirer une réelle valeur ajoutée, vous devez prévoir un budget pour obtenir une licence pour une solution RBAC à source fermée de Spotify ou la créer vous-même.</p>'}, {'', '<p>Enfin, évaluez le rythme d’innovation que vous attendez de votre solution de développement interne. Les fournisseurs de logiciels à code source fermé, deux en particulier, ont démontré un rythme d’innovation extrêmement rapide, en lançant de nombreuses premières dans le secteur. Comparez cela au rythme d’innovation de la communauté open source, car les portails de développeurs internes sont adhérents une fois installés et ont tendance à être des relations à long terme.</p>'}, {'', ""<p>Êtes-vous prêt pour le succès ? Pour parvenir à une adoption interne à grande échelle d'un portail de développeurs interne, il ne suffit pas de configurer un portail. Il faut également résoudre les problèmes qui comptent pour votre organisation, démontrer votre réussite, informer l'organisation de cette réussite et créer une boucle de rétroaction pour surmonter les défis. Pour y parvenir, vous devez connaître les meilleures pratiques, créer du contenu et des forums pour impliquer l'organisation et, en fin de compte, adopter une approche orientée produit pour garantir le succès de votre effort d'ingénierie de plateforme.</p>""}, {'', '<p>La réussite dans ce domaine combine souvent budgétisation et accès aux meilleures pratiques.</p>'}, {'', '<p>La communauté Backstage est dynamique et active, et vous pouvez adopter les meilleures pratiques en interagissant avec les membres de la communauté. Mais pour les entreprises et les champions internes qui doivent réussir à faire adopter un nouveau portail de développeurs interne, vous souhaiterez peut-être envisager des alternatives au support communautaire.</p>'}, {'', ""<p>Cela peut être accompli de plusieurs manières. Les cabinets de conseil et les praticiens individuels sont spécialisés dans ce domaine et peuvent vous coacher. Les organisations au-dessus d'une certaine taille investissent souvent dans des ressources de gestion de produits dédiées pour s'approprier cette fonction. Enfin, certains fournisseurs comme configure8 se concentrent sur la réussite plutôt que sur un outil (ou un cadre pour créer un outil comme Backstage) en fournissant des bonnes pratiques consultatives, du contenu et d'autres valeurs ajoutées fournies avec leur logiciel pour vous assurer d'atteindre des niveaux d'adoption robustes.</p>""}, {'', '<p>Enfin, dans la mesure où vos scénarios d’intérêt impliquent des actions en libre-service pour alléger TicketOps, assurez-vous d’avoir budgétisé l’investissement technique approprié pour développer les parcours d’or ou les plans directeurs qui sont vendus dans un portail de développeur interne. Certains fournisseurs comme configure8 proposent une couche de solutions qui crée des parcours d’or personnalisés pour vous. Quelle que soit la manière dont vous vous procurez les parcours d’or, vous devez équilibrer votre investissement pour vous assurer d’obtenir un excellent « rasoir » et toutes les « lames de rasoir » dont vous aurez besoin pour répondre aux besoins uniques de vos différents clients internes.</p>'}, {'', '<p>Comprenez-vous votre coût total de possession et vos alternatives ? L’une des considérations de Backstage est de comprendre son coût total de possession par rapport à sa meilleure alternative.</p>'}, {'', ""<p>Bien que Backstage soit un logiciel open source et gratuit, nous avons constaté qu'un investissement technique significatif est nécessaire pour l'installation, la personnalisation, la maintenance, le support interne et la gestion du produit. Quantifiez le coût total (c'est-à-dire le salaire, les taxes, les avantages sociaux, les frais d'installation, etc.) de chaque ressource humaine nécessaire pour réussir avec Backstage.</p>""}, {'', ""<p>Nous avons constaté que Spotify peut vous facturer des frais de licence pour les fonctionnalités essentielles que vous ne souhaitez pas créer vous-même. Il y aura également des frais d'hébergement interne et une budgétisation pour les éventuels frais de conseil.</p>""}, {'', ""<p>Plusieurs entreprises ayant évalué Backstage se rendent compte aujourd'hui que le coût total de possession des autres fournisseurs est bien plus faible et plus prévisible. Les principaux fournisseurs proposent des alternatives robustes pour le catalogue et la gestion des scores, les actions en libre-service et la gestion collaborative des coûts. Deux fournisseurs ont évolué pour offrir des fonctionnalités convaincantes et un haut degré de flexibilité. La clé est de trouver un fournisseur qui équilibre la fonctionnalité et la flexibilité globales avec la facilité d'utilisation, la rapidité de rentabilisation et le support de niveau entreprise. C'est là que configure8 a trouvé un équilibre unique en plus d'un coût de possession inférieur par rapport à Backstage.</p>""}, {'', ""<p>Enfin, toute analyse du retour sur investissement doit tenir compte des coûts d'opportunité. Voici quelques exemples courants :</p>""}, {'', ""<p>• Compromis liés au délai de mise sur le marché. La mise en place de Backstage pour votre preuve de concept peut prendre plus d’un mois, l’obtention d’une première version pouvant être publiée peut prendre 3 à 6 mois et l’adoption complète peut prendre 6 à 12 mois. Pendant cette période, vous pouvez générer une valeur commerciale nulle ou minimale. Les fournisseurs spécialisés peuvent assurer le succès plus rapidement. La valeur commerciale incrémentielle que vous pouvez générer avec un fournisseur spécialisé par rapport à Backstage en ce qui concerne la « rapidité de rentabilisation » représente un coût d’opportunité que vous devez quantifier. • Fonctionnalités abandonnées : des scénarios tels que les tableaux de bord ou le RBAC qui sont importants pour votre organisation et que vous devrez payer ou abandonner, vous devez prendre en compte ces coûts incrémentiels (ou le coût de l’absence de ces fonctionnalités) dans votre analyse. • Coût d’opportunité des ressources d’ingénierie internes : combien d’ingénieurs sont nécessaires pour prendre en charge Backstage en plus de ce qui est nécessaire pour réussir avec un fournisseur spécialisé ? Quelle valeur commerciale ces ingénieurs incrémentiels auraient-ils pu générer en prenant en charge la valeur unique que votre entreprise apporte à ses clients plutôt qu’un outil interne dans un monde d’alternatives convaincantes ? Cela devient un autre coût de possession de Backstage que vous devez comparer avec le coût d'achat d'une solution auprès d'un fournisseur.</p>""}, {'', '<p>Comprendre les coûts directs et d’opportunité de vos meilleures alternatives vous aidera à gérer votre budget de la manière la plus efficace et facilitera vos discussions internes avec les finances et les autres parties prenantes.</p>'}, {'', '<h3>Considérez attentivement tous les clones</h3>'}, {'', '<p>Étant donné la popularité initiale de Backstage, il n’est pas surprenant de voir émerger des clones construits sur Backstage. Un clone est un fournisseur qui a pris le code source de Backstage et y a ajouté des fonctionnalités, souvent un wrapper de sécurité, l’hébergement et la maintenance, des personnalisations légères de l’interface utilisateur et éventuellement le comblement d’un manque de fonctionnalités comme les tableaux de bord. Cette approche peut alléger une partie des charges de Backstage open source en échange des coûts de licence que vous engagez. Il est important de comprendre si le clone est le mieux adapté à vos besoins uniques compte tenu du paysage hautement concurrentiel des fournisseurs qui est désormais disponible. Une façon simple d’y penser pourrait être de comparer Jenkins à Jenkins hébergé par rapport à Jenkins avec un wrapper de sécurité du fournisseur par rapport à ce que les entreprises obtiennent avec Azure DevOps et GitHub Actions.</p>'}, {'', '<h3>Réflexions finales</h3>'}, {'', '<p>La création d’un portail de développement interne à partir d’un framework open source peut être gratifiante pour les organisations qui peuvent tirer profit d’une personnalisation poussée, ce qui est impossible ailleurs. Pour de nombreuses organisations, cependant, il est important de bien évaluer l’effort requis pour réussir avec Backstage et de le comparer à vos alternatives dans la communauté des fournisseurs. Bien que Backstage offre certains avantages, les équipes DevEx et de plateforme rencontrent de plus en plus de succès plus rapidement et à un coût total de possession inférieur via des fournisseurs spécialisés comme configure8.</p>'}]"
Une enquête montre que l'adoption de l'environnement de développement cloud prend de l'ampleur,"[{'', '<p>Une enquête mondiale a révélé que même si 95 % des développeurs et des chefs d’entreprise connaissent les environnements de développement cloud (CDE), les raisons qui les poussent à en adopter un varient considérablement. L’enquête a interrogé 223 développeurs et chefs d’entreprise travaillant pour des organisations comptant plus de 2 000 employés et 250 développeurs.</p>'}, {'', ""<p>Menée par Coder, un fournisseur d'un CDE, l'enquête a par exemple révélé que si 15 % des personnes interrogées ont cité l'optimisation du processus de configuration de l'environnement de développement comme raison de l'adoption d'un CDE, 14 % ont cité la simplification des tâches de gestion telles que l'intégration ou l'amélioration de l'accès à distance.</p>""}, {'', ""<p>Selon Rob Whiteley, PDG de Coder, même si les CDE améliorent clairement la productivité des équipes de développement d'applications et renforcent la sécurité en réduisant la quantité de code source exécuté sur les ordinateurs portables et de bureau, les organisations souhaitant adopter les CDE doivent faire face à une résistance culturelle importante. Les développeurs d'applications ont historiquement mis en place leurs propres environnements de développement sur leurs propres machines locales. Cette approche signifie cependant qu'en plus de rendre la collaboration plus difficile, les organisations doivent également consacrer beaucoup de temps et d'efforts au verrouillage de chaque point de terminaison utilisé par leurs développeurs, a noté Whiteley.</p>""}, {'', ""<p>En fait, ces problèmes sont la raison pour laquelle de nombreuses équipes d'ingénierie de plateforme formées pour centraliser la gestion des flux de travail DevOps mettent en œuvre des CDE, a-t-il ajouté. L'objectif ultime est d'établir un ensemble de garde-fous tout en permettant aux développeurs de répondre eux-mêmes à leurs besoins en outils, a noté Whiteley.</p>""}, {'', ""<p>En fait, de nombreuses équipes DevOps ont déjà créé une version maison d'un CDE qu'elles maintiennent actuellement. Coder et d'autres fournisseurs de CDE préconisent l'utilisation d'une plateforme prise en charge par un fournisseur plutôt que de consacrer des ressources à la prise en charge d'un CDE personnalisé, a déclaré Whiteley.</p>""}, {'', '<p>Les workflows DevOps continuent d’évoluer à l’ère de l’intelligence artificielle (IA), et ce n’est qu’une question de temps avant que davantage d’entreprises adoptent les CDE, a-t-il ajouté. Le volume considérable de données avec lesquelles les développeurs travailleront rendra impossible l’utilisation continue de machines locales pour créer des applications, a déclaré M. Whiteley.</p>'}, {'', '<p>Quelle que soit la motivation à l’origine de l’adoption des CDE, des changements culturels importants sont en train d’être apportés aux flux de travail DevOps, qui doivent évoluer à mesure que le développement d’applications continue de devenir de plus en plus distribué. À l’ère post-COVID-19, il n’est plus rare que les équipes de développement d’applications soient composées de professionnels de l’informatique travaillant dans des fuseaux horaires différents. Cela crée un besoin urgent de tirer parti des services cloud pour fournir le niveau de collaboration que ces équipes doivent atteindre.</p>'}, {'', '<p>On ne sait pas exactement combien d’environnements de développement de code ont été déployés, mais comme le rythme de création et de déploiement des applications continue de s’accélérer, l’époque où les entreprises pouvaient permettre à chaque développeur de gérer son propre environnement de développement touche à sa fin. Les entreprises doivent déterminer le plus rapidement possible si le code créé sera réellement exécuté dans un environnement de production. Plus on passe de temps à essayer de concilier les différences entre les environnements de développement et l’environnement de production dans lequel l’application s’exécute, plus il faudra de temps pour mener à bien un projet donné.</p>'}, {'', '<p>Malheureusement, le temps est la seule chose que la plupart des équipes DevOps ne peuvent plus se permettre de consacrer à des initiatives de développement d’applications qui, en plus de se multiplier, deviennent également beaucoup plus complexes à gérer chaque jour qui passe.</p>'}]"
Comment l'IA générative permet des plateformes de tests continus unifiées,"[{'', '<p>Il est temps de disposer d’une plateforme de tests continus unifiée.</p>'}, {'', ""<p>Les plateformes et outils de test de logiciels existants sont spécialisés dans les activités de test pour un sous-ensemble des différentes étapes des flux de valeur. Le paysage des outils de test de logiciels comprend une large gamme de solutions, chacune avec ses points forts, axées sur des aspects spécifiques du développement, de la livraison et de l'exploitation des logiciels.</p>""}, {'', ""<p>Par exemple\xa0:• Les outils de test unitaire se concentrent sur les premières étapes du développement, permettant aux développeurs de tester des unités de code individuelles pour en vérifier l'exactitude.• Les outils de test d'intégration visent à tester les interactions entre différents modules ou services au sein d'une application.• Les outils de test système sont conçus pour tester de bout en bout l'ensemble du système avant sa mise en service.• Les outils de test de performance évaluent le comportement de l'application dans des conditions de charge et de stress.• Les outils de test de sécurité se concentrent sur l'identification des vulnérabilités au sein de l'application.• Les outils de test d'acceptation utilisateur (UAT) facilitent la phase de test finale, où les utilisateurs finaux valident la solution par rapport à leurs exigences.</p>""}, {'', ""<p>Le défi consistant à réunir ces différents besoins de test au sein d'une seule plateforme de tests continus est multiple. Une telle plateforme doit s'intégrer de manière transparente à une grande variété d'outils et d'environnements de développement, prendre en charge différentes méthodologies de test et être suffisamment flexible pour s'adapter à différents processus organisationnels et normes de qualité.</p>""}, {'', '<p>Bien qu’il existe des outils d’intégration continue/déploiement continu (CI/CD) qui intègrent plusieurs étapes de test, ils le font souvent en s’intégrant à des outils de test spécialisés plutôt qu’en proposant eux-mêmes une solution de test unifiée. Ces outils CI/CD sont plus proches de l’orchestration des différentes activités de test plutôt que de leur unification sous les capacités d’une seule plateforme.</p>'}, {'', '<p>Cependant, le concept d’une plateforme de tests continus entièrement unifiée, couvrant toutes les étapes, depuis les exigences jusqu’au déploiement et aux tests en production, représente une opportunité significative d’innovation dans le domaine des outils de développement logiciel. Pour y parvenir, il faudrait probablement tirer parti des avancées dans des domaines tels que l’IA générative, comme indiqué dans mon précédent blog Application de l’IA/ML aux tests continus pour créer des processus de test adaptables et intelligents capables de couvrir l’ensemble des besoins de test de manière cohérente.</p>'}, {'', ""<h3>Avantages d'une plateforme de tests continus unifiée</h3>""}, {'', ""<p>Efficacité et rapidité améliorées : une plateforme de test unifiée intègre les tests à toutes les étapes du cycle de vie du développement logiciel (SDLC), ce qui simplifie considérablement le processus de test. Cette intégration facilite les boucles de rétroaction automatisées et continues qui identifient et corrigent rapidement les défauts, permettant aux équipes de développement d'itérer et d'améliorer rapidement. En conséquence, les équipes peuvent publier de nouvelles fonctionnalités et correctifs plus rapidement, ce qui accélère la mise sur le marché du produit et améliore la réactivité aux besoins des clients et aux évolutions du marché.</p>""}, {'', ""<p>Qualité et fiabilité améliorées : en garantissant des tests complets et cohérents sur l'ensemble de l'application, une plateforme de test unifiée joue un rôle crucial dans l'identification et l'atténuation des problèmes au début du cycle de développement. Cette détection précoce permet de maintenir un niveau élevé de qualité et de fiabilité du logiciel, ce qui accroît la satisfaction et la confiance des utilisateurs dans le produit. L'application cohérente des normes de qualité à toutes les phases de test contribue à la robustesse et à la fiabilité du produit logiciel.</p>""}, {'', ""<p>Économies de coûts et maintenance réduite : l'automatisation du processus de test via une plateforme CT unifiée optimise non seulement l'utilisation des ressources, mais réduit également considérablement les coûts associés aux tests manuels et à la correction des défauts à un stade avancé. La détection précoce des défauts se traduit par des coûts de réparation inférieurs et le processus rationalisé réduit le délai global de mise sur le marché. De plus, les logiciels fiables et de haute qualité nécessitent moins de maintenance, ce qui réduit encore les coûts à long terme et libère des ressources pour les efforts d'innovation et de développement.</p>""}, {'', ""<p>Collaboration améliorée entre les équipes : une plateforme CT unifiée favorise une culture de collaboration et de transparence entre les équipes de développement, de test et d'exploitation. En fournissant un cadre et des outils communs pour toutes les activités de test, elle élimine les silos et permet une communication et une collaboration transparentes tout au long du cycle de développement logiciel. Cette collaboration améliorée garantit que les équipes sont alignées sur les objectifs du projet, peuvent partager leurs idées plus efficacement et travailler ensemble pour identifier et résoudre les problèmes plus efficacement, ce qui conduit à de meilleurs résultats.</p>""}, {'', '<p>Ensemble, ces avantages soulignent comment une plateforme de tests continus unifiée peut transformer le processus de développement logiciel, le rendant plus efficace, rentable et collaboratif tout en garantissant la livraison de produits logiciels fiables et de haute qualité.</p>'}, {'', '<p>Bien que de nombreux outils traitent de parties spécifiques du cycle de vie des tests, la vision d’une plateforme unique qui unifie de manière transparente toutes les étapes des tests, de la définition des exigences à la production, reste un objectif ambitieux. Il s’agit d’une lacune dans le paysage technologique actuel qui présente à la fois un défi et une opportunité de développement futur.</p>'}, {'', '<h3>Défis pour les plateformes de tests continus unifiées</h3>'}, {'', ""<p>Créer une plateforme de tests continus qui unifie les activités de test pour toutes les étapes du flux de valeur de bout en bout est un défi complexe en raison de plusieurs facteurs :1. Diversité des technologies et des outils : les environnements de développement de logiciels modernes sont très divers, intégrant divers langages de programmation, cadres et technologies. Créer une plateforme qui s'intègre parfaitement à toutes ces technologies est un défi.2. Points d'intégration complexes : les tests continus doivent s'intégrer à plusieurs étapes du pipeline de développement, notamment le développement, le déploiement et les opérations. Chacune de ces étapes peut utiliser des outils et des processus différents, ce qui rend difficile la création d'une solution unique.3. Variables des mesures de qualité : différentes équipes et différents projets peuvent avoir différentes définitions de la qualité, des critères de réussite et des mesures de performance. Une plateforme de tests unifiée doit être hautement personnalisable pour répondre à ces différents besoins.4. Gestion du changement : l'adoption d'une nouvelle plateforme nécessite des changements dans les processus et les flux de travail de l'organisation. La résistance au changement est courante dans les organisations, et la transition vers une nouvelle façon de tester peut être accueillie avec scepticisme et inertie.5. Évolutivité et performances : garantir que la plateforme puisse évoluer pour répondre aux besoins de test des grandes organisations avec des milliers de tests exécutés simultanément est un défi technique. Les problèmes de performances peuvent devenir un goulot d'étranglement, affectant l'efficacité globale du processus de développement. 6. Sécurité et conformité : l'intégration des tests à toutes les étapes du développement introduit également des défis de sécurité et de conformité. La plateforme doit garantir que les données sensibles sont protégées et que les pratiques de test sont conformes aux exigences réglementaires. 7. Contraintes de coûts et de ressources : le développement, la maintenance et la prise en charge d'une plateforme de tests continus unifiée nécessitent un investissement important. Les organisations peuvent hésiter à engager les ressources nécessaires sans preuve claire du retour sur investissement. 8. Évolution des pratiques : les pratiques et les outils de développement de logiciels évoluent constamment. Maintenir la plateforme à jour avec les dernières pratiques et technologies nécessite des efforts et une innovation continus.</p>""}, {'', ""<p>Malgré ces défis, la valeur des tests continus tout au long du cycle de développement logiciel est de plus en plus reconnue. Certaines entreprises et communautés open source progressent dans cette direction, en créant des solutions de test plus intégrées et plus flexibles. Cependant, la mise en place d'une plateforme entièrement unifiée qui réponde à tous ces défis est un effort continu et représente une opportunité significative d'innovation dans le secteur du développement et des tests de logiciels.</p>""}, {'', ""<h3>L'IA générative peut faciliter une plateforme de tests continus unifiée</h3>""}, {'', ""<p>L’IA générative peut jouer un rôle important pour surmonter les défis associés à la création d’une plateforme de tests continus qui unifie les activités de test pour toutes les étapes du flux de valeur de bout en bout. Voici comment l’IA générative peut relever chacun des défis :1. Diversité des technologies et des outils : l’IA générative peut être formée sur un large éventail de langages de programmation, de cadres et de technologies pour comprendre et générer du code ou des scripts de test. Cette capacité lui permet de s’adapter à différents environnements et de créer des supports de test compatibles avec divers outils et technologies.2. Points d’intégration complexes : l’IA peut analyser le flux de travail des pipelines de développement et suggérer des points d’intégration optimaux pour les tests. En apprenant à partir de différentes configurations CI/CD (intégration continue/déploiement continu), l’IA peut recommander les meilleures pratiques pour intégrer les tests de manière transparente dans les flux de travail existants.3. Différentes mesures de qualité : les modèles d’IA générative peuvent être personnalisés pour comprendre et appliquer différentes mesures de qualité et différents critères de réussite en fonction des exigences spécifiques du projet. En s’entraînant sur divers ensembles de données, ces modèles peuvent s’adapter à diverses définitions de la qualité et générer des tests ou des analyses pertinents.4. Français : Gestion du changement : l'IA peut aider au processus de gestion du changement en simulant les résultats de l'adoption de nouvelles plateformes de test, offrant ainsi des avantages fondés sur des preuves et atténuant la résistance au changement. De plus, les analyses pilotées par l'IA peuvent mettre en évidence les gains d'efficacité et les améliorations de qualité pour soutenir la transition. 5. Évolutivité et performances : l'IA générative peut optimiser les processus de test en identifiant les redondances et en suggérant des améliorations, améliorant ainsi les performances. En outre, l'IA peut allouer dynamiquement des ressources en fonction des besoins de test, garantissant l'évolutivité sans compromettre l'efficacité. 6. Sécurité et conformité : les modèles d'IA peuvent être formés pour identifier et signaler les problèmes potentiels de sécurité et de conformité dans le processus de test. En apprenant en permanence des dernières normes de sécurité et réglementations de conformité, l'IA peut contribuer à garantir que les pratiques de test répondent aux exigences nécessaires. 7. Contraintes de coûts et de ressources : en automatisant la génération et l'optimisation des cas de test, l'IA générative peut réduire considérablement l'effort manuel requis, réduisant ainsi les coûts et les demandes de ressources. L'IA peut également aider à hiérarchiser les efforts de test en fonction de l'évaluation des risques, garantissant que les ressources sont concentrées là où elles sont le plus nécessaires. Évolution des pratiques : les modèles d'IA générative sont intrinsèquement adaptables et peuvent apprendre en permanence des nouvelles pratiques de développement, des nouveaux outils et des nouvelles technologies. Cela garantit que la plateforme de test reste à jour avec les dernières avancées en matière de développement logiciel.</p>""}, {'', '<p>L’IA générative a le potentiel de transformer les tests continus en fournissant des solutions adaptatives, efficaces et intelligentes aux défis complexes de l’unification des activités de test sur l’ensemble du flux de valeur de bout en bout. Cependant, la réalisation de ce potentiel nécessite une conception minutieuse, une formation approfondie des modèles d’IA et une gestion continue pour garantir que les systèmes d’IA restent efficaces et adaptés à l’évolution des besoins en matière de tests.</p>'}, {'', ""<h3>Résumé : Appel à l'action</h3>""}, {'', '<p>La nécessité d’une plateforme de tests continus (CT) unifiée n’a jamais été aussi urgente. Les plateformes de tests actuelles, chacune experte dans son domaine, ne couvrent que des fragments du cycle de vie du développement logiciel (SDLC), ce qui conduit à un processus de test décousu et inefficace. Cette fragmentation ralentit non seulement le développement et les livraisons, mais compromet également la qualité et la sécurité du produit final. Le rêve d’une plateforme unique qui intègre de manière transparente toutes les étapes des tests, des exigences au déploiement et aux tests en production, représente un bond monumental vers l’efficacité, la sécurité et la qualité du développement logiciel.</p>'}, {'', '<p>Les défis liés à la création d’une telle plateforme sont multiples, allant de la diversité des technologies et des outils à la nature évolutive des pratiques de développement logiciel. Chaque défi, de l’intégration de technologies diverses et de la gestion du changement au sein des organisations à la garantie de l’évolutivité et de la conformité, ajoute de la complexité au développement d’une plateforme de test unifiée. Pourtant, les avantages potentiels de surmonter ces obstacles sont immenses, promettant une amélioration significative de la vitesse et de la qualité de la livraison des logiciels. La reconnaissance de ces avantages par l’industrie est de plus en plus grande, comme en témoignent les efforts de certaines entreprises et communautés open source qui évoluent vers des solutions de test plus intégrées et plus flexibles.</p>'}, {'', '<p>L’IA générative apparaît comme une lueur d’espoir dans cette quête, offrant des solutions innovantes aux défis multiformes de l’unification des activités de test. En exploitant la puissance de l’IA générative, l’industrie peut faire face à la diversité des outils, intégrer des étapes de test complexes, s’adapter à des mesures de qualité variables, gérer les changements organisationnels, évoluer efficacement, garantir la sécurité et la conformité et évoluer avec les pratiques de développement logiciel. La voie à suivre nécessite un effort concerté, mais les investissements dans les innovations en matière de tests pilotés par l’IA peuvent concrétiser la vision d’une plateforme de tests complète, unifiée et continue. Il ne s’agit pas seulement d’une opportunité d’amélioration, mais d’un appel à l’action pour que l’industrie redéfinisse l’avenir des plateformes d’ingénierie.</p>'}]"
Ingénierie des plateformes : le changement technologique de 2024,"[{'', ""<p>L'ingénierie des plateformes, un moteur essentiel dans un paysage technologique en constante évolution, est sur le point de connaître un essor significatif en 2024. Cet article explore les raisons de sa popularité croissante, les types d'organisations susceptibles d'en bénéficier, les inconvénients potentiels et le rôle crucial de la gestion des données de test (TDM) dans ce contexte.</p>""}, {'', '<h3>Qu’est-ce qui motive la demande en ingénierie de plateforme ?</h3>'}, {'', '<p>L’ingénierie des plateformes devrait prendre un essor considérable en 2024, car elle offre des avantages cruciaux pour accélérer la valeur commerciale, réduire les charges cognitives et améliorer l’efficacité des processus de développement et de gestion des applications. Selon Gartner, d’ici 2026, environ 80 % des organisations d’ingénierie logicielle mettront en place des équipes de plateformes en tant que fournisseurs internes de services, composants et outils réutilisables pour la fourniture d’applications. Cette tendance souligne l’adoption et l’importance croissantes de l’ingénierie des plateformes dans le monde de la technologie.</p>'}, {'', '<p>La popularité croissante de l’ingénierie de plate-forme peut être attribuée à plusieurs facteurs :</p>'}, {'', ""<p>• Productivité des développeurs : l'amélioration de la productivité des développeurs est un facteur clé, représentant environ 21 % des raisons de sa popularité.• Implémentation du pipeline CI/CD : environ 20 % de la popularité est motivée par le besoin d'intégration continue efficace et de pipelines de livraison continue.• Standardisation des outils et des processus : 20 % supplémentaires proviennent de la nécessité de standardiser les outils et les processus.• Améliorations de la sécurité : les améliorations de la sécurité contribuent à 20 % de sa popularité.• Infrastructure en tant que code (IaC) : enfin, 19 % sont motivés par l'adoption de méthodologies d'infrastructure en tant que code (IaC).</p>""}, {'', ""<p>En outre, l'ingénierie de plateforme permet de remédier aux inefficacités causées par la décentralisation des outils et des processus dans DevOps, en particulier à mesure que les organisations évoluent. Cette approche fournit un cadre plus organisé et centralisé, essentiel pour gérer les complexités des environnements de développement logiciel modernes.</p>""}, {'', '<p>Comme le souligne Gartner, la capacité de l’ingénierie de plateforme à offrir une expérience de libre-service fluide aux clients et aux partenaires commerciaux est un facteur important de sa popularité croissante. Cette adaptabilité est cruciale à une époque où la technologie et les exigences du marché évoluent rapidement, obligeant les entreprises à rester agiles et compétitives.</p>'}, {'', '<p>Ces facteurs pris collectivement font de l’ingénierie des plateformes une approche stratégique pour les organisations d’ingénierie logicielle, en particulier à l’horizon 2024 et au-delà.</p>'}, {''}, {'', '<h3>Principaux avantages de l’ingénierie de plateforme</h3>'}, {'', '<p>Même si cela ne convient pas à toutes les organisations, certaines de leurs caractéristiques communes sont les plus susceptibles d’en bénéficier.</p>'}, {'', '<p>Si votre organisation cherche à :</p>'}, {'', ""<p>• Réduire la prolifération des outils en standardisant les outils et les services• Améliorer la productivité grâce à l'automatisation, ce qui conduit à des cycles de développement plus rapides• Établir des cadres de gouvernance pour le développement de logiciels• Augmenter l'évolutivité grâce à des technologies telles que la conteneurisation et l'orchestration• Améliorer la sécurité en intégrant des outils d'analyse de sécurité automatisés dans le pipeline CI/CD,</p>""}, {'', '<p>L’ingénierie de plateforme peut alors être la solution qu’il vous faut. Ces avantages sont particulièrement pertinents pour les entreprises confrontées à des cycles de développement logiciel complexes et celles qui ont besoin de solutions robustes, évolutives et sécurisées.</p>'}, {'<h3>Inconvénients de l’ingénierie de plateforme</h3>', ''}, {'', ""<p>Cependant, l'ingénierie des plateformes n'est pas sans défis. La complexité croissante de l'infrastructure peut être décourageante pour les petites équipes. Une forte dépendance à la plateforme peut entraîner des difficultés lors du changement de technologie ou de fournisseur. De plus, les problèmes de compatibilité avec les outils et technologies de développement existants peuvent ralentir les processus.</p>""}, {'', '<h3>Le rôle crucial de la gestion des données de test (TDM)</h3>'}, {'', '<p>Dans un environnement d’ingénierie de plateforme, la gestion des données de test (TDM) devient un élément essentiel. L’importance de la TDM réside dans son rôle dans le développement agile, garantissant la précision et la fiabilité des tests, améliorant ainsi l’efficacité du processus de test. Les méthodologies agiles, qui divisent les projets en sprints plus petits, s’appuient fortement sur la TDM pour fournir des données de test conformes et de haute qualité dans un modèle d’intégration et de déploiement continu.</p>'}, {'', ""<p>TDM relève le défi de la gestion des données fragmentées entre différentes applications, environnements et niveaux de conformité. Une approche TDM basée sur les entités permet aux testeurs de fournir des données de test à la demande, rationalisant ainsi le processus de test et réduisant le risque de violation des données. En fin de compte, les pratiques TDM réduisent considérablement le temps, les coûts et les risques liés à la livraison de logiciels, ce qui en fait un élément essentiel des stratégies d'ingénierie de plateforme modernes.</p>""}, {'', '<p>À mesure que la technologie évolue rapidement, le rôle de l’ingénierie de plateforme, renforcée par une TDM efficace, devient de plus en plus crucial. Les entreprises qui ne s’adaptent pas à ces changements en intégrant des composants tels que la TDM dans leurs stratégies d’ingénierie de plateforme risquent de prendre du retard sur le marché technologique concurrentiel. Par conséquent, l’adoption de ces innovations est essentielle pour survivre et réussir dans le paysage technologique actuel en constante évolution.</p>'}]"
Cinq excellentes opportunités d'emploi DevOps,"[{'', '<p>DevOps.com fournit désormais un rapport hebdomadaire sur les emplois DevOps à travers lequel les opportunités pour les professionnels DevOps seront mises en évidence pour mieux servir notre public.</p>'}, {'', '<p>Notre objectif en ces temps économiques difficiles est de permettre aux professionnels DevOps de faire progresser plus facilement leur carrière.</p>'}, {'', '<p>Bien entendu, le vivier de talents DevOps disponibles est encore relativement limité, donc lorsqu’un professionnel DevOps assume un nouveau rôle, cela a tendance à créer une opportunité pour les autres.</p>'}, {'', '<p>Les cinq offres d’emploi partagées cette semaine sont sélectionnées en fonction de l’entreprise qui cherche à embaucher, du segment industriel vertical et, naturellement, de l’échelle salariale proposée.</p>'}, {'', '<p>Nous nous engageons également à fournir des informations supplémentaires sur l’état du marché du travail DevOps. En attendant, voici quelques informations à prendre en compte :</p>'}, {'', '<p>LinkedIn</p>'}, {'', '<p>Ingénieur du personnel de FanaticsRemotePlatform176\xa0000 $ à 264\xa0000 $</p>'}, {'', '<p>SimplyHired.com</p>'}, {'', '<p>Code.orgIngénieur en infrastructure à distance145\xa0800 $ à 162\xa0000 $</p>'}, {'', '<p>Indeed.com</p>'}, {'', '<p>Mayo ClinicRochester, MinnesotaIngénieur MLOps/DevOps senior128\xa0502 $ à 186\xa0264 $</p>'}, {'', '<p>CareerBuilder.com</p>'}, {'', '<p>State Street Corp. Quincy, MassachusettsIngénieur Azure DevOps/Vice-président adjoint100\xa0000 à 160\xa0000 $</p>'}, {'', '<p>Dés.com</p>'}, {'', '<p>Metropolitan Transit Authority (MTA)New York, New YorkIngénieur logiciel spécialisé – DevOps110\xa0748 $ à 130\xa0719 $</p>'}]"
Comment GitHub aborde l'expérience des développeurs internes,"[{'', ""<p>L'expérience des développeurs (DX) est au cœur de toutes les conversations, et ce pour de nombreuses raisons. Une DX de qualité est intrinsèquement liée à une productivité, une rapidité et une satisfaction accrues des développeurs. Les organisations qui investissent dans la DX ont tendance à constater des améliorations d'efficacité et une diminution du taux de rotation du personnel, ce qui contribue à réduire les coûts.</p>""}, {'', '<p>De nombreux efforts sont donc déployés pour améliorer l’expérience des développeurs avec leurs flux de travail et leurs outils internes. Mais quels sont les bons modèles d’expérience de développement dans la pratique ? Pour cela, tournons-nous vers GitHub.</p>'}, {'', '<p>GitHub, le référentiel de code et la plateforme de contrôle de version omniprésents, est naturellement l’outil préféré des développeurs. En interne, GitHub cherche constamment à améliorer l’expérience de ses propres développeurs en matière de création, de déploiement et de maintenance de logiciels. Ci-dessous, nous allons découvrir comment GitHub aborde la transformation numérique et insuffle ces caractéristiques dans sa culture de développement logiciel.</p>'}, {'', '<p>La pratique la plus cruciale de GitHub est de permettre aux développeurs d’entrer dans l’état de flux et d’y rester. Dr Eirini Kalliamvakou, chercheuse chez GitHub, définit le « travail en profondeur » comme le temps réservé à la concentration. « Pendant ce temps, vous subissez le moins d’interruptions possible », a-t-elle déclaré. Selon Kalliamvakou, les employés de GitHub bloquent activement du temps pour le travail en profondeur – certaines équipes ont même des « jeudis sans réunion ».</p>'}, {'', '<p>Des recherches ont démontré les bénéfices de consacrer du temps à un travail approfondi. Comme je l’ai déjà évoqué, une étude interne réalisée auprès d’employés de Microsoft a révélé que les développeurs qui consacraient officiellement du temps à la concentration pour coder ont connu une augmentation significative de leur satisfaction globale et de leur productivité perçue.</p>'}, {'', ""<p>Plus récemment, GitHub s'est associé au groupe de recherche DX pour une étude, DevEx In Action, qui évalue les résultats observables de l'amélioration de l'expérience des développeurs. Les résultats ont montré que les développeurs qui consacrent beaucoup de temps au travail approfondi bénéficient d'une augmentation de productivité de 50 %.</p>""}, {'', '<p>Il n’est pas surprenant que les développeurs de GitHub aient adopté GitHub Copilot, l’outil de saisie semi-automatique de code basé sur le Codex d’OpenAI. Selon Jon Peck, responsable senior des relations avec les développeurs et de la défense des intérêts des entreprises chez GitHub, Copilot est véritablement intégré dans la plupart de ses tâches quotidiennes. « Copilot vit avec moi dans n’importe quel environnement de développement que j’utilise », a-t-il déclaré.</p>'}, {'', '<p>Aujourd’hui, la technologie permet de créer des fonctions complètes et viables, qui tiennent compte du contexte et du style, a-t-il déclaré. De plus, Copilot Chat s’intègre désormais à Jetbrains et à VSCode, ce qui le rapproche de l’environnement de développement habituel du programmeur. Peck souligne en particulier les gains d’expérience des développeurs qui l’utilisent pour des activités ennuyeuses, telles que la création de tests unitaires pour un nouveau code ou la synthèse automatique des fonctionnalités des composants.</p>'}, {'', ""<p>L'automatisation est un autre moyen par lequel GitHub améliore la productivité et l'expérience de ses développeurs. « L'automatisation permet aux développeurs de rester plus longtemps dans le flux, en les interrompant uniquement lorsque quelque chose nécessite leur attention », a déclaré Peck.</p>""}, {'', ""<p>Par exemple, les équipes utilisent GitHub Actions pour toutes sortes de déclencheurs, comme les réponses aux nouveaux commits de code ou la modification de l'état des problèmes, a déclaré Peck. L'exécution de tests asynchrones en arrière-plan et la notification au développeur uniquement en cas de problème leur permettent de rester concentrés sur le code pendant des périodes plus longues. De plus, la communication asynchrone autour des demandes d'extraction et des révisions de code peut garantir que les membres de l'équipe collaborent efficacement pendant leur temps libre.</p>""}, {'', ""<p>Ces caractéristiques sont souvent adoptées dans les cultures de développement de logiciels cloud-native comme GitHub. Plus vous pouvez exécuter des tests et des processus de build en arrière-plan, mieux c'est, explique Kalliamvakou. De plus, réduire la fatigue des notifications est un autre moyen de garantir que les développeurs ne sont pas interrompus à moins que quelque chose ne nécessite vraiment leur attention.</p>""}, {'', '<p>Pour optimiser l’expérience des développeurs, il est judicieux de recueillir leurs commentaires. Par exemple, chez GitHub, l’équipe en charge de l’expérience des développeurs mène régulièrement des enquêtes internes pour mettre en évidence les points à améliorer. Cela permet d’évaluer la satisfaction des développeurs quant à la quantité de travail approfondi qu’ils effectuent. De même, LinkedIn recueille régulièrement des informations qualitatives à l’aide d’un cadre interne pour évaluer la satisfaction des développeurs à l’égard de leurs outils.</p>'}, {'', '<p>GitHub propose également une newsletter pour faire connaître les nouvelles fonctionnalités internes introduites et pour faire le point régulièrement sur l’état d’avancement des choses. « Lorsque vous lancez une initiative d’expérience développeur, vous devez faire connaître cette initiative et suivre la réaction des utilisateurs au fil du temps », a déclaré Kalliamvakou. Garder un œil actif sur le succès de ces initiatives est un bon moyen de vérifier l’état de santé de l’entreprise pour éviter l’épuisement professionnel.</p>'}, {'', '<p>Un autre domaine dans lequel GitHub se concentre sur l’amélioration de l’expérience des développeurs est celui de la sécurité. Traditionnellement, les systèmes effectuent une analyse de sécurité une fois que tout le code a été validé. Cependant, cela peut être très pénible à résoudre, en particulier lorsque vous devez rechercher plusieurs dépendances et que vous n’avez pas une grande visibilité.</p>'}, {'', '<p>« Plus vous rapprochez l’analyse de sécurité du point où la vulnérabilité a été créée, plus il sera facile d’y remédier », a déclaré Peck. Déplacer vers la gauche pour détecter les vulnérabilités potentielles sur le moment et proposer des options de « correction en un clic » peut améliorer considérablement l’expérience du développeur. Il a également décrit comment GitHub a conçu les fonctionnalités de sécurité avancées de dogfood GitHub pour effectuer des tâches telles que l’analyse continue des secrets, la désinfection des entrées et l’analyse basée sur des modèles.</p>'}, {'', '<p>Il est intéressant de noter que l’étude DevEx in Action a révélé que les développeurs qui déclarent avoir un haut niveau de compréhension de leur code se sentent 42 % plus productifs que ceux qui ont une compréhension faible ou inexistante. Alors, comment pouvons-nous améliorer notre compréhension du code ?</p>'}, {'', '<p>Il arrive souvent qu’un développeur oublie le contexte de quelque chose qu’il a écrit l’année dernière, ou qu’un élément programmé par un coéquipier semble obscur. C’est un autre domaine dans lequel l’IA pourrait être utilisée pour générer automatiquement de la documentation. Cependant, l’avenir de la compréhension du code va au-delà de la simple documentation statique.</p>'}, {'', '<p>« La tendance est à l’apprentissage là où ils se trouvent, pour les maintenir dans le flux », a déclaré Kalliamvakou. Elle indique que les informations basées sur l’IA intégrées dans l’IDE \u200b\u200bqui expliquent ce que signifie un programme ou une fonction rendent le flux de travail du développeur très intuitif.</p>'}, {'', '<p>Les développeurs sont souvent confrontés au problème « ça marche sur ma machine », qui peut constituer un obstacle majeur à leur expérience. En substance, les applications dépendent de bien plus que du code : elles dépendent de l’environnement, des dépendances locales, de ses configurations et du périphérique qui exécute le programme.</p>'}, {'', ""<p>En utilisant un IDE basé sur le cloud comme Codespaces, explique Peck, les développeurs GitHub peuvent créer un conteneur dans le cloud et conserver les mêmes configurations. L'avantage de cette solution est que vous pouvez également conserver votre expérience d'édition locale, a-t-il ajouté.</p>""}, {'', '<p>Selon Kalliamvakou, les avantages de l’expérience des développeurs ne sont plus seulement anecdotiques : nous disposons désormais de preuves concrètes pour étayer ces affirmations. Par exemple, les développeurs qui trouvent leur travail intéressant se sentent 30 % plus productifs, selon le rapport susmentionné.</p>'}, {'', ""<p>Il semble que le fait d'avoir à ses côtés un assistant de codage IA formé sur un modèle de langage étendu (LLM) puisse favoriser la satisfaction des développeurs et les gains de productivité. L'introduction d'outils basés sur l'IA peut aider à automatiser une partie du travail insatisfaisant, laissant plus de temps pour des activités plus engageantes, a déclaré Kalliamvakou.</p>""}, {'', '<p>Mais même si l’IA peut résoudre de nombreux problèmes, il y aura toujours un certain degré de labeur. Kalliamvakou recommande de répartir les tâches fastidieuses entre plusieurs employés et d’éviter de les confier à une seule personne ! Chez GitHub, l’équipe DevEx analyse également l’efficacité des réunions pour éliminer celles qui ne sont pas de haute qualité, ce qui peut également favoriser l’engagement.</p>'}]"
Comment réussir l'ingénierie de la plateforme,"[{'', '<p>Si vous souhaitez que votre entreprise s’oriente vers une transformation numérique plus efficace et rationalisée, l’ingénierie de plateforme est la seule solution. Cependant, sans feuille de route définie, la plupart des entreprises auront rapidement l’impression de s’être aventurées dans le désert technologique sans GPS. Selon le rapport State of DevOps 2023, il faut en moyenne trois ans pour que l’ingénierie de plateforme commence à produire des résultats tangibles, mais en réalité, la majorité des entreprises auront du mal à y parvenir pendant beaucoup plus longtemps.</p>'}, {'', ""<p>Une bonne ingénierie de plateforme aidera les organisations à créer un environnement qui encourage leurs équipes à développer des solutions plus efficaces et plus fiables, ce qui, à terme, favorisera la croissance de l'entreprise. Ci-dessous, j'ai présenté certaines des étapes essentielles que les organisations doivent suivre pour s'assurer que les bonnes mesures de protection sont mises en place pour profiter de tous les avantages de l'ingénierie de plateforme tout en évitant certains des pièges courants.</p>""}, {'', '<h3>Étape 1 : Définir la gouvernance – Créer de l’ordre au milieu du chaos</h3>'}, {'', '<p>Avant de vous lancer dans l’ingénierie de plateforme, il est essentiel de définir un cadre de gouvernance efficace. De nombreuses organisations tentent de mettre en œuvre de nouvelles stratégies sans avoir une compréhension claire du qui, du comment et du quoi de leurs processus, ce qui a pour résultat net de créer davantage de complexité.</p>'}, {'', '<p>Commencez le processus en déterminant les rôles et les autorisations de votre équipe et établissez un centre d’excellence. Ce faisant, chaque membre de l’équipe connaîtra ses responsabilités et ses domaines d’expertise avec une structure claire et complète de contrôle d’accès et d’autorisation. Ensuite, créez des politiques et des procédures de gouvernance claires qui serviront de lignes directrices sur la manière dont la plateforme sera gérée et maintenue afin de garantir la cohérence et la sécurité tout au long du cycle de vie de la plateforme.</p>'}, {'', '<h3>Étape 2 : Configurez votre catalogue de services – Créez un buffet numérique</h3>'}, {'', '<p>En cherchant d’abord à améliorer l’expérience des développeurs, les organisations obtiendront de meilleurs résultats en matière d’ingénierie de plateforme. C’est pourquoi l’un des éléments les plus importants consiste à définir ce qui figure au menu numérique. Une fois que vous avez établi une approche efficace de la gouvernance, choisissez les services que vous souhaitez intégrer à votre catalogue de services et à votre portail en libre-service dans une approche GitOps complète, gérée par l’équipe de la plateforme.</p>'}, {'', '<p>Une plateforme efficace permet à chacun d’interagir avec les outils et le cloud sans avoir besoin d’être un expert technique. C’est comme avoir un buffet technologique où chacun peut choisir son repas, mais seulement si le portail en libre-service est intuitif et dispose d’une interface centrée sur l’utilisateur. Des contrôles d’accès doivent être intégrés à la plateforme avec une gouvernance prédéfinie afin que les utilisateurs ne puissent demander que les services pour lesquels ils disposent des autorisations appropriées.</p>'}, {'', '<h3>Étape 3 : Améliorer l’observabilité – Faire la lumière sur le cloud</h3>'}, {'', '<p>À l’ère de la transformation numérique, il faut voir pour croire, et c’est là que l’observabilité a un rôle à jouer. L’amélioration de l’observabilité est essentielle pour obtenir des informations sur les performances et le comportement de la plateforme, ce qui implique l’intégration d’outils tels que la surveillance des événements et des projets, la transparence des coûts du cloud, les performances des applications, la santé de l’infrastructure et les interactions des utilisateurs.</p>'}, {'', ""<p>Dans un environnement cloud en pleine croissance, l'observabilité permet aux équipes de suivre l'évolution des coûts, de l'utilisation, de la disponibilité, des performances et de la sécurité dans une infrastructure cloud en constante évolution. Une fois qu'un projet a été déployé, il doit être géré et maintenu chez tous les fournisseurs de cloud, ce qui est essentiel pour réduire les coûts au minimum, mais constitue souvent une tâche énorme et compliquée.</p>""}, {'', ""<p>Pour gérer cela efficacement, il faut surveiller les indicateurs clés de performance (KPI) et configurer des alertes pour les événements critiques, et utiliser des journaux et des outils d'analyse pour obtenir une visibilité sur le comportement des applications, suivre les erreurs et résoudre les problèmes plus efficacement. Enfin, la mise en œuvre de systèmes de traçage capables de suivre le flux de requêtes sur différents microservices et composants permet d'identifier les goulots d'étranglement des performances, de comprendre les problèmes de latence et d'optimiser le comportement du système.</p>""}, {'', '<h3>Étape 4\xa0: Branchez les pipelines CI/CD – Tous à bord de l’Automation Express</h3>'}, {'', ""<p>L'étape suivante consiste à dynamiser la plateforme en connectant les pipelines CI/CD. Ces pipelines, connectés à votre Git, généreront des modèles avec automatisation intégrée, qui permettent de tester automatiquement les modifications de code chaque fois que les développeurs valident ou fusionnent du code dans le référentiel. Ce faisant, cela réduit considérablement les erreurs humaines et garantit la cohérence tout au long du processus.</p>""}, {'', '<p>Des mécanismes de suivi et de rétroaction continus dans les pipelines CI/CD garantissent que seul un code fiable et bien testé arrive en production. Ces pipelines sont orchestrés via des API, ce qui rendra le parcours numérique plus rapide et plus fluide.</p>'}, {'', '<h3>Étape 5 : Mettre en place FinOps et GreenOps – Un exercice d’équilibre pour un avenir meilleur</h3>'}, {'', '<p>Enfin, pour réussir l’ingénierie de la plateforme, il faut autant maîtriser les bases techniques que prendre ses responsabilités. En combinant les approches FinOps et GreenOps et en plaçant la durabilité au niveau de l’orchestration pour permettre aux utilisateurs de l’ensemble de l’organisation de consommer moins d’infrastructure, les organisations peuvent réduire les coûts de livraison de logiciels/cloud et même leurs émissions de carbone.</p>'}, {'', '<p>Comme je l’ai déjà écrit dans ces pages, les stratégies FinOps et GreenOps vont de pair : l’intégration de la gestion des coûts du cloud et de la gestion de l’empreinte carbone est le moyen le plus efficace de garantir que votre parcours soit non seulement efficace mais également respectueux de l’environnement.</p>'}, {'', '<p>Et c’est tout. En définissant la gouvernance, en mettant en place un portail libre-service efficace, en améliorant l’observabilité, en connectant les pipelines CI/CD et en adoptant FinOps et GreenOps, les organisations seront sur la bonne voie pour créer une plateforme non seulement efficace mais également respectueuse de l’environnement.</p>'}]"
L'ère de l'ingénierie des plateformes : à quoi s'attendre en 2024,"[{'', '<p>L’ingénierie de plateforme a gagné en popularité ces dernières années, certains la comparant à la prochaine itération de DevOps. Gartner l’a qualifiée de l’une des principales tendances stratégiques de 2024 et a prédit que 80 % des grandes entreprises d’ingénierie logicielle créeront des équipes d’ingénierie de plateforme d’ici 2026. Mais qu’est-ce qui motive cette dynamique ?</p>'}, {'', '<p>Je pense que c’est le résultat de la maturité de l’adoption du cloud par les entreprises de toutes tailles et de toutes formes. Alors que de plus en plus d’entreprises ont commencé à utiliser le cloud et ont adopté DevOps comme moyen de commercialiser rapidement des fonctionnalités, il est devenu évident qu’il fallait repenser la manière dont l’infrastructure et les plateformes sous-jacentes étaient utilisées pour les applications d’entreprise. L’ingénierie de plateforme est apparue comme la nouvelle voie pour créer et gérer l’infrastructure applicative sous-jacente.</p>'}, {'', '<p>L’année 2020 a été une année de croissance pour l’ingénierie des plateformes, mais à l’ère actuelle du cloud hybride et de l’IA générative, nous voyons de nouvelles tendances émerger dans ce domaine. Alors, quelle est la prochaine étape ? Voici ce que nous nous attendons à voir à l’approche de 2024.</p>'}, {'', '<h3>Modèle de financement centré sur le produit</h3>'}, {'', '<p>Traditionnellement, l’informatique était considérée comme un gardien et un centre de coûts. Avec le cloud, les développeurs ont acquis plus de pouvoir pour se procurer l’infrastructure dont ils avaient besoin pour leurs applications. Les entreprises ont pris conscience de l’intérêt de moderniser leurs opérations informatiques pour répondre à ce besoin moderne et ont voulu transformer leurs opérations cloud pour faire partie de l’équipe d’innovation. Elles considèrent l’ingénierie de plateforme comme l’approche transformationnelle qui aligne les opérations cloud avec les développeurs et les utilisateurs professionnels. Cependant, l’ingénierie de plateforme nécessite un modèle de financement centré sur le produit au lieu des modèles centrés sur le projet comme par le passé, car un modèle centré sur le produit n’a pas de date de début et de fin comme les projets et s’aligne étroitement sur le cycle de vie du produit avec un financement continu. Les entreprises doivent reconnaître ce changement par rapport au statu quo dans son ensemble, et nous nous attendons à ce que cette prise de conscience se produise pour les entreprises en 2024.</p>'}, {'', ""<h3>L'ingénierie de plateforme comme extension de DevOps</h3>""}, {'', '<p>Les entreprises ont adopté DevOps pour accélérer le développement de leurs applications, mais elles ont continué à utiliser les modèles traditionnels de contrôle des opérations dans le cloud. L’adoption de garde-fous traditionnels dans le cloud a créé des frictions et ralenti l’innovation. Avec son modèle « as code », l’ingénierie de plateforme est bien adaptée pour réduire les frictions et accélérer l’innovation. Nous nous attendons à ce que les organisations adoptent l’ingénierie de plateforme comme l’extension naturelle de DevOps pour fonctionner au rythme de l’entreprise.</p>'}, {'', ""<h3>Capacités de l'IA générative</h3>""}, {'', '<p>L’IA générative est au sommet de son hype à la fin de l’année 2023, et en 2024, nous verrons des cas d’utilisation prêts à l’emploi pour les entreprises prendre vie. L’IA générative complétera l’ingénierie de plateforme avec des assistants de code pour créer une infrastructure en tant que code, une gouvernance en tant que code et une sécurité en tant que code pour garantir des opérations fluides dès le jour 2. Nous verrons également des chatbots s’appuyer sur l’IA générative pour aider les SRE et autres professionnels de l’ingénierie de plateforme avec la documentation, les manuels d’infrastructure et d’autres cas d’utilisation pertinents.</p>'}, {'', '<p>2024 est une année cruciale pour les entreprises, car elles doivent repousser les forces économiques et adopter les avancées technologiques pour être compétitives sur le marché mondial. La plupart des dirigeants considèrent l’ingénierie des plateformes comme la principale tendance stratégique sur laquelle se concentrer, car elle apporte productivité et efficacité des coûts à leur organisation. Ce n’est qu’un début, et la décennie à venir sera définie par l’ingénierie des plateformes, posant des bases solides pour l’innovation pilotée par l’IA dans les entreprises.</p>'}]"
L'état des portails de développeurs internes (IDP),"[{'', '<p>Les groupes informatiques ont du mal à trouver et à retenir des développeurs de logiciels talentueux depuis un certain temps. Et dans un contexte de contraction de l’économie technologique, les leaders technologiques tentent de trouver des moyens de faire plus avec moins. L’un des effets de cette situation a été l’augmentation des investissements dans les portails de développeurs internes (IDP) pour soutenir les équipes d’ingénierie. Ces IDP centralisent les outils et standardisent les flux de travail DevOps courants pour aider à réaliser les ambitions de l’ingénierie de plateforme.</p>'}, {'', '<p>L’objectif des IDP est d’améliorer l’agilité logicielle et, ce faisant, d’améliorer la productivité des développeurs. Mais est-ce le cas ? Quel est le statut de ces initiatives IDP ? Et surtout, ces initiatives ont-elles réussi à améliorer la productivité sans sacrifier l’expérience ou la satisfaction des développeurs ?</p>'}, {'', '<p>Nous allons tenter de répondre à ces questions ci-dessous. Nous examinerons l’état des portails de développeurs internes et verrons comment ils optimisent la productivité des développeurs et améliorent leur expérience.</p>'}, {'', ""<p>En 2023, l'ingénierie de la productivité des développeurs est devenue un sujet de discussion majeur. Bien que les cadres de mesure de la productivité des développeurs aient suscité un débat houleux au sein de la communauté des logiciels, les organisations semblent toujours faire de la productivité des développeurs un objectif prioritaire.</p>""}, {'', '<p>Par exemple, 87 % des personnes interrogées ont déclaré que l’amélioration de la productivité des développeurs était une mesure de développement logiciel de premier plan, selon un livre blanc de 2023 commandé par Humanitec. Il s’agissait de l’objectif logiciel prioritaire numéro un, devant la gestion des retards informatiques, l’amélioration de la gouvernance de la chaîne d’approvisionnement et l’accélération du temps de cycle.</p>'}, {'', '<p>L’un des moyens de favoriser la productivité consiste à rationaliser le flux de travail des développeurs grâce à des portails de développeurs internes. Cette tendance n’en est qu’à ses débuts, mais nous avons vu la tendance s’inverser en 2023. Presque toutes les organisations ont déjà commencé à mettre en œuvre l’ingénierie de plateforme, et « 85 % des personnes interrogées ont commencé à mettre en œuvre des portails de développeurs internes ou prévoient de le faire l’année prochaine », selon le rapport 2024 State of Internal Developer Portals de Port.</p>'}, {'', '<p>Il est difficile de parler de portails de développeurs sans mentionner Backstage, le projet CNCF en cours d’incubation et open source de Spotify. Backstage a connu une adoption significative en 2023. (Au moment de la rédaction de cet article, il s’agit du 12e projet CNCF le plus actif en termes d’activité GitHub).</p>'}, {'', '<p>L’émergence des architectures de référence IDP a également contribué à donner une image plus claire de ce à quoi ressemblent les portails de gestion des infrastructures dans la pratique. Cela dit, le marché est encore en phase de maturation et les organisations ne savent pas exactement ce que constitue un IDP. Ainsi, 53 % d’entre elles utilisent diverses formes de portails de développeurs, notamment des produits internes, Backstage et commerciaux, selon le rapport Port.</p>'}, {'', '<p>L’augmentation de la productivité des développeurs est le principal indicateur de réussite d’une initiative de portail interne pour développeurs. Cependant, déterminer comment calculer cette productivité reste une cible mouvante. L’un de ces indicateurs pourrait être le suivi du temps consacré aux activités à forte intensité de main-d’œuvre, en particulier les tâches non liées au codage, qui atteignent leur maximum pour les équipes qui n’adoptent pas l’ingénierie de plateforme. Pour les organisations qui ne sont pas engagées dans l’ingénierie de plateforme, 78 % des répondants ont déclaré que les développeurs consacraient au moins trois heures par jour à des tâches non essentielles, selon le rapport de Port.</p>'}, {'', '<p>La productivité peut également être mesurée en termes de fréquence de déploiement, ou de temps nécessaire à chaque déploiement. Mais au-delà des mesures traditionnelles de productivité des développeurs, les responsables informatiques peuvent prendre en compte d’autres indicateurs pour évaluer la productivité, explique Paolo Negri, CTO et cofondateur de Contentful. Le premier est l’aspect « santé » de la productivité, a-t-il déclaré. « Si vous considérez uniquement le développement de logiciels, où se situe mon organisation par rapport à une sorte de référence de productivité par rapport à d’autres organisations similaires ? »</p>'}, {'', '<p>Deuxièmement, il faut réfléchir à la productivité interfonctionnelle. « Quelle est la productivité ou la performance la plus élevée que nous pouvons atteindre en tenant compte des objectifs de l’entreprise ? », a ajouté Negri. « Cette ligne de pensée s’intéresse davantage aux aspects essentiels pour accélérer le flux de travail interfonctionnel en réduisant la dépendance d’une équipe unique au processus de développement, ce qui permet à une équipe de se concentrer sur des activités à plus haut rendement. »</p>'}, {'', '<p>La manière dont cette productivité est mesurée fait encore l’objet de débats, mais de plus en plus de personnes semblent utiliser des mesures qualitatives plutôt que quantitatives. 75 % des personnes interrogées dans l’étude Port mentionnée ci-dessus ont déclaré préférer les enquêtes ou les rapports personnalisés aux cadres standards comme DORA ou SPACE. LinkedIn, par exemple, a réalisé un travail intéressant en créant une plateforme d’enquête interne personnalisée pour recueillir des commentaires afin d’améliorer la satisfaction des développeurs à l’égard des outils internes.</p>'}, {'', '<p>« Les ingénieurs de plateforme peuvent améliorer considérablement l’expérience des développeurs en créant une boucle d’information étroite pour identifier les points sensibles et recueillir les commentaires des développeurs », a ajouté Conor Bronsdon, animateur du podcast Dev Interrupted de LinearB. « Cela nécessite à la fois de solliciter activement l’avis des développeurs via des enquêtes et d’utiliser des mesures quantitatives telles que les temps de construction locaux et le temps de cycle pour comprendre et suivre les blocages du flux de travail. »</p>'}, {'', '<p>Il est intéressant de noter que les développeurs affirment passer autant de temps à attendre les builds et les tests qu’à écrire du code, selon une enquête GitHub menée auprès de 500 entreprises de plus de 1\xa0000 employés (juin\xa02023). L’espoir est que les IDP puissent libérer une équipe pour réduire le temps consacré à ces tâches et se concentrer sur des rendements plus élevés. Un élément essentiel de cette habilitation sera de réduire les frictions des développeurs avec leur chaîne d’outils.</p>'}, {'', '<p>« L’ingénierie de la productivité des développeurs consiste à maximiser le temps disponible pour un travail productif en minimisant le temps que les développeurs consacrent à des tâches pénibles et frustrantes comme l’attente de longs cycles de rétroaction et le dépannage évitable et inefficace », a déclaré Hans Dockter, PDG de Gradle. Il existe donc un bon argument pour voir comment nous pouvons réduire les frictions dans le flux de travail des développeurs.</p>'}, {'', '<p>Dockter encourage avant tout les leaders technologiques à se concentrer sur la réduction des changements de contexte, l’un des principaux facteurs de fatigue cognitive. « La plupart des dirigeants d’aujourd’hui ne comprennent pas pleinement le flux de travail et l’expérience mentale de leurs développeurs, car la plupart d’entre eux ne ressentent pas directement les difficultés », a-t-il déclaré. « Afin d’améliorer la productivité des développeurs et d’obtenir la meilleure expérience possible, je pense qu’il est essentiel que les dirigeants technologiques prennent en compte la science cognitive qui détermine le comportement des développeurs. »</p>'}, {'', ""<p>Il peut être difficile de gérer des équipes de développement logiciel et de maintenir un rendement constant en période de crise économique. Cependant, nombreux sont ceux qui misent sur les IDP et l'ingénierie de plateforme pour conserver (voire accroître) l'agilité logicielle.</p>""}, {'', '<p>« Les plateformes de développement intégrées (IDP) avec prise en charge de l’ingénierie de plateforme pour étendre l’automatisation du flux de travail tout au long du cycle de vie du développement logiciel peuvent rationaliser le processus de développement, augmentant ainsi directement la productivité », a déclaré Bronsdon. « En automatisant les tâches de routine, en fournissant des environnements de développement standardisés et en facilitant l’intégration et le déploiement continus, l’ingénierie de plateforme génère des avantages en termes d’efficacité dans l’ensemble de l’organisation de R&D. »</p>'}, {'', ""<p>Quelle que soit la mise en œuvre exacte, le concept IDP reste une méthode populaire qui permet aux développeurs de se concentrer davantage sur la création de valeur commerciale et moins sur la logistique du processus de développement. Je m'attends donc à ce que des développements continus et des repères émergent dans cette nouvelle ère de DevOps tout au long de l'année à venir.</p>""}]"
AWS re:Invent 2023 : environnements de développement cloud,"[{'', ""<p>Orateur 1\xa0: C'est Techstrong TV.</p>""}, {'', '<p>Shira Rubinoff : Je m’appelle Shira Rubinoff. Je suis responsable de la cybersécurité au sein du groupe Techstrong. Je suis ici avec Rob Whiteley, PDG de Coder. Rob, c’est un plaisir d’être avec vous aujourd’hui.</p>'}, {'', ""<p>Rob Whiteley : Oui, merci beaucoup de m'avoir accueilli. C'est super.</p>""}, {'', '<p>Shira Rubinoff : Rob, nous avons eu de très bonnes conversations et j’aimerais que tu partages avec le public qui est Coder, quelle est ton histoire ? S’il te plaît, explique qui tu es, comment tu as commencé. Partons de là.</p>'}, {'', '<p>Rob Whiteley : Oui, j’espère que vous avez environ deux heures. Non, je plaisante. Je vais vous donner la version courte.</p>'}, {'', '<p>Shira Rubinoff : Bien sûr.</p>'}, {'', '<p>Rob Whiteley\xa0: Coder est une solution d’environnement de développement cloud, ou CDE, selon la terminologie du secteur. Cela signifie essentiellement que nous essayons de résoudre le problème consistant à aider les entreprises à transférer leur développement des ordinateurs portables locaux vers un environnement plus natif du cloud, où vous demandez aux développeurs d’accéder au cloud et d’y développer.</p>'}, {'', '<p>Le développement local sur des ordinateurs portables est coûteux, sujet aux erreurs et on se retrouve souvent avec le constat suivant : « Eh bien, ça a bien fonctionné sur ma machine, mais quand je livre le code, ça ne fonctionne plus. » Nous essayons donc de résoudre ce problème. Nous existons depuis environ sept ans et avons commencé notre vie sous la forme d’un projet open source appelé Code Server, qui est un IDE. Depuis, nous avons évolué pour proposer une solution plus complète dans cet espace CDE.</p>'}, {'', '<p>Shira Rubinoff : Eh bien, c’est formidable et certainement très nécessaire. Plongeons-nous dans les avantages de cette mesure en matière de sécurité, car ils sont nombreux et très nécessaires. J’aimerais que votre public le comprenne.</p>'}, {'', '<p>Rob Whiteley : Oui. Oui. Non, je suis très heureux d’être ici parce que je pense que c’est un élément sous-estimé, n’est-ce pas ? Donc, pour moi, si je prends du recul, la raison pour laquelle les entreprises investissent dans le développement cloud est la productivité des développeurs, n’est-ce pas ? Seulement environ un tiers du temps d’un développeur est réellement consacré au codage. Les deux tiers restants sont consacrés à des tâches opérationnelles et administratives. En passant au cloud, vous obtenez des gains de productivité. Super. C’est pour cela que je suis venu.</p>'}, {'', '<p>La raison pour laquelle je reste sur le cloud est généralement liée à des avantages liés à la sécurité. Je veux dire par là que beaucoup de nos clients nous ont dit : « Un développeur a oublié son ordinateur portable dans un taxi » ou « Il n’avait pas accès à Internet lorsqu’il était en déplacement ». En mettant les choses dans le cloud, vous pouvez vous connecter à partir de n’importe quel appareil et récupérer le code source des ordinateurs portables. C’est désormais une source majeure de propriété intellectuelle pour la plupart des entreprises. Les applications font fonctionner votre entreprise, et récupérer le code source des ordinateurs portables est donc un énorme avantage.</p>'}, {'', '<p>Ce n’est que le début. C’est généralement ce qui déclenche le processus. Par exemple, dans l’une de nos grandes banques, c’est le RSSI qui nous a fait appel uniquement pour résoudre le problème de récupération du code source des ordinateurs portables.</p>'}, {'', '<p>Shira Rubinoff : C’est un énorme problème.</p>'}, {'', '<p>Rob Whiteley : C’est vrai. Je pense que cela en vaut la peine, mais une fois que vous êtes dans le cloud, vous disposez de meilleurs contrôles d’accès, vous pouvez gérer les accès en fonction des rôles, vous pouvez réfléchir davantage au chiffrement et à la manière dont vous le sécurisez, et vous pouvez appliquer certaines formes de SSH. Il y a donc de nombreux avantages, et je ne pense pas que les clients s’en rendent compte avant d’avoir commencé. Nous espérons notamment aborder ce sujet en amont. La sécurité est en fait un facteur majeur de la transition vers le cloud. Il ne s’agit pas seulement de gains de productivité.</p>'}, {'', '<p>Shira Rubinoff : Certainement. Parlons de l’absence d’étapes supplémentaires dont nous parlions plus tôt. Le facteur humain est un élément essentiel de la sécurité pour les organisations et les entreprises technologiques et pour tous les types de sécurité. Et avec le codage, il y a de nombreuses étapes différentes à suivre pour arriver à ce que vous voulez faire. Lorsqu’il y a des étapes supplémentaires et que les gens sont pressés, ils ne veulent pas le faire. Ils veulent le contourner, le casser, le laisser ouvert pour ne pas avoir à y penser, laisser des mots de passe traîner, toutes sortes de choses. Alors, s’il vous plaît, dites-moi ce que fait Coder et pourquoi.</p>'}, {'', '<p>Rob Whiteley : Oui. Non, c’est génial. En fait, Ammar, le cofondateur de Coder, a une phrase selon laquelle les développeurs sont intrinsèquement paresseux. Il ne veut pas manquer de respect à cela. C’est juste qu’ils sont…</p>'}, {'', '<p>Shira Rubinoff : C’est tout le monde, n’est-ce pas ? C’est le multitâche. Je ne dirais pas que ce sont des codeurs. Les codeurs sont fabuleux.</p>'}, {'', '<p>Rob Whiteley : Je pense que l’éthique du codage consiste en partie à faire les choses de la manière la plus élégante et la plus simple possible. C’est ce qui fait un bon code. Si vous transférez la charge de la sécurité au développeur, comme nous, dans l’industrie, aimons ce concept de shift left, qui consiste essentiellement à déplacer les choses plus tôt dans le cycle de développement. Cela signifie en réalité que vous demandez au développeur d’en faire plus, de prendre plus de mesures, d’intégrer la sécurité dans son flux de travail. Mais cela va à l’encontre de cette éthique de la paresse et de la volonté de faire les choses efficacement.</p>'}, {'', '<p>Je pense que l’une des choses que nous avons remarquées est qu’il faut inverser un peu la situation. Passons maintenant au développeur. Et si nous pouvions rendre le développeur plus productif et simplement intégrer la sécurité de manière transparente ? Il n’a pas à s’en soucier. Comment cela fonctionne-t-il ? Eh bien, si je me connecte à un cloud pour faire mon développement, j’hérite de toute l’infrastructure de sécurité que j’ai déjà mise en place pour mon cloud, mon pare-feu, mes contrôles d’accès, mon chiffrement, mes sauvegardes, tout ce qui maintient cet environnement que j’ai probablement déjà mis en production. Je déplace désormais automatiquement ces contrôles de sécurité vers mon environnement de développement, qui était auparavant constitué d’ordinateurs portables, et je devais m’assurer que mon antivirus ou ma gestion des appareils mobiles gardaient cet appareil sécurisé. Mon développeur a dû lutter contre cela. Il a dû travailler dur, il a dû mettre à jour les choses, il a dû s’assurer.</p>'}, {'', ""<p>Nous réduisons ce travail, mais plus important encore, nous retirons la sécurité de la plaque, de la tête. Si vous accédez au cloud, il est plus sécurisé. Point final. C'est pourquoi je pense que cela fonctionne mieux que de devoir envoyer les développeurs suivre davantage de formations en sécurité. Oui, vous devez écrire du code propre et avoir moins de bugs, mais c'est sur cela que le développeur devrait se concentrer, pas sur le respect de vos politiques de sécurité, ce qui est trop fastidieux.</p>""}, {'', '<p>Shira Rubinoff : Certainement. J’ai une solide expérience en sécurité et je peux vous dire que les meilleures technologies du marché sont dotées d’une sécurité intégrée et ne nécessitent aucune étape supplémentaire. Vous voulez quelque chose de meilleur, de plus puissant, de plus rapide, sans que la responsabilité ne repose sur l’utilisateur, c’est-à-dire le développeur ou quelqu’un qui doit en être responsable. Vous voulez vous assurer qu’il fait bien son travail sans avoir à le surcharger avec ces éléments supplémentaires.</p>'}, {'', '<p>Pensez à notre vie quotidienne. Nous avançons à cent à l’heure, et les développeurs doivent garder cela à l’esprit. Ils doivent répondre à tout, courir et voyager, tout ce qu’ils font. La dernière chose à laquelle vous voulez qu’ils pensent est : « Attendez une seconde, est-ce que j’ai fait toutes ces étapes ? » Parce que la plupart du temps, ils ne l’ont pas fait. C’est donc une excellente chose que vous avez certainement intégrée.</p>'}, {'', '<p>En ce qui concerne les concurrents que vous voyez ou auxquels vous êtes confrontés lorsque vous êtes sur le marché, quels sont, selon vous, vos principaux facteurs de différenciation sur le marché ?</p>'}, {'', '<p>Rob Whiteley : Oui, bien sûr. Je pense que pour nous, il existe deux façons principales de déployer ces solutions. Elles sont auto-hébergées, ce qui signifie que vous téléchargez le logiciel, c’est Coder. Nous commençons donc en open source, et vous pouvez passer à la version commerciale si vous le souhaitez, mais vous possédez le logiciel, vous le déployez. Ou vous pouvez l’utiliser en tant que SaaS. Le problème avec cet espace en particulier lorsqu’il est déployé en tant que SaaS, ce qui est le cas de presque tous nos concurrents, c’est que je dispose désormais de nombreuses métadonnées très sensibles sur mes développeurs. Quels environnements utilisent-ils ? Quels outils utilisent-ils ? À quelle fréquence sont-ils dans ces environnements ? Quelles langues ? Quelles versions ? Tout cela pourrait être utilisé pour exploiter l’application. Ces métadonnées circulent donc désormais vers un cloud tiers, ce qui, soit dit en passant, m’a obligé à percer un trou dans le pare-feu juste pour le faire, uniquement pour qu’ils perforent un trou directement dans mon environnement afin de provisionner le poste de travail du développeur.</p>'}, {'', '<p>Pour nous, le fait de tout héberger dans votre cloud, que ce soit sur site ou dans un environnement public, peu importe, est ce qui a été la plus grande différence inhérente. J’adore le SaaS. Le SaaS est l’avenir, mais dans un marché d’adoption précoce comme celui-ci, il y a une nette tendance vers l’auto-hébergement. La plupart de ces organisations peuvent de toute façon exécuter des logiciels aussi bien qu’un fournisseur de cloud, et elles préfèrent donc le faire elles-mêmes. C’est donc la première différence.</p>'}, {'', '<p>Deuxièmement, je dirais que la majorité des solutions sur ce marché, et encore une fois, je l’appellerai CDE ou environnement de développement cloud, sont conçues en pensant à l’opérateur, à l’équipe DevOps back-end qui doit posséder l’infrastructure. Nous avons essayé d’aborder la question du point de vue du développeur. Qu’est-ce qui va créer la meilleure expérience pour le développeur ? Parce que l’une des choses que vous avez évoquées est que si vous compliquez les choses, elles ne seront pas adoptées.</p>'}, {'', ""<p>Shira Rubinoff : C'est vrai.</p>""}, {'', '<p>Rob Whiteley : Nous ne vivons plus dans un monde où nous pouvons forcer les développeurs à adhérer. Ils doivent en quelque sorte adhérer. Donc si l’expérience n’est pas bonne, ils n’adopteront tout simplement pas cette structure.</p>'}, {'', '<p>Shira Rubinoff : Eh bien, c’est l’état d’esprit des développeurs, et c’est très bien que vous vous y soyez mis. Beaucoup de gens disent : « Eh bien, c’est ce que nous faisons pour tout le monde. » J’aime le fait que vous vous soyez vraiment concentré sur ce que font les développeurs ? Comment aiment-ils travailler ? Et que vous travailliez dans ce sens. Lorsqu’ils parlent de sécurité en général ou de la façon dont fonctionne réellement un système, pensez à votre base d’utilisateurs, travaillez comme eux et construisez la sécurité autour de cela.</p>'}, {'', '<p>Rob Whiteley : Exactement.</p>'}, {'', '<p>Shira Rubinoff : N’essayez pas de les forcer à faire quelque chose. C’est donc essentiel.</p>'}, {'', '<p>Rob Whiteley : Oui, exactement. Notre état d’esprit est simple. Si vous simplifiez l’expérience du développeur et que vous éliminez les problèmes de sécurité, il l’adoptera. Vous bénéficierez alors enfin de tous les avantages dont nous avons parlé. Le pire, à mon avis, c’est que vous déployiez la solution et qu’il continue à travailler sur son ordinateur portable comme si rien n’avait changé parce qu’il trouve la nouvelle solution trop difficile à utiliser.</p>'}, {'', '<p>Shira Rubinoff : C’est vrai. Je sais que vous avez fait une annonce très intéressante en septembre. J’aimerais que vous la réitériez et que vous en parliez un peu à notre public, ainsi que que vous nous fassiez également allusion à de nouvelles choses à venir.</p>'}, {'', '<p>Rob Whiteley : Oui. Oui. Nous sommes très enthousiastes.</p>'}, {'', '<p>Shira Rubinoff : Si vous pouviez nous donner un petit aperçu ici.</p>'}, {'', '<p>Rob Whiteley : Oui, en septembre dernier, nous avons annoncé quelque chose appelé le Coder Registry. Considérez cela comme un marché pour les plugins Coder. La raison pour laquelle c’est important est que nous croyons au libre choix des développeurs. Les développeurs doivent choisir les outils qui les rendent les plus productifs. Ce que nous avons fait, c’est que nous avons créé ce registre pour que n’importe qui dans l’écosystème des outils de développement puisse écrire un plugin pour Coder. Si vous voulez pouvoir fournir votre outil à un développeur… Parce que nous possédons en quelque sorte le globe oculaire de ce développeur, si vous voulez… Nous sommes l’outil auquel ils se connectent pour faire leur travail afin que nous puissions commercialiser différents outils. Si vous souhaitez utiliser un outil d’IA de génération de code pour vous aider, nous pouvons le faire. En faisant ces plugins, nous créerons un écosystème beaucoup plus robuste, je pense, dans lequel ce n’est pas au développeur de faire cette intégration.</p>'}, {'', '<p>Nous ne sommes plus qu’une tuile sur laquelle ils peuvent cliquer pour dire : « Je veux cet outil ». Mais nous donnons toujours à l’équipe de la plateforme la possibilité de le gérer. S’ils ne veulent pas introduire cet outil, ils peuvent simplement l’exclure de la liste de sélection des développeurs. Ce registre est ce que nous avons annoncé. Nous créons de plus en plus de supports tiers pour cela. C’est en quelque sorte le point important. C’était donc une grande annonce.</p>'}, {'', '<p>Ce que je peux dire pour l’année prochaine et qui sera, selon moi, la plus passionnante, c’est que nous essayons vraiment de nous concentrer sur la façon dont nous pouvons former ces équipes de plateformes qui sont responsables de la productivité des développeurs, de la sécurisation du code source et de leur fournir un ensemble d’informations beaucoup plus avancées. L’une des choses que fait Coder, c’est que nous collectons une grande partie de ces métadonnées et, comme je l’ai dit, nous ne les envoyons pas dans le cloud. Elles restent locales dans votre environnement.</p>'}, {'', '<p>Shira Rubinoff : C’est important.</p>'}, {'', '<p>Rob Whiteley : Eh bien, c’est un ensemble de données riche. Nous pourrions faire beaucoup de choses avec cet ensemble de données pour faciliter la vie des opérateurs. L’une des choses que nous examinerons au début de l’année prochaine est de savoir comment nous pouvons exposer ces données de manière à ce que vous puissiez déterminer « Si j’augmente la quantité de calcul, à quoi cela correspondrait-il pour un résultat ? » En mettant cela en place en amont, nous pouvons rendre la tâche beaucoup plus facile pour les opérateurs. Encore une fois, si nous pouvons éliminer les frictions de l’expérience des développeurs, ils l’adopteront. C’est donc en partie ce que nous étudions.</p>'}, {'', '<p>Shira Rubinoff : Eh bien, c’est très excitant. Restez tous à l’écoute pour cette annonce à venir. Avez-vous autre chose à ajouter pour notre public ? Quelque chose qu’ils devraient prendre en compte ou des conseils utiles concernant la conférence AWS ou votre entreprise elle-même que vous aimeriez partager avec nous ?</p>'}, {'', '<p>Rob Whiteley : Oui, je pense que le plus important serait… L’environnement de développement cloud est nouveau. C’est un marché émergent. Je pense que beaucoup d’entreprises ne l’étudient pas nécessairement parce qu’elles pensent que ce n’est pas adapté à leur situation ou qu’elles ne veulent pas forcer les développeurs à changer de comportement. Je pense que ce que nous avons constaté, c’est que chaque client qui l’a fait a rapidement pris de l’expansion. Pour moi, mon plus grand appel à l’action est d’aller y jeter un œil. Je veux dire, c’est gratuit. C’est ouvert. Vous pouvez commencer. Je ne vous demande même pas d’acheter quoi que ce soit.</p>'}, {'', '<p>Mais je pense que le simple fait de passer du local au cloud, d’obtenir ces avantages en matière de sécurité et de démontrer que l’entreprise en est gagnante est la première étape de ce qui sera, je pense, un changement radical dans la dernière étape que le cloud n’a pas encore réellement atteinte, à savoir les développeurs. Les environnements de production sont dans le cloud depuis un certain temps maintenant. Les environnements de développement sont les prochains. Je pense donc qu’il est temps de s’y mettre.</p>'}, {'', '<p>Shira Rubinoff : C’est formidable. Merci beaucoup d’avoir partagé votre point de vue avec notre public.</p>'}]"
Massdriver ajoute des contrôles de coûts cloud à sa plateforme de développement interne,"[{'', ""<p>Massdriver a mis à disposition des outils de suivi des coûts du cloud et de génération d'une nomenclature d'infrastructure cloud (Cloud IBOM) pour les plateformes de développement internes (IDP) dans le cadre d'un effort continu visant à rationaliser les flux de travail d'ingénierie de la plateforme.</p>""}, {'', '<p>Le Cloud IBOM permet, en quelques clics, de cartographier des ressources cloud hybrides telles que des bases de données, des pipelines de machine learning et des frameworks de calcul sans serveur.</p>'}, {'', '<p>Selon Cory O’Daniel, PDG de Massdriver, ces outils réduisent les dépenses en évitant les erreurs de configuration qui entraînent des coûts inattendus. Plutôt que de fournir uniquement un résumé des coûts engagés, les outils fournis par Massdriver indiquent le coût des services cloud au moment où le développeur les appelle, a-t-il noté.</p>'}, {'', '<p>En l’absence de ce contexte, les développeurs surprovisionneront presque toujours les ressources cloud pour garantir une disponibilité maximale, quel que soit le coût, explique O’Daniel. L’objectif global est d’aider les entreprises à réduire le coût total de l’informatique en fournissant aux développeurs des informations sur les coûts en temps réel, a-t-il ajouté.</p>'}, {'', '<p>Plus tôt cette année, la société a lancé un IDP qui comprend des outils d’ingénierie de plateforme qui permettent, par exemple, aux équipes DevOps de créer un catalogue de composants d’infrastructure auxquels les développeurs peuvent accéder dans le cadre des garde-fous qu’ils définissent. La plateforme comprend également des outils d’observabilité et de visualisation qui simplifient le suivi des métriques et la surveillance de l’utilisation des ressources cloud.</p>'}, {''}, {'', '<p>Massdriver plaide en faveur d’un IDP qui fournit des outils et des flux de travail intégrés permettant aux organisations d’adopter l’ingénierie de plateforme comme méthodologie de gestion des flux de travail DevOps à grande échelle. L’objectif est de réduire le niveau d’expertise requis pour créer et déployer des applications dans des environnements de cloud computing, a déclaré O’Daniel. L’une des principales raisons pour lesquelles il n’y a pas plus d’applications exécutées dans le cloud aujourd’hui est que ces environnements sont encore trop complexes pour être gérés avec succès, a-t-il ajouté. Un IDP fait abstraction de cette complexité dans un ensemble de flux de travail qui ont été approuvés par une équipe DevOps, a-t-il ajouté.</p>'}, {'', '<p>Il existe bien sûr plusieurs approches pour créer un IDP, mais les approches concurrentes nécessitent trop d’efforts pour la mise en place et la maintenance, a déclaré O’Daniel. Une plateforme propriétaire offre une approche plus clé en main qui ne nécessite pas que les équipes DevOps créent leur propre plateforme ou en personnalisent une, a-t-il ajouté.</p>'}, {'', '<p>On ne sait pas exactement à quel rythme les entreprises adoptent l’ingénierie de plateforme, mais il est clair qu’il faut trouver un équilibre entre la fourniture de services informatiques gérés par une équipe centralisée et le désir d’un développeur d’adopter de nouveaux outils dès qu’il le juge nécessaire. Aujourd’hui, une trop grande part de responsabilité dans la mise en place des environnements informatiques est laissée aux développeurs, qui ont désormais moins de temps que jamais pour écrire du code.</p>'}, {'', '<p>Il n’existe bien sûr aucune garantie que la réduction de la charge cognitive des développeurs augmente la productivité. La création d’applications nécessite des moments d’inspiration, mais, en théorie, un IDP devrait laisser plus de temps à l’inspiration pour se manifester. Dans le cadre de cet effort, les équipes DevOps doivent s’efforcer en permanence d’éliminer les goulots d’étranglement qui ont émergé dans leurs flux de travail DevOps.</p>'}]"
L'ingénierie de plateforme est une « voie d'or » vers FinOps,"[{'', ""<p>Coûts du cloud. Cette simple réalité a donné lieu à toutes sortes d'outils désormais axés sur les piles informatiques modernes pour consolider les ressources, réduire le gaspillage, optimiser les gains d'efficacité identifiés et surveiller le bien-être du système au niveau le plus élevé, le plus large et le plus étendu.</p>""}, {'', '<p>Dans les paysages de cloud computing en cours de réinvention pour les fonctions d’IA générative, l’essor des pratiques d’opérations d’apprentissage automatique (MLOps) vise à aligner l’ingénierie logicielle sur l’ingénierie opérationnelle et, peut-être surtout, sur l’ingénierie des coûts.</p>'}, {'', '<p>Mais avant de nous aventurer trop profondément dans le domaine du MLOps, nous devrions peut-être nous rappeler que le FinOps (opérations alignées pour prendre en compte l’impact financier de toutes les décisions d’architecture système et d’ingénierie logicielle) est vraiment la pratique fondamentale qui devrait superviser toutes les questions dans ce domaine.</p>'}, {'', '<p>CloudBolt est une organisation qui a clairement exprimé sa volonté d’aider les entreprises à optimiser, automatiser et contrôler les environnements cloud hybrides et multicloud en fournissant des solutions de gestion financière, d’automatisation, d’orchestration et de gouvernance cloud pour les entreprises mondiales. La société publie plusieurs rapports sectoriels sur divers aspects de la complexité du multicloud.</p>'}, {'', '<p>Kyle Campos, Chief Product & Technology Officer (CPTO) de CloudBolt, suggère qu’à mesure que les entreprises continuent de se concentrer sur l’efficacité (en examinant chaque dépense et chaque outil), la plupart des DSI avant-gardistes « institutionnaliseront » l’ingénierie de la plateforme et créeront des équipes correspondantes d’ici décembre 2024. Ces équipes géreront non seulement les actifs technologiques, mais les hiérarchiseront également pour accélérer la création de valeur commerciale. Ce changement stratégique s’inscrit dans la tendance plus large du secteur qui consiste à optimiser les piles technologiques pour plus d’agilité et d’efficacité, offrant ainsi une approche multidimensionnelle des activités.</p>'}, {'', '<p>« Les silos FinOps qui génèrent des frictions et [la présence de] promesses d’optimisation non réalisées connaîtront une percée à mesure que la conversation et les solutions passeront de la « motivation » à la « facilitation » au cours de l’année à venir », a déclaré Campos. « L’ingénierie de plateforme étant pleinement adoptée comme approche technologique par la majorité des organisations informatiques avant-gardistes d’ici la fin de l’année prochaine, les pratiques FinOps deviendront une partie native du parcours vers les « chemins d’or », au même titre que la sécurité et l’observabilité, en tant que tiercé des valeurs par défaut dans le processus de livraison. »</p>'}, {'', '<p>Après des décennies de dépenses excessives en matériel, logiciels et services, Campos estime que 2024 sera l’année où le coût d’exécution d’une application deviendra la mesure de performance ultime. Pourquoi fait-il cette affirmation ? Parce que les mesures traditionnelles axées sur les produits de base (CPU, mémoire, disque, réseau) deviendront en fait moins importantes, a-t-il déclaré.</p>'}, {'', '<p>À mesure que les organisations transformées numériquement poursuivent leur progression vers le cloud, le coût des marchandises vendues sera intrinsèquement lié aux opérations et aux services mesurés à un niveau plus précis ; en examinant des outils, des services et des algorithmes spécifiques pour réaliser des économies de coûts.</p>'}, {'', '<p>« Alors que les entreprises sont confrontées à des défis financiers complexes en 2024, les consommateurs exigeront de plus en plus de flexibilité, ce qui obligera les fournisseurs de cloud à proposer des tarifs et des services plus centrés sur le consommateur », a déclaré Campos. « L’initiative FinOps FOCUS instituée cette année n’est qu’un début. La voix collective de la communauté des utilisateurs continuera de changer la donne en uniformisant les règles du jeu entre les fournisseurs de cloud et les consommateurs. »</p>'}, {'', '<p>Campos a conclu en affirmant qu’il existe actuellement une corrélation directe entre la complexité/la friction des utilisateurs et la granularité des capacités/des coûts dans l’écosystème des solutions FinOps.</p>'}, {'', '<p>« L’augmentation de ces derniers rend les premiers beaucoup plus complexes », a-t-il expliqué. « Mais les progrès de l’IA/ML continueront de bouleverser le statu quo, en abaissant les barrières et le temps de rentabilisation pour les utilisateurs. Ce qui prenait auparavant des heures de configuration personnalisée, d’essais et d’erreurs sera désormais une conversation sans friction. De plus, l’IA/ML facilitera l’inversion des solutions de coût unitaire de telle sorte que le coût unitaire soit fourni à l’utilisateur, et non pas par lui. »</p>'}]"
Une étude révèle des gains significatifs en matière d'ingénierie des plateformes,"[{'', ""<p>Une enquête menée auprès de 100 ingénieurs logiciels travaillant dans des organisations aux États-Unis et en Europe occidentale, publiée aujourd'hui, a révélé que presque tous (99 %) ont adopté l'ingénierie de plate-forme dans une certaine mesure comme méthodologie pour gérer les flux de travail DevOps à grande échelle, plus de la moitié (53 %) l'ayant fait au cours de l'année dernière.</p>""}, {'', ""<p>Menée par Port, un fournisseur d'une plateforme propriétaire de portail de développeurs internes (IDP), l'enquête a également révélé que 83 % des répondants ont également intégré les flux de travail GitOps dans leurs pratiques d'ingénierie de plateforme.</p>""}, {'', '<p>L’enquête a également révélé que 85 % des personnes interrogées travaillent pour des organisations qui ont commencé à déployer des IDP ou prévoient de le faire au cours de l’année à venir. Cependant, la définition d’un IDP n’est pas tout à fait claire : plus d’un tiers (35 %) des personnes ayant mis en place un IDP déclarent s’appuyer sur des feuilles de calcul pour gérer leur IDP. Un peu plus de la moitié (53 %) des personnes interrogées, en revanche, utilisent un IDP pour fournir aux développeurs des fonctionnalités en libre-service pour configurer des environnements afin d’écrire du code personnalisé, basé sur le logiciel open source Backstage développé à l’origine par Spotify ou sur une plateforme IDP commerciale.</p>'}, {'', ""<p>Parmi les 85 % de répondants qui utilisent ou prévoient d'utiliser un IDP, le principal critère de réussite est l'amélioration de la productivité des développeurs (56 %), suivie de la réduction du temps de déploiement (25 %). Malgré ces initiatives, seulement un peu plus d'un tiers (34 %) ont déclaré que les IDP aident les équipes DevOps à appliquer les normes d'ingénierie logicielle.</p>""}, {'', ""<p>Roni Floman, directeur marketing de Port, a déclaré que l'enquête suggère que les développeurs perdent beaucoup de temps sur des tâches qui n'ont rien à voir avec l'écriture de code. Par exemple, l'enquête révèle qu'avant d'adopter l'ingénierie de plateforme, les développeurs passaient trois à quatre heures par jour sur des activités non essentielles, ce qui faisait qu'il leur fallait des semaines pour finalement déployer un logiciel dans un environnement de production.</p>""}, {'', '<p>Le plus grand défi auquel les entreprises sont confrontées lorsqu’elles adoptent l’ingénierie de plateforme est de convaincre les développeurs qu’une équipe centralisée a à cœur leurs intérêts. De nombreuses équipes de développement d’applications ont adopté DevOps comme meilleure pratique en premier lieu pour échapper à la tyrannie d’une équipe informatique centralisée. Le problème est que, par conséquent, beaucoup d’entre elles se retrouvent désormais à gérer des tâches qui n’ont pas grand-chose à voir avec l’écriture de code. L’ingénierie de plateforme cherche à trouver un équilibre entre la possibilité pour les développeurs d’adopter de nouveaux outils lorsqu’ils le jugent nécessaire et les avantages d’avoir une équipe d’ingénierie de plateforme dédiée qui automatise les processus en leur nom.</p>'}, {'', '<p>Toutefois, à moins que les équipes d’ingénierie de la plateforme ne trouvent un moyen d’inciter les développeurs à adhérer à l’effort, il est peu probable qu’elles y parviennent, a noté Floman.</p>'}, {'', '<p>Chaque organisation devra déterminer elle-même le niveau d’ingénierie de plateforme qu’elle souhaite mettre en œuvre, compte tenu de sa culture unique. Dans la plupart des cas, une initiative d’ingénierie de plateforme commencera à petite échelle avant d’augmenter en taille et en portée à mesure que la dynamique s’intensifie. En attendant, la chose la plus importante que toute équipe d’ingénierie de plateforme doit faire à court terme est clairement de mobiliser les développeurs. Après tout, la meilleure façon d’obtenir du soutien pour tout changement apporté à un flux de travail DevOps est, comme toujours, de convaincre les développeurs que c’était leur idée au départ.</p>'}]"
KubeCon 2023 : le plan de contrôle des développeurs de CTO.ai,"[{'', ""<p>Orateur 1\xa0: C'est Techstrong TV.</p>""}, {'', ""<p>Alan Shimel : Salut à tous, nous sommes de retour. Nous sommes ici à Chicago à KubeKon. Nous terminons notre couverture de la deuxième journée, notre dernier invité pour la deuxième journée. Nous serons de retour demain, cependant. Mais laissez-moi vous présenter Kyle Campbell, n'est-ce pas ?</p>""}, {'', '<p>Kyle Campbell : C’est vrai.</p>'}, {'', ""<p>Alan Shimel : Kyle est le fondateur de cto.ai et nous allons en savoir plus sur cto.ai et un peu sur ce qu'ils font ici chez KubeKon. Mais avant cela, Kyle, tout d'abord, bienvenue.</p>""}, {'', '<p>Kyle Campbell : Merci.</p>'}, {'', '<p>Alan Shimel : Deuxièmement, écoutons un peu Kyle.</p>'}, {'', '<p>Kyle Campbell : Oui, merci.</p>'}, {'', '<p>Alan Shimel : Racontez-nous votre genre de parcours.</p>'}, {'', '<p>Kyle Campbell : Oui, tout d’abord, je suis ravi de vous rencontrer en personne. J’ai déjà parlé de mon parcours, j’ai déjà raconté cette histoire, un peu peu conventionnelle. J’ai grandi en Nouvelle-Écosse au Canada, dans une petite ville. Je suis sur Internet depuis l’âge de huit ans et je ne m’intéressais pas du tout au passé formel. J’ai donc créé des logiciels dès les premiers jours du boom des .com. Je ne m’y intéresse peut-être pas, mais j’ai eu des cicatrices. Mais j’étais un ingénieur logiciel autodidacte, donc l’open source a été la clé de mon succès et de bons outils de développement.</p>'}, {'', '<p>Alan Shimel : Bien sûr.</p>'}, {'', ""<p>Kyle Campbell : J'ai évolué dans l'ère du cloud et de l'open source, puis j'ai commencé à fonder des plateformes de développement en 2014. La première entreprise que j'ai créée était une plateforme de développement, dans le domaine de l'immobilier. Zillow l'a acquise en huit mois environ, ce qui était intéressant.</p>""}, {'', '<p>Alan Shimel : Très.</p>'}, {'', ""<p>Kyle Campbell : J'ai ensuite lancé avec succès une agence DevOps et j'ai commencé à découvrir qu'il y avait beaucoup d'opportunités pour les plateformes de développement de nouvelle génération, ce qui m'a conduit à cto.ai.</p>""}, {'', '<p>Alan Shimel : Excellent, mec. Quelle belle histoire aussi. Tu es toujours en Nouvelle-Écosse ?</p>'}, {'', '<p>Kyle Campbell : Je ne le suis pas. J’ai déménagé à l’autre bout du pays. Je vis maintenant en Colombie-Britannique.</p>'}, {'', '<p>Alan Shimel : Tant mieux pour vous.</p>'}, {'', ""<p>Kyle Campbell : J'adore le plein air. Je passe beaucoup de temps avec mon fils à camper, à pêcher et à essayer de sortir et de profiter de la beauté de la nature.</p>""}, {'', '<p>Alan Shimel : J’adore. Ouais, non, c’est magnifique. Je veux dire, ce n’est pas que la Nouvelle-Écosse n’est pas belle. C’est brutal en hiver, mais c’est un beau pays, une belle partie du pays.</p>'}, {'', '<p>Parlons de cto.ai maintenant. Écoutez, j’ai moi-même fondé plusieurs entreprises. Tous les fondateurs que j’ai interviewés ou avec qui j’ai discuté au cours des 30 dernières années ne se sont pas contentés de dire : « Oh, j’ai envie de fonder une entreprise aujourd’hui. » C’est un peu comme Richard Dreyfuss dans Rencontres du troisième type, n’est-ce pas ? Il y a quelque chose qui vous pousse à vous dire : « Je dois le faire. Il faut que cela soit fait. » Qu’est-ce qui vous a poussé à vous lancer dans cto.ai ?</p>'}, {'', '<p>Kyle Campbell : Oui. Comme je l’ai décrit, une grande partie de mon parcours s’est déroulée en autodidacte et sur les épaules de géants. Et ce qui était vraiment important pour moi, c’était l’expérience de développeur et la facilité d’utilisation des outils dès le début de ma carrière, car cela m’a permis de vraiment développer mes compétences en tant que développeur et de suivre ces personnes qui avaient des diplômes en informatique, des masters et tout ce genre de choses, n’est-ce pas ?</p>'}, {'', '<p>Alan Shimel : Certainement.</p>'}, {'', '<p>Kyle Campbell : Lorsque j’ai réfléchi à cela et à l’avenir ainsi qu’à ma passion pour l’ingénierie des plateformes, il m’est apparu évident que ce qui manquait sur le marché était un ensemble d’outils conçus pour embrasser l’avenir de l’expérience des développeurs et la façon dont l’IA peut avoir un impact sur celui-ci, du code au cloud.</p>'}, {'', ""<p>Donc, vers 2016, j'ai commencé à expérimenter avec l'idée, et en 2020, j'ai décidé de lancer officiellement cto.ai en tant que plan de contrôle des développeurs, c'est ce que nous appelons. Il faut l'envisager comme une plate-forme CI/CD moderne où les équipes peuvent composer leur flux de travail entre la construction, le déploiement, les tests et la prévisualisation. Et lorsqu'elles travaillent avec ces outils, elles peuvent interagir avec eux dans la CLI Slack, GitHub, communiquer avec ces flux de travail via une sorte de modèle basé sur des invites. Et nous collectons des données de télémétrie qui leur permettent ensuite de les mesurer par rapport aux métriques DORA, qui sont...</p>""}, {'', ""<p>Alan Shimel : C'est la norme.</p>""}, {'', '<p>Kyle Campbell : … oui, c’est une norme et je pense, franchement, qu’il s’agit d’une manière empirique de mesurer l’expérience du développeur. Nous n’essayons pas de mesurer la productivité des développeurs. Nous pensons que ce n’est pas la bonne question à poser. La question est de savoir comment mesurer l’expérience du développeur de la même manière que je mesurerais l’expérience de mes utilisateurs avec mon produit. Et si ce que nous faisons consiste à créer des plateformes pour permettre l’expérience du développeur, nous avons besoin d’une méthode intégrée native pour la mesurer. Et nous pensons également que cette mesure est un moyen d’ouvrir la voie à un avenir entre la façon dont les développeurs expérimentent ces outils et la façon dont l’IA peut soutenir l’expérience du développeur et améliorer les développeurs avec, franchement, des super pouvoirs au fil du temps.</p>'}, {'', ""<p>Alan Shimel : J'ai récemment assisté à quelques conférences d'utilisateurs de grandes entreprises DevOps, et c'était intéressant. Les trois conférences utilisent toutes la même mesure, à savoir que les développeurs ne consacrent qu'environ 30 % de leur temps au développement.</p>""}, {'', ""<p>Kyle Campbell : Rapport sur l'état de DevOps 2018, je crois ?</p>""}, {'', '<p>Alan Shimel : Mais c’était là et cela a été confirmé dans plusieurs autres. Qu’en pensez-vous ? Est-ce une bonne chose, une mauvaise chose, quelque chose dont nous pouvons être fiers, quelque chose sur lequel nous devrions travailler ? Quel est le problème ?</p>'}, {'', ""<p>Kyle Campbell : Oui. Je pense que d'un point de vue assez pratique, en tant qu'ingénieur logiciel, mon objectif est de proposer des fonctionnalités que les clients apprécient et qui génèrent de la valeur pour l'entreprise. Et si je peux le faire à une vitesse plus élevée tout en maintenant la qualité, je pense que c'est une façon pour moi, en tant qu'individu, de contribuer au succès de l'entreprise et de l'équipe.</p>""}, {'', '<p>Mais je pense que ce qui se passe souvent, c’est que nous sommes vraiment obsédés par le développeur individuel. Et je pense que tout développeur qui considère l’idée qu’il ne passe que 30 % de son temps à ajouter de la valeur n’appréciera pas beaucoup cela. Je pense que nous devons réorienter la conversation en nous éloignant de la productivité du développeur pour nous concentrer sur l’habilitation du développeur et examiner les outils et le processus que l’expérience du développeur favorise et la façon dont les développeurs passent du code au cloud et nous demander, en tant que chef d’entreprise, en tant que CTO avec qui nous travaillons beaucoup, si les outils et le flux de travail avec lesquels nous soutenons notre équipe et cette expérience du développeur, ce qu’ils vivent, génèrent des résultats ?</p>'}, {'', '<p>Et je pense que les mesures DORA sont un moyen empirique de mesurer cela, évidemment, par le biais d’anecdotes et d’enquêtes, mais ce sur quoi nous avons vraiment innové, c’est de créer une intégration verticale avec la chaîne d’outils qui vous permet de mesurer cela en temps réel et d’itérer ensuite rapidement sur les outils. Et certaines équipes peuvent vraiment s’en approprier elles-mêmes. Et ce que les entreprises devraient faire, c’est prendre du recul et se demander : « Oui, comment pouvons-nous maximiser le temps que les développeurs passent sans individualiser cette considération ? » Il y a une boucle interne et une boucle externe. Et ce que DORA est vraiment utile pour mesurer la boucle externe de la façon dont nous travaillons en agrégation d’équipes. Nous avons besoin de plateformes de nouvelle génération qui rendent ces données et ce contexte plus accessibles aux chefs d’entreprise.</p>'}, {'', '<p>Alan Shimel : Je ne suis pas en désaccord avec ce que vous avez dit, donc nous sommes d’accord avec cela. Avez-vous publié des nouvelles ou quoi que ce soit ? Que se passe-t-il autour de l’émission ?</p>'}, {'', '<p>Kyle Campbell : Oui, depuis quelque temps, nous nous concentrons sur la création d’un flux de travail et d’outils solides comme le roc pour établir les prémisses de cette vision CI/CD composable de nouvelle génération. Nous avons intégré ces données dans DORA Metrics. Vous pouvez désormais générer des rapports sur les métriques DORA en temps réel, enregistrer ces rapports, les partager avec qui vous voulez, en tête-à-tête, en rétrospective, donc c’est à vous de voir. Et nous commençons à expérimenter un peu de science des données, mais quelque chose de vraiment passionnant que nous venons de mettre en production la semaine dernière. Nous n’avons pas encore vraiment fait d’annonce officielle, donc je suppose que c’est une première, car nous avons publié notre premier type d’IA-</p>'}, {'', ""<p>Alan Shimel : Je l'ai entendu ici en premier.</p>""}, {'', '<p>Kyle Campbell : … oui, je l’ai entendu ici en premier. Notre premier ensemble de fonctionnalités d’IA, qui est la révision de code par IA. Et ce que nous faisons, c’est que nous laissons les équipes commencer avec ce genre de modèles de base pour savoir comment exploiter l’IA dans le processus de révision de code afin d’obtenir les commentaires des développeurs plus tôt. Mais ensuite, nous voulons qu’ils puissent améliorer ce modèle pour le personnaliser vraiment selon leurs préférences sur leur propre type de matériel, pour personnaliser ces modèles afin de prendre en compte leurs propres exigences spécialisées et évidemment protéger les informations propriétaires qui sont au cœur de leurs convictions. Nous avons donc lancé cela en production, en l’annonçant ici en premier.</p>'}, {'', '<p>Alan Shimel : Vraiment ?</p>'}, {'', '<p>Kyle Campbell : Nous recherchons des adopteurs précoces qui peuvent en quelque sorte le tester et nous aider à le tester et à déterminer comment nous pouvons encore améliorer le contexte de ce modèle avec certaines des informations dérivées de ces mesures DORA également.</p>'}, {'', ""<p>Alan Shimel : Donc, ici, nous parlons à cette caméra. Aux développeurs. Vous cherchez à ce qu'ils testent un peu le sujet.</p>""}, {'', '<p>Kyle Campbell : C’est vrai. Ouais.</p>'}, {'', '<p>Alan Shimel : Comment font-ils ?</p>'}, {'', '<p>Kyle Campbell : Vous êtes développeur et vous vous intéressez à la manière dont l’IA peut prendre en charge votre flux de travail CICD, en particulier la révision de code. Venez sur cto.ai, inscrivez-vous, envoyez un message. J’aimerais vous parler personnellement, vous qui êtes vous-même développeur de logiciels, et j’aimerais savoir ce que vous en pensez, quelles sont selon vous les limites et les hésitations, et plus important encore, comment pensez-vous que nous pouvons améliorer cela avec d’autres cas d’utilisation ? Parce que je pense qu’il existe tout un ensemble de cas d’utilisation passionnants pour l’IA et la livraison de logiciels qui vont aider les développeurs de logiciels.</p>'}, {'', '<p>Alan Shimel : Absolument.</p>'}, {'', '<p>Kyle Campbell : Nous embrassons-</p>'}, {'', '<p>Alan Shimel : J’ai eu cette conversation dans une interview plus tôt aujourd’hui. Écoutez, pour les gens qui ne sont pas des techniciens, vous leur montrez une interface d’IA et certains des tours de passe-passe qu’elle pourrait faire et ils pensent que c’est magique.</p>'}, {'', '<p>Kyle Campbell : Oui. Arthur C. Clark.</p>'}, {'', '<p>Alan Shimel\xa0:\xa0C’est vrai.</p>'}, {'', '<p>Kyle Campbell : Ouais.</p>'}, {'', '<p>Alan Shimel : Nous savons que ce n’est pas de la magie. Il s’agit simplement de mettre un mot devant un autre ou un seul… mais il existe certaines utilisations pour lesquelles l’IA excelle vraiment. Elle n’est donc pas destinée à remplacer Google Search ni aux développeurs de logiciels.</p>'}, {'', '<p>Kyle Campbell : Non.</p>'}, {'', '<p>Alan Shimel : Mais cela peut permettre aux développeurs de logiciels d’être 10 fois plus efficaces. Et l’une des façons dont cela pourrait vous apprendre à coder, n’est-ce pas ? Vous pourriez y mettre du code et dire : « Expliquez-le-moi. » Cela pourrait réviser du code, n’est-ce pas ? Donc soit il écrit du code, soit il révise du code. Vous ne devriez probablement pas lui faire faire les deux, mais…</p>'}, {'', '<p>Kyle Campbell : C’est vrai.</p>'}, {'', '<p>Alan Shimel : C’est vrai. Mais si c’est pour l’utiliser à des fins de révision, quelle belle utilisation de la technologie.</p>'}, {'', '<p>Kyle Campbell : Oui. Nous avons vu que le facteur le plus important qui ralentit la mise sur le marché des fonctionnalités pour les développeurs est l’attente de la révision du code. Et même si je ne suis pas sûr que ce sera la même chose que la programmation en binôme et l’entrée dans les détails, si nous pouvons nous assurer que chaque développeur peut obtenir une révision du code instantanément en ajoutant simplement une étiquette, puis que nous pouvons adapter la qualité de cette révision du code avec plus de contexte, soutenu par la façon dont l’équipe pense la qualité. Je pense que c’est un exemple important de la façon dont nous pouvons accélérer les besoins des développeurs et l’expérience des développeurs avec l’IA, tout en laissant tout le contrôle entre les mains du développeur pour décider : « Est-ce rationnel ? Est-ce une hallucination ? Est-ce le moment opportun ? » Mais la boucle de rétroaction que nous pouvons obtenir à partir de cela est, je pense, une très bonne tête de pont et c’est pourquoi nous avons choisi de commencer par là.</p>'}, {'', '<p>Alan Shimel : Absolument. Et nous verrons bien. Je ne pense pas que nous ayons encore effleuré la surface de la manière dont nous allons utiliser l’IA, mais c’est certainement une première étape décisive que nous allons franchir, n’est-ce pas ?</p>'}, {'', '<p>Kyle Campbell : Je pense que oui. Et je pense que les autres éditeurs de logiciels doivent réfléchir à la manière de réorganiser leur offre pour se préparer à cette situation, car je pense qu’il existe une certaine hésitation à l’égard de l’IA. Je pense que c’est tout à fait normal. Mais si nous pouvons réfléchir à la manière dont nous pouvons en prendre le contrôle, à la manière dont les logiciels peuvent en prendre le contrôle, les ingénieurs logiciels sont parfaitement adaptés pour être parmi les premiers à gérer ces problèmes. Tout comme nous le voyons avec l’IA et la génération de code, je pense que la prochaine étape naturelle consiste à la soutenir dans le cadre de la révision du code. Nous commencerons ensuite à examiner comment des déploiements plus intelligents, des retours en arrière plus intelligents et, bien évidemment, des opérations peuvent être améliorés grâce à l’IA. Et cet endroit est un parfait exemple de personnes essayant de résoudre ce problème.</p>'}, {'', '<p>Alan Shimel : Absolument. Absolument. Et voici l’autre chose. Je travaille dans le domaine de la technologie depuis longtemps. On ne remet pas ces génies dans leurs bouteilles.</p>'}, {'', ""<p>Kyle Campbell : C'est vrai.</p>""}, {'', '<p>Alan Shimel : Une fois qu’ils sont dehors, ils sont dehors. Et donc, que cela vous plaise ou non, ils sont là. Et je pense que vous pourriez… si vous ne pouvez pas les battre, les rejoindre.</p>'}, {'', '<p>Kyle Campbell : On ne peut pas arrêter la loi de Moore. C’est la voie que le monde suit.</p>'}, {'', '<p>Alan Shimel : Non, ce n’est pas le cas. Tu ne vas pas t’arrêter. C’est comme ça. C’est génial, mec. J’adore l’attitude. J’adore t’avoir ici. Cto.ai. Encore une fois, si tu cherches à aider Kyle, donne un coup de pouce à ce produit de révision de code IA qu’ils ont lancé, c’est fantastique. Alors, allons y jeter un œil.</p>'}, {'', '<p>Kyle Campbell : Très bien. Merci beaucoup.</p>'}, {'', ""<p>Alan Shimel : Très bien. Salut Kyle, profite bien du reste de ton KubeKon. cto.ai termine la deuxième journée à Chicago au KubeKon. Nous serons là demain matin. Je pense que nous commencerons à 10 heures. Nous ne serons là que jusqu'à 14 heures demain parce que ça se termine tôt demain. Mais nous serons de retour avec un chargement complet. Nous espérons que tu as apprécié ta journée. On Demand est disponible sur Techstrong TV. Et donc si tu as raté quelque chose, n'hésite pas à y jeter un œil. C'est Alan Shimel. Nous sommes en rupture de stock.</p>""}]"
KubeCon 2023 : Productivité des développeurs et ingénierie de la plateforme,"[{''}, {'', ""<p>Mitch Ashley\xa0: Bonjour à tous. Bienvenue à KubeCon ici à Chi-Town, Chicago, 2023. Je suis rejoint par Venkat Ramakrishnan. Bienvenue. Avec Pure Storage, n'est-ce pas\xa0?</p>""}, {'', '<p>Venkat Ramakrishnan\xa0: Oui.</p>'}, {'', ""<p>Mitch Ashley\xa0:\xa0Super. Et vous êtes responsable du produit et de l'ingénierie\xa0?</p>""}, {'', '<p>Venkat Ramakrishnan\xa0: Oui. Ouais.</p>'}, {'', '<p>Mitch Ashley\xa0:\xa0Cool. Vous voulez donc parler de quelque chose dont je veux parler, à savoir l’ingénierie des plateformes.</p>'}, {'', '<p>Venkat Ramakrishnan\xa0:\xa0Ouais.</p>'}, {'', '<p>Mitch Ashley\xa0:\xa0Nous avons fait du travail dans ce domaine. Et à mon avis, le moment est idéal pour l’ingénierie de plateforme, car l’accent est mis sur la productivité des développeurs et la réorientation… DevOps, c’est à peu près tout le monde qui aide les développeurs à faire leur travail. Il semble que l’ingénierie de plateforme ait vraiment su s’adapter de manière à ce que les organisations aient besoin de cette fonction dans leur entreprise.</p>'}, {'', '<p>Venkat Ramakrishnan\xa0: Oui, absolument. Si vous regardez la plupart des entreprises modernes, l’une des ressources les plus précieuses dont elles disposent en tant que développeurs, c’est de s’assurer que la communauté des développeurs prospère, est capable de faire progresser leurs produits et leurs expériences, et est capable de tirer le meilleur parti de l’infrastructure sans être ralentie par celle-ci.</p>'}, {'', '<p>Et je plaisante toujours en disant que si les données sont le nouveau pétrole, alors les développeurs sont les nouveaux moteurs de l’économie. Donc, vous voulez vraiment… chaque entreprise est une entreprise de logiciels moderne, une entreprise numérique. Et si les développeurs ne font pas avancer l’entreprise, elle va être laissée pour compte. Il faut donc considérer les choses sous cet angle pour comprendre pourquoi l’ingénierie des plateformes est si essentielle, même à l’ère post-pandémique. Cela a vraiment accéléré les choses. Nous faisions tous partie de la communauté des conteneurs Kubernetes.</p>'}, {'', '<p>On a pu voir certains des premiers adoptants, des innovateurs qui essayaient de faire avancer l’histoire de la plateforme, de consolider les services. Mais comme de plus en plus de personnes travaillaient à distance, que de plus en plus de tâches devaient être consolidées et que de plus en plus d’équipes devaient être consolidées, nous avons vu au cours des deux dernières années, à l’ère post-COVID, la discipline de la plateforme. La discipline de l’ingénierie de plateforme a vraiment pris une place importante et nous voyons beaucoup de clients gérer des organisations d’ingénierie de plateforme qui centralisent toutes les capacités DevOps qu’ils avaient distribuées auparavant, mais qui apportent également beaucoup de contrôle sur les coûts, la sécurité, la gouvernance, les aspects réglementaires, etc.</p>'}, {'', '<p>Cela leur donne donc un guichet unique pour tout ce dont les développeurs ont besoin pour créer et exécuter leurs applications. Et c’est là que Kubernetes brille vraiment. Si vous regardez en arrière et regardez, la vision de Kubernetes et des conteneurs est de permettre aux développeurs de créer, d’expédier et d’exécuter n’importe où. Mais maintenant, tous les outils ont mûri et toutes les organisations, beaucoup des plus grandes organisations du monde, les ont adoptés. Nous constatons donc une tonne d’adoption, une tonne de croissance du point de vue des personnes qui exécutent leurs applications et créent ces plateformes de développement internes et tout. Oui.</p>'}, {'', ""<p>Mitch Ashley\xa0:\xa0Nous faisons abstraction des choses, et les conteneurs simplifient les choses pour rendre certaines parties des pipelines plus efficaces et plus faciles. Cette complexité va ailleurs. Elle est peut-être remplacée par la technologie, mais nous devons toujours concevoir les environnements de test, de développement et de production dans lesquels nous allons évoluer. Nous voulons qu'ils soient identiques ou aussi similaires que possible. C'est un domaine de l'ingénierie des plateformes. Il existe également des portails de développeurs et ce genre de choses.</p>""}, {'', '<p>Venkat Ramakrishnan\xa0:\xa0Ouais, ouais.</p>'}, {'', '<p>Mitch Ashley\xa0:\xa0Vous concentrez-vous davantage sur la plateforme dans l’ingénierie de plateforme, ou vous étendez-vous à d’autres parties\xa0?</p>'}, {'', '<p>Venkat Ramakrishnan\xa0: Non, c’est très bien. Notre mission est de faciliter la vie des ingénieurs de plateforme, mais nos clients sont assurément des développeurs. Vous voyez, l’équipe d’ingénierie de plateforme existe pour servir leurs équipes d’application et pour les aider à créer, livrer et exécuter plus rapidement. Mais il y a essentiellement deux grandes personnalités pour lesquelles nous créons nos produits. La première, la personnalité principale pour nous, est l’ingénieur de plateforme, et ce sont nos super-héros, comme je les appelle. Je viens de sortir d’une réunion avec un client et ils viennent de me dire qu’ils ont environ 6\xa0500 à 8\xa0000 développeurs sur une plateforme qui crée environ 100\xa0000 builds par jour, et que leur équipe n’est composée que de huit personnes.</p>'}, {'', '<p>Mitch Ashley\xa0:\xa0Waouh, c’est impressionnant.</p>'}, {'', '<p>Venkat Ramakrishnan\xa0: C’est à ce point que le ratio développeur/ingénieur plateforme est déséquilibré. Il y a les grandes équipes de développement, mais aussi les très petites équipes plateforme. Elles ont donc besoin de beaucoup d’automatisation. Elles ont besoin de beaucoup d’observabilité. Elles doivent pouvoir contrôler ou gérer la plateforme par le biais de politiques, de règles et d’automatisations basées sur des règles, etc. Notre travail consiste donc à fournir cela aux organisations d’ingénierie de plateforme, aux ingénieurs plateforme, afin qu’ils puissent avoir une bonne qualité de vie.</p>'}, {'', '<p>Mais en même temps, le client de nos clients est également très important pour nous, et la plupart du temps, ou presque, ce sont les développeurs qui sont les clients de la plateforme. Ce sont des utilisateurs d’applications ou des clients de la plateforme. Nous avons donc conçu le système de manière à ce que ces équipes d’application aient une visibilité sur ce qui se passe dans l’infrastructure. Elles savent comment gérer l’infrastructure sans avoir à déposer de tickets, ce qui leur permet même de faire en sorte que l’infrastructure sous-jacente se comporte comme elles le souhaitent, par exemple en ce qui concerne les performances ou la façon dont les données sont placées, comment elles sont sauvegardées, comment vous pouvez effectuer une reprise après sinistre ou même comment migrer entre les versions.</p>'}, {'', '<p>Il y a donc beaucoup de choses que nous laissons à notre communauté de développeurs le soin de contrôler et de gérer afin qu’ils puissent tirer le meilleur parti de leur plateforme. Et nous ne nous arrêtons pas là. Nous avons également laissé nos développeurs et nos équipes de plateforme protéger la plateforme au niveau des données, au niveau des applications, mais nous avons également mis ces capacités à la disposition des développeurs. Ainsi, si vous êtes un développeur utilisant un espace de noms. Vos applications s’exécutent dans un espace de noms Kubernetes, vous pouvez le sauvegarder. Ou, en tant qu’administrateur de plateforme, vous pouvez sauvegarder l’intégralité du cluster Kubernetes et vous pouvez également restaurer uniquement l’espace de noms si vous le souhaitez.</p>'}, {'', '<p>Nous proposons donc différents contrôles précis pour les deux utilisateurs. Et ce n’est pas tout. Nous ne nous sommes pas arrêtés là. L’une des charges de travail les plus difficiles à exécuter, quel que soit l’environnement, est une base de données. Et il y avait beaucoup de peur, d’incertitude et de doute sur la question de savoir si une base de données pouvait vraiment fonctionner dans Kubernetes. Si vous revenez aux débuts de Kubernetes. Portworx est l’une des entreprises qui a déclaré avec insistance : « Vous pouvez absolument exécuter des bases de données sur Kubernetes. » Et elles sont en fait les mieux adaptées, en particulier les bases de données cloud natives modernes, elles sont les mieux adaptées à l’exécution dans Kubernetes car elles ont besoin d’évolutivité, d’agilité et d’élasticité. Et quoi de mieux qu’une infrastructure Kubernetes pour vous offrir cela ?</p>'}, {'', '<p>Mitch Ashley\xa0:\xa0La distribution. Toutes sortes de choses.</p>'}, {'', '<p>Venkat Ramakrishnan\xa0: Exactement. Nous avons donc livré les services de données Portworx, qui facilitent l’automatisation d’environ 14 bases de données importantes, et nous continuons d’en ajouter d’autres et d’exécuter ces bases de données du jour zéro au jour deux. Installation, mise à niveau, correctifs, jusqu’à la reprise après sinistre des bases de données. Vous pouvez littéralement déployer un Postgres en cluster en quelques minutes et définir une politique de reprise après sinistre afin que si toute votre région AWS tombe en panne, vous puissiez redémarrer dans une autre région en quelques minutes. En quelques minutes, le RPO est atteint.</p>'}, {'', '<p>Mitch Ashley\xa0:\xa0Laissez-moi vous demander, et ce n’est peut-être pas une ligne fine, où commence et où se termine le travail d’un ingénieur de plateforme et où commence celui d’un développeur\xa0? Quel est ce point médian où ils se croisent\xa0? Parce que l’ingénierie de plateforme ne consiste pas simplement à prendre un tas de choses aux développeurs et à le faire pour eux. Les développeurs doivent toujours avoir leur propre façon de… Je pourrais vouloir créer un environnement. «\xa0Créez ceci pour moi. Je ne veux pas ouvrir de ticket.\xa0» Comme au bon vieux temps. Alors, à quoi ressemble cette partie médiane\xa0?</p>'}, {'', '<p>Venkat Ramakrishnan\xa0: Oui. Je pense que cela va dans le sens du principe selon lequel si vous le construisez, vous en êtes le propriétaire. Cela signifie qu’un ingénieur de plateforme ne va pas faire le travail du développeur à sa place, et le développeur ne va pas non plus faire le travail de l’ingénieur de plateforme à sa place. La ligne de démarcation se dessine comme si un ingénieur de plateforme regardait… Si vous regardez la vision de l’ingénieur de plateforme de son travail quotidien, il s’assure que son environnement est opérationnel et stable, qu’il est capable de proposer des applications, que les utilisateurs sont capables de déployer des applications et de les faire fonctionner dans le cadre du SLA promis, et il regarde l’environnement dans son ensemble.</p>'}, {'', '<p>Mais le point de vue d’un développeur est qu’il dispose d’un ou de plusieurs espaces de noms et d’une disposition pour ceux-ci. Il y a une authentification. Sa vision est très étroite. Il se concentre sur son application. Il se concentre sur les performances de son application. Il se concentre sur la disponibilité de son application. Il se concentre sur la façon dont il peut effectuer des mises à jour de correctifs de code et des mises à jour de sécurité pour son application. Il s’assure que son application fonctionne conformément aux exigences de la plateforme. Elle est sécurisée, elle dispose de tous les derniers correctifs CVE, elle est capable d’obtenir les performances dont elle a besoin. Elle est sauvegardée à son niveau et tout ça.</p>'}, {'', '<p>Le champ d’action du développeur est donc très limité à son ensemble d’applications, mais un ingénieur de plateforme… un développeur ne va pas dire à l’ingénieur de plateforme : « Vous savez quoi ? Vous devez ajouter plus de nœuds à votre plateforme Kubernetes. »</p>'}, {'', '<p>Mitch Ashley\xa0:\xa0Ils ne veulent pas avoir à le faire.</p>'}, {'', '<p>Venkat Ramakrishnan\xa0: C’est le travail de l’ingénieur de la plateforme. Oui. Ils ne veulent pas avoir à le faire. Ils ne devraient pas le faire, car c’est invisible pour eux. Donc, ce que Kubernetes fournit essentiellement, c’est une infrastructure invisible pour les développeurs. Et pour l’ingénieur de la plateforme, l’ingénieur de la plateforme ne va pas aller voir le développeur pour lui dire\xa0: «\xa0Vous savez quoi\xa0? Vous devriez réécrire votre code, car c’est ainsi que fonctionne cette plateforme\xa0».</p>'}, {'', '<p>Mitch Ashley\xa0:\xa0Cela ne se passera pas bien s’ils le font.</p>'}, {'', '<p>Venkat Ramakrishnan\xa0: Exactement. Et nous savons tous que la plus grande inertie dans les entreprises, en particulier dans les entreprises de logiciels, est qu’aucun développeur ne veut réécrire son application parce qu’elle doit fonctionner pour une infrastructure différente. C’est ce que pensent de nombreuses entreprises d’infrastructure. Oh, j’ai une nouvelle API. Tous les développeurs d’applications vont réécrire ces applications pour les utiliser avec la nouvelle API.</p>'}, {'', '<p>Mitch Ashley\xa0:\xa0Bonne chance.</p>'}, {'', '<p>Venkat Ramakrishnan\xa0: C’est pourquoi Posit survit encore. Nous avons eu un débat houleux hier lors d’un de nos dîners communautaires. Les gens demandaient\xa0: «\xa0Pourquoi Posit existe-t-il toujours\xa0? Pourquoi Block existe-t-il toujours\xa0?\xa0» Parce qu’il existe des milliards d’applications qui utilisent ces interfaces et qui fonctionnent. Et je suppose qu’Object est une nouvelle interface et que nous la prenons en charge, mais bon, j’ai vu…</p>'}, {'', '<p>Mitch Ashley\xa0: Et personne ne va payer pour le déplacer. Pourquoi\xa0? Quelle est la valeur\xa0?</p>'}, {'', '<p>Venkat Ramakrishnan\xa0:\xa0Vous allez donc voir que l’ingénieur de plateforme ne peut pas dicter au développeur d’applications de dire\xa0: «\xa0Allez modifier votre application\xa0». C’est la raison pour laquelle je parle des bases de données. Vous pouvez proposer un ensemble de services de données, mais un développeur va intervenir, en particulier si vous créez une application new age, vous essayez d’apporter une nouvelle expérience numérique. Il va dire\xa0: «\xa0Regardez, je vais utiliser cette base de données à haut débit qui peut vraiment effectuer tant de transactions ou quelques centaines de nœuds, ou quelques centaines ou milliards d’utilisateurs.\xa0» Et le service de données que vous proposez peut ne pas être évolutif pour fournir cela.</p>'}, {'', '<p>C’est pourquoi nous avons créé ces services de données Portworx, car nous voulons offrir les bases de données les plus appréciées des développeurs et leur permettre de les exécuter facilement afin qu’ils n’aient pas à se battre avec la plateforme et l’équipe et tout cela pour savoir quels services de données ils doivent exécuter pour eux. Car encore une fois, les ingénieurs de plateforme ne sont pas des experts en bases de données. Et les administrateurs de bases de données ne sont pas des experts en plateforme. Nous voulions donc combiner cela et offrir cela aux équipes de bases de données, aux équipes d’application, pour savoir comment exécuter ces bases de données modernes et comment les simplifier. Encore une fois, c’est la séparation claire des responsabilités. J’espère que cela a répondu à votre question.</p>'}, {'', '<p>Mitch Ashley\xa0: Oui. Non, très bien. Parlez-nous un peu de ce que fait votre produit. Quelle est votre approche du marché ? Comment aidez-vous les ingénieurs de plateforme à faire leur part dans cet écosystème\xa0?</p>'}, {'', ""<p>Venkat Ramakrishnan\xa0: Oui. Comme je l’ai mentionné, notre produit est une plateforme de données complète. Elle comprend des services de stockage, de sauvegarde et de données. Nous continuons à l’ajouter et à l’étendre afin de pouvoir collaborer avec nos clients à pratiquement toutes les étapes du parcours. Par exemple, si votre client débute tôt, vous avez un client d’entreprise, vous exécutez un tas de charges de travail, mais bon, vous n’êtes pas prêt à exécuter des services de données. Vous n’êtes pas prêt à exécuter des applications critiques, mais vous exécutez une tonne d’applications dans votre… nous pouvons donc commencer par vous protéger les données, ou la protection des données Kubernetes, par exemple en vous permettant de sauvegarder et de restaurer vos clusters Kubernetes. Vous êtes toujours conforme à vos politiques de sécurité. Vous protégez votre Kubernetes ou les opérations de la plateforme contre les attaques de ransomware, et toutes les autres choses intéressantes. Mais nous pouvons partir de là et dire : « Vous savez quoi ? « Vous pouvez exécuter toutes vos applications critiques, y compris votre pile d'applications complète, vos bases de données, et obtenir une sauvegarde en cas de reprise après sinistre avec notre logiciel de gestion des données d'entreprise Portworx. »</p>""}, {'', '<p>Et troisièmement, vous pouvez ajouter à la plateforme et exécuter n’importe quelle base de données de votre choix à partir de notre catalogue, que vous pouvez immédiatement lancer et faire en sorte que vos équipes d’application l’utilisent. Notre mission première est de servir les équipes de plateforme, comme je l’ai mentionné. Nous vendons donc principalement à des équipes de plateforme. Si vous regardez tous nos gros clients qui gèrent des milliers de nœuds d’installations Portworx, ils sont tous principalement des équipes de plateforme. Mais presque toujours, les développeurs consomment cette plateforme, l’application. Nous considérons donc les développeurs comme une communauté d’utilisateurs qui exploitent réellement le service qui leur est proposé ou qui leur est fourni. Nous sommes l’équipe de la plateforme.</p>'}, {'', '<p>C’est un peu comme si vous étiez dans une entreprise, le service informatique vous propose un service et nous, les employés de l’entreprise, utilisons ce service informatique. Par exemple, il peut s’agir d’un e-mail, ou de quelque chose comme Slack. C’est la même chose. Une équipe de plateforme offre tout un tas de services à une communauté de développeurs qui créent des applications, et c’est ce qui fait tourner l’entreprise. Oui. Nous vendons donc aux équipes de plateforme, mais nous les consommons principalement par les développeurs d’applications.</p>'}, {'', ""<p>Mitch Ashley\xa0:\xa0Super. Et Portworx, s'agit-il d'une offre cloud SaaS ou est-ce que vous proposez également une offre sur site\xa0?</p>""}, {'', '<p>Venkat Ramakrishnan\xa0:\xa0Nous proposons des solutions sur site et SaaS. Honnêtement, nous avons constaté une croissance explosive dans les deux cas.</p>'}, {'', ""<p>Mitch Ashley\xa0:\xa0Et il faut vraiment être dans les deux pour servir les pièces centrales, n'est-ce pas\xa0?</p>""}, {'', '<p>Venkat Ramakrishnan\xa0: Exactement.</p>'}, {'', '<p>Mitch Ashley\xa0: Très bien. Les gens peuvent-ils tester les pneus comme avec le service cloud\xa0? Ou comment les gens peuvent-ils découvrir ce que vous pouvez faire avec Portworx\xa0?</p>'}, {'', '<p>Venkat Ramakrishnan\xa0: Oui, ils peuvent y aller… par exemple, nous avons une version Essentials de Portworx. Ils peuvent aller sur central.Portworx.com et télécharger notre version Essentials. Nous avons une sauvegarde en tant que service. Il existe une version gratuite de la sauvegarde avec laquelle ils peuvent jouer. Vous pouvez simplement vous inscrire, vous inscrire vous-même. Vous n’avez pas besoin de nous contacter. Vous pouvez simplement vous rendre sur un portail, vous inscrire vous-même. Le pointer vers votre cluster Kubernetes, votre espace de noms. Vous pouvez le sauvegarder autant que vous le souhaitez… nous avons des limites supérieures, mais c’est assez libéral. Ensuite, nous avons une version d’essai.</p>'}, {'', '<p>S’ils veulent jouer avec l’entreprise, avec toute la suite de la plateforme, ils peuvent télécharger une version d’essai. Les fonctionnalités sont illimitées, mais la durée de jeu et d’exécution est limitée à un certain nombre de jours. N’hésitez pas à nous contacter, nous serons ravis de discuter avec vous. Nous souhaitons nous associer au parcours de nos clients. Notre mission est de faciliter la vie des ingénieurs de la plateforme. De leur redonner du temps. De leur permettre de gérer la complexité à grande échelle, et de pouvoir fournir ces services et respecter leurs SLA tout en servant une large communauté de développeurs. Donc, encore une fois, nous sommes de leur côté. Ils sont nos super-héros et nous sommes là pour les servir.</p>'}, {'', '<p>Mitch Ashley\xa0: Je suis sûr qu’ils apprécient l’aide. Dites-nous à nouveau l’URL pour aller vérifier cela. L’adresse Web. Où vont-ils\xa0?</p>'}, {'', '<p>Venkat Ramakrishnan\xa0: Oui. C’est ww.Portworx.com, ww.Portworx.com. Ils peuvent également se rendre sur central.Portworx.com pour nous consulter. Il y a aussi une tonne d’aide sur docs.Portworx.com. Il y a donc une tonne de ressources. Et nous avons notre chaîne YouTube, qui est Portworx. Vous pouvez vous y abonner. Il y a une tonne de vidéos d’apprentissage. Si vous allez sur Portworx.com, il y a aussi des laboratoires sandbox avec lesquels ils peuvent jouer afin de pouvoir déployer et exécuter immédiatement. Il y a donc une tonne de ressources pour se mettre à niveau. Et vous pouvez toujours nous affronter sur slack.Portworx.com également, et venir poser des questions également.</p>'}, {'', '<p>Mitch Ashley\xa0: C’est toujours génial de pouvoir y aller, de l’essayer et de voir ce que ça donne. Comment ça marche. Il n’y a rien de tel que de mettre la main à la pâte. Eh bien, merci, Venkat. C’est un grand plaisir de discuter avec vous. J’espère que vous passerez un bon reste de l’émission.</p>'}, {'', '<p>Venkat Ramakrishnan\xa0:\xa0Ouais.</p>'}, {'', '<p>Mitch Ashley\xa0: Et si vous parlez de Kubernetes, vous êtes au bon endroit.</p>'}, {'', '<p>Venkat Ramakrishnan\xa0: Absolument. Je suis toujours heureux et enthousiaste d’être ici. Merci de m’avoir invité.</p>'}, {'', '<p>Mitch Ashley\xa0: Bien sûr.</p>'}, {'', '<p>Venkat Ramakrishnan\xa0: C’est toujours un plaisir de parler à Techstrong.</p>'}, {'', '<p>Mitch Ashley\xa0: Tu reviens toujours.</p>'}, {'', '<p>Venkat Ramakrishnan\xa0: Absolument.</p>'}, {'', '<p>Mitch Ashley\xa0: Oui, c’est un domaine en pleine évolution, n’est-ce pas\xa0?</p>'}, {'', ""<p>Venkat Ramakrishnan\xa0:\xa0C'est vrai. C'est vrai.</p>""}, {'', ""<p>Mitch Ashley\xa0:\xa0Et ça bouge vite aussi. C'est donc toujours bien de se tenir au courant de ce qui se passe.</p>""}, {'', '<p>Venkat Ramakrishnan\xa0:\xa0Et cette émission est devenue bien plus grande et bien plus fréquentée, n’est-ce pas\xa0? C’est une émission incroyable.</p>'}, {'', '<p>Mitch Ashley\xa0:\xa0C’est vraiment incroyable. C’est vraiment incroyable. Merci encore.</p>'}, {'', '<p>Venkat Ramakrishnan\xa0:\xa0Merci.</p>'}, {'', '<p>Mitch Ashley\xa0: Nous reviendrons avec d’autres excellentes interviews, tout comme avec Venkat, et nous parlerons de qui sait quoi d’autre. Nous verrons, mais je parie que cela aura quelque chose à voir avec Kubernetes ou le cloud ou les deux. Nous vous parlerons donc bientôt. Ne partez pas. Nous serons de retour dans quelques minutes.</p>'}]"
KubeCon 2023 : plateformes de développement internes et productivité des développeurs,"[{''}, {''}, {'', '<p>Mitch Ashley\xa0: Bonjour à tous. Bienvenue à Chicago pour KubeCon 2023. Nous en sommes au deuxième jour de notre diffusion en direct sur Techstrong TV sur tous nos sites, DevOps.com, Security Boulevard, Techstrong.TV et Cloud Native Now. Nous avons donc d’autres excellentes interviews. Alan vient d’en terminer quelques-unes et je suis en train de préparer Amit Gorvin. Enchanté de vous rencontrer.</p>'}, {'', '<p>Amit Eyal : Très heureux de vous rencontrer.</p>'}, {'', '<p>Mitch Ashley : Et tu es avec, dis le nom.</p>'}, {'', '<p>Amit Eyal\xa0:\xa0Kubiya AI.</p>'}, {'', '<p>Mitch Ashley : Kubiya AI, d’accord. Ça fait beaucoup de syllabes, mais d’accord. Je plaisante.</p>'}, {'', '<p>Amit Eyal : Nous faisons cela exprès pour semer la confusion.</p>'}, {'', '<p>Mitch Ashley : Vous savez quoi ? Une fois que vous l’entendez, c’est unique, n’est-ce pas ? Vous vous souvenez de ces [inaudible 00:00:45].</p>'}, {'', '<p>Amit Eyal : Cent pour cent.</p>'}, {'', '<p>Mitch Ashley : Exactement. D’accord, je sais que vous en avez déjà parlé et que vous avez parlé des personnes déplacées à l’intérieur du pays. C’est un domaine qui connaît une croissance très rapide, n’est-ce pas ? Tout le monde se concentre sur…</p>'}, {'', '<p>Amit Eyal : Absolument.</p>'}, {'', ""<p>Mitch Ashley : Comment pouvons-nous aider les développeurs, les ingénieurs de plateforme, les personnes qui travaillent ensemble pour simplifier la vie, le libre-service, ce genre de choses, les portails et tout ça. Je sais que vous avez fait quelques annonces récemment. Parlez-en, mais peut-être pourriez-vous expliquer ce que vous faites tous et pourquoi c'est un peu différent.</p>""}, {'', '<p>Amit Eyal : Bien sûr. Je pense que pour prendre un peu de recul, l’une des choses que beaucoup d’entreprises du secteur, y compris nous-mêmes, font, c’est de savoir comment créer un meilleur échange entre les développeurs et les DevOps, les opérateurs d’un côté, les développeurs de l’autre, où vous avez la voie royale vers la productivité. Les IDP, les plateformes de développement internes, sont connues dans les coulisses pour être le principal leader dans ce domaine pour débloquer la productivité des laboratoires.</p>'}, {'', '<p>L’une des choses que nous avons découvertes chez Kubiya, et que nous avons essentiellement essayé de résoudre, c’est que le libre-service n’est efficace que dans la mesure où l’interaction entre les utilisateurs et les outils est optimale. Et là où cela échoue souvent, c’est parce qu’il s’agit d’un échange unidirectionnel entre l’utilisateur et l’outil. Si vous échouez à l’authentification, échouez à l’autorisation, ne connaissez pas les variables que vous devez transmettre ou si quelque chose ne fonctionne pas comme prévu, vous avez une question, vous devez revenir en arrière et créer une file d’attente de tickets Jira pour pouvoir aller chercher l’aide d’un être humain.</p>'}, {'', '<p>Donc, ce qui a porté ses fruits l’année dernière, et bien évidemment ChatGPT a fait de la sensibilisation sur le marché pour cela, c’est l’IA conversationnelle, un échange bidirectionnel entre les machines et les humains. Nous avons marié ces deux mondes. Nous avons créé cet échange entre vos utilisateurs, développeurs et opérateurs avec les outils de manière à ce que vous puissiez avoir une conversation complète avec eux.</p>'}, {'', ""<p>Vous pouvez lancer un déploiement Kubernetes, il vous demandera combien vous avez répliqué. Vous pouvez l'augmenter ou le réduire. Vous pouvez l'utiliser pour votre file d'attente de tickets Jira, par exemple, et lui poser des questions sur qui possède quoi, où ils sont affectés à la création, à la notation, tout ce que vous seriez capable de faire en tant qu'humain, mais dans ce cas, vous avez juste une conversation. Parce qu'imaginez avoir de l'autre côté, quelqu'un comme Fred, John, Bob sur le canal d'astreinte qui a une conversation avec vous. Nous remplaçons cela par l'assistant virtuel.</p>""}, {'', '<p>Mitch Ashley : Intéressant. Donc, remplacer le modèle d’assistant virtuel. Il semble que vous décriviez également, si j’ai bien compris, que le personnel technique du développeur peut s’interfacer via ChatGPT pour dire… Je veux dire, ce qui est bien avec ChatGPT, c’est que vous pouvez lui parler comme à un langage de programmation, juste en logique conversationnelle, n’est-ce pas ? Je veux que vous créiez cela pour moi avec bla, bla, bla. Est-ce que cela fait partie de ce que vous dites ? Vous pouvez lui donner des instructions ou des demandes ?</p>'}, {'', '<p>Amit Eyal : Une petite information, nous n’utilisons pas ChatGPT. C’est une expérience de type ChatGPT.</p>'}, {'', '<p>Mitch Ashley : Je vois.</p>'}, {'', '<p>Amit Eyal : C’est probablement la meilleure façon d’ancrer les gens dans-</p>'}, {'', ""<p>Mitch Ashley : J'ai sauté dessus.</p>""}, {'', '<p>Amit Eyal : C’est très bien. Nous utilisons environ huit modèles de langage différents, à la fois grands et petits. Nous utilisons en fait un élément d’IA conversationnelle avec GPT-4 pour Azure, juste pour information, mais c’est différent. L’expérience est similaire à celle de ChatGPT. Sauf qu’ici, ce sont vos outils, vos programmes, vos scripts, essentiellement un IDP personnalisé, si vous pouvez, pour chaque personne de l’organisation, chaque individu de l’organisation.</p>'}, {'', '<p>Mitch Ashley : D’accord. Alors, utilisez-vous des modèles de langage génératifs d’IA à grande échelle, ou s’agit-il simplement de l’expérience que vous proposez avec la technologie que vous utilisez ?</p>'}, {'', '<p>Amit Eyal : Certainement. Les grands modèles de langage permettent cet échange naturel, essentiellement les avantages des grands modèles de langage et de la partie générative que nous utilisons. Mais nous les exploitons également et les étayons avec des systèmes basés sur des règles, avec des pratiques d’ingénierie solides. C’est ainsi que nous obtenons l’avantage d’un résultat attendu d’un côté. Nos utilisateurs s’attendent à un résultat attendu. Essentiellement, il faut que ce qu’ils obtiennent de l’autre côté soit déterministe.</p>'}, {'', ""<p>Mitch Ashley : Je suis content que vous ayez dit ça, car je pensais à ChatGPT. C'est sympa, mais vous n'obtenez pas la même réponse, le même résultat. Il n'est pas forcément garanti que ce soit cohérent, précis. Donc, cela a du sens du point de vue de l'échafaudage du backend.</p>""}, {'', '<p>Amit Eyal : Et il existe des systèmes d’autorisation et de règles en place pour exploiter le LLM afin qu’il fonctionne comme vous l’attendez.</p>'}, {'', '<p>Mitch Ashley : Très bien. Vous avez parlé de l’expérience du développeur, et je comprends parfaitement la dissonance cognitive entre le fait de travailler dans mon IDE et de faire mon truc de développeur, et puis je dois passer par ce système de billetterie ou par un portail d’exploitation bizarre qui n’a pas été conçu pour les développeurs, mais je peux quand même l’utiliser dans un sens très large.</p>'}, {'', ""<p>Amit Eyal : C'est peut-être vrai, mais cela dépend uniquement de la façon dont il a été entretenu, maintenu ou de ce que vous y avez réellement mis. Vous devez donc toujours le programmer pour qu'il agisse comme vous l'attendez.</p>""}, {'', '<p>Mitch Ashley : Et cela fait partie du flux aussi, n’est-ce pas ?</p>'}, {'', '<p>Amit Eyal : Ouais.</p>'}, {'', '<p>Mitch Ashley : Je ne connais peut-être pas les réponses aux 10 questions au début de l’interaction, mais je dois commencer et terminer à un moment donné du processus. Mais il s’agit du flux de travail du développeur, pas du formulaire à remplir. N’est-ce pas ?</p>'}, {'', '<p>Amit Eyal : Cent pour cent.</p>'}, {'', ""<p>Mitch Ashley : Super. Alors, l'annonce, résumons-la à nouveau pour nous car j'ai marché dessus avec le truc GPT.</p>""}, {'', '<p>Amit Eyal : Bien sûr, oui. Non, ça va. Nous avons donc lancé l’année dernière notre première plateforme, qui ressemblait davantage à la prochaine génération d’automatisation des flux de travail, car elle visait davantage à connecter les gens au monde de l’IA conversationnelle dans le monde des IDP. À partir de là, et en particulier avec l’évolution des grands modèles linguistiques, nous avons adopté une approche basée sur les agents, dans laquelle nous utilisons différents modèles et différents agents formés sur différents modèles afin de cartographier efficacement différentes fonctions au sein d’une transaction, presque comme le cerveau humain et les lobes interagissant les uns avec les autres.</p>'}, {'', '<p>Ainsi, lorsque vous interagissez avec le système, selon la manière dont vous l’abordez, cela peut être : montrez-moi comment je fais quelque chose. Par exemple, configurez un VPN. Cela peut être dans vos documents. Nous utilisons donc évidemment les techniques RAG pour entrer dans un train sur votre base de connaissances. Mais si vous allez exécuter une requête sur une source de données ou une base de données, ou peut-être me montrer tous mes tickets Jira pour le sprint en cours pour certains… C’est un modèle de langage différent qui interagit avec vous.</p>'}, {'', ""<p>Et puis, si vous voulez créer un Terraform, créer un déploiement Kubernetes, détruire Terraform, c'est une base d'action qui doit également vérifier les autorisations. Nous utilisons un agent de politique ouvert. Nous utilisons également toutes les techniques de masquage des données, et nous avons évidemment l'orchestration, une machine d'état qui va orchestrer tout cela avec la logique métier de l'organisation. Donc, il se passe beaucoup de choses en coulisses.</p>""}, {'', '<p>Mitch Ashley : Il y en a. Eh bien, il y a beaucoup de demandes différentes de la part des utilisateurs.</p>'}, {'', '<p>Amit Eyal : Pour l’utilisateur, c’est transparent. Vous avez un emoji cérébral qui réfléchit et vous lit ensuite ce qu’il fait. C’est donc une interaction très amusante, une interaction humaine, mais en coulisses, il se passe beaucoup de choses.</p>'}, {'', ""<p>Mitch Ashley : Très bien. Est-ce que vous intégrez les IDE et allez jusqu'au bout ou est-ce un portail séparé ?</p>""}, {'', '<p>Amit Eyal : Les utilisateurs peuvent intégrer leurs IDE. Ils peuvent se connecter. Nous avons un SDK et tout est basé sur une API ouverte. Donc, en gros, vous pouvez vous connecter à n’importe quel outil.</p>'}, {'', '<p>Mitch Ashley : Je pense que c’est vraiment sympa et important parce que d’une part, c’est pratique. Et aussi, si vous fournissez des informations que vous pouvez fournir sous forme de code structuré ou auto-rempli, vous pouvez imiter les comportements de ce que vous faites dans un IDE qui… Très naturel. Donc, je n’ai pas besoin d’aller chercher une API ou ce qu’est cet appel ; c’est juste là. Ou je n’ai pas besoin d’aller chercher comment cette chose s’appelle dans ma requête ; c’est juste là.</p>'}, {'', '<p>Amit Eyal : D’accord. Je pense donc que le seul point que j’aimerais souligner est que les gens envisagent l’approche copilote ou l’approche ChatGPT comme un outil de complétion de code. Ce que nous faisons, c’est que nous créons un outil de complétion des opérations. Essentiellement, toute la prochaine étape consiste à savoir ce que vous feriez, comment déployer le code, exécuter vos CI/CD et le maintenir pour les opérations du deuxième jour, c’est donc là que vous pouvez penser à combler cette lacune.</p>'}, {'', ""<p>Mitch Ashley : Ok, intéressant. Alors, félicitations pour l'annonce.</p>""}, {'', '<p>Amit Eyal : Merci.</p>'}, {'', '<p>Mitch Ashley : C’est fantastique. Et à chaque fois que vous venez, vous avez de nouvelles choses en cours et qui se passent. Je suis curieux, peut-être en avez-vous parlé à Alan ou à Mike, votre histoire, vous êtes PDG, fondateur de l’entreprise. Comment avez-vous commencé à vous engager sur cette voie ? Était-ce parce que je faisais cela dans une grande entreprise et que c’était comme un énorme besoin, je vais aller de l’avant ou ?</p>'}, {'', '<p>Amit Eyal : C’est une excellente question. La première fois que nous y sommes allés, c’était moi et mon cofondateur, qui est lui-même un opérateur qui a construit une plateforme en libre-service pour son équipe de plateforme dans sa précédente entreprise chez Bluevine, et je venais d’AWS où je gérais les partenariats pour tous les DevOps et DevSecOps-</p>'}, {'', '<p>Mitch Ashley : Joli couple.</p>'}, {'', '<p>Amit Eyal : Les acteurs de premier plan. J’ai donc vu qu’ils avaient peut-être des difficultés avec certains de ces services en libre-service, et j’ai essayé de les aider. Et c’est là que la thèse du côté commercial, selon laquelle il s’agit d’un problème beaucoup plus vaste et que nous avons résolu ce problème nous-mêmes, s’est concrétisée.</p>'}, {'', '<p>Mitch Ashley : Intéressant. Parlez-nous un peu de votre expérience en matière de partenariat. Est-ce que cela s’est concrétisé en tant qu’élément important de l’entreprise ? J’imagine que lorsque vous intégrez de nombreuses choses, vous pouvez simplement consulter l’API. Vous souhaitez établir des relations, des partenariats avec d’autres entreprises.</p>'}, {'', '<p>Amit Eyal : Nous disposons donc d’un écosystème très solide de partenaires qui souhaitent vraiment discuter. J’ai vécu essentiellement cette expérience d’assistant sur leur plateforme. Et en ce moment, nous laissons entrer dans la file d’attente quelques-uns des partenaires stratégiques avec lesquels nous pensons pouvoir avoir une forte adéquation, mais aussi la maintenir. Au fur et à mesure que nous agrandissons notre équipe, nous allons avoir un écosystème beaucoup plus large. Nous avons déjà probablement 10 à 15 acteurs très forts dans le secteur qui souhaitent établir un partenariat. Je pense donc que cela ne fera que croître à partir de ce moment-là.</p>'}, {'', '<p>Mitch Ashley : Bien, bien. Amit, tu le sais déjà. J’ai appris cette leçon sur l’un des produits sur lesquels j’ai travaillé, lorsque tu n’as pas conçu ton produit en pensant à un écosystème de partenaires. Tu arrives à un point où tu te dis : « OK, nous ne pouvons pas faire ce que nous voulons faire. Nous devons revenir en arrière et ajouter cela. Donc, en ayant cette perspective en amont, si c’est un endroit où tu veux aller à un moment donné du cycle de vie, c’est… »</p>'}, {'', '<p>Amit Eyal : C’est exactement ça. Notre écosystème est conçu pour être un dialogue bidirectionnel avec tous les outils. Ainsi, si nos clients le demandent, il n’y a aucune raison pour que le fournisseur de l’autre côté n’ait pas une meilleure adéquation afin que nous puissions offrir une meilleure expérience utilisateur pour les deux parties.</p>'}, {'', '<p>Mitch Ashley : Je suis curieux, vendez-vous vos produits à la communauté du développement technique ? Vendez-vous davantage à l’ingénierie de plateforme et aux professionnels des outils DevOps opérationnels ? Sur quoi avez-vous tendance à vous concentrer ?</p>'}, {'', '<p>Amit Eyal : Je dirais que la décision est généralement prise conjointement. Les équipes de plateforme doivent généralement se mettre d’accord avec les responsables de l’ingénierie. Ainsi, qu’il s’agisse d’un responsable de l’ingénierie qui nous présente à l’équipe de la plateforme ou d’une équipe de la plateforme qui nous présente au responsable de l’ingénierie, nous voyons généralement les deux parties impliquées au moins dans la validation. Mais le moyen le plus simple de commencer est vraiment de travailler avec les équipes de la plateforme, car ce sont elles qui sont généralement les utilisateurs expérimentés du système. Ce sont elles qui créent et échafaudent les configurations de ces systèmes, il est donc plus facile pour elles de commencer, de les tester en interne sur elles-mêmes. Et très facilement, nous voyons généralement qu’elles le déploient auprès du reste de l’équipe d’ingénierie.</p>'}, {'', '<p>Mitch Ashley : Ils ont déjà une bonne idée du type de demandes qui leur sont adressées.</p>'}, {'', '<p>Amit Eyal : Précisément.</p>'}, {'', '<p>Mitch Ashley : Et cela rendrait cela plus facile pour les gens.</p>'}, {'', '<p>Amit Eyal : Sinon, l’inverse serait de me laisser faire et de mettre en place une équipe de plateforme. C’est donc une approche plus directe [inaudible 00:13:09].</p>'}, {'', ""<p>Mitch Ashley : C'est logique et c'est bien. Je peux tout à fait comprendre. Cela commence par : « Eh bien, quelque chose pourrait faciliter le fait de se soulager d'une charge...</p>""}, {'', '<p>Amit Eyal : Précisément.</p>'}, {'', ""<p>Mitch Ashley : L'équipe d'ingénierie de la plateforme » ou eux disant : « Hé-</p>""}, {'', '<p>Amit Eyal : Nous sommes là pour les aider.</p>'}, {'', '<p>Mitch Ashley : … c’est une façon plus simple de travailler avec ça. » N’est-ce pas ?</p>'}, {'', '<p>Amit Eyal : Nous sommes là pour les renforcer. Tout le monde n’a pas le luxe d’embaucher trois à cinq personnes supplémentaires l’année prochaine, mais si vous pouviez le faire avec une ou deux personnes et ensuite Kubiya, vous pourriez maintenant évoluer.</p>'}, {'', '<p>Mitch Ashley : Super. Si vous regardez un peu en avant, peut-être dans les six prochains mois ou peut-être dans les douze prochains mois, six mois environ, que se passe-t-il dans l’industrie, peut-être vers où vous vous dirigez, à quoi pensez-vous ensuite ?</p>'}, {'', '<p>Amit Eyal : Les dernières annonces d’OpenAI sont très intéressantes, car de notre point de vue, elles nous donnent encore plus d’outils pour atteindre notre objectif final, qui est, selon moi, un opérateur autonome. Un opérateur autonome qui sera capable de retirer des tâches de bout en bout aux êtres humains et de leur permettre de faire le meilleur usage possible de leur temps. Cela nécessitera toujours une interaction humaine dans la boucle et nous ne nous attendons pas à ce que cela se fasse du jour au lendemain. Mais je pense que beaucoup de choses qui ont été annoncées récemment sont déjà en phase avec ce que nous avons fait, mais cela ne fait que faciliter les choses et ouvrir de plus en plus de cas d’utilisation.</p>'}, {'', '<p>Mitch Ashley : Il semble qu’OpenAI ait fait un travail plutôt décent : ne jetez pas tout là-bas, ajoutez progressivement des fonctionnalités lorsque vous êtes prêt à les gérer.</p>'}, {'', '<p>Amit Eyal : Oui. Comme nous sommes dans ce domaine depuis quelques années, nous avons déjà réfléchi à la manière dont notre plateforme sera nativement conversationnelle. Cela nous permet d’évoluer très rapidement dans ce domaine.</p>'}, {'', '<p>Mitch Ashley : Super. Eh bien, dites aux gens où ils peuvent aller le voir ?</p>'}, {'', '<p>Amit Eyal : Absolument.</p>'}, {'', '<p>Mitch Ashley : Peuvent-ils obtenir une démonstration en ligne ou que pouvons-nous faire pour les aider ?</p>'}, {'', '<p>Amit Eyal : C’est une belle partie. Nous ouvrons notre plateforme en libre-service. Vous pouvez donc vous inscrire sur Kubiya.ai, www.kubiya.ai. Inscrivez-vous pour un essai gratuit. Nous avons également un sandbox où vous pouvez, sans avoir besoin de saisir les identifiants de votre organisation, quoi que ce soit. C’est une expérience sécurisée. Évidemment, c’est un peu plus limité, mais vous pouvez aller échanger avec Kubi, notre assistant DevOps, et vraiment profiter de l’expérience avant de vous engager à participer à un essai.</p>'}, {'', '<p>Mitch Ashley\xa0: Appréciez le bac à sable. Vous ne voulez pas encore l’utiliser sur votre infrastructure. Examinons-le et mettons-le en pratique.</p>'}, {'', ""<p>Amit Eyal : Il s'agit principalement de lecture seule. Nous avons quelques actions d'écriture, mais en général, nous voulons nous assurer que le contenu est classé PG pour ce que les gens peuvent faire.</p>""}, {'', ""<p>Mitch Ashley : Exactement. Exactement. Super. Et ça s'écrit K-U-B-I-Y-A.ai.</p>""}, {'', '<p>Amit Eyal : Oui, oui.</p>'}, {'', '<p>Mitch Ashley : Super.</p>'}, {'', '<p>Amit Eyal : Merci beaucoup.</p>'}, {'', ""<p>Mitch Ashley : C'était sur le... Je suis sûr que c'était sur ton titre mais-</p>""}, {'', '<p>Amit Eyal : Oui, il serait là.</p>'}, {'', ""<p>Mitch Ashley : Amit, c'est un plaisir de discuter avec vous et félicitations pour les nouveaux progrès, la nouvelle version et les nouvelles fonctionnalités, vers quoi vous vous dirigez maintenant. Un marché formidable à conquérir.</p>""}, {'', ""<p>Amit Eyal : Je l'apprécie et j'apprécie que vous ayez passé du temps ici aujourd'hui.</p>""}, {'', '<p>Mitch Ashley\xa0: Bien sûr. C’est un plaisir de vous avoir parmi nous. Pour nos amis, pour les personnes présentes dans le public, Amit représente certaines des grandes choses qui se produisent non seulement à KubeCon mais aussi dans notre secteur, et les progrès qui se produisent en général. Alors, restez à l’écoute. Nous avons d’autres excellentes interviews à venir. Ne partez pas. Nous serons là dans quelques minutes. Alors, nous reviendrons.</p>'}]"
Améliorer l'expérience des développeurs (DevEx) dans l'ingénierie des plateformes,"[{'', ""<p>L'expérience du développeur (DevEx) est un élément indispensable qui définit le parcours des développeurs de logiciels. Elle englobe un éventail de facteurs, allant des outils et processus utilisés par les développeurs à l'éthique culturelle et à l'environnement de travail dans lequel ils évoluent. Dans le domaine de l'ingénierie des plateformes, DevEx constitue un élément essentiel qui non seulement amplifie la productivité des développeurs, mais accélère également le calendrier de livraison des logiciels.</p>""}, {'', ""<h3>Comprendre l'ingénierie des plateformes</h3>""}, {'', ""<p>L'ingénierie de plateforme est une approche du développement logiciel qui vise à construire une plateforme adaptée aux besoins d'une myriade d'équipes au sein d'une organisation. Cette plateforme fonctionne comme un socle, fournissant une suite d'outils et de services qui permettent aux développeurs de créer et de déployer leurs applications. L'essor de l'ingénierie de plateforme découle de son plaidoyer en faveur d'une plateforme de développement gérée de manière centralisée, éliminant ainsi efficacement la nécessité pour des équipes disparates de construire et de gérer leurs propres plateformes. Cette approche garantit la rationalisation des tâches d'infrastructure critiques telles que la sécurité, la gouvernance et l'observabilité, favorisant l'efficacité en limitant les efforts en double.</p>""}, {'', ""<h3>Importance de DevEx dans l'ingénierie des plateformes</h3>""}, {'', '<p>DevEx revêt une importance primordiale dans l’ingénierie des plateformes en raison de son rôle catalyseur dans l’amélioration de la productivité des développeurs et la réduction des délais de livraison des logiciels. En fournissant une plateforme de développement interne préconfigurée et en définissant un « parcours d’or » clair pour les équipes DevOps, l’ingénierie des plateformes répond à la complexité labyrinthique et aux lacunes de compétences qui prévalent dans les configurations traditionnelles. Par conséquent, la main-d’œuvre devient moins dépendante des subtilités de l’infrastructure, orchestrant un processus unifié pour la gestion de la flotte d’une organisation, éliminant les silos opérationnels et minimisant les efforts redondants.</p>'}, {'', ""<h3>Stratégies pour augmenter le DevEx dans l'ingénierie des plateformes</h3>""}, {'', ""<p>L'élévation du DevEx dans l'ingénierie des plateformes nécessite une approche à multiples facettes :</p>""}, {'', ""<p>1. Automatisation : Le rôle central de l’automatisation dans l’ingénierie des plateformes ne peut être surestimé. Des outils tels que Puppet, Chef et Ansible jouent un rôle essentiel dans la réduction des erreurs manuelles et la rationalisation du processus de développement en automatisant la gestion de l’infrastructure et de la configuration. 2. Évolution des rôles de DevEx et d’ingénieur de plateforme : L’évolution du paysage des organisations d’ingénierie souligne l’importance accrue accordée au renforcement de l’expérience des développeurs par la construction ou le perfectionnement de plateformes de développeurs internes. On s’attend à ce que les titres se métamorphosent pour refléter avec précision l’évolution des responsabilités de ces personnes. 3. Abstraction de l’infrastructure pour les développeurs : Le mouvement dominant vers le « glissement vers la gauche » a obligé les développeurs à se débattre avec un éventail croissant d’outils et de flux de travail complexes. En 2024, l’accent sera probablement mis sur l’abstraction de l’infrastructure pour les développeurs, les libérant des subtilités des plateformes et des ensembles d’outils sous-jacents. 4. Implémentations rationalisées du catalogue de services : Il est prévisible qu’un effort concerté soit déployé pour rendre les implémentations du catalogue de services plus conviviales. Cette initiative vise à fournir aux organisations la capacité d'offrir une expérience utilisateur améliorée à leur clientèle. 5. Montée en puissance des équipes de plateformes dédiées : Dans les années à venir, il y aura probablement une augmentation explosive de la création d'équipes de plateformes dédiées au sein des organisations d'ingénierie logicielle. Ces équipes sont prêtes à servir de fournisseurs internes de services, de composants et d'outils réutilisables adaptés à la livraison d'applications. Cette augmentation du nombre d'équipes de plateformes dédiées contribuera à rationaliser et à normaliser les processus dans les organisations, favorisant ainsi une plus grande efficacité et une plus grande cohésion dans les pratiques de développement de logiciels. 6. Partage des connaissances pour les outils et les plans directeurs : Dans un paysage regorgeant d'environ 2 000 outils dans le domaine du cloud natif, discerner les combinaisons d'outils optimales devient une tâche ardue. L'année à venir devrait voir une augmentation des normes et des implémentations de référence partagées au sein de la communauté, aidant les organisations à traiter leur plateforme comme un produit sur mesure parfaitement adapté aux exigences uniques de leurs développeurs.</p>""}, {'', '<p>Il est impératif de noter que la priorisation de ces tendances reste subjective, dépendante des besoins et aspirations distincts de chaque organisation.</p>'}, {'', '<h3>Conclusion</h3>'}, {'', ""<p>DevEx est une pierre angulaire de l'ingénierie des plateformes, qui permet de catalyser l'efficacité des développeurs et d'accélérer la livraison des logiciels. L'automatisation, l'évolution des rôles, l'abstraction de l'infrastructure, les catalogues de services raffinés, l'essor des équipes de plateformes dédiées et le partage des connaissances pour les outils et les plans directeurs constituent des moyens essentiels pour renforcer DevEx dans l'ingénierie des plateformes. Les organisations doivent considérer leur plateforme comme un produit sur mesure parfaitement adapté aux besoins spécifiques de leurs développeurs.</p>""}]"
Les avantages de l’infrastructure en libre-service,"[{'', ""<p>L'infrastructure en libre-service est une approche moderne de la gestion informatique qui permet aux équipes de développement de mieux contrôler leurs systèmes et services sans avoir recours à un personnel informatique dédié pour obtenir de l'aide. Cette approche offre de nombreux avantages qui aident les entreprises à devenir plus efficaces et plus agiles.</p>""}, {'', ""<h3>Qu'est-ce que l'infrastructure en libre-service ?</h3>""}, {'', ""<p>L'infrastructure en libre-service est un type de système d'infrastructure informatique qui permet aux utilisateurs d'accéder à leurs propres applications, données et ressources, de les gérer et de les entretenir sans avoir recours à un fournisseur externe. Elle combine des outils d'automatisation logicielle, des services de cloud computing, des technologies open source, des solutions de gestion de réseau et d'autres outils numériques pour fournir un environnement dans lequel les utilisateurs peuvent interagir avec la technologie selon leurs propres conditions, mais dans des limites définies (créées, par exemple, pour des raisons de sécurité).</p>""}, {'', '<p>L’infrastructure en libre-service a contribué à transformer les modèles traditionnels de prestation de services en augmentant l’autonomie de l’équipe de développement lorsqu’elle utilise la technologie à diverses fins. Grâce à l’infrastructure en libre-service, il peut être plus facile pour les entreprises de réduire les coûts associés à la maintenance des systèmes informatiques tout en améliorant la vitesse à laquelle les opérations peuvent être effectuées, car la plupart des tâches ne nécessitent pas d’intervention manuelle.</p>'}, {'', ""<p>Certaines plateformes d'infrastructure en libre-service incluent AWS Service Catalog, Google Cloud Deployment Manager, Spacelift et Backstage.</p>""}, {'', ""<p>L'infrastructure en libre-service présente quelques difficultés. La mise en place d'une architecture en libre-service efficace nécessite souvent une certaine courbe d'apprentissage. Les plateformes logicielles étant en constante évolution en raison des mises à jour ou de la sortie de nouvelles versions, il peut être difficile pour les administrateurs de se tenir au courant des changements pour garantir le bon fonctionnement de leurs systèmes. Mais l'infrastructure en libre-service offre également un certain nombre d'avantages pour compenser ces difficultés.</p>""}, {'', '<h3>Quels sont les avantages de l’infrastructure en libre-service ?</h3>'}, {'', '<p>Premièrement, l’infrastructure en libre-service offre aux équipes la flexibilité dont elles ont besoin pour développer, déployer et faire évoluer rapidement les applications selon leurs besoins. Elle élimine la charge de travail non essentielle des équipes de développement, ce qui leur permet de travailler plus rapidement et plus efficacement avec moins de ressources. Par exemple, elles n’ont pas besoin de se soucier de Terraform, mais disposent d’un service où elles sélectionnent des modèles qui répondent à leurs besoins.</p>'}, {'', ""<p>Cela permet de rationaliser les processus de livraison, réduisant ainsi le temps nécessaire aux projets ou aux applications, de la conception à la mise en production. L'infrastructure en libre-service permet d'accélérer les cycles de développement des produits et de réduire les risques en diminuant le temps global consacré au débogage et aux tests de l'infrastructure tout au long des étapes de développement. Elle supprime entièrement la gestion de l'infrastructure des équipes de développement, même lorsqu'elle est automatisée, éliminant ainsi les erreurs humaines potentielles qui pourraient entraîner des temps d'arrêt coûteux ou des problèmes de sécurité à l'avenir.</p>""}, {'', ""<p>De plus, lorsqu'il est combiné avec des outils d'automatisation et des meilleures pratiques DevOps telles que l'intégration continue/livraison continue (CI/CD), il permet d'automatiser des tâches encore plus fastidieuses, rationalisant ainsi davantage les délais de livraison globaux des produits sans sacrifier les mesures de contrôle qualité déjà en place.</p>""}, {'', '<p>En termes de collaboration, l’infrastructure en libre-service contribue à promouvoir la transparence dans l’ensemble des organisations en offrant une visibilité sur les analyses d’utilisation tout en permettant aux équipes une plus grande autonomie sur l’utilisation des actifs, ce qui augmentera les niveaux de productivité entre les départements.</p>'}, {'', '<p>Cela permet également d’établir de meilleurs canaux de communication entre les équipes d’ingénierie logicielle responsables des versions d’applications, car elles peuvent déployer des mises à jour en temps opportun grâce à l’automatisation. Il n’est pas nécessaire de longues périodes d’attente en raison de retards dans les approbations des fournisseurs ou d’autres parties prenantes, comme lors des configurations de processus manuels traditionnels.</p>'}, {'', '<p>Enfin, l’un des principaux avantages de l’infrastructure en libre-service réside dans ses capacités d’évolutivité. Que votre entreprise ait besoin d’une capacité pour quelques milliers d’utilisateurs ou pour plus de cent mille utilisateurs, elle peut gérer n’importe quelle évolutivité dont vous avez besoin à tout moment, de sorte que vous ne soyez jamais laissé pour compte en raison d’un manque de ressources informatiques lorsque la demande augmente de manière inattendue.</p>'}, {'', '<h3>Pour conclure</h3>'}, {'', ""<p>L'infrastructure en libre-service offre aux entreprises de nombreuses possibilités de moderniser facilement leurs opérations informatiques. Grâce à des fonctionnalités telles que l'évolutivité, l'automatisation et le développement agile, les entreprises peuvent bénéficier de la possibilité de déployer rapidement et efficacement des applications.</p>""}, {'', '<p>L’infrastructure informatique en libre-service réduit non seulement les coûts opérationnels, mais offre également un meilleur contrôle sur les ressources et permet aux organisations d’exploiter plus efficacement leurs ressources limitées.</p>'}]"
Halloween DevOps : trucs et friandises,"[{'', '<p>Le monde de DevOps est comme un labyrinthe, rempli de choix à chaque tournant. Certains chemins mènent à l’efficacité et au succès, tandis que d’autres peuvent conduire à des défis et des retards inattendus. Dans l’esprit d’Halloween, explorons les astuces et les friandises des choix DevOps pour garantir que votre équipe se retrouve avec un sac rempli de friandises plutôt que de mauvais tours.</p>'}, {'', '<h3>Contrôle des versions : les secrets de la cohérence, les astuces de la complexité</h3>'}, {'', '<p>Traitement : la mise en œuvre d’un système de contrôle de version robuste est essentielle pour toute équipe DevOps. Des outils comme Git offrent un moyen fiable de suivre les modifications, de collaborer sur le code et de conserver un historique de l’évolution de votre projet.</p>'}, {'', ""<p>Astuce : Cependant, la complexité de ces systèmes peut conduire à des confusions et des erreurs si elle n'est pas correctement comprise. Les stratégies de ramification, par exemple, doivent être clairement définies pour éviter les fusions chaotiques et les pertes de travail.</p>""}, {'', '<h3>Intégration continue et déploiement continu (CI/CD) : des solutions rapides, des configurations délicates</h3>'}, {'', ""<p>Traitement : les pipelines CI/CD automatisent le processus d'intégration, de test et de déploiement du code, accélérant ainsi les cycles de publication et garantissant des logiciels plus fiables.</p>""}, {'', '<p>Astuce\xa0: la configuration de ces pipelines peut être complexe et sujette aux erreurs. Les erreurs de configuration peuvent entraîner des échecs de build, des retards de publication ou même le déploiement de code bogué en production.</p>'}, {'', ""<h3>Conteneurisation : la douceur de l'isolement, attention aux frais généraux</h3>""}, {'', ""<p>Traitement\xa0: les conteneurs fournissent des environnements isolés pour les applications, garantissant la cohérence entre le développement, les tests et la production. Des outils comme Docker et Kubernetes ont révolutionné le développement et le déploiement d'applications.</p>""}, {'', ""<p>Astuce\xa0: la conteneurisation ajoute une couche supplémentaire de complexité à votre infrastructure. Une mauvaise gestion des conteneurs peut entraîner une inefficacité des ressources, et Kubernetes lui-même présente une courbe d'apprentissage abrupte.</p>""}, {'', '<h3>Surveillance et journalisation : le danger de la visibilité, le piège de la surcharge</h3>'}, {'', '<p>Traitement : une surveillance et une journalisation complètes donnent aux équipes une visibilité sur les performances et le comportement du système, permettant une résolution proactive des problèmes et une optimisation des performances.</p>'}, {'', ""<p>Astuce : le volume de journaux et de mesures peut être écrasant. Sans outils et stratégies de filtrage et d'analyse appropriés, des informations importantes peuvent être perdues dans le bruit.</p>""}, {'', '<h3>Infrastructure-as-Code (IaC) : une automatisation douce, une complexité amère</h3>'}, {'', ""<p>Traitement : les outils IaC comme Terraform et AWS CloudFormation permettent aux équipes d'automatiser et de gérer les versions de la configuration de l'infrastructure, garantissant ainsi la cohérence et réduisant les erreurs manuelles.</p>""}, {'', ""<p>Astuce\xa0: les scripts IaC peuvent devenir complexes et difficiles à maintenir. Les erreurs dans ces scripts peuvent entraîner une mauvaise configuration de l'infrastructure, des problèmes de sécurité potentiels et un gaspillage de ressources.</p>""}, {'', ""<h3>Collaboration et communication : les pièges du travail en équipe, les pièges de l'incompréhension</h3>""}, {'', '<p>Traitement : DevOps met l’accent sur l’importance de la collaboration entre les équipes de développement et d’exploitation, favorisant une culture de responsabilité partagée et d’amélioration continue.</p>'}, {'', '<p>Astuce : La mauvaise communication et le manque d’alignement entre les équipes peuvent entraîner des inefficacités, des erreurs et une rupture du processus collaboratif.</p>'}, {'', '<h3>Sécurité : le spectre invisible</h3>'}, {'', '<p>Traitement\xa0: DevSecOps permet un «\xa0shift left\xa0» en matière de sécurité, en intégrant des contrôles et des pratiques de sécurité dès le début du cycle de vie du développement, garantissant ainsi des applications plus sûres et plus sécurisées.</p>'}, {'', '<p>Astuce : Mais cette intégration nécessite une attention et une maintenance continues. Des dépendances obsolètes, des paramètres mal configurés et des pratiques de sécurité inadéquates peuvent rendre vos applications vulnérables, transformant le spectre invisible des problèmes de sécurité en une réalité effroyable.</p>'}, {'', ""<p>Et vous ? Quels domaines de DevOps sont propices à la chasse aux bonbons cette saison d'Halloween ?</p>""}, {'', '<p>Dans l’ensemble, DevOps offre une multitude d’avantages, des versions plus rapides à la collaboration améliorée en passant par des logiciels de meilleure qualité. Cependant, cela n’est pas sans défis. Naviguer dans le paysage DevOps nécessite un équilibre délicat entre l’adoption de l’automatisation et le maintien du contrôle. En étant conscients des pièges potentiels et en se concentrant sur les friandises, les équipes peuvent créer des systèmes efficaces, fiables et évolutifs. Comme un groupe de petits enfants qui se promènent dans le quartier le soir d’Halloween, le succès dans DevOps vient de la préparation, de la collaboration et de la capacité à s’adapter aux surprises inattendues. Alors, enfilez vos costumes, prenez vos sacs et préparez-vous à récupérer ces friandises DevOps !</p>'}, {'', ""<p>Source de l'image : Photo de Antonio Gabola sur Unsplash</p>""}]"
Préparation à la transition vers l'ingénierie de plateforme,"[{'', '<p>L’ingénierie de plateforme est l’une des tendances les plus en vogue dans le monde de la distribution de logiciels et devrait représenter la prochaine évolution de DevOps. Selon l’étude 2023 de Gartner sur les principales technologies stratégiques, 80 % des organisations d’ingénierie logicielle mettront en place des équipes de plateforme en tant que fournisseurs internes de services, composants et outils réutilisables pour la distribution d’applications d’ici 2026. Ce qui se passait autrefois en un jour dans le monde des développeurs doit désormais se produire en cinq minutes. Les organisations subissant une forte pression pour faire évoluer rapidement leurs opérations, en particulier dans un marché qui réclame à cor et à cri les avantages de l’intelligence artificielle (IA), l’automatisation est de plus en plus importante dans l’ingénierie de plateforme. Elle réduit les erreurs manuelles, rationalise le processus de développement et stimule la croissance et l’efficacité. À mesure que la technologie progresse rapidement, il devient de plus en plus essentiel pour les organisations de s’orienter de manière proactive dans les nouvelles tendances émergentes dans l’ingénierie de plateforme pour rester compétitives. Les clés d’une ingénierie de plateforme réussie incluent l’architecture cloud-native, l’IA et l’apprentissage automatique (ML), ainsi que des outils évolutifs qui utilisent des interfaces intuitives et des composants prédéfinis pour créer des applications fonctionnelles.</p>'}, {'', ""<h3>L'ingénierie de plateforme est la progression naturelle de DevOps</h3>""}, {'', '<p>L’ingénierie de plateforme étend les pratiques de DevOps en sortant le développement des silos et en adoptant une vue d’ensemble collaborative de la technologie d’une organisation. L’ingénierie de plateforme repose sur une infrastructure fondamentale qui permet des capacités en libre-service afin que les développeurs puissent déployer le code plus rapidement, de manière plus fiable et plus sécurisée. Si l’ingénierie de plateforme représente la prochaine vague de changement dans DevOps, il est essentiel que les dirigeants et les développeurs comprennent qu’il ne s’agit pas d’une technologie ou d’une approche unique. Le passage à l’ingénierie de plateforme représente également un changement culturel pour les développeurs et les dirigeants afin de répondre aux besoins, aux défis et au rythme rapide du développement de logiciels modernes en optimisant les processus, en accélérant la livraison et en réduisant la complexité des opérations. Pour que l’ingénierie de plateforme réussisse, il est essentiel de l’aborder dans la perspective d’une culture axée sur la technologie qui crée des solutions en tenant compte du point de vue du développeur. En cherchant d’abord à améliorer l’expérience du développeur, les entreprises peuvent obtenir de meilleurs résultats.</p>'}, {'', '<h3>Les tendances qui façonnent le paysage</h3>'}, {'', '<p>L’ingénierie des plateformes est une tendance émergente en soi, mais plusieurs éléments essentiels façonnent son impact sur les entreprises d’aujourd’hui et de demain. Pour que les entreprises puissent en tirer profit, il faut comprendre ces tendances essentielles pour garder une longueur d’avance.</p>'}, {'', ""<p>L'architecture cloud native est fondamentale pour l'ingénierie des plateformes modernes, car elle permet de concevoir et de créer des applications pour les environnements cloud. L'infrastructure en tant que code (IaC) est un élément clé, permettant le provisionnement et la gestion automatisés des ressources. Les plateformes cloud natives comme Kubernetes, Terraform ou Docker facilitent l'orchestration et la mise à l'échelle des conteneurs, tandis que l'informatique sans serveur isole entièrement la gestion de l'infrastructure. Cette tendance garantit que l'ingénierie des plateformes est agile, évolutive et rentable, avec des temps d'arrêt minimes et une maintenance plus facile.</p>""}, {'', '<p>L’automatisation est essentielle dans l’ingénierie des plateformes, car elle réduit les erreurs manuelles et rationalise les flux de travail de déploiement. Elle permet aux développeurs de créer, de tester et de déployer des logiciels plus efficacement, ce qui permet des versions de produits plus rapides et plus fiables. En fait, 43,2 % des premiers utilisateurs de l’ingénierie des plateformes ont constaté une livraison de produits plus rapide. Des outils d’automatisation comme Jenkins, Ansible et Terraform sont couramment utilisés pour automatiser le provisionnement de l’infrastructure, la gestion de la configuration et les pipelines de déploiement. Les modèles d’apprentissage automatique (ML) peuvent effectuer un dépannage automatisé en analysant les données historiques pour identifier les modèles et prédire les problèmes potentiels, en traitant les problèmes de manière proactive et en minimisant les temps d’arrêt. Cela contribue également à l’optimisation des performances. En utilisant l’IA qui peut analyser les données de performances des applications, l’automatisation fait passer l’ingénierie des plateformes à un niveau supérieur en suggérant des optimisations pour améliorer l’utilisation des ressources et l’expérience utilisateur. Les outils basés sur l’IA aident à générer du code ou des scripts, ce qui est essentiel pour réduire le temps de développement et les erreurs humaines. Les algorithmes ML renforcent le côté sécurité de l’ingénierie des plateformes, une priorité absolue pour tout développeur. Il est possible de tirer parti des tests de sécurité automatisés pendant le développement et la maintenance, et même de détecter et de répondre aux menaces de sécurité en temps réel. Lorsque l’IA gère des tâches de routine, les développeurs sont moins susceptibles de se sentir dépassés ou de souffrir d’épuisement professionnel. Cela permet aux développeurs de se concentrer sur des tâches plus créatives, la résolution de problèmes complexes et l’innovation, ce qui se traduit par des opérations plus efficaces pour les aider à atteindre des niveaux de productivité, de sécurité et de cohérence plus élevés.</p>'}, {'', '<p>Les outils de développement évolutifs qui créent des applications fonctionnelles à l’aide d’interfaces intuitives et de composants prédéfinis simplifient de plus en plus le développement et le déploiement des applications. Les plateformes low-code et no-code permettent aux développeurs de créer des applications avec un minimum d’effort de codage, en utilisant des interfaces visuelles et des modules prédéfinis. Les frameworks sans serveur comme AWS Lambda et Azure Functions permettent aux développeurs de se concentrer uniquement sur l’écriture de code. Le recours croissant aux passerelles d’interface de programmation d’application (API) et leur développement offrent un moyen simplifié de gérer et d’exposer les API, améliorant ainsi l’intégration et la connectivité des applications. Les plateformes de conteneurisation deviennent des outils tout aussi puissants. Des plateformes comme Docker et Kubernetes simplifient le déploiement d’applications dans divers environnements en proposant des conteneurs et une orchestration standardisés. L’adoption d’une suite stratégique de ces outils évolutifs nécessite une compréhension claire des objectifs du produit, des objectifs organisationnels et des capacités de développement.</p>'}, {'', ""<h3>Comment les dirigeants peuvent soutenir l'ingénierie des plateformes</h3>""}, {'', '<p>Selon les prévisions, 30 % des entreprises évaluent actuellement l’adoption de pratiques d’ingénierie de plateforme, et cette tendance ne fera que croître. Si les tendances en matière d’ingénierie de plateforme sont renforcées par la transformation numérique et les nouvelles technologies, il est essentiel de se rappeler que l’ingénierie de plateforme représente un changement dans la culture DevOps. Pour soutenir efficacement la transition, les dirigeants doivent s’engager dans une culture d’ingénierie de plateforme. Il ne suffit pas d’adopter une technologie. Elle doit être soutenue par une stratégie complète qui permet aux développeurs de bénéficier véritablement des outils et des structures de l’ingénierie de plateforme. À quoi cela ressemble-t-il ? Pour réussir, les dirigeants et les développeurs doivent encourager la collaboration et éliminer les cloisonnements entre les équipes d’exploitation et de développement. Il est possible de construire un pont entre les développeurs et les opérations en s’engageant dans la migration vers le cloud, en créant une plateforme centralisée et en investissant dans des outils collaboratifs et la stratégie pour la soutenir.</p>'}, {'', ""<p>Pour s'engager dans l'ingénierie de plateforme, il faut s'engager dans une culture collaborative initiée par le haut, renforcée par des décisions et des opérations stratégiques globales. Cela implique un apprentissage continu pour que les développeurs restent au courant des nouveaux langages, tendances, défis et priorités, en interne et en externe. Les équipes réussissent mieux lorsqu'elles utilisent des indicateurs de performance pour suivre les flux de travail qui les aident à effectuer une maintenance efficace et à s'améliorer de manière cohérente et continue.</p>""}, {'', ""<h3>Adopter le changement, c'est adopter le progrès</h3>""}, {'', '<p>Dans un monde de développement logiciel en constante évolution, il est facile de penser que chaque vague technologique remplacera la précédente. Bien entendu, les experts savent que ce n’est pas vrai. L’itération est essentielle à l’évolution des technologies, et il est important d’adopter la prochaine itération de progrès pour rester compétitif. L’automatisation et l’ingénierie de plateforme sont des outils qui permettent aux développeurs de progresser plutôt que de les remplacer. Il est important de noter que l’ingénierie de plateforme ne marque pas la fin de DevOps. Au contraire, elle porte les fondations fondamentales que DevOps a construites à un niveau supérieur pour doter les développeurs de capacités de libre-service, favoriser la collaboration, améliorer l’évolutivité, accélérer le déploiement et favoriser une croissance commerciale durable.</p>'}]"
Comment créer une plateforme de données pour des analyses ad hoc en libre-service,"[{'', '<p>En fait, il existe de nombreuses situations dans lesquelles les ingénieurs DevOps, les équipes SRE et d’autres services axés sur l’observabilité doivent explorer des volumes de données rapidement et de manière flexible. Un SRE essayant de déterminer la cause profonde de la latence peut avoir besoin d’analyser le mouvement des données entre les points de terminaison pour trouver celui qui fonctionne mal. Un autre ingénieur peut avoir besoin de comparer les performances historiques et actuelles pour trouver un comportement anormal. Enfin, un ingénieur DevOps peut avoir à analyser les mesures de performance des utilisateurs pour comprendre l’ampleur d’un ralentissement, qu’il soit mondial ou régional.</p>'}, {'', ""<p>Chaque cas d'utilisation nécessite des tableaux de bord capables de fournir des données détaillées via une grande variété de visualisations, notamment des cartes choroplèthes, des zones de pile, des graphiques à secteurs et des graphiques à barres. Idéalement, un tableau de bord permettra aux équipes d'isoler les dimensions, d'appliquer des filtres et d'explorer les données pour obtenir des informations plus approfondies.</p>""}, {'', '<p>Malheureusement, tous les tableaux de bord ne peuvent pas prendre en charge ce type d’analyse rapide et flexible. Beaucoup utilisent encore des technologies obsolètes, qui manquent de la flexibilité, de l’agilité et de l’évolutivité nécessaires pour explorer les données de manière transparente. Beaucoup ont également été conçus sans l’urgence que requièrent les données d’aujourd’hui. Après tout, au cours des années précédentes, la plupart des utilisations des tableaux de bord (comme les rapports internes) n’étaient pas sensibles au temps.</p>'}, {'', ""<p>D'autres tableaux de bord sont limités par des modèles, qui offrent un éventail limité de widgets, d'outils et de fonctionnalités d'exploration. Ces tableaux de bord peuvent sembler peu maniables et lents, inadaptés à une situation en évolution rapide comme une panne d'application.</p>""}, {'', '<h3>Exigences</h3>'}, {'', '<p>Pour véritablement tenir la promesse d’une analyse ad hoc en libre-service sur d’énormes ensembles de données, les équipes doivent travailler en temps réel et ont donc besoin d’une base de données capable de répondre rapidement.</p>'}, {'', '<p>Lorsqu’une application tombe en panne, un SRE peut ne pas savoir quoi rechercher et doit donc parcourir rapidement de nombreuses données. Alors que certains tableaux de bord s’appuient sur des solutions de contournement pour des requêtes plus rapides, telles que les pré-agrégations, le précalcul ou les cumuls, cela n’est pas possible dans ce cas, car le SRE ne saura tout simplement pas quoi rechercher. Après tout, il ne peut pas nécessairement prédire ce qui va se passer, et même s’il travaille sur des hypothèses issues de pannes précédentes, le problème actuel pourrait être très différent.</p>'}, {'', '<p>Les tableaux de bord doivent donc proposer de nombreuses fonctionnalités et visualisations. Les utilisateurs doivent pouvoir filtrer les données par heure, isoler des variables comme la localisation et zoomer sur des intervalles de temps spécifiques en quelques clics. Les tableaux de bord doivent également prendre en charge divers types de données, notamment des relations parent-enfant complexes et des colonnes imbriquées.</p>'}, {'', ""<p>De plus, les tableaux de bord doivent offrir une vision approfondie. Une plateforme de diffusion multimédia en ligne peut avoir besoin d'évaluer les indicateurs utilisateur (tels que la latence ou les temps de chargement) sur différents appareils, systèmes d'exploitation et régions pour affiner les performances. Un fournisseur de cloud doit surveiller son matériel physique pour détecter les températures élevées, les commutateurs ou périphériques réseau lents et d'autres anomalies.</p>""}, {'', '<p>Les données étant désormais si importantes pour la réussite, de plus en plus de personnes au sein d’une entreprise, notamment les data scientists, les chefs de produit et les utilisateurs externes, ont besoin d’informations basées sur les données. Dans ces situations, les tableaux de bord doivent gérer l’augmentation du trafic utilisateur et de l’activité de requête, en maintenant des réponses rapides même en cas de charge de travail. Cela est particulièrement important si l’on considère qu’une opération mono-utilisateur (comme un zoom) nécessitera plusieurs requêtes sur le backend pour s’exécuter.</p>'}, {'', '<p>Une base de données doit également évoluer de manière transparente. Si l’environnement d’une organisation génère des millions d’événements par heure, cela équivaut à des milliards d’événements par jour ou par semaine, ce qui constitue un défi pour toute base de données à ingérer, stocker, analyser et interroger. En fait, de nombreuses bases de données ne peuvent pas fournir de temps de réponse rapides tout en gérant de grands ensembles de données et un volume de requêtes élevé. Par exemple, les bases de données transactionnelles (OLTP) peuvent souvent effectuer des requêtes rapidement, mais ne peuvent pas exécuter d’analyses à grande échelle, tandis que les bases de données analytiques (OLAP) peuvent analyser des volumes massifs de données, mais pas à grande vitesse.</p>'}, {'', ""<h3>Apache Druid pour l'exploration indépendante et ad hoc des données</h3>""}, {'', ""<p>C'est là qu'intervient Apache Druid open source. En combinant l'évolutivité et les analyses avancées d'une base de données OLAP avec la vitesse d'une base de données OLTP, Druid offre une exploration rapide et en temps réel des données.</p>""}, {'', ""<p>Une fois les données ingérées, Druid les rend immédiatement disponibles pour l'interrogation et l'analyse, éliminant ainsi la nécessité de regrouper ou d'agréger les données d'une manière ou d'une autre. De plus, Druid s'intègre nativement aux technologies de streaming comme Apache Kafka et Amazon Kinesis, éliminant ainsi le besoin de solutions de contournement ou de connecteurs.</p>""}, {'', ""<p>Druid permet des visualisations interactives, offrant des temps de réponse en millisecondes, permettant une exploration plus polyvalente, élargissant la gamme de dimensions et de filtres disponibles, et même maintenant des vitesses inférieures à la seconde face à l'augmentation des volumes d'utilisateurs et de requêtes.</p>""}, {'', ""<p>L'architecture unique de Druid permet également une mise à l'échelle facile. En séparant les tâches clés entre différents types de nœuds (nœuds de données pour le stockage, nœuds maîtres pour l'ingestion et la disponibilité des données et nœuds de requête pour l'exécution des requêtes et le renvoi des résultats via la méthode de dispersion/collecte), Druid garantit que les nœuds peuvent être mis à l'échelle indépendamment en fonction des besoins. Par la suite, Druid rééquilibre également automatiquement le trafic pour garantir des performances constantes.</p>""}, {'', ""<h3>Salesforce : l'histoire d'un succès druidique</h3>""}, {'', '<p>Salesforce est un pionnier dans le domaine de la gestion de la relation client (CRM), sert 150 000 clients dans le monde et génère des milliards de revenus annuels.</p>'}, {'', ""<p>L'équipe Edge Intelligence est la division de Salesforce qui s'occupe de la tâche colossale consistant à ingérer, traiter, filtrer, agréger et interroger l'intégralité de leurs données de journal, soit des milliards à des milliards de lignes par jour. Chaque minute, Salesforce ingère 200 millions de mesures, tandis que chaque jour, Salesforce traite cinq milliards d'événements quotidiens à l'échelle mondiale. Au total, Salesforce accumule des dizaines de pétaoctets de données dans son magasin transactionnel, cinq pétaoctets de journaux dans ses centres de données et près de 200 pétaoctets dans son stockage Hadoop.</p>""}, {'', ""<p>Les équipes Salesforce utilisent Druid pour obtenir des informations en temps réel sur les performances des produits et les expériences des utilisateurs, en explorant instantanément de grands ensembles de données. De l'ingénieur au responsable de compte, tout le monde peut interroger une grande variété de dimensions, de filtres et d'agrégations pour mieux comprendre les tendances, résoudre les problèmes qui surviennent et définir des stratégies pour l'avenir.</p>""}, {'', '<p>En utilisant les capacités de compactage de Druid, Salesforce a également réduit le nombre de lignes Druid de 82 %, ce qui a entraîné une réduction globale de son empreinte de stockage de 47 % et une accélération de ses temps de réponse aux requêtes de 30 %.</p>'}]"
