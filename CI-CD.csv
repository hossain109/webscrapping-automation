Title,Content
Lancez-vous : passez au mode multijoueur pour dynamiser votre plateforme interne,"[{'', '<p>Comment les organisations peuvent-elles accroître la valeur de leurs plateformes internes sans élargir massivement leurs équipes de plateformes ?</p>'}, {'', '<p>Nous célébrons souvent le héros ou l’héroïne qui crée seul quelque chose d’ingénieux – l’innovateur non-conformiste. Cependant, la réalité nous montre constamment que la plupart des innovations sont le résultat d’un travail d’équipe. Cela est également vrai pour la conception d’une plateforme de développement interne. La plateforme interne peut offrir une valeur exceptionnelle. Cependant, pour atteindre son plein potentiel, il est logique de s’éloigner du développement de plateformes en mode solo et de passer en mode multijoueur pour dynamiser votre plateforme interne. Mais qu’est-ce que cela signifie exactement ?</p>'}, {'', '<p>Si de nombreuses organisations se lancent à fond dans la création d’une plateforme interne, elles n’exploitent pas toujours toutes les ressources et l’expertise dont elles disposent en interne. Au lieu de cela, il y a souvent plusieurs équipes cloisonnées qui construisent des plateformes qui résolvent leurs problèmes, mais elles ne communiquent pas avec d’autres équipes qui pourraient utiliser et améliorer la plateforme qu’elles construisent. S’il ne s’agit pas de plusieurs équipes avec des plateformes concurrentes ou redondantes, les organisations font parfois reposer la responsabilité du développement d’une plateforme sur les équipes de plateforme. Mais ces approches répondent-elles aux difficultés auxquelles les organisations sont confrontées et aux défis qu’elles cherchent à résoudre en adoptant une plateforme interne ?</p>'}, {""<h3>Un bref historique : de DevOps à l'ingénierie de plateforme</h3>"", ''}, {'', '<p>Par le passé, les développeurs passaient beaucoup de temps à attendre que les opérateurs leur fournissent des services ou des outils. Cela entraînait des frictions, des retards et de la frustration. Les développeurs et les opérateurs étaient isolés les uns des autres et n’avaient aucun moyen de briser les silos qui les empêchaient d’être productifs. Cela a donné naissance à DevOps, qui vise à donner aux équipes les moyens de tout créer et de tout gérer elles-mêmes.</p>'}, {'', '<p>Le DevOps fonctionne – et fonctionne bien – jusqu’à ce que les organisations évoluent et que des problèmes doubles apparaissent. Tout d’abord, les équipes DevOps finissent par se débattre avec des problèmes répétitifs et redondants, et ensuite, la charge de travail DevOps augmente et s’accélère à mesure que l’entreprise évolue. Cela signifie que les équipes DevOps ont non seulement beaucoup plus de travail à faire au quotidien, mais doivent également gérer la création et l’exécution de tout, de l’infrastructure au réseau en passant par le CI/CD et l’observabilité. Dans le même temps, les équipes DevOps doivent gérer une multitude de processus et de préoccupations internes, tels que la sécurité et la conformité. Naturellement, la surcharge cognitive est inévitable.</p>'}, {'', ""<h3>Tuer le dragon DevOps : découvrez l'ingénierie de plateforme</h3>""}, {'', '<p>Pour résoudre les défis croissants de DevOps, l’ingénierie de plateforme a émergé avec pour objectif principal d’offrir une meilleure expérience aux développeurs. Que signifie l’expérience des développeurs dans ce contexte ? Cela signifie réduire la charge cognitive des développeurs et leur permettre de se concentrer sur leur travail principal : apporter de la valeur aux clients. Mais pour y parvenir, il serait essentiel de réduire les charges de travail cognitives et littérales, et de rationaliser le travail des équipes DevOps. C’est là que l’ingénierie de plateforme et la formation d’équipes de plateforme jouent un rôle crucial.</p>'}, {'', '<p>Cette évolution n’a cependant pas complètement résolu le problème. La charge cognitive a simplement été transférée aux équipes de plateformes, chargées de fournir une plateforme de développement interne personnalisée qui réponde aux besoins des développeurs et offre des outils et des services à la demande. Pourtant, les équipes de plateformes finissent par prendre en charge un ensemble d’outils et de services déjà vaste et en plein essor et ressentent la pression de devoir fournir une expérience utilisateur agréable aux développeurs. L’ensemble du secteur fait écho à l’importance de l’expérience et des flux des développeurs lorsqu’ils créent leurs logiciels. Et si c’est vrai, comment pouvons-nous tirer le meilleur parti de ces plateformes internes sans épuiser toute l’équipe ou augmenter massivement ses effectifs ?</p>'}, {'', '<h3>Jouer en équipe</h3>'}, {'', '<p>Pour obtenir une valeur plus importante et potentiellement plus rapide avec la plateforme interne, commencez par réfléchir à la manière dont les équipes sont organisées, puis déterminez leurs rôles et responsabilités. D’après l’expérience des développeurs, des équipes DevOps et des équipes de plateforme, il est clair qu’aucun individu ou équipe ne doit (ou ne peut) tout gérer. Dans l’ouvrage phare Team Topologies, quatre types d’équipes sont décrits : équipe alignée sur le flux (équipe d’application), équipe d’activation, équipe de sous-systèmes complexes et équipe de plateforme.</p>'}, {''}, {'', ""<p>Lorsque l'idée de plateforme interne commence à s'effondrer, les équipes alignées sur les flux ont trop de charge cognitive et de bricolage dans le cadre de leur travail. Dans le même temps, avec l'ingénierie de plateforme, une grande partie du travail préparatoire et de la maintenance est transférée aux équipes de plateforme, ce qui alourdit leur charge de travail.</p>""}, {'', '<p>En comprenant ces types d’équipes et leur dynamique, nous pouvons commencer à nous demander : comment pouvons-nous partager la charge et même impliquer d’autres équipes dans le travail de construction de la plateforme interne ? Comment pouvons-nous répartir la responsabilité de manière plus équitable et de manière à offrir une valeur maximale ?</p>'}, {'', '<h3>Jouer sur la plateforme avec une approche multijoueur</h3>'}, {'', '<p>C’est là que l’intérêt d’une approche multijoueur pour la création de plateformes se fait sentir. Si les développeurs et les équipes de plateformes détiennent une grande partie de l’expertise nécessaire pour créer une plateforme interne efficace, la responsabilité de créer une plateforme utile capable de susciter une adhésion et une adoption quasi universelles n’incombe pas entièrement à l’un ou l’autre. Les autres équipes de l’organisation peuvent avoir une grande influence sur le développement de la plateforme en y apportant leurs propres compétences, expertise et connaissances, ce qui profite en fin de compte à l’organisation.</p>'}, {'', '<h3>Prêt, joueur 1</h3>'}, {'', '<p>Un exemple d’injection de compétences et d’expertise spécifiques dans une plateforme est celui où plusieurs équipes d’application ont besoin d’accéder à une base de données. Dans un monde idéal, les développeurs auraient un accès à la demande, par exemple à une base de données Postgres en tant que service. Ils pourraient en fournir une à chaque fois qu’ils en ont besoin. De nombreuses organisations disposent en interne d’une équipe spécialisée dans les bases de données qui sait exactement comment fonctionne Postgres, comment il doit être configuré et comment les équipes alignées sur les flux doivent l’utiliser.</p>'}, {'', '<p>Il n’y a aucune raison pour que les développeurs ou les équipes de plateforme d’une entreprise soient obligés de posséder des connaissances approfondies en matière de bases de données lorsqu’une équipe de sous-systèmes composée d’experts en la matière peut facilement proposer cette option de base de données en tant que service au sein de la plateforme. En déchargeant cette charge, les équipes d’application peuvent obtenir ce dont elles ont besoin de la plateforme, et les équipes de plateforme peuvent facilement utiliser l’option de base de données en tant que service comme élément de base au sein de la plateforme, qu’elles peuvent regrouper avec d’autres services, le tout en s’appuyant sur l’expertise de l’équipe de base de données.</p>'}, {'<h3>Prêt Joueur Deux</h3>', ''}, {'', '<p>Un deuxième exemple concerne une exigence spécifique plutôt qu’un outil spécifique. Les processus de sécurité, par exemple, sont des exigences qui doivent être intégrées à de nombreuses charges de travail de livraison de logiciels. Mais il est probable que les ressources de sécurité ne soient pas suffisantes pour les intégrer aux équipes chargées des applications et des plateformes. Au lieu de cela, l’approche multi-acteurs permet aux équipes de sécurité de sauver la mise en définissant des processus et en les intégrant à la plateforme interne.</p>'}, {'', '<p>Encore une fois, les équipes de développement et de plateforme ne sont pas obligées de comprendre tous les tenants et aboutissants de la sécurité, mais peuvent suivre les étapes et les garde-fous définis par l’équipe de sécurité au fur et à mesure de leur intégration dans les charges de travail de la plateforme.</p>'}, {'', '<h3>Tout le monde peut contribuer à rendre la plateforme interne plus puissante</h3>'}, {'', '<p>L’utilisation de compétences et de connaissances spécialisées provenant de toute l’organisation et leur intégration dans la plateforme interne de manière simple à utiliser constituent une stratégie efficace pour réduire la charge cognitive au sein des équipes.</p>'}, {'', '<p>Cette approche, parfois appelée « sourcing interne » ou « démocratisation de la plateforme », s’est révélée très fructueuse. L’essentiel est de tirer le meilleur parti des talents, des compétences et des connaissances dont dispose votre organisation. Cela permet non seulement de soulager la charge cognitive et d’éviter de devoir constituer une équipe massive ou de submerger une petite équipe avec trop d’informations, mais cela ouvre également la voie à l’adhésion et à l’appropriation de la plateforme à l’échelle de l’organisation.</p>'}, {'', ""<h3>Comment démarrer avec l'approche multijoueur</h3>""}, {'', ""<p>L'ingénierie des plateformes étant considérée comme une pratique sociotechnique, il est important d'impliquer à la fois les personnes et la technologie. Considérez à nouveau les topologies d'équipe et les trois modes d'interaction décrits dans le livre : la collaboration, le x-as-a-service et la facilitation.</p>""}, {''}, {'', '<h3>Collaboration : Jouons ensemble</h3>'}, {'', '<p>Pour comprendre à quoi pourrait ressembler une plateforme interne multijoueur, les équipes doivent se concentrer sur la collaboration. Elles doivent se comprendre, développer l’empathie et s’aligner. La collaboration est le meilleur type d’interaction pour comprendre les exigences, telles que les besoins des équipes d’application, la façon dont elles aimeraient utiliser la plateforme et si et comment les équipes d’application souhaiteraient contribuer aux outils et services qu’elles ont elles-mêmes développés dans la plateforme.</p>'}, {'', ""<p>L'équipe de la plateforme doit également collaborer avec les autres équipes, en particulier les spécialistes qui fournissent des outils et des services spécifiques. Cela permet de mieux comprendre ce que sont ces services, comment ils peuvent être produits et intégrés à la plateforme, ainsi que ce que ces équipes doivent consommer des autres équipes. Pour faciliter ce travail, une équipe de facilitation de courte durée axée sur la facilitation dans toute l'organisation peut être utile pour cartographier les connexions et les collaborations.</p>""}, {'', '<h3>X-as-a-Service : là où le mode multijoueur prend vie</h3>'}, {'', ""<p>Le mode multijoueur prend vie lorsque les équipes de l'organisation sont habilitées à apporter leur expertise à la plateforme. L'équipe de la plateforme peut le permettre. L'objectif final est de faire en sorte que la collaboration mène à une expertise qui se transforme en outils et services à la demande X-as-a-service.</p>""}, {'', ""<p>En utilisant l'exemple précédent, nous pouvons voir que la base de données Postgres est proposée en tant que service au sein de la plateforme et qu'elle bénéficie de l'expertise en sécurité apportée par l'équipe de sécurité intégrée à la plateforme. Les consommateurs de la plateforme interagissent désormais avec la plateforme en mode x-as-a-service, ce qui signifie qu'ils obtiennent l'outil dont ils ont besoin à la demande avec une sécurité renforcée déjà intégrée.</p>""}, {''}, {'', '<p>D’un point de vue technique également, l’équipe de la plateforme doit tenir compte de la facilité avec laquelle les autres équipes peuvent contribuer à la plateforme. S’agit-il d’une boîte noire que personne ne comprend ou est-il facile pour les autres équipes de contribuer ? Quel est ce processus de contribution, non seulement sous forme d’interaction ponctuelle en amont, mais également de manière continue pour garantir que la plateforme reste à jour et adaptée à ses objectifs ? Ce processus est-il compris et documenté ?</p>'}, {'', '<p>C’est l’un des domaines dans lesquels une infrastructure de plateforme peut prendre en charge ce modèle multijoueur et même permettre le mode multijoueur prêt à l’emploi. Si votre plateforme est constituée de blocs de construction composables fournis et maintenus par des équipes d’experts, elle permettra à l’équipe de plateforme d’utiliser ces blocs de construction pour composer des routes pavées ou des chemins d’or. Ceux-ci peuvent ensuite être utilisés par les équipes d’application pour une expérience de développement encore plus simple et plus fluide. En fin de compte, la plateforme interne finit par être supérieure à la somme de ses parties et devient un moteur d’innovation.</p>'}, {'', ""<h3>Le jeu n'est pas terminé : passez en mode multijoueur pour obtenir plus de jetons</h3>""}, {'', ""<p>Le passage au mode multijoueur est au cœur de la volonté de permettre une meilleure expérience aux développeurs et de réduire la charge cognitive des équipes de développement et de plateforme. Et même si ces équipes sont celles qui en bénéficient le plus en apparence, une plateforme interne inspirée du multijoueur peut bénéficier de la participation de l'ensemble de l'organisation.</p>""}, {'', '<p>En passant du mode solo au mode multijoueur, chacun peut devenir un héros de plateforme interne.</p>'}, {''}]"
Les opérations en tant que code : transformer l'excellence opérationnelle,"[{'', '<p>La transformation numérique et l’infrastructure cloud native sont inévitables. Cette évolution implique la nécessité de gérer les opérations avec la même rigueur et la même automatisation que celles appliquées à l’infrastructure ou à la sécurité. De nombreuses organisations ont adopté l’idée que tout se trouve dans un pipeline et que tout est sous forme de code. Alors que les ingénieurs de plateforme et d’autres équipes ont créé des solutions pour créer et fournir des applications et les cadres nécessaires pour les exécuter, les opérations réelles de fourniture de services sont souvent décousues et purement réactives.</p>'}, {'', '<p>Saisissez les opérations sous forme de code.</p>'}, {'', ""<p>En exploitant des outils tels que Terraform avec des pipelines d'automatisation et CI/CD, les équipes d'ingénierie de fiabilité du site (SRE), DevOps et DevSecOps peuvent standardiser et automatiser les tâches opérationnelles, garantissant ainsi la cohérence, l'efficacité et la fiabilité.</p>""}, {'', ""<p>Les opérations en tant que code étendent les principes de l'infrastructure en tant que code (IaC) aux procédures opérationnelles. Elles impliquent la définition, la gestion et l'exécution de tâches opérationnelles (telles que la définition de politiques d'escalade, la définition de manuels d'exécution et l'exécution de manuels de jeu) à l'aide de codes et d'outils d'automatisation. Cette approche garantit que les pratiques opérationnelles sont reproductibles, contrôlées par version et peuvent être exécutées avec une intervention humaine minimale.</p>""}, {'', '<h3>Éviter les goulots d’étranglement</h3>'}, {'', '<p>L’un des principaux avantages de l’approche des opérations en tant que code est la possibilité de supprimer la dépendance vis-à-vis des équipes centralisées. À mesure que le besoin de rapidité dans les pratiques DevOps augmente, ces équipes ne peuvent plus dépendre de l’ITSM centralisé ou d’autres équipes pour leurs besoins d’intégration de nouvelles fonctions de surveillance, d’enrichissement d’événements ou de création de nouveaux runbooks.</p>'}, {'', '<p>De même, les équipes centralisées qui consacrent des compétences coûteuses et spécialisées à la surveillance des intégrations, à la gestion des événements, à l’enrichissement et à l’automatisation, qui peuvent être gérées via des opérations en tant que code, n’ont guère de sens sur le plan économique. Ces équipes, en particulier dans les grandes organisations, sont déjà surchargées et le retard de travail augmente de jour en jour. Tirer parti de Terraform ou d’autres mécanismes pour atteindre les mêmes objectifs tout en offrant de meilleurs résultats est plus logique pour toutes les équipes travaillant ensemble.</p>'}, {'', '<h3>Tirer parti des pipelines et de Terraform pour les opérations</h3>'}, {'', ""<p>Terraform, traditionnellement utilisé pour l'IaC, est devenu la lingua franca de DevOps. En écrivant des configurations Terraform, les équipes peuvent automatiser le provisionnement et la gestion non seulement de l'infrastructure, mais aussi des flux de travail opérationnels qui garantissent l'excellence opérationnelle. Par exemple, les configurations Terraform peuvent définir des tâches telles que les définitions de services, la configuration des utilisateurs, des équipes et des rôles, la définition des politiques et des calendriers d'escalade, la définition de la corrélation et de l'orchestration des événements et la définition de l'automatisation telle que les runbooks et les diagnostics automatisés.</p>""}, {'', '<p>Les pipelines CI/CD jouent un rôle crucial dans les opérations en tant que code. En intégrant des tâches opérationnelles dans les pipelines CI/CD, vous pouvez garantir que les modifications sont testées, examinées et déployées de manière contrôlée et automatisée.</p>'}, {'', ""<p>Les portes de qualité sont traditionnellement utilisées pour les révisions de code, les tests automatisés, les contrôles de sécurité, etc. Pour les opérations en tant que code, elles peuvent être exploitées à des fins de normalisation en garantissant la cohérence des fonctionnalités de base telles que les normes de service, les niveaux de politiques d'escalade, les exigences minimales pour les runbooks, etc. En effectuant des contrôles de conformité pour garantir que les changements opérationnels sont conformes aux politiques internes et aux réglementations externes. Et à terme, elles peuvent être utilisées pour évaluer les applications en termes de préparation opérationnelle.</p>""}, {'', '<h3>Avantages des opérations en tant que code</h3>'}, {'', '<p>Les organisations qui déploient des opérations sous forme de code verront plusieurs avantages, dont beaucoup avec un retour sur investissement (ROI) immédiat.</p>'}, {'', '<p>La réduction des efforts est essentielle. Trop de temps est consacré à « ClickOps », et en abandonnant les configurations manuelles, vous pouvez consacrer plus de temps à l’automatisation et libérer des ressources pour des tâches plus utiles. Vous pouvez également réduire le risque opérationnel en garantissant la traçabilité des modifications apportées aux configurations, au contrôle des versions et aux modèles qui réduisent le risque d’erreur. De même, vous pouvez opérationnaliser la gouvernance et la conformité en exploitant les analyseurs, les contrôles de qualité et les modèles approuvés, tandis que la direction peut définir des normes minimales acceptables et les résultats attendus.</p>'}, {'', ""<p>L'excellence opérationnelle est améliorée en réduisant la fréquence, la gravité et la durée des pannes, en garantissant des résultats reproductibles et une réduction des erreurs. Vous pouvez vous éloigner des connaissances tribales en donnant aux cadres supérieurs une méthode simplifiée et reproductible pour enregistrer leurs connaissances innées afin de les réutiliser et en créant un contexte pour le personnel subalterne.</p>""}, {'', '<p>L’expérience du développeur est améliorée en réduisant le temps de montée en puissance des nouveaux membres de l’équipe, ce qui leur permet de se concentrer davantage sur le travail à forte valeur ajoutée et le renforcement des capacités, tout en consacrant moins de temps à la recherche de «\xa0comment faire\xa0» ou à la remontée des connaissances vers des experts. Plus important encore, vous pouvez amorcer une transition de l’exécution à la construction en réduisant le temps consacré à maintenir les lumières allumées et à poursuivre les travaux de réparation, tandis que le personnel senior peut se concentrer sur la réduction de la dette technologique (ou l’exploitation de la richesse technologique si vous êtes optimiste) pour offrir une expérience client exceptionnelle.</p>'}, {'<h3>Commencer</h3>', ''}, {'', '<p>Le déploiement réussi des opérations en tant que code implique plusieurs étapes clés\xa0:</p>'}, {'', '<li>Tout d’abord, vous devez définir le succès. Comment allez-vous mesurer l’efficacité de vos opérations ? Pensez au-delà du temps moyen de réparation (MTTR). Qu’en est-il du coût de maintien des lumières ou de réduction du temps et du coût des travaux de réparation ? Comment pouvez-vous mieux vous attaquer à la dette technologique ?</li>'}, {'', '<li>Ensuite, évaluez les opérations actuelles et identifiez les domaines initiaux qui peuvent bénéficier de l’automatisation et des modèles. Quels sont les risques que vous pouvez immédiatement éliminer ou les résultats que vous pourriez influencer en standardisant les opérations ?</li>'}, {'', ""<li>Ensuite, assurez-vous de disposer des outils appropriés qui correspondent à votre environnement et à vos objectifs architecturaux et que vos équipes sont formées à ces outils et aux meilleures pratiques associées. Vous souhaiterez établir un centre d'excellence en constituant des équipes de passionnés et d'experts qui peuvent vous aider avec les questions-réponses, devenir les gardiens des modèles et contribuer à la création d'améliorations continues en matière d'automatisation et d'orchestration.</li>""}, {'', '<li>Enfin, concentrez-vous sur une mise en œuvre progressive en commençant par des domaines simples mais impactants, puis développez-vous en utilisant l’amélioration continue pour revoir et améliorer régulièrement vos processus en fonction des commentaires et des mesures.</li>'}, {'', '<h3>Et ensuite ?</h3>'}, {'', '<p>Les opérations en tant que code représentent la prochaine frontière de la gestion informatique, offrant la promesse de cohérence, d’efficacité et de fiabilité dans les tâches opérationnelles. En tirant parti de Terraform, des pipelines CI/CD et d’outils robustes, vous pouvez amener vos équipes à adopter cette approche transformatrice. Bien que des défis existent, ils sont surmontables grâce à une planification, une exécution et une amélioration continue minutieuses. Les opérations en tant que code peuvent être une pierre angulaire de l’excellence opérationnelle, permettant à vos équipes de passer d’un monde de travail et de dépannage à la création de capacités qui vous aideront à gagner sur le marché, à mieux servir vos équipes et, surtout, vos clients.</p>'}]"
Le coût croissant des incidents numériques : comprendre et atténuer l'impact des pannes,"[{'', '<p>Les perturbations numériques ont atteint des niveaux alarmants. La réponse aux incidents dans les environnements d’application modernes est fréquente, chronophage et demande beaucoup de travail. Notre équipe a une expérience directe de la gestion des impacts à grande échelle de ces perturbations et pannes, ayant passé des décennies dans les opérations informatiques. PagerDuty a récemment publié une étude1 qui met en lumière à quel point nos systèmes et pratiques de réponse aux incidents existants sont défaillants. La récente débâcle de Crowdstrike en est une preuve supplémentaire. Malgré tous les investissements dans l’observabilité, les opérations d’IA, l’automatisation et les manuels, la situation ne s’améliore pas. À certains égards, elle est pire : nous collectons de plus en plus de données et nous sommes surchargés d’outils, ce qui crée une confusion entre les utilisateurs et les équipes qui ont du mal à comprendre l’environnement holistique et toutes ses interdépendances. Avec un temps de résolution moyen de 175 minutes, chaque incident numérique ayant un impact sur le client coûte à la fois du temps et de l’argent. Le secteur doit réinitialiser et revoir les processus actuels afin que nous puissions évoluer et changer la trajectoire.</p>'}, {'', ""<h3>L'impact des pannes et des temps d'arrêt des applications</h3>""}, {'', '<p>Les pannes sapent la confiance des clients. 90 % des responsables informatiques déclarent que les perturbations numériques ont réduit la confiance des clients. La protection des données sensibles, la garantie d’une restauration rapide du service et la fourniture de mises à jour en temps réel aux clients sont essentielles pour maintenir la confiance en cas d’incident numérique. Des analyses approfondies et orientées vers l’action sont essentielles après l’incident pour éviter qu’il ne se reproduise. Et — au risque de renforcer l’évidence — les organisations informatiques doivent mettre en place des pratiques opérationnelles pour minimiser les pannes en premier lieu.</p>'}, {'', ""<p>Pourtant, même si les responsables informatiques comprennent les implications sur la confiance des clients, la fréquence des incidents continue d'augmenter. 59 % des responsables informatiques signalent une augmentation des incidents ayant un impact sur les clients, et la situation ne s'améliorera pas à moins que nous ne changions la façon dont nous observons et atténuons les problèmes dans nos applications.</p>""}, {'', '<h3>L’automatisation peut aider, mais son adoption est lente</h3>'}, {'', '<p>Malgré la menace croissante, de nombreuses organisations sont à la traîne en matière d’automatisation de la réponse aux incidents :</p>'}, {'', '<li>Plus de 70 % des responsables informatiques signalent que les tâches clés de réponse aux incidents ne sont pas encore entièrement automatisées</li>'}, {'', '<li>38 % du temps des intervenants est consacré à la gestion des processus manuels de réponse aux incidents</li>'}, {'', '<li>Les organisations dotées de processus manuels mettent en moyenne trois heures et 58 minutes pour résoudre les incidents ayant un impact sur les clients, contre deux heures et 40 minutes pour celles dotées de processus automatisés.</li>'}, {'', '<p>Il n’est pas nécessaire d’être un expert en informatique pour savoir que passer près de la moitié de son temps sur des processus manuels est un gaspillage de ressources. Et les entreprises qui ont automatisé leurs opérations mettent encore près de trois heures à résoudre les incidents. Pourquoi la réponse aux incidents est-elle toujours aussi lente ?</p>'}, {'', '<p>Il ne s’agit pas seulement d’automatiser les processus. Nous devons également accélérer l’automatisation des décisions, en nous appuyant sur une compréhension approfondie de l’état des applications et de l’infrastructure.</p>'}, {'', '<h3>IA causale pour DevOps : le chaînon manquant</h3>'}, {'', '<p>L’IA causale pour DevOps promet de créer un pont entre l’observabilité et la réponse automatisée aux incidents numériques. Par « IA causale pour DevOps », j’entends un logiciel de raisonnement causal qui applique l’apprentissage automatique (ML) pour capturer automatiquement les relations de cause à effet. L’IA causale a le potentiel d’aider les équipes de développement et d’exploitation à mieux planifier les modifications apportées au code, aux configurations ou aux modèles de charge, afin qu’elles puissent rester concentrées sur la réalisation des objectifs de niveau de service et d’entreprise au lieu de lutter contre les incendies.</p>'}, {'', '<p>Avec Causal AI for DevOps, de nombreuses tâches de réponse aux incidents qui sont actuellement manuelles peuvent être automatisées\xa0:</p>'}, {'', '<li>Lorsque les entités de service sont dégradées ou défaillantes et affectent d’autres entités qui composent les services métier, un logiciel de raisonnement causal fait apparaître la relation entre le problème et les symptômes qu’il provoque.</li>'}, {'', ""<li>L'équipe responsable du service défaillant ou dégradé est immédiatement informée afin de pouvoir se mettre au travail pour résoudre le problème. Certains problèmes peuvent être résolus automatiquement.</li>""}, {'', ""<li>Des notifications peuvent être envoyées aux utilisateurs finaux et aux autres parties prenantes, leur faisant savoir que leurs services sont affectés, ainsi qu'une explication des raisons pour lesquelles cela s'est produit et du moment où les choses reviendront à la normale.</li>""}, {'', '<li>La documentation post-mortem est générée automatiquement.</li>'}, {'', ""<li>Il n'existe plus de processus de tri complexes qui nécessiteraient autrement l'intervention de plusieurs équipes et responsables. Les incidents et pannes numériques sont réduits et l'analyse des causes profondes est automatisée, ce qui permet aux équipes DevOps de passer moins de temps à résoudre les problèmes et plus de temps à expédier le code.</li>""}, {'', ""<h3>Il est temps d'automatiser la réponse aux incidents</h3>""}, {'', ""<p>Il est temps de passer d'une réponse manuelle aux incidents à une réponse automatisée. L'IA causale pour DevOps peut aider les équipes à prévenir les pannes, à réduire les risques, à diminuer les coûts et à instaurer une confiance durable des clients. C'est un sujet qui nous tient à cœur chez Causely, où nous construisons une plateforme de raisonnement causal pour aider les organisations à garantir la fiabilité continue des applications et à éliminer le dépannage humain. Vous pouvez en savoir plus sur nous et notre plateforme sur Causely.io.</p>""}]"
CISA : une faille critique de Jenkins exploitée dans des attaques de ransomware,"[{'', '<p>Une faille de sécurité critique dans le populaire serveur d’automatisation open source Jenkins figure sur la liste des vulnérabilités connues de la Cybersecurity and Infrastructure Security Agency (CISA) après avoir été exploitée dans un ransomware et d’autres attaques.</p>'}, {'', ""<p>L'agence de cybersécurité la plus importante du gouvernement américain a ajouté le bug - identifié comme CVE-2024-23897 et avec un score de gravité CVSS de 9,8 sur 10 - à son catalogue de vulnérabilités exploitées connues, ce qui met en garde les agences fédérales contre la nécessité de sécuriser leurs serveurs Jenkins, bien que la CISA ait également averti toutes les organisations exploitant de tels serveurs de s'assurer qu'ils sont sécurisés.</p>""}, {'', ""<p>La vulnérabilité dans l'interface de ligne de commande (CLI) de Jenkins est une faille de parcours de chemin causée par une faiblesse dans l'analyseur de commandes args4j, qui peut être exploitée par des acteurs malveillants pour obtenir l'exécution de code à distance (RCE) et pour lire des fichiers arbitraires sur le serveur Jenkins.</p>""}, {'', ""<p>Le serveur Jenkins basé sur Java, maintenu par CloudBees et la communauté Jenkins, est utilisé par les développeurs dans leur intégration continue et leur développement continu (CI/CD) et automatise les étapes du cycle de vie du développement logiciel, y compris le développement et le déploiement. L'outil, soutenu par des sociétés telles qu'Amazon Web Services (AWS), GitHub et JFrog, compte plus d'un million d'utilisateurs.</p>""}, {'', '<h3>Une faille devient publique et est corrigée</h3>'}, {'', '<p>Yaniv Nizry, chercheur en vulnérabilité chez SonarSource, développeur de logiciels open source, a été le premier à signaler cette faille de sécurité en janvier, soulignant qu’avec une part de marché d’environ 44 %, « la popularité de Jenkins est évidente. Cela signifie que l’impact potentiel des vulnérabilités de sécurité de Jenkins est important ».</p>'}, {'', ""<p>Un correctif a été publié en janvier avec Jenkins 2.442, LTS 2.426.3 en désactivant la fonctionnalité d'analyse de commandes, les responsables expliquant que Jenkins est livré avec une CLI intégrée pour accéder à Jenkins à partir d'un environnement de script ou de shell. Il utilise la bibliothèque args4j pour analyser les arguments et les options de commande sur le contrôleur Jenkins lors du traitement des commandes CLI.</p>""}, {'', ""<p>« Cet analyseur de commandes possède une fonctionnalité qui remplace un caractère @ suivi d'un chemin de fichier dans un argument par le contenu du fichier (expandAtFiles) », ont écrit les responsables. « Cela permet aux attaquants de lire des fichiers arbitraires sur le système de fichiers du contrôleur Jenkins en utilisant l'encodage de caractères par défaut du processus du contrôleur Jenkins. »</p>""}, {'', '<h3>Les cybercriminels se lancent</h3>'}, {'', ""<p>Des preuves de concept (POC) auraient commencé à émerger peu après la publication du correctif par Jenkins. Les chercheurs de Trend Micro ont signalé en mars qu'ils avaient observé plusieurs attaques exploitant la faille, 28 des 44 adresses IP sources des attaques provenant des Pays-Bas, les autres de pays comme Singapour et l'Allemagne. La plupart des cibles se trouvaient en Afrique du Sud.</p>""}, {'', '<p>Ils ont également constaté des cas où des exploits RCE étaient échangés.</p>'}, {'', '<p>D’autres chercheurs ont découvert des attaques plus récentes exploitant la vulnérabilité de Jenkins. En juillet, CloudSEK a signalé une attaque de la chaîne d’approvisionnement contre Born Group, une agence internationale de conseil et d’expérience client basée à New York, par le groupe de menaces IntelBroker, spécialisé dans les violations de données, l’extorsion et la vente d’accès à des systèmes compromis.</p>'}, {'', ""<p>Les chercheurs de CloudSEK ont déclaré qu'IntelBroker avait exploité CVE-2024-23897 pour obtenir un accès initial via un serveur Jenkins vulnérable avant d'accéder au référentiel GitHub de Born Group.</p>""}, {'', '<h3>Attaque de ransomware en Inde</h3>'}, {'', ""<p>Plus tôt ce mois-ci, les chercheurs du Juniper Threat Lab ont écrit sur une attaque de ransomware contre Brontoo Technology Solutions, une société de services et de conseil informatique en Inde qui collabore avec C-Edge Technologies, une coentreprise entre Tata Consultancy Services et la State Bank of India. Juniper et CloudSEK ont attribué l'attaque au groupe de ransomware RansomXXX, qui existe depuis 2018, opère depuis la Russie ou l'Europe de l'Est et cible les agences gouvernementales, les banques et les organismes de santé.</p>""}, {'', '<p>L’attaque a perturbé les paiements de détail dans les banques indiennes. Une fois encore, les acteurs malveillants ont obtenu un accès initial à l’environnement informatique de Brontoo via la vulnérabilité Jenkins.</p>'}, {'', '<p>« Cette vulnérabilité permet à un utilisateur non authentifié de lire les premières lignes de n’importe quel fichier du système de fichiers », ont écrit les chercheurs. « Elle existe parce que la fonction intégrée de l’analyseur de commandes n’a pas été désactivée par défaut. Si elle est exploitée avec succès, cette vulnérabilité peut entraîner la fuite de fichiers et de données sensibles, l’exécution potentielle de commandes et permettre une attaque par ransomware. »</p>'}]"
GitHub oriente Copilot Autofix vers l'œil de la tempête de sécurité de l'IA,"[{'', ""<p>GitHub, société de gestion de versions distribuées et de plateforme de collaboration, a présenté son nouvel outil Copilot Autofix. Ce service logiciel basé sur l'IA s'adresse aux développeurs qui doivent remédier aux vulnérabilités logicielles dans le code destiné aux applications traditionnelles et dans celles imprégnées d'une injection de nouvelles ou existantes races d'IA.</p>""}, {'', '<p>Copilot Autofix fait partie du groupe de produits de la plateforme GitHub Advanced Security (GHAS). Mentionnée pour la première fois au printemps de cette année, la technologie était au stade de la version bêta publique avant de passer ce mois-ci à une version complète.</p>'}, {'', '<p>Ce produit finalisé de première version intègre le moteur d’analyse de code CodeQL de GitHub, un moteur d’analyse de code développé par GitHub pour automatiser les contrôles de sécurité afin que les développeurs puissent analyser et afficher les résultats sous forme d’alertes d’analyse de code. Il intègre également GPT-4o, un modèle de langage multimodal de grande taille qui offre des fonctionnalités de conversation en temps réel et de génération de texte. Copilot Autofix propose également des heuristiques et la technologie dispose de son propre ensemble d’API pour permettre aux équipes d’implémenter son ensemble d’outils et de créer des suggestions de code (et des extraits de code) pour corriger et remédier aux vulnérabilités. Les développeurs peuvent accepter, modifier ou rejeter les suggestions de code proposées.</p>'}, {'', '<h3>Trouvé oui, corrigé peut-être</h3>'}, {'', '<p>« Les outils d’analyse de code détectent les vulnérabilités, mais ils ne s’attaquent pas au problème fondamental [de la réparation des logiciels] : la correction nécessite une expertise en sécurité et du temps, deux ressources précieuses dont on manque cruellement. En d’autres termes, le problème n’est pas de trouver les vulnérabilités, mais de les corriger », a déclaré Mike Hanley, responsable de la sécurité et vice-président senior de l’ingénierie chez GitHub.</p>'}, {'', ""<p>En utilisant son nouveau slogan « Trouvé signifie corrigé » pour aborder ce point précis, Hanley explique que Copilot Autofix analyse les vulnérabilités dans le code, explique pourquoi elles sont importantes… et propose des suggestions de code qui aident les développeurs à corriger les vulnérabilités aussi rapidement qu'elles sont découvertes.</p>""}, {'', '<blockquote>« Au cours de la version bêta publique, nous avons constaté que les développeurs corrigeaient les vulnérabilités du code plus de trois fois plus rapidement que ceux qui le faisaient manuellement, ce qui constitue un exemple frappant de la manière dont les agents d’IA peuvent radicalement simplifier et accélérer le développement de logiciels sécurisés », s’enthousiasme Hanley. « Les développeurs peuvent éviter les nouvelles vulnérabilités dans leur code grâce à Copilot Autofix dans la demande d’extraction, et désormais également réduire l’arriéré de la dette de sécurité en générant des correctifs pour les vulnérabilités existantes. »</blockquote>'}, {'', '<p>« Au cours de la version bêta publique, nous avons constaté que les développeurs corrigeaient les vulnérabilités du code plus de trois fois plus rapidement que ceux qui le faisaient manuellement, ce qui constitue un exemple frappant de la manière dont les agents d’IA peuvent radicalement simplifier et accélérer le développement de logiciels sécurisés », s’enthousiasme Hanley. « Les développeurs peuvent éviter les nouvelles vulnérabilités dans leur code grâce à Copilot Autofix dans la demande d’extraction, et désormais également réduire l’arriéré de la dette de sécurité en générant des correctifs pour les vulnérabilités existantes. »</p>'}, {'', '<p>La division GHAS de GitHub a déclaré qu’elle avait de grands projets pour Copilot Autofix et ses ensembles d’outils de plateforme associés. Elle s’efforce d’améliorer la portée et la précision de « l’analyse secrète » actuelle. L’organisation définit l’analyse secrète comme une fonctionnalité de sécurité qui permet de détecter et d’empêcher l’inclusion accidentelle d’informations sensibles telles que les clés API, les mots de passe, les jetons et autres secrets dans le référentiel de code d’une équipe DevOps. L’approche de GitHub ici signifie que l’analyse secrète analyse les validations de code dans les référentiels pour les types de secrets connus afin que les administrateurs du référentiel puissent être alertés en cas de détection. Hanley de GitHub a également déclaré que l’équipe développe de nouveaux flux de travail qui font évoluer Copilot Autofix pour les organisations ayant un volume élevé de dettes de sécurité, le tout sur des plateformes de développement familières.</p>'}, {'', ""<p>Avec une prise en charge initiale de JavaScript, TypeScript, Java et Python, Copilot Autofix étend désormais également la prise en charge à C#, C/C++, Go, Kotlin, Swift et Ruby. Disponible gratuitement pour les développeurs travaillant sur des projets open source, les clients GitHub Enterprise Cloud payants qui s'abonnent à GHAS trouveront Copilot Autofix activé par défaut dans leurs paramètres GHAS.</p>""}, {'', ""<h3>L'IA pour le bien, pour le bien ?</h3>""}, {'', '<p>Compte tenu de la vitesse à laquelle les équipes DevOps modernes doivent tenter de créer des offres logicielles fonctionnelles et du fait que tous les groupes ne disposent pas d’un gourou de la sécurité, GitHub affirme que le marché est mûr pour un outil alimenté par l’IA capable de remédier à ce niveau. À une époque où les professionnels moins techniques sont « préoccupés par l’impact de l’IA », il s’agit peut-être d’un bon exemple d’intelligence d’automatisation utilisée pour résoudre des problèmes plutôt que pour en créer de nouveaux.</p>'}]"
Endor Labs ajoute des outils d'analyse et de correctifs pour sécuriser les logiciels open source,"[{'', ""<p>Endor Labs a révélé aujourd'hui, lors de la conférence Black Hat USA 2024, qu'il avait ajouté la capacité de déterminer à quel point il pourrait s'avérer difficile de mettre à niveau un progiciel open source, y compris son potentiel à casser une application, à sa plateforme de sécurisation des chaînes d'approvisionnement en logiciels.</p>""}, {'', ""<p>De plus, la société ajoute des correctifs Endor Magic pour permettre aux équipes DevSecOps d'appliquer les correctifs créés dans une version ultérieure à une version précédente du module si elles déterminent que la mise à niveau de ce module serait trop difficile.</p>""}, {'', ""<p>Jenn Gile, directrice du marketing produit chez Endor Labs, a déclaré que ces capacités d'analyse fourniront aux équipes DevSecOps le contexte nécessaire pour déterminer quand mettre à niveau un module en fonction du niveau potentiel de perturbation qu'un nouveau module pourrait introduire dans un environnement informatique.</p>""}, {''}, {'', ""<p>Les outils d'analyse de la composition logicielle (SCA) existants sont certes capables d'identifier les vulnérabilités, mais ils ne disposent pas des conseils de correction essentiels dont les entreprises ont besoin pour prendre la décision de procéder à une mise à niveau, a-t-elle ajouté. Endor Labs, au contraire, utilise des analyses appliquées au moment de la création d'une version pour déterminer exactement quelles dépendances tierces sont utilisées et comment elles interagissent avec le code de l'application, a noté Gile.</p>""}, {'', '<p>En conséquence, la plupart des organisations choisiront de ne pas mettre à niveau un module en supposant que le niveau de risque pour l’entreprise est trop élevé, a-t-elle ajouté.</p>'}, {'', '<p>À l’inverse, les équipes DevSecOps peuvent décider de procéder à une mise à niveau pour découvrir que l’effort requis était bien plus important que prévu. Il n’est pas rare que les équipes DevSecOps annulent une mise à niveau une fois ce niveau de difficulté déterminé.</p>'}, {'', '<p>Les dernières fonctionnalités ajoutées à la plateforme Endor Labs fournissent les renseignements exploitables dont les équipes DevSecOps travaillant en collaboration avec leurs collègues de cybersécurité ont besoin pour prendre des décisions de mise à niveau plus éclairées, a déclaré Gile.</p>'}, {'', ""<p>S'il est déterminé que la mise à niveau d'un module est trop difficile, les équipes DevSecOps ont alors la possibilité de déployer le code source et les correctifs associés, y compris les étapes de test, de création et de déploiement, à l'aide des correctifs magiques Endor.</p>""}, {'', '<p>La sécurité des logiciels open source est devenue un enjeu majeur, car les entreprises qui s’appuient sur ces applications qui incluent du code open source ne peuvent pas facilement développer un correctif à chaque fois qu’une nouvelle vulnérabilité est découverte. Dans de nombreux cas, comme dans le cas de la célèbre vulnérabilité du shell Log4J qui a affecté les applications Java, les équipes informatiques découvrent que le code affecté est maintenu par une petite équipe de bénévoles non rémunérés qui n’ont ni le temps ni la motivation pour développer rapidement un correctif.</p>'}, {'', '<p>Alors que la communauté open source met en commun ses ressources pour relever ce type de défi à l’avenir, de nombreuses organisations informatiques d’entreprise réévaluent leurs applications dans l’intention de devenir moins dépendantes des logiciels open source qui ne sont pas maintenus par une grande équipe de développeurs contribuant activement au projet.</p>'}, {'', '<p>D’une manière ou d’une autre, il arrivera certainement un moment où une nouvelle vulnérabilité zero-day sera découverte et les équipes DevSecOps devront immédiatement la corriger avant que les cybercriminels ne l’exploitent. C’est particulièrement important à une époque où le temps entre la divulgation d’une vulnérabilité et les premières attaques qui l’exploitent se mesure désormais en heures.</p>'}]"
Se concentrer sur les fondamentaux ou sur l'innovation ? La réussite de DevOps nécessite les deux,"[{'', '<p>Les équipes DevOps sont confrontées à un fossé entre les traditionalistes et les innovateurs, un fossé qui s’accentue à mesure que la pression monte pour que les équipes tirent parti des dernières avancées. Les traditionalistes se battent pour préserver les fondamentaux, pour conserver une certaine appréciation des débuts de DevOps et pour ne pas faire de concessions (il suffit de prendre l’exemple de CrowdStrike). De l’autre côté, les innovateurs sont prêts à saisir leurs moments de gloire avec les dernières avancées.</p>'}, {'', '<p>Pour que la fonction DevOps soit performante, vous devez accorder la même importance aux deux. La fusée DevOps sera toujours lancée, mais le succès de son voyage dépend en grande partie du travail effectué avant le début du compte à rebours. Comment les équipes peuvent-elles adopter l’état d’esprit de la NASA ? Plongeons-nous dans le vif du sujet…</p>'}, {'', '<h3>Retour aux fondamentaux : des builds fiables et reproductibles</h3>'}, {'', '<ol>Processus sous contrôle : pour garantir que la fusée décolle correctement à chaque fois, la « norme » doit être aussi standard que possible. Les processus perdent généralement leur standardisation lors de l’assemblage et du déploiement de la build. Souvent, il existe différentes techniques et différents environnements par rapport à la création d’un chemin de promotion unique qui construit chaque environnement – \u200b\u200bquel que soit son objectif – de la même manière. Chaque build vers un environnement inférieur est essentiellement une « répétition » de sorte qu’au moment où vous arrivez à la production, vous avez probablement construit au moins deux choses différentes dans les mêmes scripts avec la même approche. Dans DevOps, la routine est primordiale, mais vous devez tout traiter comme un moment potentiel de CrowdStrike.</ol>'}, {'', '<li>Processus sous contrôle : pour garantir que la fusée décolle correctement à chaque fois, la « norme » doit être aussi standard que possible. Les processus perdent généralement leur standardisation lors de l’assemblage et du déploiement de la build. Souvent, il existe différentes techniques et différents environnements par rapport à la création d’un chemin de promotion unique qui construit chaque environnement – \u200b\u200bquel que soit son objectif – de la même manière. Chaque build vers un environnement inférieur est essentiellement une « répétition » de sorte qu’au moment où vous arrivez à la production, vous avez probablement construit au moins deux choses différentes dans les mêmes scripts avec la même approche. Dans DevOps, la routine est primordiale, mais vous devez tout traiter comme un moment potentiel de CrowdStrike.</li>'}, {''}, {'', ""<ol>Capture de bout en bout : les responsables de la construction adoptent de nouvelles approches en cours de route qui ne vivent souvent que dans leur esprit. Malheureusement, ces stratégies ne sont pas toujours adaptées à la façon dont ces environnements évoluent au fil du temps, car ils sont continuellement actualisés en termes de code et de matériel. Même si cela peut sembler fastidieux, les équipes DevOps doivent avoir une image de bout en bout réaliste et vérifiée des environnements. Cela peut prendre la forme de récits, d'éléments visuels, de vidéos enregistrées, etc. Mais l'objectif ici est de capturer. Selon le rapport 2023 State of DevOps, la qualité de la documentation est le moteur de la mise en œuvre réussie de chaque capacité technique étudiée, de l'intégration continue au développement basé sur le tronc en passant par les pratiques de fiabilité.</ol>""}, {'', ""<li>Capture de bout en bout : les responsables de la construction adoptent de nouvelles approches en cours de route qui ne vivent souvent que dans leur esprit. Malheureusement, ces stratégies ne sont pas toujours adaptées à la façon dont ces environnements évoluent au fil du temps, car ils sont continuellement actualisés en termes de code et de matériel. Même si cela peut sembler fastidieux, les équipes DevOps doivent avoir une image de bout en bout réaliste et vérifiée des environnements. Cela peut prendre la forme de récits, d'éléments visuels, de vidéos enregistrées, etc. Mais l'objectif ici est de capturer. Selon le rapport 2023 State of DevOps, la qualité de la documentation est le moteur de la mise en œuvre réussie de chaque capacité technique étudiée, de l'intégration continue au développement basé sur le tronc en passant par les pratiques de fiabilité.</li>""}, {''}, {'', '<ol>Réduction des étapes manuelles : Dans la mesure du possible, l’intervention humaine ne doit être utilisée que pour les points de décision, et non pour terminer ou corriger une tâche « fastidieuse ». Ce concept est parfois mal compris. Dans le cadre de l’intégration continue/livraison continue, l’expression « intégration continue » signifie idéalement qu’en appuyant sur un bouton, vous pouvez promouvoir du code et qu’il peut passer par tous les environnements du chemin de promotion et atterrir en production. Si vous le deviez, vous pourriez cliquer sur un bouton pour que le changement apparaisse en production à la vitesse du cycle de vos processus automatiques. Le « déploiement continu », en revanche, est une décision humaine de faire exactement cela. Il est relativement rare qu’un processus de build soit si routinier, à faible risque ou si urgent que vous choisissiez d’éliminer l’intervention humaine. En général, la meilleure pratique consiste à disposer de builds hautement automatisés avec des notifications et des alertes automatisées pour savoir quand les humains doivent intervenir.</ol>'}, {'', '<li>Réduction des étapes manuelles : Dans la mesure du possible, l’intervention humaine ne doit être utilisée que pour les points de décision, et non pour terminer ou corriger une tâche « fastidieuse ». Ce concept est parfois mal compris. Dans le cadre de l’intégration continue/livraison continue, l’expression « intégration continue » signifie idéalement qu’en appuyant sur un bouton, vous pouvez promouvoir du code et qu’il peut passer par tous les environnements du chemin de promotion et atterrir en production. Si vous le deviez, vous pourriez cliquer sur un bouton pour que le changement apparaisse en production à la vitesse du cycle de vos processus automatiques. Le « déploiement continu », en revanche, est une décision humaine de faire exactement cela. Il est relativement rare qu’un processus de build soit si routinier, à faible risque ou si urgent que vous choisissiez d’éliminer l’intervention humaine. En général, la meilleure pratique consiste à disposer de builds hautement automatisés avec des notifications et des alertes automatisées pour savoir quand les humains doivent intervenir.</li>'}, {'', '<h3>Innovation : un regard sur les opportunités</h3>'}, {'', ""<ol>Infrastructure as Code : l'un des défis des builds est la nécessité de résoudre plusieurs problèmes en même temps, comme une promotion de fonctionnalité par l'équipe de développement ; des défauts qui pourraient devoir être corrigés en fonction de l'expérience en production ou dans des environnements inférieurs ; des correctifs et des actualisations sur les systèmes sous-jacents. La possibilité de collision ou d'omission est relativement élevée dans un environnement dynamique. L'infrastructure as code (IaC) résout cet obstacle frustrant à la réussite de la première build en construisant la pile entière à partir de zéro à chaque fois. L'IaC présuppose que vous disposez de conteneurs et que vous pouvez créer et déployer des éléments virtuellement, mais il s'agit d'une innovation incroyablement puissante dans DevOps en raison de sa capacité à garantir des builds propres et complètes dès la première fois, à chaque fois.</ol>""}, {'', ""<li>Infrastructure as Code : l'un des défis des builds est la nécessité de résoudre plusieurs problèmes en même temps, comme une promotion de fonctionnalité par l'équipe de développement ; des défauts qui pourraient devoir être corrigés en fonction de l'expérience en production ou dans des environnements inférieurs ; des correctifs et des actualisations sur les systèmes sous-jacents. La possibilité de collision ou d'omission est relativement élevée dans un environnement dynamique. L'infrastructure as code (IaC) résout cet obstacle frustrant à la réussite de la première build en construisant la pile entière à partir de zéro à chaque fois. L'IaC présuppose que vous disposez de conteneurs et que vous pouvez créer et déployer des éléments virtuellement, mais il s'agit d'une innovation incroyablement puissante dans DevOps en raison de sa capacité à garantir des builds propres et complètes dès la première fois, à chaque fois.</li>""}, {''}, {'', '<ol>Créer des builds adaptatifs via MLOps : une partie importante de chaque build consiste à déterminer le « package » de validation et de vérification qui accompagnera le code source déployé. Il est impératif de pouvoir identifier les risques, les ralentissements et les points de défaillance. Mais en l’absence de moyens d’évaluer ces éléments en fonction de chaque version, la décision par défaut est d’inclure l’intégralité de la vérification dans chaque version, ce qui allonge le cycle de build et de publication. Un bon point de départ est d’appliquer le ML/AI pour créer des builds adaptatifs capables d’automatiser les vérifications et de garantir que les critères sont respectés et qu’ils sont sécurisés et performants. L’alimentation de cette infrastructure d’optimisation de build avec le pipeline de données en cours garantit qu’elle reste pertinente et optimisée en temps réel. Bien qu’une grande partie de ces tâches puisse être effectuée manuellement par des humains qualifiés, ce sont des tâches pour lesquelles le ML/AI est un choix naturel.</ol>'}, {'', '<li>Créer des builds adaptatifs via MLOps : une partie importante de chaque build consiste à déterminer le « package » de validation et de vérification qui accompagnera le code source déployé. Il est impératif de pouvoir identifier les risques, les ralentissements et les points de défaillance. Mais en l’absence de moyens d’évaluer ces éléments en fonction de chaque version, la décision par défaut est d’inclure l’intégralité de la vérification dans chaque version, ce qui allonge le cycle de build et de publication. Un bon point de départ est d’appliquer le ML/AI pour créer des builds adaptatifs capables d’automatiser les vérifications et de garantir que les critères sont respectés et qu’ils sont sécurisés et performants. L’alimentation de cette infrastructure d’optimisation de build avec le pipeline de données en cours garantit qu’elle reste pertinente et optimisée en temps réel. Bien qu’une grande partie de ces tâches puisse être effectuée manuellement par des humains qualifiés, ce sont des tâches pour lesquelles le ML/AI est un choix naturel.</li>'}, {''}, {'', '<ol>Intégrer l’ingénierie de fiabilité du site (SRE) : l’ingénierie de fiabilité du site (SRE) représente la prochaine étape de DevOps, car elle brise les silos et crée une continuité entre ceux qui créent le code et ceux qui interagissent directement avec les clients. Une façon d’intégrer l’ingénierie de fiabilité du site (SRE) dans DevOps est de créer une liste de rotation dans laquelle chaque développeur effectue périodiquement une « tournée de service » au sein du service d’assistance, par exemple six semaines et au deuxième niveau de support. Une « tournée de service » réciproque consistant à placer le personnel du service d’assistance dans les équipes de développement en tant qu’analystes commerciaux ou développeurs (selon leur formation technique) est tout aussi bénéfique. La collaboration et les connaissances générées par un modèle SRE ont le potentiel de transformer l’expérience de l’utilisateur final.</ol>'}, {'', '<li>Intégrer l’ingénierie de fiabilité du site (SRE) : l’ingénierie de fiabilité du site (SRE) représente la prochaine étape de DevOps, car elle brise les silos et crée une continuité entre ceux qui créent le code et ceux qui interagissent directement avec les clients. Une façon d’intégrer l’ingénierie de fiabilité du site (SRE) dans DevOps est de créer une liste de rotation dans laquelle chaque développeur effectue périodiquement une « tournée de service » au sein du service d’assistance, par exemple six semaines et au deuxième niveau de support. Une « tournée de service » réciproque consistant à placer le personnel du service d’assistance dans les équipes de développement en tant qu’analystes commerciaux ou développeurs (selon leur formation technique) est tout aussi bénéfique. La collaboration et les connaissances générées par un modèle SRE ont le potentiel de transformer l’expérience de l’utilisateur final.</li>'}, {'', '<h3>Conclusion</h3>'}, {'', '<p>La « tension opérationnelle » entre les traditionalistes et les innovateurs de DevOps pourrait s’accroître à l’heure actuelle ; après tout, le secret de DevOps a été de faire de ce qui est difficile une routine.</p>'}, {'', '<p>Mais la bonne nouvelle est que les équipes ont besoin des deux. Il s’agit simplement de trouver des moyens de réunir ces mentalités, d’expliquer l’impact que ces deux éléments ont sur une fonction DevOps réussie et d’identifier des moyens de dynamiser les efforts DevOps en accordant une crédibilité égale aux deux.</p>'}]"
Une enquête révèle que la vitesse de déploiement des logiciels dépasse la sécurité,"[{'', ""<p>Une enquête menée auprès de 5 315 contributeurs individuels et dirigeants du développement, des opérations informatiques et de la sécurité révèle que les deux tiers (66 %) d'entre eux publient des logiciels plus rapidement qu'il y a un an.</p>""}, {'', ""<p>Réalisée par le cabinet d'études de marché Omdia pour le compte de GitLab, l'enquête révèle également que parmi les 40 % des développeurs ayant participé à l'enquête, 24 % ont déclaré qu'ils poussaient du code dans des environnements de production au moins une fois par jour, et 13 % poussaient du code plusieurs fois par jour.</p>""}, {'', '<p>Dans le même temps, plus des trois quarts (78 %) des personnes interrogées ont déclaré qu’elles utilisaient déjà l’intelligence artificielle (IA) ou qu’elles prévoyaient de l’utiliser dans les deux prochaines années pour développer des logiciels. Plus de la moitié (55 %) ont reconnu que l’introduction de l’IA dans le cycle de développement logiciel était risquée, la confidentialité et la sécurité des données étant la principale préoccupation.</p>'}, {'', '<p>Malheureusement, l’enquête suggère également que les meilleures pratiques en matière de sécurisation des logiciels sont encore en retard. L’enquête révèle que parmi les 27 % de personnes interrogées qui travaillent dans le domaine de la sécurité, seulement 38 % déclarent avoir transféré la responsabilité de la sécurité des applications aux développeurs, et seulement 34 % déclarent proposer une formation à la sécurité aux développeurs.</p>'}, {'', '<p>Du côté positif, 32 % prévoient de commencer à proposer cette formation cette année, et plus d’un quart (26 %) de ceux qui utilisent actuellement l’IA pour le développement d’applications ont identifié l’amélioration de la sécurité comme l’un des principaux avantages de l’IA. Plus de la moitié (52 %) ont déclaré qu’ils étaient intéressés par l’utilisation ou prévoyaient de s’appuyer sur les explications de l’IA sur les vulnérabilités de sécurité pour améliorer le code.</p>'}, {'', ""<p>Dans l'ensemble, plus des deux tiers des développeurs (67 %) ont déclaré que plus d'un quart du code sur lequel ils travaillent provient de bibliothèques open source, et 40 % d'entre eux ont indiqué que les composants logiciels open source constituaient plus de la moitié du code de leur application. À l'heure actuelle, cependant, seuls 20 % d'entre eux travaillent pour des organisations utilisant des nomenclatures de logiciels (SBOM).</p>""}, {'', ""<p>Plus inquiétant encore, seulement 34 % déclarent utiliser des outils de test de sécurité des applications dynamiques (DAST), suivis de près par 33 % utilisant des outils de test de sécurité des applications statiques (SAST), l'analyse des conteneurs (29 %) et la détection des secrets (24 %).</p>""}, {'', '<p>Plus de la moitié des personnes interrogées dans le domaine de la sécurité ont également déclaré avoir du mal à convaincre les équipes de développement de donner la priorité à la correction des vulnérabilités, et 52 % ont indiqué que la bureaucratie ralentit souvent leurs efforts pour corriger rapidement les vulnérabilités.</p>'}, {'', ""<p>Josh Lemos, responsable de la sécurité des systèmes d'information de GitLab, a déclaré que même si des progrès ont été réalisés en termes d'adoption des meilleurs processus DevSecOps, il reste encore beaucoup à faire. La sécurité des applications ne fera que devenir un problème plus urgent à mesure que la quantité de code créée à l'aide d'outils d'IA augmentera, a-t-il ajouté.</p>""}, {'', '<h3>Le rythme de développement des logiciels</h3>'}, {'', '<p>Plutôt que de simplement traiter la sécurité des applications comme une autre porte à ajouter à un flux de travail DevOps, les organisations doivent, autant que possible, fournir aux développeurs le contexte dont ils ont besoin pour résoudre les problèmes de sécurité au moment où ils écrivent du code, a déclaré Lemos.</p>'}, {'', '<p>Le défi, bien sûr, est de trouver des moyens de continuer à créer des logiciels plus sûrs sans ralentir le rythme auquel ils sont créés, a-t-il noté.</p>'}, {'', ""<p>Chaque organisation devra naturellement déterminer elle-même le rythme auquel elle souhaite développer ses logiciels. Cependant, plus une entreprise dépend des logiciels pour générer des revenus, plus la pression pour créer de nouvelles applications tout en mettant à jour en permanence les applications existantes ne cesse d'augmenter.</p>""}]"
Les risques de l’oligopole du cloud,"[{'', '<p>Le marché mondial des technologies est de plus en plus dominé par quelques entreprises, chacune luttant pour renforcer sa position concurrentielle. Pour illustrer clairement cette tendance, une analyse récente montre qu’Amazon Web Services (AWS), Microsoft Azure et Google Cloud contrôlent plus des deux tiers de l’ensemble du secteur du cloud public. Dans ce que l’on appelle désormais « l’oligopole du cloud », la consolidation du marché a créé des défis inattendus pour les entreprises. Dans leur quête d’une activité distribuée et plus résiliente, de nombreuses organisations se sont exposées par inadvertance au risque de concentration, la plupart des produits SaaS sur lesquels elles s’appuient étant hébergés chez les mêmes fournisseurs de cloud public.</p>'}, {'', '<p>Les régulateurs ont exprimé à plusieurs reprises leurs inquiétudes quant à ce risque et au risque que les entreprises deviennent « prisonnières » de leurs fournisseurs. L’Ofcom, par exemple, a publié en 2023 un rapport faisant état de ses inquiétudes quant aux pratiques d’entreprises comme AWS et Microsoft. Il s’agit notamment des frais de sortie élevés, des obstacles techniques à l’interopérabilité et de l’épée à double tranchant des remises sur les dépenses engagées.</p>'}, {'', '<p>Dans un contexte où les marchés sont en pleine mutation et où une réglementation stricte n’est pas susceptible d’entrer en vigueur prochainement, il incombe aux entreprises qui investissent dans le cloud public d’évaluer leur propre exposition à ces risques. À cette fin, nous avons examiné de plus près la dynamique et l’avenir de l’oligopole du cloud, ainsi que les mesures que les entreprises peuvent prendre pour protéger leurs intérêts.</p>'}, {'<h3>Le risque du cloud et de la concentration</h3>', ''}, {'', '<p>Le risque de concentration est à son maximum lorsqu’une organisation dépend d’un nombre limité de fournisseurs, ce qui peut entraîner des perturbations importantes en cas de problèmes. Dans le contexte du cloud, le recours à plusieurs fournisseurs peut créer l’illusion d’une diversité de la chaîne d’approvisionnement si leur hébergeur cloud partagé tombe soudainement en panne – cette illusion se brise.</p>'}, {'', '<p>En pratique, le risque de concentration rend les entreprises plus vulnérables aux défaillances de leurs fournisseurs. Si un fournisseur de cloud subit une interruption inattendue – qu’il s’agisse d’une faille de sécurité, d’une pénurie dans la chaîne d’approvisionnement ou d’une catastrophe naturelle –, toutes les entreprises qui dépendent de ses services en subiront également les conséquences. Bien entendu, une interruption potentielle peut avoir de graves conséquences financières ou en termes de réputation.</p>'}, {'', '<p>En outre, le risque de concentration ne concerne pas uniquement les entreprises individuelles, il peut avoir des répercussions sur des secteurs entiers. Si une part importante d’entreprises similaires dépend d’un seul fournisseur de cloud, une perturbation pourrait se propager à l’ensemble du marché avec des conséquences généralisées et imprévisibles.</p>'}, {'', ""<h3>Éviter la dépendance vis-à-vis d'un fournisseur</h3>""}, {'', ""<p>Le verrouillage du fournisseur est également un effet secondaire malheureux, bien que presque certainement intentionnel, de l'oligopole du cloud. Le verrouillage du cloud se produit lorsqu'une entreprise devient dépendante d'un seul fournisseur de cloud en raison de coûts cachés, de problèmes d'interopérabilité ou d'incitations financières qui peuvent rendre difficile le passage à un autre fournisseur.</p>""}, {'', '<p>Par exemple, les principaux fournisseurs fixent des frais de sortie (c’est-à-dire des frais pour transférer des données hors d’un cloud), ce qui rend le changement de fournisseur coûteux. Les restrictions techniques peuvent également rendre difficile la configuration des données et des applications pour qu’elles fonctionnent sur différents clouds et ancrer davantage les clients dans l’écosystème d’un seul fournisseur.</p>'}, {'', '<p>De plus, les fournisseurs de cloud sont désireux d’offrir des remises sur les dépenses engagées. Si ces dernières offrent un avantage considérable en termes d’économies, elles peuvent également inciter les clients à concentrer leur utilisation du cloud auprès d’un seul fournisseur, renforçant ainsi la dépendance vis-à-vis du fournisseur.</p>'}, {'', '<h3>Renforcer la résilience dans un marché concentré</h3>'}, {'', '<p>Pour atténuer les risques associés à l’oligopole du cloud, les entreprises doivent adopter des stratégies qui renforcent la résilience et réduisent la dépendance à un fournisseur unique. Voici quelques approches clés :</p>'}, {'', '<ol>Audits réguliers et évaluations des risques</ol>'}, {'', '<li>Audits réguliers et évaluations des risques</li>'}, {'', '<p>Les entreprises doivent procéder à des audits réguliers de leurs chaînes d’approvisionnement afin d’identifier les vulnérabilités. Cela implique non seulement d’évaluer les fournisseurs directs, mais aussi d’examiner les chaînes d’approvisionnement de ces fournisseurs. Comprendre où se situent les risques permet aux entreprises de prendre des mesures proactives pour y faire face.</p>'}, {'', '<ol>Diversification des fournisseurs de cloud</ol>'}, {'', '<li>Diversification des fournisseurs de cloud</li>'}, {'', ""<p>Même si le recours à plusieurs fournisseurs de cloud présente des défis techniques et financiers, la diversification est essentielle pour la résilience. Le recours à différents fournisseurs pour les systèmes de production et les sauvegardes peut contribuer à atténuer le risque d'un point de défaillance unique.</p>""}, {'', ""<p>Le basculement immédiat vers un autre cloud peut ne pas être pratique ou rentable, mais disposer de copies de données en dehors de l'environnement de production est également essentiel à la résilience. Au minimum, la configuration de sauvegardes vers un autre cloud est une pratique peu coûteuse mais utile.</p>""}, {'', '<ol>Construction sur plusieurs zones et régions de disponibilité</ol>'}, {'', '<li>Construction sur plusieurs zones et régions de disponibilité</li>'}, {'', ""<p>Les entreprises doivent structurer leurs environnements cloud sur plusieurs zones de disponibilité au sein d'une même région afin de limiter les impacts des pannes localisées. Le déploiement sur différentes régions géographiques peut également contribuer à améliorer la résilience, bien qu'il s'agisse d'une opération plus coûteuse et plus complexe.</p>""}, {'', ""<ol>Adopter l'interopérabilité et les conteneurs</ol>""}, {'', ""<li>Adopter l'interopérabilité et les conteneurs</li>""}, {'', ""<p>Les technologies telles que les conteneurs peuvent faciliter la résilience multicloud en facilitant le déplacement des applications et des données entre différents environnements cloud. Les conteneurs encapsulent les applications et leurs dépendances, leur permettant de s'exécuter de manière cohérente sur différentes plateformes cloud.</p>""}, {'', '<p>Les méthodes et outils d’infrastructure en tant que code (IaC) permettent également la création et la gestion d’infrastructures qui peuvent être répliquées rapidement et efficacement dans différents environnements cloud.</p>'}, {'', ""<h3>L'avenir du cloud et de l'IA</h3>""}, {'', '<p>Alors que l’adoption et la complexité de l’intelligence artificielle (IA) continuent de s’accélérer, l’influence de l’oligopole du cloud se fait déjà sentir. Le stockage, l’infrastructure et le calcul dans le cloud sont essentiels au développement de l’IA, car ils fournissent les moyens de développer et de former les modèles d’IA les plus avancés.</p>'}, {'', '<p>Cela signifie toutefois que les principaux fournisseurs de cloud exercent un contrôle considérable sur l’avenir de l’IA, ce qui risque d’étouffer la concurrence et l’innovation dans ce domaine. L’IA devenant de plus en plus essentielle à divers secteurs, cette dépendance pourrait exacerber les risques de concentration existants.</p>'}, {'', '<p>Les décideurs politiques et les acteurs de l’industrie doivent donc envisager des mesures visant à promouvoir la concurrence et à garantir un environnement sain et concurrentiel pour le développement de l’IA.</p>'}, {'<h3>Réflexions finales</h3>', ''}, {'', '<p>L’oligopole du cloud, dans la mesure où il étouffe la concurrence, représente un domaine potentiellement négligé dans lequel les entreprises doivent évaluer leurs engagements en matière de chaîne d’approvisionnement. Si la croissance de ces fournisseurs a rendu le cloud public plus accessible que jamais, leur ascension a finalement exposé les entreprises et le marché dans son ensemble à de nouvelles vulnérabilités.</p>'}, {'', '<p>Dans ces circonstances, les entreprises doivent adopter des stratégies pour diversifier leur utilisation du cloud, renforcer leur résilience et atténuer les risques. Heureusement, les organismes de réglementation commencent déjà à s’en rendre compte et joueront un rôle crucial pour garantir que le marché reste compétitif et équitable.</p>'}, {'', '<p>Alors que nous nous tournons vers l’avenir, notamment avec l’essor de l’IA, il deviendra encore plus crucial de répondre à ces questions pour garantir que chaque entreprise maintienne sa continuité et sa résilience dans un paysage technologique en évolution rapide.</p>'}]"
Cinq leçons apprises à la dure face à l’écran bleu mondial de la mort,"[{'', '<p>En tant que professionnel de la gestion de produits, je suis formé pour comprendre les difficultés des clients, en particulier ceux qui rencontrent des problèmes de temps d’arrêt en raison d’une mauvaise qualité des logiciels. Vendredi dernier, j’ai vécu le problème de première main lorsque je suis arrivé tôt à l’aéroport d’Heathrow, pour découvrir que la panne de Microsoft avait cloué au sol des vols dans le monde entier. Mon vol de Londres n’a été retardé que de quelques heures, mais mon vol de correspondance a été complètement annulé. Comme c’était le dernier vol de la journée vers ma ville natale, j’ai dû trouver une chambre d’hôtel pour la nuit, mais toutes les chambres dans un rayon de 50 kilomètres de l’aéroport étaient complètes. Tout cela est dû à un bug dans un petit fichier pas beaucoup plus gros que cet article, qui a eu un impact sur des millions de personnes à travers le monde.</p>'}, {'', '<p>La plupart des versions de logiciels contiennent des défauts. Je n’ai jamais vu de code de production qui ne présentait aucun problème, en particulier dans les applications d’entreprise. Les découvertes qui sont publiées en production sont généralement des bugs cosmétiques ou mineurs qui affectent une fonctionnalité rarement utilisée. Cependant, des défauts comme celui qui a fait tomber les compagnies aériennes sont une toute autre situation. Il est maintenant temps de réfléchir aux leçons que nous pouvons tirer de cet événement.</p>'}, {'', '<p>Leçon 1 : le décalage vers la droite n’est PAS facultatif. Le décalage vers la droite est le concept que vous devez tester en production, après la sortie. On peut se demander si cela a été fait dans le cas de la panne de la semaine dernière. Même si les tests ont été effectués dans les environnements de pré-version, vous ne pouvez pas être sûr que vos environnements de pré-version sont identiques à ceux de production. Vous devez au moins effectuer des tests de détection en production. Mieux encore, exécutez régulièrement des tests de régression. Même lors de tests en production, tester un seul processus ou composant peut ne pas suffire. Ce qui nous amène à la leçon suivante.</p>'}, {'', ""<p>Deuxième leçon : l'intégration et les tests de bout en bout du système sont indispensables. La plupart des applications d'entreprise actuelles sont construites à partir d'un ensemble complexe de services et de systèmes interconnectés. Chaque service isolé peut fonctionner exactement comme prévu, mais lorsque vous le testez avec tous les autres éléments mobiles, vous pouvez constater que de petits retards ici et là perturbent le processus de bout en bout. Ces problèmes liés au temps sont exacerbés par des volumes inhabituellement élevés, ce qui nous amène à la leçon suivante.</p>""}, {'', ""<p>Troisième leçon : les tests de performance et de charge sont également importants. Les tests de performance et de charge peuvent ne pas sembler importants dans des circonstances normales, mais lors de la reprise après une panne, on assiste généralement à une vague d'utilisateurs essayant de poursuivre le travail retardé. On pourrait considérer cela comme un effet en cascade, mais la situation se présente souvent à la fin du mois et du trimestre pour certaines entreprises.</p>""}, {'', '<p>Dans le cas de l’incident de la semaine dernière, un grand nombre de vols annulés a entraîné une augmentation du volume sur le système de réservation des compagnies aériennes. Un système qui n’a peut-être pas été affecté par le bug réel est affecté par un niveau d’activité inhabituel. Les tests de charge ne sont pas une tâche facile et il n’est peut-être pas nécessaire de les effectuer régulièrement, mais vous ne devez pas attendre qu’une catastrophe se produise pour apprendre qu’un volume supérieur de 20 % à la moyenne rendra un système inutilisable.</p>'}, {'', '<p>Quatrième leçon : vous n’êtes pas obligé de diffuser le logiciel à tout le monde en même temps. Deux concepts auraient permis d’éviter une crise mondiale. Le premier est le « dogfooding », c’est-à-dire le fait de manger sa propre nourriture pour chien. Il s’agit de la pratique consistant à utiliser son propre logiciel avant de le diffuser à d’autres. De cette façon, en cas de problèmes majeurs, vous les détectez rapidement et vous limitez les souffrances au sein de votre propre organisation.</p>'}, {'', ""<p>Le deuxième concept est appelé « versions canari ». Tout comme les mineurs emmenaient autrefois des canaris dans les mines de charbon pour les avertir d'une condition mortelle, vous pouvez également diffuser du code à un petit sous-ensemble d'utilisateurs externes pendant une courte période pour garantir qu'une version fonctionne en dehors de votre organisation. Une fois encore, vous limitez l'impact négatif.</p>""}, {'', '<p>Cinquième leçon : le coût des pannes de système a des répercussions en cascade. Il ne s’agit pas seulement de la perte de productivité pendant le temps d’arrêt, mais aussi de l’effet domino. Par exemple, un temps d’arrêt dans le système de saisie des commandes peut entraîner des problèmes avec le système de gestion des stocks, ce qui peut retarder les expéditions et avoir un impact sur le chiffre d’affaires et les résultats financiers. La perte de revenus et de bénéfices peut amener les actionnaires et les clients à voter avec leurs pieds.</p>'}, {'', ""<p>La perte de productivité est aggravée par la nécessité de faire des heures supplémentaires pour rattraper le retard. Pour les travailleurs horaires, cela représente un coût direct pour l'entreprise. Pour les employés salariés, comme les développeurs et les testeurs, cela signifie un impact négatif sur leur satisfaction au travail et sur le temps passé loin de leur famille. L'augmentation du turnover dans le secteur informatique a également un coût.</p>""}, {'', ""<p>La baisse de la satisfaction client est un autre coût indirect de la mauvaise qualité qui n'est pas toujours directement lié à une panne. Même si vos concurrents ont été touchés par le même incident, vos clients ne le savent peut-être pas. Ils voyageaient avec vous ce jour-là.</p>""}, {'', ""<p>La prochaine fois que vous négocierez un budget pour les outils et les ressources de test, n'oubliez pas qu'il y a non seulement des coûts importants liés aux temps d'arrêt, mais aussi de nombreux coûts indirects. Parfois, je pense que le coût des outils et du personnel de test devrait être classé dans la catégorie des assurances, car c'est le cas. Le fait que les bugs entraînent très rarement des temps d'arrêt signifie que la douleur n'est généralement pas aiguë. Nous vivons avec cela. Nos clients vivent avec cela. Jusqu'au jour où cela vous met à terre.</p>""}, {'', '<p>N’attendez pas que la douleur soit insupportable. Investissez dans des personnes, des processus et des outils pour vous assurer de ne jamais avoir à expliquer pourquoi un bug dans un petit fichier a eu un impact sur le monde entier.</p>'}]"
Une enquête révèle des signes inquiétants d'insécurité dans la chaîne d'approvisionnement des logiciels,"[{'', ""<p>Une enquête mondiale menée auprès de 1 224 professionnels de l'ingénierie logicielle travaillant dans des organisations de plus de 1 000 employés suggère qu'un écart important est apparu entre ce que les cadres supérieurs estiment être fait pour améliorer la sécurité des applications et ce qui se passe réellement.</p>""}, {'', ""<p>Réalisée par Atomik Research pour le compte de JFrog, fournisseur d'une plateforme DevSecOps, l'enquête révèle que 67 % des 331 cadres et managers interrogés estiment que des analyses de sécurité au niveau du code sont effectuées régulièrement, mais seulement 41 % des développeurs sont du même avis.</p>""}, {'', '<p>88 % des dirigeants estiment également que l’intelligence artificielle (IA) et les outils d’apprentissage automatique sont utilisés pour les processus d’analyse et de correction de la sécurité. Cependant, seules 60 % des équipes DevSecOps ont déclaré utiliser réellement ces outils. Un peu plus de 90 % des dirigeants pensent également qu’ils utilisent des modèles d’apprentissage automatique dans leurs applications logicielles, alors que seulement 63 % des développeurs confirment cette affirmation.</p>'}, {'', '<p>De même, 92 % des dirigeants affirment que leurs organisations disposent d’outils pour détecter les packages open source malveillants, alors que seulement 70 % des développeurs sont du même avis.</p>'}, {'', ""<p>Paul Davis, responsable de la sécurité des systèmes d'information chez JFrog Field, a déclaré que le fossé entre les dirigeants et les développeurs de base est probablement dû au simple fait que de nombreuses organisations sont encore en train de repenser leurs workflows DevSecOps. Le défi est que chaque organisation commence ce voyage à partir d'un point de départ différent et progresse donc à des rythmes différents, souvent difficiles à quantifier.</p>""}, {'', '<p>Le fait que seulement 30 % des personnes interrogées aient identifié la nécessité de remédier aux vulnérabilités de leur chaîne d’approvisionnement de logiciels comme une préoccupation majeure en matière de sécurité complique encore les choses. On ne comprend pas vraiment pourquoi moins d’un tiers des personnes interrogées s’inquiètent des vulnérabilités malgré leur prévalence, mais les résultats de l’enquête suggèrent qu’il existe soit un niveau de confiance excessif, soit que d’autres problèmes plus urgents pourraient prendre le pas.</p>'}, {""<h3>Sécuriser les chaînes d'approvisionnement de logiciels avec ou sans réglementation</h3>"", ''}, {'', '<p>On ne sait pas exactement dans quelle mesure la réglementation pourrait inciter les entreprises à mieux sécuriser leurs chaînes d’approvisionnement en logiciels. L’Union européenne (UE), par exemple, fait avancer le Cyber \u200b\u200bResilience Act (CRA) qui oblige les entreprises à sécuriser la chaîne d’approvisionnement de tout logiciel vendu, mais d’autres pays n’ont pas encore suivi l’exemple. En l’absence d’une réglementation réelle, il appartient à chaque entreprise de déterminer la meilleure façon de sécuriser sa chaîne d’approvisionnement en logiciels contre les cyberattaques.</p>'}, {'', '<p>La forme la plus courante de ces attaques consiste généralement à voler les identifiants nécessaires pour accéder à un environnement de développement d’applications. Une fois que les cybercriminels y ont accès, ils tentent pendant une longue période d’intégrer des logiciels malveillants dans un flux de travail dans l’espoir que ces derniers parviennent à s’infiltrer dans plusieurs applications en aval. Dans le même temps, les cybercriminels ciblent également les référentiels de logiciels open source en se faisant passer pour des contributeurs intéressés, avant d’injecter de la même manière des logiciels malveillants dans un outil ou un composant logiciel susceptible d’être utilisé par des milliers d’organisations.</p>'}, {'', '<p>Espérons que les progrès de l’intelligence artificielle (IA) permettront bientôt d’identifier et de remédier plus facilement à ce type de menaces. En attendant, le fossé entre les développeurs et les responsables quant aux mesures prises pour garantir la sécurité des chaînes d’approvisionnement en logiciels est pour le moins déconcertant.</p>'}]"
Construisons enfin une base de données fiable et continue ! Nous le méritons,"[{'', ""<p>Au cours des dernières décennies, nous avons connu des transformations importantes, renforçant notre capacité à améliorer la distribution de logiciels et à créer de nouvelles méthodologies et cadres pour améliorer la collaboration entre les équipes. Malgré ces avancées, le cycle de vie du développement logiciel (SDLC) actuel est encore loin d'être parfait. Les équipes investissent beaucoup de temps dans la transmission des artefacts, et les contrôles précoces du pipeline sont inefficaces ou inexistants.</p>""}, {'', '<p>Parallèlement, nous souhaitons parvenir à un cycle de développement logiciel fiable et robuste. Nous voulons que nos déploiements ne soient pas bloqués, que nos applications ne tombent pas en panne et que nos bases de données ne ralentissent pas. Nous voulons une fiabilité continue autour des déploiements, des applications et des bases de données. Bien que nous ayons travaillé dur pour garantir que nos pipelines CI/CD soient rapides et que nous ayons appris à déployer et à tester des applications de manière fiable, nous n’avons pas fait progresser notre monde des bases de données. Il est temps d’obtenir également une fiabilité continue autour des bases de données.</p>'}, {'', '<p>Pour ce faire, les développeurs doivent être propriétaires de leurs bases de données. Une fois qu’ils en auront pris possession, ils seront prêts à optimiser les pipelines, ce qui permettra d’assurer une fiabilité continue des bases de données. Ce changement de propriété doit être délibérément piloté par les responsables techniques. Le potentiel des ingénieurs de plateforme à révolutionner le secteur en mettant en œuvre des mesures proactives pour protéger les bases de données est évident. Cependant, il est essentiel de disposer des bons outils et processus. Voyons comment procéder et ce dont nous avons besoin.</p>'}, {'', '<p>Il y a vingt ans, les environnements cloud étaient inexistants et la majorité des logiciels fonctionnaient sur des serveurs sur site. Les applications étaient limitées à quelques blocs, comprenant généralement une base de données, quelques serveurs Web et un stockage de fichiers. À cette époque, le dépannage était relativement simple, car les journaux et les traces étaient facilement accessibles en cas de bug.</p>'}, {'', ""<p>Cependant, nos capacités de livraison de logiciels étaient entravées par des processus inefficaces. Les développeurs travaillaient sur des modifications qui étaient ensuite encapsulées dans des ensembles de modifications et transmises aux ingénieurs système pour le déploiement. Cette séparation signifiait que les développeurs n'étaient pas activement impliqués dans les phases de déploiement et de maintenance. Lorsque des bugs survenaient, les ingénieurs système devaient intervenir, ce qui entraînait des processus de correction prolongés en raison de barrières de communication.</p>""}, {'', ""<p>Reconnaissant l'inefficacité de l'exclusion des développeurs du déploiement et de la maintenance, le concept de DevOps a été introduit, mettant l'accent sur la collaboration entre les développeurs et les ingénieurs système. Cependant, il est devenu évident que la collaboration à elle seule ne suffisait pas. Les ingénieurs DevOps ont émergé, visant à fusionner les compétences pour un développement et un déploiement plus fluides. Ces ingénieurs peuvent désormais développer du code métier, le déployer et gérer l'infrastructure cloud à l'aide de solutions d'infrastructure en tant que code (IaC) équipées d'outils et de processus pour des opérations efficaces.</p>""}, {'', '<p>Le paysage a considérablement évolué depuis, avec l’adoption de microservices, de bases de données indépendantes pour chaque petite application et une complexité accrue des communications entre services. L’identification des bugs est devenue un défi, compte tenu de la nature distribuée des systèmes et des signaux dispersés dans l’ensemble de l’écosystème. Bien que le déploiement des composants se soit accéléré, la gestion de cette complexité reste difficile. Les solutions efficaces pour prévenir les problèmes de production, les processus de débogage rationalisés et les équipes évolutives restent difficiles à trouver.</p>'}, {'', ""<p>À l'ère des applications à base de données unique, une équipe d'administrateurs de bases de données suffisait à gérer des tâches telles que la configuration, la réplication, la maintenance, le partitionnement et les optimisations des bases de données. Cependant, avec la fréquence actuelle de déploiement de nombreuses applications au quotidien, l'identification des problèmes de performances et la mise à l'échelle des équipes d'administrateurs de bases de données présentent des défis. L'absence d'outils et de processus adéquats aggrave la situation.</p>""}, {'', '<p>Les ingénieurs de plateforme doivent désormais relever ces défis. Tout comme nous avons appris à réunir les développeurs et les ingénieurs système dans le domaine de DevOps, l’étape suivante consiste à donner aux développeurs la responsabilité de tous les composants, y compris les bases de données. Bien que les développeurs gèrent déjà le déploiement, les pipelines CI/CD et l’IaC, ils manquent de contrôle sur les bases de données en raison de problèmes de surveillance, d’outils de dépannage insuffisants et d’un manque d’alignement avec les pipelines CI/CD pour éviter les changements erronés en production. Les ingénieurs de plateforme sont prêts à combler ces lacunes. La section suivante explore comment ils peuvent influencer ce changement.</p>'}, {'', ""<h3>Comment créer la fiabilité d'une base de données</h3>""}, {'', '<p>Voici les trois parties que les ingénieurs de plate-forme doivent couvrir pour renforcer la fiabilité dans le domaine des bases de données\xa0:</p>'}, {'', '<li>Des outils et des processus qui fonctionnent à tous les niveaux du pipeline</li>'}, {'', '<li>Observabilité et surveillance sémantique des bases de données</li>'}, {'', '<li>Dépannage automatisé.</li>'}, {'', '<p>Passons en revue chacun de ces domaines pour comprendre ce dont nous avons besoin.</p>'}, {'', '<h3>Outils et processus</h3>'}, {'', ""<p>Différents problèmes concernant les bases de données peuvent survenir sans que les développeurs ne s'en aperçoivent. Il s'agit notamment du problème des requêtes N+1, d'un nombre d'index insuffisant ou excessif, des défis liés au chargement rapide par rapport au chargement paresseux dans les ORM, des migrations de schéma et de l'inadéquation de l'impédance, pour n'en citer que quelques-uns.</p>""}, {'', '<p>Il est essentiel de reconnaître que les développeurs ne sont pas en mesure de prévenir ces problèmes de manière proactive. Ils manquent d’outils et de processus efficaces pour identifier les problèmes de performances lors du développement de leurs applications. Les tests des bases de données sont souvent insuffisants, comme nous l’avons vu dans notre article sur la façon de tester les bases de données. Cette insuffisance est due aux limites des solutions CI/CD actuelles et à la pyramide de tests, qui peinent à détecter ces problèmes. Les tests unitaires et d’intégration se concentrent principalement sur l’exactitude des données et ne traitent pas de problèmes tels que le problème des requêtes N+1, l’utilisation d’index ou l’impact sur les performances lors de l’utilisation d’une expression de table commune (CTE). Bien que les tests de charge puissent offrir des informations, ils sont effectués tard dans le pipeline, vers la fin du processus de déploiement, ce qui n’aide guère les développeurs en termes d’efficacité temporelle.</p>'}, {'', '<p>Nous avons besoin de garde-fous robustes pour les bases de données afin de permettre aux développeurs d’identifier ces problèmes dès le début du processus de développement et de déplacer les contrôles vers la gauche autant que possible. Ces garde-fous peuvent identifier des problèmes tels que des index inutilisés, des configurations incorrectes, des problèmes de performances et des paramètres inappropriés dans les systèmes de mappage objet-relationnel (ORM) précisément au moment où les développeurs écrivent leur code. En mettant en œuvre ces mesures avant de valider les modifications du code, le délai d’exécution est considérablement réduit. Cette approche permet aux développeurs de s’approprier les performances de leurs bases de données, en leur fournissant les outils nécessaires sans entraver leur productivité. Étant donné que la propriété est conservée au sein d’une seule équipe, le délai d’exécution augmente considérablement, ce qui conduit à une plus grande fiabilité.</p>'}, {'', '<h3>Observabilité pour les bases de données</h3>'}, {'', '<p>Une autre dimension à prendre en compte pour renforcer la fiabilité est la surveillance. Les solutions de surveillance actuelles ne sont pas parfaites : elles inondent les utilisateurs de données brutes, agrègent les signaux, masquent les problèmes au sein de cohortes d’utilisateurs spécifiques ou entravent le débogage pour identifier les problèmes.</p>'}, {'', '<p>Pour permettre aux développeurs de prendre le contrôle de leurs bases de données, il est nécessaire de développer des outils adaptés aux activités liées aux bases de données et aux flux de travail des développeurs. Les outils de surveillance des bases de données doivent comprendre les migrations de schémas, les tâches de maintenance, les diverses méthodes d’hébergement, les applications multi-locataires, les extensions de base de données, les configurations et de nombreuses autres facettes. Il devient impossible d’exiger des développeurs qu’ils prennent le contrôle de leurs bases de données si les outils de surveillance les submergent de données brutes dépourvues d’explications sur le fonctionnement réel du système.</p>'}, {'', '<p>Néanmoins, les ingénieurs de plateforme peuvent passer de la simple télémétrie et de la surveillance à la compréhension et à l’observabilité, comme expliqué dans notre article sur l’observabilité. En intégrant des outils compatibles avec les bases de données, les ingénieurs de plateforme peuvent permettre aux développeurs de les utiliser. Une fois mis en œuvre, les développeurs peuvent surveiller efficacement leurs bases de données et obtenir des informations sur leur évolution au fil du temps.</p>'}, {'', '<h3>Dépannage automatisé</h3>'}, {'', '<p>Les développeurs ne peuvent pas s’approprier leurs bases de données s’ils sont accablés de tâches manuelles et à forte intensité de main-d’œuvre. Des activités telles que la définition de seuils, la configuration d’alarmes, la révision de tableaux de bord ou la corrélation de requêtes avec des commandes REST peuvent toutes être automatisées. Au lieu de compter sur des systèmes de surveillance pour signaler des problèmes génériques tels qu’une « utilisation élevée du processeur », nous avons besoin de récits complets tels que « nous avons déployé ces modifications en production, modifiant la distribution des données, ce qui a conduit à l’échec de l’application à utiliser un index en raison d’un plan d’exécution obsolète lors de l’exécution de la requête dans cette partie particulière du code ». Ce compte rendu détaillé est ce dont nous avons besoin.</p>'}, {'', '<p>Les ingénieurs de plateforme doivent fournir aux développeurs des outils qui racontent l’histoire dans son intégralité plutôt que de simplement élucider les symptômes. Cette approche permet aux développeurs de résoudre les problèmes plus rapidement, en évitant le processus de dépannage laborieux, notamment la collecte de journaux à partir de diverses sources et l’utilisation de grep pour rechercher des identifiants de corrélation. L’automatisation basée sur notre connaissance des bases de données est essentielle. Diverses stratégies visant à améliorer les performances des bases de données, comme indiqué dans nos discussions précédentes, doivent être automatisées au sein du système. Une fois ces trois domaines renforcés par des garde-fous de base de données, les développeurs peuvent à nouveau prendre en charge leurs bases de données. Explorons les avantages que cette approche peut apporter.</p>'}, {'', '<h3>Avantages du changement de propriétaire</h3>'}, {'', '<p>Le principal avantage de la mise en œuvre de barrières de sécurité pour les bases de données et de la possibilité pour les développeurs de s’approprier leurs bases de données est l’évolutivité. Cette approche élimine les contraintes des équipes, libère tout leur potentiel et leur permet de fonctionner à leur vitesse optimale. En supprimant la nécessité de collaborer avec d’autres équipes qui manquent de contexte complet, les développeurs peuvent travailler plus rapidement, réduisant ainsi les frais de communication. Tout comme nous avons reconnu que la rationalisation de la communication entre les développeurs et les ingénieurs système était la première étape menant à l’évolution vers les ingénieurs DevOps, l’objectif ici est d’éliminer la dépendance vis-à-vis des autres équipes. Les développeurs ne dépendent plus des ingénieurs système ou des administrateurs de bases de données ; ils peuvent gérer et maintenir leurs bases de données de manière indépendante.</p>'}, {'', ""<p>Cela se traduit par un processus d'évolution considérablement accéléré. Chaque base de données étant désormais la propriété du propriétaire du microservice concerné, tout problème de base de données est rapidement traité et résolu par le propriétaire. Il n'est pas nécessaire de centraliser la gestion des performances ou de maintenir des équipes d'administrateurs de base de données capables d'optimiser mais incapables de suivre le rythme du développement.</p>""}, {'', '<p>Un autre aspect remarquable est la réduction du facteur bus. La concentration des connaissances sur la base de données au sein d’une seule équipe d’administrateurs de base de données permet d’atténuer les inquiétudes liées à la rotation du personnel ou aux vacances prolongées. Les transferts de tâches de base de données peuvent être gérés comme des flux de travail de développement réguliers, conformément aux principes de la méthodologie agile. Les tâches liées à la base de données s’intègrent parfaitement à la méthodologie Scrum.</p>'}, {'', ""<p>En fin de compte, les développeurs qui prennent en charge leurs bases de données minimisent le temps nécessaire pour identifier et résoudre les problèmes de base de données. Les développeurs sont libérés du fardeau des tâches lentes et banales. Grâce à la surveillance sémantique, ils identifient rapidement les problèmes, le dépannage automatisé fournit une compréhension complète du problème et ils peuvent corriger les problèmes de manière indépendante. Cela élimine le besoin de salles de guerre ou de ponts d'appel pour déchiffrer la situation.</p>""}, {'', '<h3>Ce qui nous attend</h3>'}, {'', ""<p>La fiabilité continue est un impératif quelle que soit la taille de l'entreprise. Le changement de propriétaire offre un moyen d'y parvenir. Les garde-fous de base de données marquent le début d'une nouvelle ère pour les développeurs et les bases de données, mais ce n'est que le point de départ. Avec l'intégration de l'apprentissage automatique (ML), le dépannage automatisé peut évoluer vers des modifications de code automatisées. Similaires à l'analyse de code statique qui identifie les problèmes courants dans les langages de programmation, les outils peuvent générer des requêtes d'extraction automatisées pour résoudre les problèmes typiques, en exploitant les données de base de données de production capturées automatiquement. Plutôt que d'initier un ticket pour les modifications de configuration ORM, les garde-fous de base de données peuvent modifier le code de manière autonome, en recherchant l'approbation comme une formalité.</p>""}, {'', '<p>À mesure que les développeurs prennent en charge leurs bases de données, ils peuvent utiliser les meilleures pratiques CI/CD pour améliorer l’état de la base de données. La pyramide de tests s’étendra au-delà de la vérification des exigences métier pour englober « comment faire cela », garantissant non seulement des actions correctes mais aussi une mise en œuvre correcte.</p>'}, {'', '<p>En fin de compte, cette approche réduira les goulots d’étranglement de la communication entre les équipes et les rôles, en passant de DevOps à DevDbOps. C’est le chemin que nous devons suivre pour libérer tout le potentiel des développeurs.</p>'}, {'', '<h3>Résumé</h3>'}, {'', '<p>Ces dernières années, le paysage mondial est devenu beaucoup plus complexe. La prolifération des bases de données, des services, des canaux de communication et des composants dynamiques a accru la complexité. À l’instar de l’évolution vers DevOps et de la mise en œuvre de CI/CD à l’aide de l’infrastructure en tant que code (IaC) pour un déploiement accéléré des changements, l’intégration de garde-fous de base de données est essentielle pour permettre aux développeurs de s’approprier leurs bases de données. Il incombe aux ingénieurs de plateforme de défendre et de mettre en œuvre cette approche innovante au sein de leurs organisations.</p>'}]"
La politique en tant que code régira vos pipelines de CD,"[{'', ""<p>La peur de perdre le contrôle ou de ne pas être en conformité a freiné de trop nombreuses transformations DevOps. Dans la plupart des organisations, y compris toutes celles réglementées, un certain degré de gouvernance est nécessaire pour gérer les versions de production. Les politiques relatives aux versions peuvent inclure les contrôles de sécurité nécessaires, les barres de qualité, les fenêtres de déploiement ou les approbations. La conformité aux politiques d'architecture et les impacts sur les coûts peuvent également être pertinents pour les pipelines régissant les changements d'infrastructure.</p>""}, {'', ""<p>Il peut être difficile de garantir le respect de ces politiques. Il y a encore quelques années, lorsque les cycles de publication mensuels ou trimestriels étaient la norme, les politiques pouvaient être appliquées manuellement. Un responsable de publication recueillait les preuves de conformité et un groupe d'approbation examinait ces preuves. Ce n'est qu'en cas de conformité suffisante que l'application était autorisée à être publiée.</p>""}, {'', ""<p>Au fur et à mesure que nous avons développé des pipelines de livraison continue plus matures, notre cadence de livraison a rendu la révision manuelle de chaque version intenable. Un processus long et coûteux peut fonctionner une fois par trimestre, mais lorsqu'il est publié plusieurs fois par semaine, il ne fonctionne plus.</p>""}, {'', '<p>Il est courant d’intégrer la gouvernance dans des pipelines de distribution en évolution rapide. Cependant, ce n’est pas quelque chose que le secteur maîtrise parfaitement. Nous disposons de bons conseils de groupes tels que Capital One sur ce que les organisations devraient faire, mais la mise à l’échelle et le maintien de la gouvernance à grande vitesse doivent encore être améliorés.</p>'}, {'', '<h3>Le défi de la gouvernance traditionnelle des pipelines</h3>'}, {'', ""<p>Cédant aux impératifs de rapidité des entreprises, les organisations ont eu recours à un ensemble de techniques qui se chevauchent pour atténuer les risques que les anciens processus tentaient de traiter. Il s'agit notamment de :</p>""}, {'', ""<p>• Formation à la sécurité pour les développeurs : si moins de bugs de sécurité sont écrits, moins de bugs s'échapperont.• Automatisation des vérifications dans les pipelines : il est demandé aux équipes d'application d'implémenter des analyses de sécurité automatisées et d'appliquer des politiques de promotion dans leurs pipelines en fonction de normes centrales.• Examen du code : après avoir formé les développeurs aux politiques de sécurité et de gouvernance, les équipes peuvent exiger un examen du code, souvent via le processus de demande d'extraction, des modifications pour augmenter la conformité.• Examens de conformité réguliers : le CAB peut être remplacé par un examen de l'architecture de gouvernance qui garantit périodiquement (souvent trimestriellement) que les contrôles appropriés sont en place.</p>""}, {'', ""<p>Malheureusement, ce patchwork échoue souvent. Pour plusieurs raisons, mais le plus souvent à cause de la pression exercée par l'entreprise pour publier davantage de versions, ce qui pousse les développeurs à commencer à raccourcir les vérifications. Après l'incident inévitable ou l'échec de la vérification, les organisations prennent souvent des mesures régressives telles que la remise du contrôle des pipelines aux équipes centrales, réduisant ainsi l'autonomie de l'équipe d'application.</p>""}, {'', '<p>Les organisations qui adoptent cette approche disparate auront également besoin d’aide pour relever les défis posés par les écarts entre les équipes centrales qui gèrent les politiques et les équipes d’applications distribuées qui doivent les mettre en œuvre. Les politiques sont mal comprises, les changements tardent à être mis en œuvre et la conformité peut être inégale.</p>'}, {'', '<h3>La gouvernance à grande vitesse nécessite davantage d’automatisation</h3>'}, {'', ""<p>Nous avons besoin d'un moyen pour les décideurs politiques de l'appliquer à l'échelle d'une organisation tout en minimisant l'impact quotidien sur l'innovation et la productivité des développeurs. Une solution dans ce domaine devrait répondre à une série de besoins\xa0: 1. Vérifications automatisées\xa0: la conformité aux politiques pourrait être validée en quelques instants, sans ralentir de manière significative les pipelines de CD. 2. Peut centraliser la définition\xa0: les équipes GRC (gouvernance, risque et conformité) doivent être en mesure de définir des politiques. 3. Déploiement rapide\xa0: les changements de politique doivent être déployés rapidement et efficacement dans toute l'organisation. 4. Compréhensibilité\xa0: les auteurs de politiques et les équipes d'application régies par des politiques doivent être en mesure de comprendre une politique et ses impacts. 5. Maintenir l'autonomie de l'équipe d'application\xa0: les développeurs des équipes d'application doivent être en mesure d'apporter des modifications conformes aux politiques à leurs pipelines sans avoir besoin de l'aide d'une autre équipe.</p>""}, {'', ""<p>Ces besoins suggèrent à leur tour les caractéristiques d'une solution : ● Fonctionne « sur » et non « dans » un pipeline : si les politiques sont définies à l'intérieur des pipelines, il est difficile de donner de l'autonomie aux développeurs. De plus, apporter des modifications aux politiques signifie que des dizaines, des centaines ou des milliers de pipelines doivent être mis à jour rapidement et fidèlement, ce qui n'est pas une tâche facile. Par conséquent, la définition et l'application des politiques doivent exister en dehors des pipelines. ● Automatisation pour les équipes GRC : l'automatisation des politiques doit être créée par les équipes GRC tout en restant largement compréhensible.</p>""}, {'<h3>La politique en tant que code : la solution ?</h3>', ''}, {'', '<p>Avec une approche de type « policy-as-code » (PaC), comme l’utilisation de projets comme Open Policy Agent (OPA) de la CNCF, les équipes GRC et de plateforme peuvent établir des politiques à l’aide d’un DSL (langage spécifique au domaine) accessible. Bien qu’un code soit compréhensible par une machine, les éléments DSL garantissent qu’il restera compréhensible par l’homme.</p>'}, {'', '<p>En enveloppant un moteur PaC autour de vos pipelines, vous pouvez garantir des politiques telles que l’exigence d’analyses de sécurité. Si le pipeline ne comporte pas l’étape d’analyse, il peut être bloqué. Si une étape qui compare les résultats à vos politiques n’est pas présente, elle peut être bloquée. L’équipe d’application reste libre de contrôler la manière dont ses pipelines sont créés et déployés, mais si elle ne dispose pas des vérifications requises, elle ne pourra pas atteindre la production. L’équipe d’application possède les éléments d’application et l’équipe GRC possède les éléments GRC.</p>'}, {'', '<h3>La politique en tant que code a le vent en poupe</h3>'}, {'', ""<p>Avec des projets PaC tels qu'Open Policy Agent (OPA) diplômés du CNCF et une alternative, Kyverno, l'incubation de PaC se transforme en courant dominant et commence à croître dans son adoption au sein des outils DevOps.</p>""}, {'', '<p>Aujourd’hui, nous voyons des éléments de PaC adoptés par les principales plateformes DevOps\xa0:</p>'}, {'', ""<p>● Azure DevOps\xa0: les stratégies Azure DevOps ont des fonctionnalités de code avec un modèle propriétaire basé sur JSON. Les stratégies peuvent être exécutées via Powershell afin de pouvoir être exécutées via des pipelines.● GitLab\xa0: PaC a été introduit pour les analyses de sécurité. Les stratégies sont définies dans la documentation JSON propriétaire et sont exécutées sur des événements de demande de fusion ou selon une planification.● Harness\xa0: le projet CNCF, OPA, alimente l'approche PaC de Harness. Les stratégies sont disponibles sur toute la plateforme DevOps, de la garantie que les analyses de sécurité sont exécutées à l'exigence d'approbations plus importantes pour les exécutions d'infrastructure en tant que code qui augmentent considérablement les coûts.</p>""}, {'', '<h3>Planifiez votre utilisation maintenant</h3>'}, {'', '<p>Cette approche commence à prendre de l’ampleur dans le secteur. Il est donc temps d’en apprendre davantage sur des technologies telles que l’OPA de la CNCF. Si vous utilisez des outils avec PaC, commencez à élaborer des politiques et, dans le cas contraire, envisagez d’expérimenter ces outils. Vous pouvez trouver l’offre gratuite de Harness si vous aimez l’OPA. Azure DevOps et GitLab proposent des offres gratuites si JSON vous convient.</p>'}]"
Une enquête révèle une adoption significative des outils d'IA pour créer des logiciels,"[{'', ""<p>Une enquête mondiale menée auprès de 5 315 professionnels de l'informatique, dont 804 cadres dirigeants et 1 439 experts en sécurité, révèle que 39 % des répondants déclarent travailler pour des organisations utilisant actuellement l'intelligence artificielle (IA) pour créer des logiciels.</p>""}, {'', ""<p>Réalisée par le cabinet d'études de marché Omdia pour le compte de GitLab, l'enquête révèle également que 39 % supplémentaires prévoient d'utiliser l'IA pour créer des logiciels dans les deux prochaines années.</p>""}, {'', '<p>Parmi ceux qui utilisent actuellement l’IA, les principaux cas d’utilisation sont la génération de code et les suggestions de code (47 %), suivies des explications sur le fonctionnement du code (40 %) et des résumés des modifications du code (38 %).</p>'}, {'', ""<p>Les autres cas d'utilisation future de l'IA identifiés par les répondants incluent la prévision des mesures de productivité et l'identification des anomalies à 38 % chacune, suivies d'explications sur la manière dont une vulnérabilité peut être exploitée et comment y remédier (37 %), et des chatbots qui permettent aux utilisateurs de poser des questions en langage naturel (36 %).</p>""}, {'', ""<h3>Utilisation de l'IA</h3>""}, {'', '<p>Ashley Kramer, directrice de la stratégie de GitLab, a déclaré que l’enquête montre clairement que les développeurs font un usage intensif de l’IA pour automatiser les tâches routinières et que ce n’est désormais qu’une question de temps avant que la vitesse accrue à laquelle le code est écrit ne crée des défis de gestion de la base de code pour les équipes DevOps. Ces problèmes devront être résolus à l’aide d’outils et de fonctionnalités d’IA qui sont intégrés à chaque mise à jour successive des plateformes DevSecOps modernes, a-t-elle ajouté.</p>'}, {'', '<p>Dans l’ensemble, l’enquête révèle que les développeurs passent en moyenne moins d’un quart de leur temps à écrire du code, le reste étant consacré à des réunions et des tâches administratives (15 %), à l’amélioration du code existant (15 %), à la compréhension du code (13 %), aux tests (12 %), à la maintenance du code (11 %) et à l’identification et à l’atténuation des vulnérabilités de sécurité (10 %).</p>'}, {'', '<p>Au total, 62 % des cadres dirigeants estiment qu’il est essentiel d’intégrer l’IA dans le développement de logiciels pour éviter de prendre du retard. Cependant, plus de la moitié (56 %) d’entre eux ont également indiqué que l’introduction de l’IA dans le cycle de vie du développement logiciel était également risquée.</p>'}, {'', '<p>Plus de la moitié (55 %) des cadres dirigeants estiment que la productivité des développeurs est importante pour la réussite de leur entreprise, et 57 % d’entre eux estiment que la mesure de la productivité des développeurs est essentielle à la croissance de l’entreprise. Cependant, seuls 42 % des cadres dirigeants mesurent actuellement la productivité des développeurs au sein de leur entreprise et sont satisfaits de leur approche. Plus d’un tiers (36 %) estiment que leurs méthodes de mesure de la productivité des développeurs sont défectueuses, tandis que 15 % souhaitent mesurer la productivité des développeurs mais ne savent pas comment s’y prendre.</p>'}, {'', ""<p>Les répondants de niveau C dont les organisations utilisent une plateforme (56 %) pour créer et déployer des logiciels étaient beaucoup plus susceptibles que ceux n'utilisant pas de plateforme (33 %) d'être satisfaits de leur approche actuelle pour mesurer la productivité des développeurs, selon l'enquête.</p>""}, {'', '<p>En général, près des deux tiers (64 %) des répondants travaillent également pour des organisations qui souhaitent consolider leur chaîne d’outils, mais seulement 17 % d’entre eux ont déjà commencé à le faire. Près des trois quarts (74 %) des répondants dont les organisations utilisent actuellement l’IA pour le développement de logiciels ont déclaré qu’ils souhaitaient consolider leur chaîne d’outils.</p>'}, {'', ""<p>Plus des deux tiers (67 %) ont déclaré que le cycle de vie du développement logiciel dans leur organisation est en grande partie ou entièrement automatisé et plus de la moitié des répondants (55 %) exécutent désormais également 50 % ou plus de leurs charges de travail dans le cloud, selon l'enquête.</p>""}, {'', ""<p>Un quart des personnes interrogées ont également déclaré qu'elles étaient les principales responsables de la sécurité des applications. Cependant, plus de la moitié (58 %) des personnes interrogées dans le domaine de la sécurité ont déclaré avoir du mal à faire en sorte que le développement donne la priorité à la correction des vulnérabilités, tandis que 55 % ont indiqué que les vulnérabilités de sécurité sont principalement découvertes par l'équipe de sécurité après la fusion du code dans un environnement de test. L'enquête renforce le simple fait que la sécurité des applications nécessite un effort d'équipe, a déclaré Kramer.</p>""}, {'', '<p>Il est difficile de dire avec certitude à ce stade comment le développement et le déploiement des applications seront transformés par l’IA, mais une chose est sûre : il est peu probable que la plupart des flux de travail DevSecOps existants soient en mesure de suivre le rythme.</p>'}]"
JFrog acquiert Qwak pour fusionner les workflows MLOps et DevOps,"[{'', ""<p>JFrog a annoncé aujourd'hui l'acquisition de Qwak pour ajouter une plateforme d'opérations d'apprentissage automatique (MLOps) à son portefeuille existant d'outils et de plateformes DevOps. Les équipes de science des données s'appuient généralement sur une plateforme MLOps pour gérer les flux de travail utilisés pour créer et déployer un modèle d'intelligence artificielle (IA) et JFrog a déjà intégré sa plateforme DevOps principale avec la plateforme MLOPs créée par Qwak.</p>""}, {'', '<p>Yoav Landman, CTO de JFrog, a déclaré que ce n’était désormais qu’une question de temps avant que les processus MLOps et DevOps ne convergent, à mesure que de plus en plus d’applications sont imprégnées de modèles d’IA. L’acquisition de Qwak donne à JFrog une plateforme MLOps mature qui, par exemple, fournit les capacités de gestion des versions et d’immuabilité nécessaires pour faire progresser cette convergence, a-t-il ajouté.</p>'}, {''}, {'', ""<p>Malgré l'acquisition de Qwak, JFrog continuera à proposer des intégrations avec d'autres plateformes MLOps telles qu'AWS Sagemaker et MLflow de DataBricks. Cependant, à mesure que le développement d'applications continue d'évoluer, de plus en plus d'entreprises vont avoir besoin d'une intégration transparente de MLOps et de DevOps dans leur chaîne d'approvisionnement logicielle, a noté Landman.</p>""}, {'', '<p>De nombreux workflows MLOps créés par les équipes de science des données reproduisent bon nombre des mêmes processus déjà utilisés par les équipes DevOps. Par exemple, un magasin de fonctionnalités fournit un mécanisme de partage de modèles et de code de la même manière que les équipes DevOps utilisent un référentiel Git. Au fil du temps, les organisations devront soit intégrer ces référentiels, soit les remplacer à mesure que les workflows deviennent plus intégrés.</p>'}, {'', '<p>Bien entendu, les entreprises qui souhaitent fusionner leurs équipes MLOps et DevOps devront également faire face à des défis culturels importants. De nombreuses équipes DevOps déploient du code plusieurs fois par jour. En comparaison, les équipes de science des données ont besoin de plusieurs mois pour créer, tester et déployer un modèle d’IA. De plus, une fois déployé, un modèle d’IA a tendance à dériver à mesure que des données supplémentaires sont analysées. Cette dérive peut conduire les algorithmes à formuler des recommandations et à prendre des décisions qui, au fil du temps, peuvent devenir de plus en plus sous-optimales. Les équipes de science des données doivent donc mettre à jour les modèles d’IA à l’aide de workflows qui doivent être intégrés à un workflow DevOps.</p>'}, {'', '<p>Les responsables informatiques avisés doivent veiller à ce que le fossé culturel actuel entre les équipes de science des données et celles de DevOps ne s’élargisse pas. Après tout, la question n’est pas tant de savoir si les flux de travail DevOps et MLOps convergeront, mais plutôt de savoir quand et dans quelle mesure. Plus ce fossé perdure, plus l’inertie à surmonter pour le combler sera grande.</p>'}, {'', '<p>Bien entendu, de nombreux fournisseurs de plateformes MLOps ont tout intérêt à justifier leurs investissements dans leurs plateformes. Mais à l’heure où les entreprises sont soumises à une pression économique plus forte que jamais pour réduire leurs coûts, il n’y a peut-être pas de meilleur moment que le présent pour identifier un ensemble de flux de travail de plus en plus redondants les uns par rapport aux autres. Le fait est que la création, la mise à jour, la sécurisation et le déploiement de modes d’IA sont des processus répétables qui se prêtent bien à l’automatisation. De nombreuses équipes de science des données préféreraient que quelqu’un d’autre au sein de l’organisation informatique gère ce processus à leur place.</p>'}]"
Les mises à jour de Broadcom sur le portefeuille VMware rationalisent les flux de travail DevOps,"[{'', ""<p>Broadcom a étendu aujourd'hui VMware Cloud Foundation (VCF) pour fournir des modèles qui promettent de simplifier la tâche des équipes DevOps pour fournir des catalogues en libre-service aux développeurs créant des applications sur le cloud privé basé sur une plate-forme intégrée de calcul, de stockage et de réseau.</p>""}, {'', '<p>De plus, la version 5.2 de VCF ajoute la prise en charge de la mise à jour corrective en direct des différents composants, la possibilité de mettre à niveau indépendamment tout cluster Kubernetes connecté au service Tanzu Kubernetes Grid, une instance de VCF optimisée pour être déployée dans un environnement de calcul de pointe. Ces outils simplifient les migrations à partir de plates-formes VMware autonomes telles que vSAN et prennent en charge les unités de traitement de données doubles (DPU) pour augmenter la résilience.</p>'}, {'', '<p>Dans le même temps, Broadcom a annoncé que VMware vSphere est remplacé par VMware vSphere Foundation, une mise à jour qui ajoute de nombreuses fonctionnalités identiques à une plate-forme conçue pour déployer des fonctionnalités en libre-service ainsi que la prise en charge des correctifs en direct.</p>'}, {'', ""<p>VCF est au cœur d'un effort continu lancé à l'origine par VMware, avant son acquisition par Broadcom, qui a simplifié l'acquisition, l'installation et la gestion de l'ensemble de la pile de logiciels VMware. VMware vSphere Foundation, en revanche, fournit une suite logicielle moins complète pour la création d'un environnement informatique virtuel.</p>""}, {'', ""<p>Prashanth Shenoy, vice-président du marketing produit pour la division VCF de Broadcom, a déclaré que VCF regroupe 168 produits, offres groupées et éditions qui étaient auparavant proposés via plus de 9 000 configurations dans un service d'abonnement qui réduit le coût total de possession de 40 à 50 % par rapport à un cloud public ou à une plate-forme d'infrastructure à trois niveaux. Les prix de VCF ont été réduits de moitié pour rendre cette approche plus attrayante pour les clients, a noté Shenoy.</p>""}, {'', ""<p>Broadcom cherche à contrer l'idée selon laquelle la société aurait augmenté ses prix lorsqu'elle a supprimé les licences perpétuelles pour les logiciels VMware. En réalité, VMware était le dernier grand fournisseur de logiciels à proposer encore une licence perpétuelle. Dans ce contexte, la société est désormais au même niveau que tous les autres fournisseurs de logiciels, a déclaré Shenoy.</p>""}, {'', '<p>De plus, les tarifs des logiciels VMware sont désormais plus cohérents sur plusieurs plates-formes, a-t-il ajouté. Auparavant, les tarifs de VMware avaient tendance à varier considérablement en fonction de la plate-forme sur laquelle ils étaient exécutés, a noté Shenoy. Désormais, les entreprises peuvent plus facilement transférer les licences sur plusieurs plates-formes comme elles le souhaitent, a-t-il ajouté.</p>'}, {'', '<p>L’objectif ultime est de fournir aux ingénieurs de plateforme et aux développeurs d’applications une expérience fluide, a déclaré Shenoy. On ne sait pas exactement combien d’entreprises utilisent aujourd’hui VCF par rapport à VMware vSphere, mais quelle que soit la plateforme choisie, Broadcom continue d’investir dans la recherche et le développement. Broadcom réinvestit 15 % des 34 milliards de dollars de revenus générés par VMware l’année dernière dans la recherche et le développement, a déclaré Shenoy.</p>'}, {'', '<p>Chaque entreprise devra déterminer elle-même dans quelle mesure elle souhaite continuer à s’appuyer sur le portefeuille VMware, mais la migration d’une plate-forme à une autre est toujours coûteuse. Les équipes informatiques doivent déterminer dans quelle mesure effectuer cette transition ou choisir de simplement créer de nouvelles applications sur une plate-forme concurrente. Quelle que soit l’approche adoptée, le coût total de l’informatique tend à augmenter à mesure que le nombre de plates-formes à prendre en charge augmente.</p>'}]"
Ingénierie de plateforme : l'évolution vers DevOps-as-a-Service,"[{'', ""<p>Dans le paysage en constante évolution du développement logiciel et des opérations informatiques, de nouvelles méthodologies et de nouveaux cadres apparaissent fréquemment pour répondre aux défis d'efficacité, d'évolutivité et de fiabilité. Parmi ceux-ci, l'ingénierie de plateforme et DevOps ont suscité une attention particulière.</p>""}, {'', ""<p>Dans mon précédent blog, Mesurer la valeur de DevOps en tant que service, j'ai déclaré : « En termes généraux, DaaS correspond aux capacités DevOps mises à la disposition des utilisateurs de ces capacités via des portails et des API. »</p>""}, {'', '<p>Ce blog soutiendra que l’ingénierie de plate-forme est essentiellement une évolution de DevOps vers un modèle plus structuré et orienté services, offrant une approche standardisée et évolutive pour la mise en œuvre des pratiques DevOps dans les organisations.</p>'}, {'', '<h3>Comprendre DevOps</h3>'}, {'', ""<p>Principes DevOps : DevOps est une approche culturelle et méthodologique qui met l'accent sur la collaboration entre les équipes de développement et d'exploitation. Ses principes fondamentaux sont les suivants :</p>""}, {'', ""<p>Collaboration et communication : briser les silos entre le développement et les opérations pour favoriser une culture de collaboration continue.Intégration continue et livraison continue (CI/CD) : automatisation des processus d'intégration et de déploiement pour garantir une livraison rapide et fiable des logiciels.Infrastructure en tant que code (IaC) : gestion et provisionnement de l'infrastructure informatique via du code plutôt que des processus manuels.Surveillance et observabilité : surveillance continue des applications et de l'infrastructure pour garantir les performances, la fiabilité et la disponibilité.</p>""}, {'', '<p>Défis liés à la mise en œuvre de DevOps : Bien que les avantages de DevOps soient évidents, de nombreuses organisations ont du mal à le mettre en œuvre. Les défis courants incluent :</p>'}, {'', ""<p>Intégration d'outils : l'intégration de divers outils et technologies nécessaires au CI/CD, à la surveillance et à l'IaC peut être complexe et prendre du temps. Lacunes en matière de compétences : les organisations manquent souvent des compétences et de l'expertise nécessaires pour mettre en œuvre et gérer efficacement les pratiques DevOps. Changement culturel : faire évoluer la culture organisationnelle pour adopter les principes DevOps peut être difficile et lent.</p>""}, {""<h3>L'essor de l'ingénierie de plateforme</h3>"", ''}, {'', ""<p>Qu'est-ce que l'ingénierie de plateforme ? L'ingénierie de plateforme se concentre sur la création et la maintenance de plateformes intégrées qui fournissent les outils, les environnements et les services nécessaires pour prendre en charge l'ensemble du cycle de vie du développement logiciel. Ces plateformes sont conçues pour rationaliser le développement, les tests, le déploiement et les opérations en offrant un ensemble cohérent et standardisé de services.</p>""}, {'', ""<p>Composants clés de l'ingénierie de plate-forme\xa0:</p>""}, {'', '<p>Interfaces en libre-service : fournir aux développeurs un accès facile aux ressources nécessaires, telles que les environnements, les bases de données et les pipelines CI/CD.</p>'}, {'', '<p>Automatisation : mise en œuvre de l’automatisation pour gérer l’infrastructure, les déploiements et la surveillance, réduisant ainsi les efforts manuels et les erreurs.</p>'}, {'', '<p>Normalisation : création d’environnements et de processus standardisés pour garantir la cohérence et la fiabilité des projets et des équipes.</p>'}, {'', '<p>Évolutivité : concevoir des plateformes qui s’adaptent aux besoins de l’organisation, prenant en charge plusieurs équipes et divers projets.</p>'}, {'', '<h3>Ingénierie de plateforme en tant que DevOps-as-a-Service</h3>'}, {'', ""<p>Définition de DevOps en tant que service : DevOps en tant que service (DaaS) peut être compris comme une approche orientée services pour fournir des fonctionnalités DevOps. Il s'agit de fournir un ensemble d'outils et de pratiques standardisés et évolutifs sous forme de service géré, permettant aux organisations de tirer parti des principes DevOps sans avoir à créer et à maintenir elles-mêmes l'infrastructure sous-jacente.</p>""}, {'', ""<p>Similitudes entre l'ingénierie de plateforme et DevOps-as-a-Service\xa0:</p>""}, {'', ""<p>1. Standardisation et automatisation : l'ingénierie de plateforme et le DaaS mettent tous deux l'accent sur la standardisation et l'automatisation. En fournissant des outils et des environnements standardisés, l'ingénierie de plateforme simplifie les processus d'intégration et de déploiement, de la même manière que le DaaS fournit un ensemble unifié de fonctionnalités DevOps en tant que service.</p>""}, {'', ""<p>2. Capacités en libre-service : l'ingénierie de plateforme fournit des interfaces en libre-service qui permettent aux développeurs d'accéder aux ressources dont ils ont besoin sans attendre les équipes d'exploitation. Cela correspond au modèle DaaS, où les services sont facilement accessibles aux utilisateurs via un portail en libre-service.</p>""}, {'', '<p>3. Concentrez-vous sur l’efficacité et l’évolutivité : les deux approches visent à améliorer l’efficacité et l’évolutivité. L’ingénierie de plateforme crée des plateformes évolutives qui peuvent prendre en charge plusieurs équipes et projets, tandis que DaaS offre des capacités DevOps évolutives qui peuvent évoluer en fonction des besoins de l’organisation.</p>'}, {'', ""<p>4. Réduction de la complexité et des lacunes en matière de compétences : en proposant une plateforme gérée avec des outils intégrés, l'ingénierie de plateforme réduit la complexité de la mise en œuvre des pratiques DevOps et comble les lacunes en matière de compétences au sein des organisations. De même, DaaS fournit une solution clé en main qui simplifie l'adoption et la gestion de DevOps.</p>""}, {'', ""<p>5. Collaboration et intégration améliorées : l'ingénierie de plateforme favorise la collaboration en fournissant une plateforme unifiée qui intègre divers outils et services, favorisant ainsi une communication et une coordination fluides au sein de l'équipe. Cela reflète la nature collaborative du DaaS, qui intègre et orchestre les outils DevOps pour soutenir la livraison et les opérations continues.</p>""}, {'', ""<p>Avantages de l'ingénierie de plateforme en tant que DevOps en tant que service1. Adoption accélérée : l'ingénierie de plateforme accélère l'adoption des pratiques DevOps en fournissant une plateforme prête à l'emploi qui intègre les meilleures pratiques et outils. Les organisations peuvent rapidement intégrer des équipes et commencer à exploiter les capacités DevOps sans installation ni configuration approfondies.</p>""}, {'', ""<p>2. Cohérence et fiabilité : les plateformes standardisées garantissent la cohérence et la fiabilité des projets et des équipes. Cela réduit le risque d'erreurs et de divergences résultant d'intégrations d'outils ad hoc et de processus manuels.</p>""}, {'', ""<p>3. Concentrez-vous sur les compétences de base : en déchargeant la responsabilité de la création et de la maintenance de l'infrastructure DevOps sur une équipe de plateforme centralisée, les organisations peuvent permettre aux équipes de développement et d'exploitation de se concentrer sur leurs compétences de base : développer et fournir des logiciels de haute qualité.</p>""}, {'', ""<p>4. Expérience de développement améliorée : les interfaces en libre-service et les processus automatisés améliorent l'expérience du développeur en offrant un accès rapide et facile aux ressources nécessaires. Cela réduit les frictions et accélère les cycles de développement.</p>""}, {'', '<p>5. Évolutivité et flexibilité : les plateformes évolutives peuvent s’adapter aux besoins croissants des organisations, en prenant en charge des projets plus vastes et plus complexes. Cette flexibilité garantit que la plateforme peut évoluer en fonction des besoins de l’organisation.</p>'}, {'', '<h3>Défis et considérations</h3>'}, {'', ""<p>1. Investissement initial : la création d'une plateforme complète nécessite un investissement initial en temps et en ressources. Les organisations doivent planifier et allouer soigneusement les ressources pour garantir une mise en œuvre réussie.</p>""}, {'', ""<p>2. Amélioration continue : les plateformes doivent être continuellement améliorées et mises à jour pour intégrer de nouveaux outils, technologies et meilleures pratiques. Cela nécessite un effort et un engagement continus de la part de l'équipe de la plateforme.</p>""}, {'', '<p>3. Équilibrer la standardisation et la flexibilité : si la standardisation est bénéfique, il est essentiel de trouver un équilibre entre standardisation et flexibilité. Les plateformes doivent être adaptables pour répondre aux divers besoins des différentes équipes et des différents projets.</p>'}, {'', '<h3>Conclusion</h3>'}, {'', ""<p>L'ingénierie de plateforme représente l'évolution de DevOps vers un modèle plus structuré et orienté services, incarnant efficacement les principes de DevOps en tant que service. En fournissant des plateformes standardisées et évolutives avec des outils et des services intégrés, l'ingénierie de plateforme simplifie l'adoption et la gestion des pratiques DevOps, en répondant aux défis courants tels que l'intégration des outils, les lacunes en matière de compétences et le changement culturel.</p>""}, {'', ""<p>Alors que les entreprises cherchent des moyens d'améliorer l'efficacité, l'évolutivité et la fiabilité de leur développement logiciel et de leurs opérations informatiques, l'ingénierie de plateforme en tant que DevOps-as-a-service offre une solution convaincante. En accélérant l'adoption, en garantissant la cohérence et en améliorant l'expérience des développeurs, l'ingénierie de plateforme permet aux entreprises de tirer pleinement parti des avantages de DevOps et de favoriser l'innovation et l'amélioration continues.</p>""}, {'', '<p>En conclusion, l’ingénierie de plateforme ne remplace pas DevOps mais constitue une évolution qui offre une approche structurée et évolutive pour fournir des fonctionnalités DevOps en tant que service. Cette perspective met en évidence la nature complémentaire de l’ingénierie de plateforme et de DevOps, soulignant l’importance de réussir la transformation numérique.</p>'}]"
Evolution de l'ECM : le passage à GraphQL pour une efficacité améliorée des API,"[{'', '<p>Selon un rapport Forrester commandé par Adobe, « les décideurs ne considèrent plus les processus de documents numériques comme une « solution ponctuelle », mais comme une partie intégrante du paysage informatique de leur organisation. » Le rapport montre que « 65 % des personnes interrogées déclarent que l’intégration des processus de documents numériques aux applications de productivité quotidiennes est très importante ou une exigence essentielle. » À l’heure où 328,77 millions de téraoctets de données sont créés chaque jour, l’accès aux fichiers et la collaboration deviennent primordiaux pour les processus métier et mettent en évidence le besoin de gestion de contenu d’entreprise (ECM), un élément clé des solutions d’automatisation intelligente.</p>'}, {'', ""<p>Les outils ECM capturent, gèrent, stockent et diffusent du contenu et des documents. Ils garantissent que les informations sont facilement accessibles et utiles tout au long du cycle de vie du contenu. En outre, les outils ECM permettent à tous les membres de l'organisation d'accéder facilement aux informations nécessaires pour prendre des décisions, mener à bien des projets et travailler efficacement. L'apprentissage automatique, les services basés sur le cloud et les fonctionnalités mobiles présentent de nouveaux défis en matière de gestion de contenu d'entreprise. Cependant, le bon système ECM contribuera à automatiser les processus, à augmenter la productivité et à améliorer la collaboration.</p>""}, {'', '<p>Le partage de contenu signifie que l’intégration ECM n’est pas une considération secondaire : elle est essentielle pour permettre la cohérence, la collaboration et la rationalisation de l’ensemble du processus d’interaction des données. Cependant, au fil des ans, deux camps distincts d’interfaces de programmation d’application (API) ont émergé pour les intégrations ECM : GraphQL et REST (acronyme de REpresentational State Transfer). Cet article, étayé par des recherches et des pratiques, examinera pourquoi GraphQL est devenu préféré aux API REST.</p>'}, {'', '<h3>API REST vs. GraphQL</h3>'}, {'', ""<p>Supposons que vous souhaitiez interagir avec une application pour récupérer des informations ou exécuter une fonction. Une API vous permet de communiquer efficacement en créant une demande et une réponse entre le client Web moderne et le serveur ou un microservice. L'application qui soumet la demande est le client, et le serveur ou le service répond. Par exemple, la conception de l'API pour un service météorologique pourrait spécifier que l'utilisateur fournisse un code postal et que le service réponde avec les températures maximales et minimales pour cette zone spécifique.</p>""}, {'', '<p>Par rapport aux API REST, GraphQL offre une approche plus efficace pour spécifier et récupérer des données sans gaspiller les ressources réseau et la bande passante. Bien que les API REST soient considérées comme la « colonne vertébrale d’Internet », elles présentent des inconvénients.</p>'}, {'', '<p>Parmi les critiques, on trouve :</p>'}, {'', ""<p>● Surextraction et sous-extraction des données : la limitation de la surextraction favorise le gaspillage de la bande passante et des ressources de traitement, tandis que la sous-extraction entraîne des demandes de données supplémentaires car toutes les informations demandées ne sont pas disponibles dans une seule demande de point de terminaison REST. ● Plusieurs allers-retours : fréquemment, les API REST nécessitent de nombreux allers-retours vers le serveur pour récupérer les données. L'activité yo-yo augmente la latence, affecte les temps de réponse et entrave l'efficacité des opérations de récupération de contenu. ● Manque de flexibilité dans la récupération et la manipulation des données : les API traditionnelles exposent généralement des points de terminaison prédéfinis et des structures de données fixes, ce qui limite la flexibilité pour la récupération et la manipulation du contenu. Ce manque de flexibilité pose des problèmes lorsque des besoins ou des transformations de données spécifiques surviennent. ● Limitations de l'architecture : il n'existe pas de normes pour la mise en œuvre des API REST, il existe des directives pour le style architectural. Les API traditionnelles sont souvent construites sur des architectures monolithiques, où toutes les fonctionnalités et l'accès aux données sont étroitement couplés. Cela limite la capacité à faire évoluer et à faire évoluer les composants individuels de manière indépendante, ce qui entrave l'agilité et l'innovation.</p>""}, {'', ""<p>Développé par Facebook, GraphQL est un langage open source de requête et de manipulation de données pour les API. GraphQL extrait efficacement les informations à l'aide d'une seule requête, ce qui permet d'économiser la bande passante et de réduire les requêtes en cascade. Grâce à une diffusion de contenu optimisée, des intégrations transparentes et une productivité accrue des développeurs, GraphQL permet aux entreprises d'exploiter tout le potentiel de leurs systèmes de contenu, leur offrant ainsi un avantage concurrentiel.</p>""}, {'', ""<p>GraphQL permet la récupération de données déclarative, où un client peut spécifier les données dont il a besoin à partir d'une API. Au lieu de plusieurs points de terminaison qui renvoient des données distinctes, un serveur GraphQL expose un seul point de terminaison et répond précisément aux données demandées par le client. Étant donné qu'un serveur GraphQL peut récupérer des données à partir de sources individuelles et les présenter dans un graphique unifié, il n'est lié à aucune base de données ou moteur de stockage spécifique.</p>""}, {'', '<h3>REST vs. GraphQL : les différences consolidées</h3>'}, {'', ""<p>● REST est un ensemble de règles qui définit l'échange de données structurées entre un client et un serveur. GraphQL est un langage de requête, un style d'architecture et des outils pour créer et manipuler des API.● REST convient aux sources de données simples avec des ressources bien définies, tandis que GraphQL convient aux sources de données volumineuses, complexes et interdépendantes.● REST possède plusieurs points de terminaison sous la forme d'URL pour définir les ressources. GraphQL possède un seul point de terminaison URL.● REST renvoie des données dans une structure fixe définie par le serveur. GraphQL renvoie des données dans une structure flexible définie par le client.● Les données REST sont « faiblement typées », le client doit donc décider comment interpréter les données formatées lorsqu'elles sont renvoyées. Les données GraphQL sont « fortement typées », le client reçoit des données dans des formats prédéterminés et mutuellement compris.</p>""}, {'', '<p>En comparaison, GraphQL est un meilleur choix pour la gestion de contenu que l’API REST. L’efficacité, la flexibilité et l’expérience améliorée des développeurs de GraphQL sont des avantages et des éléments qui pourraient changer la donne pour votre organisation. L’adoption de GraphQL ouvre des possibilités pour gérer et diffuser du contenu plus efficacement.</p>'}, {'', '<h3>Conclusion</h3>'}, {'', '<p>Dans une enquête réalisée auprès de 450 décideurs informatiques et commerciaux de haut niveau basés en Amérique du Nord, en Europe et en Asie-Pacifique, responsables des processus de documents numériques dans leurs organisations, 61 % ont déclaré que l’adoption d’outils de traitement de documents numériques était soit « très importante », soit une « exigence critique ». L’ECM répond à ces exigences en automatisant les processus et en augmentant la productivité. Cependant, la sélection d’API pour garantir ces avantages peut avoir un impact profond sur la qualité de votre gestion documentaire.</p>'}, {'', ""<p>Au fil du temps, il est devenu évident que les développeurs préfèrent GraphQL aux API REST pour les interfaces client Web afin de diffuser et d'utiliser le contenu plus efficacement. Par exemple, des entreprises comme Netflix et Shopify ont adopté avec succès GraphQL pour leurs besoins de gestion de contenu, améliorant ainsi l'efficacité et la satisfaction des clients. Faire le bon choix peut faire la différence entre des utilisateurs finaux satisfaits et une équipe DevOps recevant une avalanche de tickets d'incident.</p>""}]"
Shreds.AI lance une plateforme LLM destinée à l'ingénierie logicielle,"[{'', ""<p>Shreds.AI a dévoilé aujourd'hui une plateforme d'intelligence artificielle (IA) générative du même nom basée sur un grand modèle de langage (LLM) qu'elle a formé pour automatiser spécifiquement les tâches d'ingénierie logicielle. Disponible en version bêta, la plateforme Shreds.AI peut attribuer des tâches à huit autres LLM en invoquant les interfaces de programmation d'applications (API) qu'elles exposent.</p>""}, {'', ""<p>Selon Soufiane Amar, PDG de Shreds.AI, plutôt que de produire de petites quantités de code, la plateforme a été formée pour créer les dizaines de milliers de lignes et de fichiers de code nécessaires pour piloter des flux de travail d'ingénierie logicielle complexes. La plateforme Shreds.AI orchestrera également avec précision l'intégration de divers composants logiciels pour créer une application, car elle a été formée à l'aide d'outils que les développeurs utilisent régulièrement, a-t-il ajouté.</p>""}, {'', ""<p>Un développeur saisit une description simple en langage naturel du logiciel qu'il souhaite que Shreds.AI crée, et la plateforme génère ensuite des diagrammes d'architecture et le code des fonctionnalités indépendantes et isolées appelées shreds. Les équipes DevOps n'ont plus qu'à valider le code avant de l'utiliser, un processus que Shreds.AI simplifie grâce à un réseau de développeurs indépendants que les organisations peuvent engager pour examiner le code, a déclaré Amar.</p>""}, {'', '<p>Shreds.AI est une méta-IA, dans la mesure où, en plus de générer du code et de raisonner sur plusieurs processus, elle est capable de classer les LLM tiers en fonction de leur capacité à effectuer des tâches spécifiques, a-t-il noté.</p>'}, {''}, {'', '<p>La plateforme Shreds.AI est déjà testée par le conglomérat automobile Stellantis et le Réseau de Transport d’Électricité (RTE), le gestionnaire du réseau de transport d’électricité français. Shreds.AI estime qu’une application qui aurait pu coûter auparavant 1 million de dollars peut désormais être créée pour moins de 30 000 dollars. L’entreprise affirme que Shreds.AI réduit le délai de mise sur le marché des logiciels, ainsi que la taille des équipes et les coûts, de plus de 80 % par rapport aux méthodes de développement de logiciels traditionnelles. En permettant la maintenance automatique, elle résout également le problème de l’obsolescence des logiciels. Elle augmente la durée de vie des logiciels de plus de 60 %, par exemple en facilitant la conversion du langage de programmation utilisé pour créer cette application en un autre langage que davantage de développeurs connaissent.</p>'}, {'', '<p>L’intégration de l’IA dans les workflows DevOps est encore trop récente et le code généré par ces plateformes doit encore être géré. Le défi est que, grâce à l’essor de l’IA, on s’attend à ce que la quantité de logiciels créés et déployés au cours des deux prochaines années dépasse la quantité de logiciels déployés au cours des deux dernières décennies. La seule façon de suivre ce rythme de développement sera d’appliquer également l’IA à la gestion des workflows DevOps.</p>'}, {'', '<p>Dans l’espoir d’éliminer autant de tâches que possible, les équipes DevOps doivent identifier les processus manuels qu’elles effectuent régulièrement aujourd’hui, en vue d’y appliquer l’IA demain. Après tout, l’objectif initial de l’adoption de DevOps était d’automatiser impitoyablement autant de processus d’ingénierie logicielle que possible, afin de permettre la création et le déploiement d’un plus grand nombre d’applications le plus rapidement possible.</p>'}]"
Défis des tests ETL et comment les surmonter,"[{'', ""<p>Les processus d'extraction, de transformation et de chargement (ETL), qui sont essentiels à l'intégration des données, constituent la clé de voûte de la consolidation des données provenant de sources multiples dans un référentiel unifié. Malgré leur rôle essentiel, les processus ETL sont sujets à des difficultés, en particulier pendant la phase de test, lorsque la garantie de la qualité, de l'intégrité et des performances des données devient primordiale.</p>""}, {'', ""<p>Les tests ETL sont essentiels pour identifier et corriger les erreurs, les incohérences et les inefficacités avant que les données ne soient finalisées à des fins d'analyse et de prise de décision. Cet article examine les obstacles courants rencontrés lors des tests ETL et fournit des stratégies concrètes pour surmonter ces défis de manière efficace, garantissant ainsi un processus d'intégration de données transparent et fiable.</p>""}, {'', '<h3>Comprendre les tests ETL</h3>'}, {'', ""<p>Les tests ETL sont un élément essentiel du processus d'intégration des données, conçu pour valider la transformation, l'extraction et le chargement de données provenant de diverses sources dans une base de données cible ou un entrepôt de données désigné. Son objectif principal est de garantir que les données transférées via le pipeline ETL sont exactes, cohérentes et complètes. Cela implique de vérifier que tous les enregistrements sont correctement extraits des systèmes sources, que les transformations sont conformes aux règles et à la logique métier, et que le chargement final dans le système cible reflète précisément le résultat escompté sans aucune perte ou corruption de données.</p>""}, {'', ""<p>Les tests ETL sont essentiels pour maintenir la qualité des données, ce qui est fondamental pour des analyses et une veille stratégique fiables. Ils englobent plusieurs activités vitales, notamment les contrôles d'exhaustivité des données, la validation de la transformation des données et la vérification du flux de données de bout en bout, toutes visant à identifier et à atténuer les anomalies de données et à garantir l'intégrité et la facilité d'utilisation des données commerciales critiques.</p>""}, {'', '<h3>Défis courants en matière de tests ETL</h3>'}, {'', ""<p>Les tests ETL sont confrontés à plusieurs défis qui peuvent compliquer le processus d'intégration des données, ce qui a un impact sur la précision et la fiabilité des analyses et des données de veille économique. Ces défis incluent :</p>""}, {'', ""<p>• Logique de transformation complexe : la validation de la logique métier qui transforme les données peut être complexe, en particulier lorsqu'il s'agit de règles complexes et de sources de données multiples. Pour garantir que toutes les transformations sont correctement appliquées, il faut une compréhension détaillée des données et des processus métier qu'elles prennent en charge.</p>""}, {'', ""<p>• Volume et évolutivité des données : avec la croissance exponentielle des données, tester l'évolutivité et les performances des processus ETL sous de gros volumes devient une tâche ardue. Les testeurs doivent s'assurer que le processus ETL peut gérer la charge de données actuelle et évoluer en fonction de la croissance future.</p>""}, {'', ""<p>• Problèmes de qualité des données : les problèmes de qualité des données inhérents, tels que les valeurs manquantes, les doublons et les incohérences entre les systèmes sources, posent des défis considérables. L'identification et la résolution de ces problèmes au cours du processus ETL sont essentielles pour maintenir l'intégrité de l'entrepôt de données.</p>""}, {'', ""<p>• Intégration avec plusieurs sources de données : le processus ETL implique souvent l'intégration de données provenant de sources disparates, chacune avec son propre format et ses propres normes. Assurer une intégration transparente et une représentation cohérente des données dans toutes les sources nécessite une planification et des tests méticuleux.</p>""}, {'', ""<p>• Performances et optimisation : il est essentiel de tester les goulots d'étranglement des performances et d'optimiser le processus ETL pour plus de rapidité et d'efficacité, en particulier pour les besoins de traitement des données en temps réel. L'identification de l'équilibre optimal entre performances et utilisation des ressources est un défi crucial.</p>""}, {'', ""<p>Relever ces défis avec succès exige une approche stratégique des tests ETL, englobant une planification minutieuse, des techniques de test avancées et une compréhension approfondie des données et du contexte commercial qu'elles servent.</p>""}, {'', '<h3>Stratégies pour surmonter les défis des tests ETL</h3>'}, {'', ""<p>Pour surmonter les défis liés aux tests ETL, il faut une stratégie complète qui garantisse l'intégrité des données, les performances et l'évolutivité. Voici quelques stratégies efficaces\xa0:</p>""}, {'', ""<p>• Outils de test automatisés : l'automatisation du processus de test ETL peut améliorer considérablement l'efficacité et la précision. Les outils qui prennent en charge la validation et la comparaison automatisées des données peuvent identifier rapidement les écarts et les erreurs, réduisant ainsi les efforts manuels et le risque d'oubli.</p>""}, {'', ""<p>• Outils de profilage et de qualité des données : l'utilisation précoce d'outils de profilage des données permet d'identifier les problèmes de qualité des données tels que les incohérences, les doublons et les valeurs aberrantes. L'intégration d'outils de qualité des données pour nettoyer et normaliser les données avant qu'elles n'entrent dans le pipeline ETL garantit un processus de transformation plus propre et des résultats de meilleure qualité.</p>""}, {'', ""<p>• Une approche de test modulaire : la division du processus ETL en modules ou composants plus petits et plus faciles à gérer permet des tests plus ciblés et plus efficaces. Cette approche permet aux testeurs d'isoler et de résoudre les problèmes plus efficacement, en s'assurant que chaque partie du processus fonctionne correctement avant de passer à la suivante.</p>""}, {'', ""<p>• Analyse comparative des performances : l'établissement d'analyses comparatives des performances du processus ETL permet d'identifier les goulots d'étranglement et les inefficacités. La réalisation de tests de charge et de stress sous différents volumes de données et scénarios garantit que le processus ETL peut répondre aux exigences du monde réel.</p>""}, {'', ""<p>• Intégration et tests continus : la mise en œuvre d'un pipeline d'intégration continue (CI) pour les processus ETL permet de détecter rapidement les erreurs et les problèmes d'intégration. Cette approche permet de tester et de valider en permanence les données au fur et à mesure de leur progression dans le pipeline ETL, facilitant ainsi un retour d'information immédiat et une résolution plus rapide des problèmes.</p>""}, {'', '<p>• Collaboration humaine : encourager la collaboration entre les ingénieurs de données, les testeurs et les analystes commerciaux garantit une compréhension approfondie de la logique métier et des exigences de qualité des données. Cette approche collaborative permet de concevoir des cas de test plus efficaces et de mieux comprendre la logique de transformation.</p>'}, {'', '<p>En adoptant ces stratégies, les organisations peuvent relever les défis courants rencontrés lors des tests ETL, ce qui conduit à des efforts d’intégration de données plus fiables et, en fin de compte, à des informations commerciales plus précises et exploitables.</p>'}, {'', '<h3>Bonnes pratiques pour des tests ETL efficaces</h3>'}, {'', '<p>Pour garantir des tests ETL efficaces, l’adoption d’un ensemble de bonnes pratiques est essentielle\xa0:</p>'}, {'', '<p>• Élaborez un plan de test complet : commencez par un plan de test détaillé qui décrit les stratégies de test, les objectifs et les critères pour chaque phase du processus ETL. Ce plan doit couvrir tous les aspects, de la vérification de la source de données à la logique de transformation et aux tests de charge dans la base de données cible.</p>'}, {'', ""<p>• Utiliser des données de test réalistes : utilisez un échantillon représentatif de données de production couvrant divers scénarios, y compris les cas extrêmes et les anomalies de données. Cette approche permet de découvrir les problèmes potentiels affectant l'intégrité et la facilité d'utilisation des données.</p>""}, {'', ""<p>• Automatisez autant que possible : l'automatisation des tâches de test répétitives et gourmandes en données augmente l'efficacité et la précision. Les tests de régression automatisés sont précieux pour les processus ETL en cours qui subissent des mises à jour ou des modifications.• Donnez la priorité à la qualité des données : intégrez des contrôles de qualité des données à chaque étape du processus ETL. Cela comprend la validation de l'exhaustivité, de l'exactitude et de la cohérence des données pour garantir que les données répondent aux normes prédéfinies.</p>""}, {'', ""<p>• Favoriser la collaboration au sein de l'équipe : encouragez la collaboration et la communication ouvertes entre les équipes de développement, de test et d'analyse commerciale. Une approche unifiée garantit une compréhension commune des objectifs ETL et améliore la qualité globale du processus de test.</p>""}, {'', '<p>L’adoption de ces meilleures pratiques peut considérablement améliorer les résultats des tests ETL, ce qui se traduit par des données plus fiables, plus précises et de meilleure qualité pour la veille économique et l’analyse.</p>'}, {'', '<h3>Derniers mots</h3>'}, {'', ""<p>L'intégration de tests ETL efficaces dans le processus d'intégration des données est essentielle pour garantir des résultats de données fiables et de haute qualité. Les organisations peuvent améliorer leurs efforts de tests ETL en relevant les défis courants avec des solutions stratégiques et en adhérant aux meilleures pratiques, ouvrant ainsi la voie à des analyses approfondies et à des décisions commerciales éclairées.</p>""}]"
Aperçu des processus de tests de sécurité continus pour DevSecOps,"[{'', ""<p>Comme vous le savez probablement déjà, DevSecOps est une méthodologie de développement logiciel qui combine le développement (Dev), la sécurité (Sec) et les opérations (Ops) à toutes les phases du cycle de vie du développement logiciel (SDLC). Elle intègre les contrôles de sécurité dans le processus de développement et comble les écarts entre les équipes de développement, de sécurité et d'exploitation.</p>""}, {'', '<p>Grâce aux pratiques DevSecOps, vous pouvez créer un environnement plus sécurisé, sécuriser davantage les pipelines d’intégration et de livraison continue et produire des logiciels de haute qualité. En intégrant la sécurité dès le début du pipeline, les entreprises peuvent atteindre une productivité plus élevée. DevSecOps n’est pas une option ; compte tenu de la recrudescence des cyberattaques, il est devenu nécessaire.</p>'}, {'', ""<h3>Qu'est-ce que les tests de sécurité continus ?</h3>""}, {'', ""<p>Comme son nom l'indique, les tests de sécurité continus garantissent que les tests de sécurité ont lieu à chaque phase du cycle de vie du logiciel. Leur objectif est de protéger les applications contre les menaces et vulnérabilités potentielles en découvrant les risques, les menaces et les dangers de sécurité avant que le logiciel ne soit mis en production.</p>""}, {'', ""<p>Les tests de sécurité traditionnels vérifient si un algorithme fonctionne correctement à un moment donné. En revanche, les tests de sécurité continus détectent et corrigent les faiblesses et les failles de sécurité en permanence tout au long du cycle de vie de l'application.</p>""}, {'', ""<p>Les tests de sécurité continus évaluent l'infrastructure, les applications et les terminaux pour détecter d'éventuelles failles que les attaquants peuvent exploiter. Il s'agit d'une extension des tests continus, qui inspecte en permanence le code et les bibliothèques tierces pour détecter les problèmes de sécurité connus ou récemment découverts.</p>""}, {'', ""<p>Les outils peuvent aider le personnel à automatiser les contrôles de sécurité dans une application. Plusieurs options sont disponibles pour répondre aux différentes phases des processus de développement, d'intégration et de déploiement.</p>""}, {'', '<p>Des tests de sécurité réguliers garantissent le respect des normes du secteur. Plus important encore, ils obligent les développeurs à mettre en œuvre des mesures de sécurité de premier ordre dans leur code pour contrecarrer les menaces de sécurité potentielles à long terme.</p>'}, {'', ""<h3>Pratiques courantes pour l'intégration des tests de sécurité dans DevSecOps</h3>""}, {'', '<p>Le SLDC comporte plusieurs phases et il peut falloir un certain temps pour intégrer la méthodologie de développement traditionnelle aux processus plus récents. Quiconque souhaite intégrer les tests de sécurité dans DevSecOps doit commencer par ces pratiques.</p>'}, {'', '<h4>Exécution de contrôles de sécurité automatisés</h4>'}, {'', '<p>L’automatisation des contrôles de sécurité au cours du cycle de vie du développement logiciel est un élément clé de DevSecOps. Cette méthode permet de détecter les vulnérabilités à un stade précoce, lorsqu’il est plus facile, plus rapide et moins frustrant de résoudre les problèmes. Les contrôles de sécurité automatisés fonctionnent en arrière-plan, ce qui permet aux développeurs de se concentrer sur les autres éléments de leurs applications. C’est un peu comme si vous disposiez d’une paire d’yeux vigilants supplémentaires qui signalent les dangers potentiels.</p>'}, {'', ""<p>Quel que soit le logiciel choisi, les outils automatisés peuvent analyser le code à la recherche de vulnérabilités en temps réel pendant le processus de création ou à d'autres moments appropriés. Le retour d'information instantané permet aux équipes de résoudre les problèmes bien avant qu'ils ne deviennent des problèmes majeurs, garantissant ainsi la sécurité de la base de code.</p>""}, {'', '<h4>Intégration de la révision du code</h4>'}, {'', '<p>La révision du code est une tradition bien ancrée au sein des équipes de développement Agile, pour des raisons qui vont bien au-delà de la sécurité. C’est également un bon moyen d’intégrer des tests de sécurité continus dans le processus SDLC, en partie parce qu’il est déjà familier à de nombreuses équipes. L’intégration de la révision du code aux outils d’analyse des vulnérabilités fournit un retour immédiat sur les faiblesses de sécurité du code source.</p>'}, {'', '<h4>Améliorer la sensibilisation à la sécurité</h4>'}, {'', ""<p>Idéalement, votre organisation a déjà investi dans la formation pour aider les utilisateurs finaux et les développeurs à reconnaître les façons dont les applications et les pratiques commerciales peuvent les rendre vulnérables. Des formations régulières permettent aux développeurs de rester informés des nouvelles menaces et encouragent le partage d'informations entre les équipes. Plus ils en savent, mieux ils peuvent identifier les menaces de sécurité potentielles et réagir rapidement et efficacement.</p>""}, {'', '<h4>Surveillance continue</h4>'}, {'', '<p>Le DevSecOps ne s’arrête pas au lancement du logiciel. Que se passe-t-il une fois l’application mise en production ? La surveillance continue consiste à surveiller une application contre les menaces de sécurité potentielles en temps réel en prêtant attention aux journaux, aux événements et au trafic réseau. Elle aide les organisations à détecter immédiatement les menaces potentielles, ce qui permet d’éviter les failles de sécurité ou au moins d’y réagir rapidement.</p>'}, {'', '<h4>Modélisation des menaces</h4>'}, {'', '<p>Il faut parfois anticiper les risques. La modélisation des menaces consiste à identifier en amont les menaces potentielles pour la sécurité, pas seulement ce qui se trouve dans le code, mais aussi ce que quelqu’un pourrait essayer de faire pour accéder à l’application ou en tirer profit. Cela se fait souvent en examinant la conception et l’architecture d’une application pour détecter les vecteurs d’attaque potentiels, souvent à l’aide d’outils sophistiqués.</p>'}, {'', '<h3>N’hésitez pas : déployez dès maintenant des tests de sécurité continus</h3>'}, {'', '<p>Le succès de DevSecOps, l’intégration de la sécurité dans DevOps, nécessite un changement de perspective, ainsi que de nouvelles ressources et méthodes. Il serait judicieux d’adopter l’état d’esprit collaboratif et agile de DevOps pour rendre le processus de développement fluide et transparent et garantir que la sécurité soit aussi fluide et discrète que possible.</p>'}]"
Une introduction douce à la surveillance et à l'optimisation continues,"[{'', ""<p>La surveillance DevOps fournit une image complète et actualisée de l'état de l'environnement de production et des détails sur ses services, son infrastructure et ses applications. En collectant des données à partir de journaux et de mesures, vous pouvez surveiller la conformité et les performances à chaque étape du cycle de vie du développement logiciel.</p>""}, {'', '<p>La surveillance ne concerne pas uniquement les problèmes de production. Elle englobe plusieurs procédures, telles que la planification, le développement, les tests, le déploiement et l’exploitation.</p>'}, {'<h3>Un aperçu des types de surveillance</h3>', ''}, {'', '<p>Les utilisations de la surveillance continue se développent au même rythme que la taille des piles technologiques des entreprises. De nombreuses entreprises commencent par suivre des indicateurs fondamentaux, tels que l’utilisation du processeur ou le comportement des clients.</p>'}, {'', '<p>Surveillance de l’infrastructure : vous pouvez collecter et analyser les données de l’infrastructure informatique interne de votre organisation et utiliser ces informations pour améliorer les performances ou d’autres indicateurs. Parmi les éléments à surveiller figurent les réseaux, les serveurs, le matériel et les logiciels informatiques, les centres de données, les systèmes d’exploitation et le stockage. Les outils de surveillance de l’infrastructure les plus courants incluent SolarWinds, ManageEngine et Prometheus.</p>'}, {'', '<p>Surveillance du réseau : la surveillance du réseau se concentre sur la recherche d’erreurs, l’évaluation de l’efficacité des composants et l’amélioration de leur utilité. Tout sur le réseau est surveillé, y compris les pare-feu, les serveurs, les machines virtuelles et les routeurs. Un système de surveillance réseau dynamique peut aider à éviter les erreurs et les pannes qui pourraient nuire aux performances. Spiceworks, Cacti et Wireshark sont quelques-uns des utilitaires les plus connus.</p>'}, {'', '<p>Surveillance des performances des applications (APM) : le logiciel fonctionne-t-il suffisamment rapidement ? Si ce n’est pas le cas, d’où vient le problème ? Vous pouvez obtenir des mesures d’exécution sur les performances d’une application, telles que la disponibilité de l’application, la sécurité et la surveillance des journaux. Les solutions APM telles que DataDog, Uptrends et Splunk sont des outils courants pour explorer de nombreux types d’informations, notamment les réponses API, l’état de l’application back-end, le volume des transactions et les mesures de temps.</p>'}, {'', '<p>Suivi des coûts : le pipeline DevOps comporte de nombreux éléments, chacun pouvant être coûteux. Toute organisation qui souhaite contrôler ses coûts (c’est-à-dire tout le monde, n’est-ce pas ?) estime qu’un suivi continu de l’utilisation des ressources est un besoin essentiel. Ces indicateurs permettent de prévoir le coût global et de maximiser l’utilisation des ressources tout au long des étapes DevOps.</p>'}, {'<h3>Pourquoi la surveillance DevOps est importante</h3>', ''}, {'', '<p>La surveillance DevOps étant proactive, elle détecte les opportunités et les lacunes pour améliorer les performances des applications avant que les symptômes des défauts ne deviennent apparents. En mettant l’accent sur les domaines potentiels d’automatisation, la surveillance améliore encore la chaîne d’outils DevOps. À l’aide d’un code piloté par API, une surveillance système appropriée offre des informations pertinentes qui vous permettent de voir chaque élément de votre pile d’applications. Le processus de surveillance est amélioré lorsque des crochets de code sont intégrés à la logique de l’application.</p>'}, {'', ""<p>Automatisation : l'amélioration de la communication entre les équipes de développement et d'exploitation est le principe central de DevOps. Cependant, la coordination entre les équipes peut être interrompue en cas de manque de connectivité entre les outils. Par conséquent, vous pouvez suivre les validations et les demandes d'extraction pour suivre les problèmes Jira et alerter l'équipe de développement sur son canal de communication préféré, ou vous pouvez utiliser l'automatisation pour permettre une vue unifiée de l'ensemble du pipeline de développement.</p>""}, {'', '<p>Visibilité et transparence : un workflow comporte des milliers d’éléments mobiles, chacun fonctionnant à une taille différente et connaissant une latence et une redondance variables en raison de l’introduction de microservices et de micro frontends. Le CI/CD permet aux développeurs d’apporter des modifications fréquentes à leur code, ce qui ajoute à la complexité d’un système de production. Par conséquent, les équipes ont besoin d’une vision de l’écosystème pour réparer tout dommage causé à l’expérience utilisateur dès qu’il se produit.</p>'}, {'', '<p>Expérience de qualité : une équipe DevOps qui utilise des technologies de surveillance continue pour gérer ses systèmes peut minimiser les temps d’arrêt du système et les interruptions d’activité. Cela relève la barre de la qualité de l’expérience informatique pour les parties prenantes internes et externes, telles que les membres du personnel, les partenaires commerciaux et les clients. Cela optimise les performances commerciales au fil du temps dans chaque département de l’entreprise.</p>'}, {'', '<h3>Effectuer correctement la surveillance DevOps</h3>'}, {'', '<p>Chaque organisation a des habitudes légèrement différentes. Néanmoins, il existe quelques principes universellement applicables.</p>'}, {'', ""<p>Définissez des objectifs clairs : quel est l'objectif général de la mise en œuvre de la surveillance DevOps ? S'agit-il d'améliorer les performances du système, de résoudre des problèmes ou d'améliorer l'expérience utilisateur ? L'identification de ces objectifs vous aide à relier vos objectifs de surveillance aux résultats visés.</p>""}, {'', ""<p>Sélectionnez les outils de surveillance appropriés : l'équipe DevOps doit prendre en compte certaines variables, notamment la prise en charge des tâches de surveillance courantes, l'évolutivité, la convivialité et l'intégration. Assurez-vous qu'ils peuvent s'adapter aux besoins changeants de vos flux de travail DevOps.</p>""}, {'', '<p>Collectez des données appropriées : concentrez-vous sur la collecte et l’évaluation des indicateurs critiques. Évitez les volumes de données massifs. Toutes les données ne sont pas utiles et disposer de trop d’informations peut prêter à confusion. L’équipe DevOps doit classer ces indicateurs en fonction de leur impact direct sur les objectifs. Cela permet d’améliorer la sécurité, de réduire les taux d’erreur, d’accélérer les temps de réponse et d’optimiser l’utilisation des ressources.</p>'}, {'', '<p>Documentez tout : si vous souhaitez vraiment instaurer une culture de collaboration, documentez correctement vos procédures de suivi et encouragez le partage des connaissances entre les équipes. Cette stratégie favorise la responsabilisation et l’appropriation des procédures et activités de suivi par les membres de l’équipe.</p>'}, {'', ""<p>Examinez en permanence les résultats : l'analyse et l'amélioration continues doivent faire partie du processus. Évaluez votre plan de surveillance pour vous assurer qu'il est adaptable et qu'il correspond à vos objectifs. Utilisez les informations recueillies et les commentaires des utilisateurs pour affiner vos méthodes de surveillance.</p>""}, {'', ""<h3>Quelques exemples de cas d'utilisation pour la surveillance DevOps</h3>""}, {'', ""<p>Sans aucun doute, la surveillance DevOps présente plusieurs avantages pour toute entreprise. Mais son utilisation appropriée est nécessaire à son succès total. Voici quelques cas d'utilisation de la surveillance DevOps.</p>""}, {'', ""<p>Surveillance des workflows Git\xa0: des conflits de base de code peuvent survenir lorsque de nombreux développeurs travaillent sur le même projet. Git peut résoudre ces problèmes en utilisant des restaurations et des validations. Le processus Git surveille ces conflits et maintient un développement continu. Instrumentation de code\xa0: le processus d'ajout de code à un programme pour suivre ses performances et ses fonctionnalités est connu sous le nom d'instrumentation de code. L'observation des valeurs contextuelles et la surveillance des appels de pile sont essentielles. Les processus DevOps peuvent être évalués pour leur efficacité et leurs défauts. Dans cette situation, les tests et l'identification des bogues sont des facteurs critiques. Journaux d'intégration et de déploiement continus\xa0: cela facilite le dépannage des déploiements ayant échoué et permet de résoudre les difficultés. Consultez les fichiers journaux générés par les systèmes d'intégration continue, en particulier leurs avertissements et leurs erreurs, ainsi que les journaux de déploiement continu pour surveiller l'état général du pipeline de développement.</p>""}, {'<h3>Réflexions finales</h3>', ''}, {'', ""<p>Les services gérés par DevOps évoluent rapidement, tout comme les solutions de surveillance, compte tenu de la complexité et de la profondeur croissantes du développement logiciel. Prendre une décision éclairée concernant vos projets de développement peut vous obliger à choisir le meilleur système de surveillance qui comprenne les nombreuses parties d'une application et la manière dont elles interagissent. Il est donc conseillé de faire appel à des prestataires de services compétents pour vous assurer de prendre la bonne mesure au bon moment afin d'obtenir le retour sur investissement maximal.</p>""}]"
Une enquête sur Harness révèle une série de défis DevOps,"[{'', ""<p>Une enquête menée auprès de 500 responsables de l'ingénierie logicielle (38 %) et de praticiens révèle que près de la moitié d'entre eux déclarent qu'ils ne peuvent pas publier de code en production sans risquer des échecs, et 39 % d'entre eux indiquent que leur code ne parvient pas à être mis en production au moins la moitié du temps.</p>""}, {'', ""<p>Réalisée par Wakefield Research pour le compte de Harness, un fournisseur d'une plateforme d'intégration continue/livraison continue (CI/CD), l'enquête révèle également que lorsque le code doit être restauré, plus des deux tiers ont recours à des processus manuels.</p>""}, {'', '<p>Dans l’ensemble, 60 % des répondants ont également indiqué qu’ils continuaient à publier du code sur une base mensuelle ou trimestrielle. 59 % des développeurs ont déclaré que les exigences de sécurité des applications limitaient leur capacité à publier du code fréquemment.</p>'}, {'', ""<p>Nick Durkin, directeur technique de terrain chez Harness, a déclaré que l'une des principales raisons pour lesquelles la sécurité reste un goulot d'étranglement est principalement due au fait qu'une trop grande partie de la responsabilité en la matière a été transférée aux développeurs, au lieu de s'appuyer sur des plateformes DevSecOps imprégnées d'intelligence artificielle (IA) pour identifier et résoudre automatiquement les problèmes d'une manière qui responsabilise toutes les personnes impliquées dans le développement de logiciels.</p>""}, {'', '<p>En général, l’IA devrait être utilisée pour éliminer des tâches fastidieuses telles que la création et l’exécution de tests dans le cadre d’un effort plus vaste visant à éliminer le besoin de surveiller les outils et les plateformes, a ajouté Durkin.</p>'}, {'', '<p>Par exemple, 42 % des professionnels interrogés ont déclaré que le déploiement du code n’était ni rapide ni efficace, et 44 % ont indiqué que tester le code de bout en bout n’était pas efficace. Plus des deux tiers (67 %) ont déclaré qu’ils devaient attendre une semaine pour tester complètement le code et, même dans ce cas, 32 % ont déclaré que la couverture des tests unitaires n’était pas élevée.</p>'}, {'', '<p>Plus de la moitié (52 %) des développeurs interrogés ont attribué l’épuisement professionnel à l’une des principales raisons pour lesquelles leurs collègues ont quitté leur emploi, et un peu moins d’un quart (23 %) d’entre eux ont fait des heures supplémentaires au moins 10 jours par mois. 97 % d’entre eux ont déclaré qu’ils changeaient régulièrement de contexte entre 14 outils différents en moyenne.</p>'}, {'', '<p>Plus de la moitié (54 %) des personnes interrogées ont indiqué qu’il leur fallait plus d’une semaine pour apprendre à utiliser un nouvel outil. 59 % ont déclaré qu’il leur fallait une semaine pour créer un outillage interne. Plus d’un quart (28 %) ont déclaré qu’il leur fallait une journée pour créer un artefact.</p>'}, {'', ""<p>Près des deux tiers (62 %) des développeurs ont également connu une dérive du périmètre à mesure que les exigences s'élargissent, ce qui les rend moins confiants dans leur capacité à exécuter.</p>""}, {""<h3>Un rapport révèle que l'intégration des nouveaux employés prend trop de temps</h3>"", ''}, {'', '<p>Les nouvelles recrues ne semblent pas non plus apporter de soulagement immédiat, car leur intégration prend en moyenne 100 jours, selon 71 % des répondants.</p>'}, {'', '<p>De plus, 40 % des développeurs ont déclaré que leur organisation n’appliquait pas de bonnes politiques de sécurité et de gouvernance. Au total, 41 % d’entre eux n’ont pas de politiques de sécurité et de gouvernance automatisées, tandis que 42 % ont déclaré ne pas disposer de politiques de gestion des identités et des accès solides.</p>'}, {'', '<p>62 % des dirigeants interrogés ont déclaré qu’ils préféreraient résoudre ces problèmes en adoptant une plateforme DevOps intégrée, mais le rythme auquel cette transition se produit n’est pas clair. De nombreuses organisations ont investi massivement dans des plateformes DevOps existantes qu’elles sont souvent réticentes à abandonner et à remplacer. Dans de nombreux cas, les organisations effectuent la transition vers une plateforme DevOps moderne pour les nouveaux projets de développement d’applications tout en continuant à utiliser des plateformes existantes pour mettre à jour les applications existantes.</p>'}, {'', '<p>Rares sont ceux qui contesteraient que les flux de travail de développement logiciel sont optimaux. Le défi consiste à déterminer la meilleure façon d’avancer par rapport à la situation actuelle de la plupart des organisations, de manière à minimiser autant que possible les perturbations à un moment où les organisations n’ont jamais été aussi dépendantes des logiciels.</p>'}]"
Les bases de l'infrastructure en tant que code : un guide pour DevOps,"[{'', ""<p>Le cloud computing et DevOps révolutionnent la manière dont les entreprises conçoivent, construisent et gèrent leur infrastructure informatique et leurs applications. Le concept d'infrastructure en tant que code (IaC) est au cœur de cette transformation.</p>""}, {'', ""<p>L'IaC modifie la façon dont les équipes d'exploitation et de développement informatiques collaborent pour créer, provisionner et gérer l'infrastructure et les applications. Cette pratique traite l'infrastructure comme un logiciel, ce qui permet un provisionnement, une configuration et une gestion automatisés.</p>""}, {'', ""<p>Le processus devait changer. Traditionnellement, la gestion de l'infrastructure informatique était un processus fastidieux et sujet aux erreurs. Les administrateurs système configuraient et entretenaient manuellement les serveurs, les réseaux et les systèmes de stockage, ce qui entraînait des incohérences et des goulots d'étranglement opérationnels. À mesure que les entreprises adoptaient le cloud computing, elles étaient motivées à répondre à leurs besoins d'agilité et d'automatisation.</p>""}, {'', ""<p>L'IaC consiste essentiellement à définir l'infrastructure et les composants applicatifs à l'aide de code, qui peuvent ensuite être contrôlés par version, testés et déployés de la même manière que les applications logicielles. L'IaC met l'accent sur l'utilisation de code déclaratif ou impératif pour définir l'infrastructure, ce qui permet d'automatiser des tâches autrefois manuelles et chronophages.</p>""}, {'', '<p>Parmi ses avantages :</p>'}, {'', ""<li>Automatisation et cohérence : l'IaC permet aux organisations d'automatiser le provisionnement et la gestion de l'infrastructure. Le résultat est que les configurations restent cohérentes dans tous les environnements, réduisant ainsi le risque de dérive de configuration et d'erreurs humaines.</li>""}, {'', ""<li>Contrôle des versions : tout comme le code logiciel, les scripts IaC peuvent être stockés dans des systèmes de contrôle des versions. Cela permet de suivre les modifications au fil du temps, de revenir aux configurations précédentes et de collaborer avec les membres de l'équipe.</li>""}, {'', ""<li>Évolutivité : l'IaC facilite l'adaptation de l'infrastructure à la demande. Il est plus facile d'allouer et de désallouer des ressources de manière dynamique.</li>""}, {'', ""<li>Rapidité : Le temps nécessaire à la mise en service et à la configuration de l'infrastructure est considérablement réduit. Cette agilité permet aux organisations de répondre rapidement à l'évolution des besoins métier.</li>""}, {'', '<li>Réutilisabilité : IaC encourage la création de modèles et de modules réutilisables, ce qui réduit la duplication des efforts.</li>'}, {'', ""<li>Tests et validation : le code d'infrastructure peut être testé automatiquement, garantissant que les configurations sont correctes et répondent aux normes de sécurité et de conformité avant le déploiement.</li>""}, {'', '<h3>IaC dans le Cloud Computing</h3>'}, {'', ""<p>Les plateformes de cloud computing telles qu'Amazon Web Services (AWS), Microsoft Azure et Google Cloud Platform (GCP) ont joué un rôle essentiel dans la popularisation de l'IaC. Ces plateformes fournissent une large gamme de services et d'API qui peuvent être orchestrés et gérés via du code.</p>""}, {'', '<p>Par exemple, Netflix, le géant mondial du streaming, s’appuie fortement sur l’infrastructure cloud pour diffuser du contenu à des millions de téléspectateurs dans le monde entier. Il a adopté une architecture de microservices et utilise un outil appelé « Spinnaker » pour la diffusion continue. Spinnaker utilise l’IaC pour définir et gérer l’infrastructure requise pour déployer et faire évoluer les microservices. Les ingénieurs de Netflix écrivent du code pour décrire l’infrastructure d’une application, ce qui leur permet de créer, de modifier et de supprimer des ressources à la demande. Cette approche a permis à Netflix de réaliser un déploiement rapide, de réduire les temps d’arrêt et de maintenir une haute disponibilité tout en gérant une infrastructure complexe et dynamique.</p>'}, {'', '<p>À titre d’exemple, Shopify, la plateforme de commerce électronique qui alimente des milliers de boutiques en ligne, utilise l’IaC pour gérer son infrastructure auprès de plusieurs fournisseurs de cloud. Shopify utilise Terraform, un outil IaC populaire, pour définir et provisionner les ressources cloud. Avec Terraform, Shopify peut définir son infrastructure de manière claire et concise, garantissant ainsi la cohérence et la répétabilité. Tout est défini dans le code, que le personnel de Shopify souhaite créer une nouvelle instance, configurer un équilibreur de charge ou faire évoluer une application. Cette approche a permis à Shopify d’évoluer rapidement, de maintenir la stabilité et de s’adapter en toute transparence aux demandes changeantes des clients.</p>'}, {'', '<h3>IaC dans DevOps</h3>'}, {'', ""<p>DevOps s'aligne parfaitement avec IaC :</p>""}, {'', ""<li>Intégration continue/livraison continue (CI/CD)\xa0: l'IaC est la pierre angulaire des pipelines CI/CD. Dans un environnement DevOps, les modifications apportées à l'infrastructure et aux applications sont intégrées et testées en continu. Les scripts IaC définissent l'état souhaité de l'infrastructure, garantissant sa cohérence à toutes les étapes du développement.</li>""}, {'', ""<li>Collaboration : les développeurs peuvent définir les exigences d'infrastructure parallèlement au code d'application, ce qui conduit à une compréhension partagée des besoins d'infrastructure et réduit le problème « cela fonctionne sur ma machine ».</li>""}, {'', ""<li>Infrastructure immuable : IaC promeut le concept d'infrastructure immuable, où les serveurs et les ressources ne sont pas modifiés sur place mais remplacés par de nouvelles instances lorsque des modifications sont nécessaires. Cette approche garantit la cohérence et simplifie les procédures de restauration.</li>""}, {'', ""<li>Tests d'infrastructure : l'infrastructure en tant que code peut être soumise à des tests automatisés, notamment des analyses de sécurité et des contrôles de conformité. Cette approche permet d'identifier et de corriger les problèmes avant qu'ils n'atteignent la production.</li>""}, {'', '<li>Boucle de rétroaction : les problèmes ou divergences d’infrastructure sont détectés tôt dans le processus de développement, réduisant ainsi les coûts et les efforts nécessaires pour les résoudre.</li>'}, {'', ""<p>À mesure que la technologie continue d'évoluer, le rôle de l'IaC dans le cloud computing et DevOps ne fera que gagner en importance. Les organisations qui adoptent l'IaC peuvent obtenir un avantage concurrentiel en accélérant leurs cycles de développement, en réduisant les frais opérationnels et en améliorant la fiabilité de l'infrastructure.</p>""}, {'', '<p>L’adoption de l’infrastructure en tant que code n’est pas seulement une tendance, mais une nécessité à l’ère du cloud et de DevOps. Elle permet aux organisations de créer et de gérer l’infrastructure avec la même rigueur et la même discipline que le développement de logiciels, ouvrant ainsi la voie à un avenir plus agile et plus efficace dans les opérations informatiques.</p>'}]"
Spotify va fournir une instance d'opinion de Backstage IDP,"[{'', ""<p>Spotify a annoncé aujourd'hui qu'il mettait à disposition du marché une instance de son portail de développement interne (IDP) basé sur la plateforme open source Backstage que la société avait précédemment contribué à la Cloud Native Computing Foundation (CNCF).</p>""}, {'', ""<p>Pia Nilsson, directrice principale de l'ingénierie chez Spotify, a déclaré que Spotify Portal for Backstage, disponible en version bêta privée aujourd'hui, fournit aux équipes d'ingénierie de la plateforme un cadre low-code/no-code pour la mise en place d'un IDP.</p>""}, {'', ""<p>De plus, Spotify met désormais à disposition des organisations ayant adopté Backstage un support d'entreprise, ainsi que des plug-ins supplémentaires qui simplifient les intégrations avec des outils et des plateformes de fournisseurs tiers tels que New Relic, Atlassian, Snyk et Datadog.</p>""}, {'', ""<p>Enfin, Spotify ajoute des fonctionnalités pour gérer les hackathons et faire remonter les informations sur l'utilisation. Un prochain plug-in promet également de permettre d'ajouter des entités de données à un catalogue de logiciels résidant dans Backstage.</p>""}, {''}, {'', ""<p>Backstage a gagné en popularité en tant qu'IDP principalement parce qu'il fournit un cadre extensible pour centraliser la gestion des environnements de développement.</p>""}, {'', '<h3>Mise en œuvre du portail Spotify</h3>'}, {'', ""<p>Certaines entreprises ont trouvé difficile de créer et de déployer Backstage. Spotify propose donc désormais une implémentation plus réfléchie du portail Spotify, plus simple à déployer, à configurer et à gérer à l'aide de modèles, a noté Nilsson.</p>""}, {'', ""<p>Les applications étant devenues plus difficiles à créer, l'accent est mis sur l'amélioration de la productivité des développeurs. Dans le même temps, les IDP permettent aux équipes d'ingénierie de plateforme de rationaliser le nombre d'outils utilisés de manière à réduire le coût total du développement logiciel.</p>""}, {'', '<p>De nombreux développeurs souhaitent ajouter des outils selon leurs besoins. Les équipes DevOps doivent donc trouver un moyen de trouver un équilibre entre la réduction de la charge cognitive à laquelle les développeurs sont confrontés et ce qui pourrait devenir une approche trop lourde de la centralisation des flux de travail DevOps. Si les développeurs talentueux commencent à se diriger vers la sortie parce qu’ils n’aiment pas l’expérience proposée, les responsables informatiques auront du mal à les remplacer.</p>'}, {'', '<h3>Donner plus de temps aux équipes DevOps</h3>'}, {'', '<p>L’idée est que les IDP, utilisés dans le contexte de l’ingénierie de plateforme, donneront aux développeurs plus de temps pour se concentrer sur l’écriture de la logique métier. Cependant, le développement d’applications est autant un art qu’une science. Ce n’est pas parce que les développeurs ont plus de temps que les idées et l’inspiration nécessaires pour écrire du code s’ensuivent automatiquement. Les IDP et l’ingénierie de plateforme, à tout le moins, créent cependant la possibilité pour les développeurs d’écrire plus de logique métier plus rapidement. Le degré auquel cela se produit varie naturellement d’une organisation à l’autre.</p>'}, {'', ""<p>Chaque organisation devra décider dans quelle mesure elle souhaite adopter l'ingénierie de plateforme, mais quel que soit son niveau d'engagement, les IDP peuvent jouer un rôle essentiel en simplifiant l'intégration des développeurs dans de nouveaux projets, a noté Nillson. En outre, un IDP permet aux équipes DevOps de suivre plus facilement des indicateurs tels que la vitesse à laquelle le code est mis à jour, a-t-elle déclaré.</p>""}, {'', '<p>Les IDP ne sont bien sûr pas nécessairement idéaux, mais avec l’essor de Backstage, il y a clairement désormais plus de standardisation, car les équipes DevOps continuent de chercher à éliminer autant de frictions que possible.</p>'}]"
Le rôle de l'IA dans la sécurisation des chaînes d'approvisionnement en logiciels et en données,"[{'', '<p>Les vulnérabilités de la chaîne d’approvisionnement pèsent lourd dans le paysage de la cybersécurité, avec des menaces et des attaques telles que SolarWinds, 3CX, Log4Shell et maintenant XZ Utils, soulignant l’impact potentiellement dévastateur de ces failles de sécurité. Ces derniers exemples d’attaques de logiciels open source (OSS) constituent un vecteur d’attaque croissant. L’étude de Capterra révèle que les attaques de la chaîne d’approvisionnement de logiciels ont touché près des deux tiers (61 %) de toutes les entreprises américaines d’avril 2022 à avril 2023.</p>'}, {'', '<p>Il faut s’attendre à une accélération des attaques sur la chaîne d’approvisionnement des logiciels open source, les attaquants automatisant les attaques dans les projets de logiciels open source courants et les gestionnaires de packages. De nombreux RSSI et équipes DevSecOps ne sont pas préparés à mettre en œuvre des contrôles dans leurs systèmes de build existants pour atténuer ces menaces. En 2024, les équipes DevSecOps s’éloigneront des modèles de sécurité shift-left au profit d’un « shift-down » en utilisant l’IA pour automatiser la sécurité hors des flux de travail des développeurs.</p>'}, {'', '<p>Voici les facteurs qui alimentent l’augmentation des attaques contre la chaîne d’approvisionnement de logiciels et le rôle de l’IA pour aider les développeurs à travailler plus efficacement tout en créant du code plus sécurisé.</p>'}, {'', ""<h3>Les attaques contre la chaîne d'approvisionnement des logiciels open source vont s'accélérer</h3>""}, {'', '<p>Les bibliothèques et langages open source sont à la base de plus de 90 % des logiciels du monde. Dans une enquête américaine menée auprès de près de 300 professionnels de l’informatique et de la sécurité informatique, 94 % ont déclaré que leurs entreprises utilisaient des logiciels open source et 57 % employaient plusieurs plateformes open source. Exactement la moitié des personnes interrogées ont déclaré que le niveau de menace était « élevé » ou « extrême », tandis que 41 % le considéraient comme « modéré ». Au moment de la rédaction de cet article, les détails de la porte dérobée implantée dans la bibliothèque XZ et plusieurs autres packages OSS venaient d’être publiés. L’omniprésence de l’open source dans le monde entier est l’un des facteurs clés qui alimentent la montée des attaques contre la chaîne d’approvisionnement.</p>'}, {'', '<h3>La gouvernance des données et la chaîne d’approvisionnement des données deviendront des enjeux cruciaux</h3>'}, {'', '<p>Les professionnels de la sécurité doivent également tenir compte de la manière dont les vulnérabilités de sécurité s’étendent à leurs chaînes d’approvisionnement en données. Bien que les organisations intègrent généralement des logiciels développés en externe dans leurs chaînes d’approvisionnement en logiciels, leurs chaînes d’approvisionnement en données ont souvent besoin de mécanismes plus clairs pour comprendre ou contextualiser les données. Contrairement aux systèmes ou fonctions structurés des logiciels, les données sont non structurées ou semi-structurées et sont soumises à un large éventail de normes réglementaires.</p>'}, {'', '<p>De nombreuses entreprises construisent des systèmes d’IA ou de machine learning à partir d’énormes pools de données provenant de sources hétérogènes. Les modèles de machine learning sur les zoos de modèles sont publiés avec une compréhension minimale du code et du contenu utilisés pour produire les modèles. Les ingénieurs logiciels doivent gérer ces modèles et ces données avec autant de soin que le code entrant dans le logiciel qu’ils créent, en prêtant attention à sa provenance.</p>'}, {'', ""<p>Les équipes DevSecOps doivent évaluer les risques liés à l'utilisation des données, en particulier lors de la création de LLM pour former des outils d'IA. Cela exige une gestion minutieuse des données au sein des modèles pour éviter la transmission accidentelle de données sensibles à des tiers comme OpenAI.</p>""}, {'', ""<p>Les organisations doivent adopter des politiques strictes décrivant l'utilisation approuvée du code généré par l'IA et, lors de l'intégration de plateformes tierces pour l'IA, effectuer une évaluation approfondie de la diligence raisonnable garantissant que leurs données ne seront pas utilisées pour la formation et le réglage fin du modèle d'IA/ML.</p>""}, {'', '<h3>L’automatisation de la sécurité par l’IA aidera les organisations à passer du « Shift-Left » au « Shift-Down »</h3>'}, {'', '<p>Le secteur a adopté le concept de shift-left il y a dix ans pour remédier aux failles de sécurité dès le début du cycle de développement logiciel et pour améliorer les flux de travail des développeurs. Les défenseurs des systèmes ont longtemps été désavantagés. L’IA a le potentiel d’égaliser les règles du jeu. Alors que les équipes DevSecOps naviguent dans les subtilités de la gouvernance des données, elles doivent également évaluer l’impact de l’évolution du paradigme du shift-left sur les postures de sécurité de leurs organisations.</p>'}, {'', '<p>Les entreprises vont commencer à aller au-delà du shift-left pour adopter l’IA afin d’automatiser entièrement les processus de sécurité et de les retirer du flux de travail du développeur. C’est ce qu’on appelle le « shift-down », car cela déplace la sécurité vers des fonctions automatisées et de niveau inférieur de la pile technologique au lieu de surcharger les développeurs avec des décisions compliquées et souvent difficiles.</p>'}, {'', '<p>Le rapport mondial DevSecOps de GitLab intitulé « L’état de l’IA dans le développement logiciel » révèle que les développeurs ne consacrent que 25 % de leur temps à la génération de code. L’IA peut améliorer leur productivité en optimisant les 75 % restants de leur charge de travail. C’est une façon de tirer parti de la capacité de l’IA à résoudre des problèmes techniques spécifiques et à améliorer l’efficacité et la productivité de l’ensemble du cycle de vie du développement logiciel.</p>'}, {'', '<p>L’année 2024 devrait être celle où les menaces croissantes pesant sur les écosystèmes OSS et affectant négativement les chaînes d’approvisionnement mondiales de logiciels ont catalysé des changements substantiels dans les stratégies de cybersécurité, notamment une dépendance accrue à l’IA pour protéger les infrastructures numériques. Le paysage de la cybersécurité est déjà en pleine transformation, avec une attention croissante portée à l’atténuation des vulnérabilités de la chaîne d’approvisionnement, à l’application de la gouvernance des données et à l’intégration de l’IA dans les mesures de sécurité. Cette transformation promet d’orienter les équipes DevSecOps vers des processus de développement logiciel axés sur l’efficacité et la sécurité.</p>'}]"
Comment migrer une plateforme d'observabilité vers l'open source,"[{'', ""<p>Il est difficile d'obtenir une observabilité complète des applications d'entreprise. Si des services tiers comme New Relic ou Datadog facilitent l'observabilité de bout en bout, à mesure que votre application devient plus complexe, les données de télémétrie le deviennent également, ce qui entraîne une augmentation des coûts. La migration vers une pile open source vous permet de contrôler les données de télémétrie et de réduire les coûts d'observabilité, malgré les défis liés aux engagements des fournisseurs de services existants.</p>""}, {'', ""<p>La migration vers des solutions open source est simple avec une architecture et des piles technologiques simples. Cependant, la transition de l'ensemble de la pile d'observabilité vers l'open source exige une planification méticuleuse, des évaluations d'outils/frameworks, des tests, une analyse des risques et une communication entre les équipes, en particulier compte tenu du nombre de microservices, de la forte dépendance au cloud et de la diversité des langages et frameworks à prendre en compte.</p>""}, {'', ""<p>Une plateforme d'observabilité cloud absorbe généralement 20 à 30 % des dépenses globales d'infrastructure, mais dans certains cas, elle peut atteindre 50 à 60 %. Si vos dépenses d'observabilité dépassent 50 %, la transition vers des solutions open source serait une option judicieuse pour atténuer les coûts de l'infrastructure technologique.</p>""}, {'', '<p>Depuis deux ans, je travaille à la migration de la plateforme d’observabilité d’un fournisseur de solutions propriétaire vers une pile open source. Plongeons-nous dans les étapes clés nécessaires à une telle migration.</p>'}, {'', '<h3>Finaliser les données et systèmes clés de télémétrie</h3>'}, {'', ""<p>Les plateformes d'observabilité gérées offrent des informations complètes sur l'état du système, ce qui nécessite un volume important de données de télémétrie de haute qualité. Par défaut, les agents chargés de capturer et de transmettre les données de télémétrie sont configurés pour recueillir autant d'informations que possible, ce qui facilite la création de tableaux de bord et de rapports complets. Cependant, cette collecte de données extensive contribue au coût global.</p>""}, {'', '<p>Sélectionnez les systèmes que vous devez réellement surveiller. Une application d’entreprise typique comprend des bases de données, des bases de données de mise en cache, des orchestrateurs de conteneurs et de nombreux services cloud. Les ingénieurs surveillent souvent plus de services que nécessaire, ce qui entraîne des complexités et des coûts inutiles. Il suffit souvent de rationaliser le périmètre de surveillance aux systèmes essentiels.</p>'}, {'', '<p>D’après mon expérience, la quantité de données de télémétrie requise est nettement inférieure à celle que les services tiers récupèrent généralement. Vous pouvez facilement diviser ces collectes de données en deux catégories\xa0: les données indispensables et les données utiles. Les données «\xa0indispensables\xa0» correspondent aux quatre signaux d’or bien établis\xa0:</p>'}, {'', '<li>Latence : le temps nécessaire pour répondre à une requête</li>'}, {'', ""<li>Trafic : le volume de requêtes qu'un système traite actuellement</li>""}, {'', ""<li>Le taux d'erreur : le nombre de requêtes qui échouent ou renvoient des réponses inattendues</li>""}, {'', '<li>Saturation des ressources : le pourcentage de ressources disponibles consommées</li>'}, {'', ""<p>Ces signaux fondamentaux servent de base aux tableaux de bord et aux alertes clés. Les plateformes d'observabilité construites sur ces signaux couvrent 80 % des cas d'utilisation typiques.</p>""}, {'', '<p>Il est essentiel de comprendre les exigences en matière de métadonnées de chaque signal. L’étendue des métadonnées capturées a un impact direct sur la complexité et le coût de la plateforme d’observabilité. Par exemple, lors de la surveillance de la latence d’un service à l’autre, tenez compte de la nécessité d’adresses IP de service, d’ID d’instance ou d’informations d’en-tête.</p>'}, {'', '<h3>Sélectionnez la pile concernée</h3>'}, {'', ""<p>Les plateformes d'observabilité gérées telles que New Relic et Datadog offrent une surveillance complète de divers composants d'entreprise. Cependant, lorsque vous migrez vers une pile open source, vous devez évaluer et intégrer les piles d'outils pour répondre à diverses exigences de surveillance.</p>""}, {'', ""<p>Un aspect important à prendre en compte est la mise à l'échelle : comment gérer l'énorme quantité de données générées chaque minute sur tous ces systèmes. Concentrez-vous sur deux fronts : la sélection de la pile pour le stockage et le traitement des données de télémétrie (journaux, métriques et traces) et la conception de méthodes pour capturer et transmettre les données de télémétrie à partir de divers systèmes.</p>""}, {'', '<h4>Pile pour les bûches</h4>'}, {'', ""<p>Vous avez besoin d'une pile capable de traiter et de stocker de manière efficace et économique le volume considérable de journaux générés par le système. Par le passé, j'utilisais la pile ELK pour stocker et rechercher des journaux, mais il s'agit d'une solution générique et non conçue spécifiquement pour les journaux.</p>""}, {'', '<p>Je recommande Loki de Grafana pour sa gestion efficace des gros volumes de journaux et LogQL, un langage proche de PromQL. Si vous connaissez PromQL, la navigation dans des données de journaux volumineuses avec LogQL est simple et intuitive.</p>'}, {'', '<h4>Pile pour les métriques</h4>'}, {'', ""<p>Prometheus est une base de données populaire pour le stockage de données de mesures de séries chronologiques. Cependant, elle présente des limitations liées à la mise à l'échelle ; elle ne peut pas être mise à l'échelle horizontalement. Des alternatives comme Thanos, Mimir de Grafana et VictoriaMetrics offrent de meilleures solutions prêtes à l'emploi pour la mise à l'échelle horizontale.</p>""}, {'', '<p>J’ai effectué quelques recherches et j’ai finalement choisi Mimir de Grafana pour stocker les métriques dans un projet de migration similaire. Cette décision était basée sur la capacité de stockage à distance de Mimir, ainsi que sur son architecture évolutive et hautement disponible.</p>'}, {'', '<h3>Pile de traçage</h3>'}, {'', '<p>Le traçage distribué est indispensable dans une architecture de microservices, pour identifier facilement les goulots d’étranglement en termes de latence et pour résoudre les bugs liés aux performances. Tempo de Grafana pourrait être une bonne option pour plusieurs raisons :</p>'}, {'', ""<li>Il peut s'intégrer de manière transparente en tant que backend pour le tableau de bord de Grafana. La consolidation des tableaux de bord dans Grafana permet d'éviter de naviguer entre plusieurs applications. Changer de plateforme uniquement pour le traçage tout en utilisant Grafana pour d'autres aspects de l'observabilité ne serait pas pratique.</li>""}, {'', ""<li>Tempo offre une expérience similaire à TraceQL lors de l'interaction avec la base de données, simplifiant le processus d'apprentissage et réduisant considérablement la courbe d'apprentissage.</li>""}, {'', '<li>Tempo est compatible avec les protocoles de traçage open source les plus répandus, tels que Zipkin et Jaeger. Si une équipe utilise déjà ces protocoles, la transition vers Tempo de Grafana se fera plus facilement.</li>'}, {'', ""<p>Vous devez installer un agent sur le système pour capturer et transmettre les données de télémétrie. Ces étapes varient en fonction du système surveillé. Les configurations d'applications d'entreprise classiques impliquent la surveillance des intégrations cloud, la surveillance des processus, des hôtes d'infrastructure, des clusters Kubernetes et la surveillance des applications. Parmi les possibilités :</p>""}, {'', '<li>Intégrations cloud : cela inclut la surveillance des services cloud tels que SQS, SNS, EMR et EC2.</li>'}, {'', ""<li>Surveillance des processus : cela implique la surveillance des processus exécutés sur des machines bare-metal. Avec l'avènement de la dockerisation et de Kubernetes, il existe désormais une manière standardisée de démarrer l'application. Dans le passé, il n'y avait pas de mécanisme fixe. Par exemple, pour exécuter une application Java, vous pouviez utiliser une application Java, la commande « java -jar », Tomcat ou OS systemctl. Dans le cas de node, il pourrait s'agir de npm ou de PM2. Chaque équipe ou service peut avoir sa propre façon de démarrer le processus.</li>""}, {'', ""<li>Hôte d'infrastructure\xa0: cela implique de surveiller la machine elle-même pour des mesures telles que l'utilisation du processeur, la mémoire, les E/S de disque et les E/S réseau ou de vérifier si la machine est hors ligne.</li>""}, {'', '<li>Surveillance Kubernetes\xa0: vous devez régulièrement surveiller un cluster Kubernetes, par exemple dans les cas où Kubernetes ne parvient pas à planifier un pod en raison de ressources insuffisantes.</li>'}, {'', ""<li>Surveillance des applications : cette surveillance se concentre sur la supervision des services créés par l'équipe. Chaque service peut différer dans son approche de développement et son choix de pile technologique, mais du point de vue de l'observabilité, ils sont généralement traités de la même manière.</li>""}, {'', ""<li>Surveillance du navigateur et des appareils mobiles\xa0: ces mesures garantissent des performances et une expérience utilisateur optimales sur différentes plateformes. La surveillance du navigateur comprend le suivi des temps de chargement des pages, des performances de rendu, des erreurs JavaScript et de l'utilisation des ressources. Pour les appareils mobiles, elle comprend la surveillance des plantages d'applications, de la latence, de l'utilisation de la batterie, des requêtes réseau et des mesures spécifiques à l'appareil.</li>""}, {'', ""<p>Avant l'avènement d'OpenTelemetry, il n'existait aucune méthode standardisée de surveillance des applications. OpenTelemetry est né de la fusion de deux projets antérieurs, OpenTracing et OpenCensus. Il s'agit d'un cadre indépendant des fournisseurs et des outils permettant d'instrumenter des applications ou des systèmes indépendamment de la langue, de l'infrastructure ou de l'environnement d'exécution. OpenTelemetry représente un effort communautaire important, et sa popularité et sa stabilité ne cessent de croître.</p>""}, {'', ""<h3>Valider la pile d'observabilité sur l'architecture d'application</h3>""}, {'', ""<p>Les applications varient en termes d'architecture et de stades de développement. Pour répondre à cette diversité, vous avez besoin de plusieurs preuves de concept (POC) sur différents systèmes, tels que l'infrastructure, le service cloud et les applications back-end et front-end.</p>""}, {'', ""<p>La migration d'une plateforme d'observabilité d'une pile technologique à une autre est simple lorsque l'architecture de l'application est monolithique. Cependant, lorsque l'architecture sous-jacente est constituée de microservices, cela devient difficile.</p>""}, {'', ""<p>Bien que l'architecture des microservices offre une flexibilité dans le choix de différentes piles technologiques pour la création de services, la conception d'une solution standard pour la capture de données de télémétrie devient un défi en raison de la diversité des piles technologiques impliquées.</p>""}, {'', '<p>Il est nécessaire de réaliser des POC pour différentes combinaisons technologiques afin de créer un outil en libre-service que les équipes individuelles peuvent suivre pour migrer facilement leurs services. Par exemple, si votre service utilise Java 11, Spring Boot 3.x.x et PostgreSQL, ces POC peuvent fournir des étapes standardisées pour permettre la surveillance des applications.</p>'}, {'', '<h3>Migrer les composants principaux</h3>'}, {'', '<p>La mise en place de la plateforme d’observabilité implique la configuration de la pile pour les métriques, les journaux et les traces, ainsi que l’installation des agents nécessaires. L’un des avantages de la migration de la plateforme d’observabilité est qu’il n’est pas nécessaire de transférer les anciennes données de télémétrie. Pendant la phase de test, vous disposerez d’une période de chevauchement pour accumuler suffisamment de données utiles pour créer des tableaux de bord et des alertes spécifiques.</p>'}, {'', ""<p>La migration des alertes et des tableaux de bord vers le nouveau système est essentielle, même lorsque la migration des données n'est pas nécessaire. En effet, les modifications apportées aux techniques sous-jacentes de capture des mesures peuvent modifier les noms des mesures et, par la suite, toutes les expressions de requête utilisées dans les alertes et les tableaux de bord. Certaines mesures peuvent ne pas être disponibles dans certains scénarios, en particulier les mesures dérivées qui peuvent être recréées à partir de mesures sous-jacentes.</p>""}, {'', ""<p>Bien que la migration manuelle des expressions de requête soit une option, un processus manuel est sujet à des erreurs et prend du temps pour un grand nombre d'alertes. J'ai connu une situation similaire avec des numéros d'alerte allant de 100 à 400.</p>""}, {'', '<p>Pour simplifier la migration, nous avons développé un script Node.js qui convertit par programmation les expressions de requête de New Relic en expressions Prometheus. Le script a effectué les étapes de haut niveau suivantes\xa0:</p>'}, {'', ""<li>Connecté au serveur API New Relic et récupéré toutes les alertes configurées pour l'intégration cloud</li>""}, {'', ""<li>Conversion des expressions d'alerte New Relic en expressions d'alerte PromQL</li>""}, {'', ""<li>J'ai écrit toutes les expressions d'alerte Prometheus dans le fichier YAML</li>""}, {'', '<p>La mise en œuvre de cette technique nous a permis de migrer avec succès toutes les alertes et les tableaux de bord en quatre ou cinq jours, un processus qui nécessiterait normalement un mois de travail manuel. De plus, les scripts ont considérablement réduit le risque d’erreur humaine.</p>'}, {'', '<h3>Tester la migration</h3>'}, {'', '<p>Vérifiez chaque alerte et chaque tableau de bord. Exécutez à la fois les anciennes et les nouvelles plateformes pendant les tests. Le fonctionnement en parallèle permet de tester en profondeur les métriques.</p>'}, {'', ""<p>Cependant, l'exécution simultanée des deux systèmes peut entraîner une baisse des performances en raison de l'envoi de données de télémétrie par les services à deux emplacements. De plus, il peut y avoir une légère augmentation des coûts jusqu'à la transition complète.</p>""}, {'', '<p>L’exécution des deux systèmes en parallèle fournit une configuration idéale pour garantir que les tableaux de bord et les alertes migrés fonctionnent correctement.</p>'}, {'', ""<p>Il est simple de tester les alertes déclenchées au cours de ce processus, mais évaluer celles qui ne se sont pas encore déclenchées constitue un défi. Dans le cadre d'un projet, j'ai réduit la valeur seuil de chaque alerte et testé ses fonctionnalités. Environ 90 % des alertes générées par des scripts personnalisés ont fonctionné de manière transparente, et seulement 10 % ont nécessité quelques ajustements manuels.</p>""}, {'', ""<h3>Migrer d'autres composants associés</h3>""}, {'', ""<p>D'autres systèmes dépendent des données générées par la plateforme d'observabilité. Par exemple :</p>""}, {'', ""<li>Systèmes de notification comme le courrier électronique, le canal Slack, etc., qui alertent en cas d'incident</li>""}, {'', '<li>Les outils de gestion des incidents comme PagerDuty offrent un moyen simplifié de gérer les incidents.</li>'}, {'', ""<p>Ces systèmes s'appuient sur la charge utile de l'alerte pour fonctionner correctement. Leurs ressources doivent être mises à jour, car un changement dans la plateforme d'observabilité entraîne un changement dans la charge utile de l'alerte. Mettez à jour les modèles des systèmes de notification pour vous assurer qu'ils s'intègrent parfaitement à la nouvelle charge utile de l'alerte.</p>""}, {'', '<p>De même, les outils de gestion des incidents comme PagerDuty nécessitent des modifications des règles de routage, des politiques d’escalade et de la planification. Heureusement, des outils de migration open source prêts à l’emploi sont disponibles pour faciliter la migration de PagerDuty vers OnCall de Grafana. Cependant, dans d’autres solutions, vous n’aurez peut-être pas accès à des outils de migration prêts à l’emploi. Dans de tels cas, une migration manuelle ou la rédaction de scripts peuvent être nécessaires.</p>'}, {'', '<h3>Calendrier de la migration</h3>'}, {'', ""<p>La planification d'une migration implique la finalisation des données et des systèmes de télémétrie clés, la sélection des piles pertinentes et la validation de la pile d'observabilité sur l'architecture de l'application. Cela prend généralement environ trois semaines.</p>""}, {'', ""<p>La durée du projet dépend de facteurs tels que le nombre d'équipes impliquées et la quantité de services actifs. Par exemple, une migration vers un environnement open source avec 100 microservices et la participation de 10 équipes différentes peut prendre quatre mois.</p>""}, {'', '<h3>Migration terminée\xa0!</h3>'}, {'', '<p>La transition de votre plateforme d’observabilité vers une pile open source offre une voie prometteuse pour réduire les coûts et renforcer le contrôle des données de télémétrie. Cependant, cette migration exige une planification et une exécution méticuleuses, englobant des étapes essentielles telles que la priorisation des fonctionnalités, la sélection de la pile, les POC, la migration des composants principaux, les tests et la migration des systèmes associés. Malgré les défis posés par la diversité des architectures et des technologies, une approche systématique, une évaluation complète et une collaboration entre les équipes peuvent faciliter un processus de migration fluide et garantir son succès.</p>'}]"
L'essor du low-code/no-code dans DevOps,"[{'', '<p>L’évolution constante du paysage technologique a entraîné un changement de paradigme dans le développement et le déploiement de logiciels. L’une des principales tendances qui gagne du terrain est l’intégration de plateformes low-code/no-code au sein de l’écosystème DevOps. Cet article explore la manière dont ces plateformes révolutionnent la manière dont les applications sont développées, déployées et gérées, favorisant la collaboration entre les développeurs et les équipes opérationnelles.</p>'}, {'', '<p>1. Démocratiser le développement :</p>'}, {'', ""<p>Les plateformes low-code/no-code permettent aux personnes ayant une expérience limitée en codage de participer activement au processus de développement d'applications. Cette démocratisation du développement accélère la livraison de logiciels en permettant aux analystes commerciaux, aux concepteurs et à d'autres non-développeurs, parfois appelés développeurs citoyens, de contribuer directement à la création d'applications.</p>""}, {'', '<p>2. Accélération du délai de mise sur le marché :</p>'}, {'', ""<p>Le cycle de vie traditionnel du développement logiciel implique souvent de longs processus de codage, ce qui entraîne des retards dans la publication des versions. Les plateformes low-code/no-code permettent un prototypage et un développement rapides grâce à des interfaces visuelles, réduisant ainsi considérablement le temps nécessaire pour faire passer les applications du concept à la production. Cette accélération s'aligne sur les principes de DevOps, favorisant une livraison continue et une mise sur le marché plus rapide.</p>""}, {'', '<p>3. Améliorer la collaboration :</p>'}, {'', ""<p>DevOps vise à éliminer les silos et à favoriser la collaboration entre les équipes de développement et d'exploitation. Les plateformes low-code/no-code comblent le fossé entre ces rôles traditionnellement distincts en fournissant un terrain d'entente où les parties prenantes techniques et non techniques peuvent collaborer de manière transparente. Cet environnement collaboratif favorise une meilleure communication, une meilleure compréhension et un meilleur alignement des objectifs.</p>""}, {'', '<p>4. Flexibilité dans le déploiement :</p>'}, {'', ""<p>Les solutions low-code/no-code sont conçues pour être indépendantes de la plateforme, ce qui permet le déploiement d'applications dans différents environnements, y compris sur site et dans le cloud. Cette flexibilité s'aligne sur le principe DevOps de l'infrastructure en tant que code (IaC), permettant aux équipes de déployer et de gérer des applications de manière cohérente sur diverses infrastructures.</p>""}, {'', '<p>5. Réduire la dépendance aux compétences spécialisées :</p>'}, {'', ""<p>Dans un environnement DevOps, il est essentiel de réduire les goulots d'étranglement et les dépendances vis-à-vis d'individus ou de rôles spécifiques. Les plateformes low-code/no-code permettent aux équipes d'être plus autonomes, réduisant ainsi la dépendance à l'égard de compétences de codage spécialisées ou de formations DevOps. Cela contribue à son tour à une culture DevOps plus résiliente et adaptable.</p>""}, {'', '<p>6. Défis et considérations :</p>'}, {'', '<p>L’adoption du low-code/no-code dans DevOps apporte de nombreux avantages, mais elle présente également des défis. Cette section aborde des considérations telles que la sécurité, l’évolutivité et le besoin potentiel de personnalisation au-delà des capacités de la plateforme.</p>'}, {'', ""<p>7. Cas d'utilisation réels\xa0:</p>""}, {'', ""<p>Découvrez des exemples concrets d'organisations qui ont intégré avec succès le low-code/no-code dans leurs workflows DevOps. Mettez en évidence les résultats positifs, les leçons apprises et l'impact sur leur cycle de vie global de développement logiciel.</p>""}, {'', '<p>Conclusion:</p>'}, {'', '<p>L’essor des plateformes low-code/no-code dans DevOps remodèle la manière dont les logiciels sont développés et déployés. En favorisant la collaboration, en accélérant les cycles de développement et en réduisant les dépendances vis-à-vis des compétences spécialisées, ces plateformes s’alignent parfaitement sur les principes de DevOps. Alors que les entreprises continuent d’adopter cette tendance, la synergie entre low-code/no-code et DevOps est sur le point de redéfinir l’avenir de la distribution d’applications.</p>'}]"
Une étude révèle des gains substantiels en matière d'ingénierie des plateformes,"[{'', '<p>Une enquête mondiale menée auprès de 500 professionnels du développement et du déploiement d’applications révèle que même si 43 % d’entre eux travaillent pour des organisations qui disposent d’une équipe de plateforme depuis trois à cinq ans, il ne semble pas y avoir beaucoup de cohérence quant à l’endroit où cette équipe rend compte au sein d’une organisation.</p>'}, {'', ""<p>L'enquête, menée par Puppet by Perforce, un fournisseur d'un cadre d'automatisation, a révélé que parmi les organisations qui disposent d'équipes de plateforme, plus de la moitié (58 %) opèrent dans le contexte d'une organisation DevOps ou de gestion d'infrastructure plus vaste, contre 40 % qui ont leur propre direction dédiée.</p>""}, {'', ""<p>David Sandilands, architecte principal des solutions pour Puppet by Perforce, a déclaré que l'enquête montre clairement que l'ingénierie de plateforme, en général, n'est pas un concept nouveau, mais il est clair que de plus en plus d'organisations tentent de réduire le niveau de travail actuellement associé à la création et au déploiement d'applications.</p>""}, {'', '<p>Peu importe où se trouvent les équipes de plateformes au sein d’une organisation, près des deux tiers des répondants (65 %) ont déclaré qu’elles étaient importantes pour leur organisation et qu’elles recevraient un financement continu. L’objectif principal de la formation de ces équipes est d’augmenter la productivité (58 %) et d’automatiser les processus standardisés (51 %), selon l’enquête. Les principaux objectifs des équipes d’ingénierie de plateformes sont de résoudre les problèmes (30 %), d’appliquer les processus de sécurité (27 %) et d’accélérer les transitions vers des environnements informatiques natifs du cloud (26 %). 70 % des répondants ont déclaré que la sécurité était intégrée à leurs plateformes dès le départ.</p>'}, {'', ""<p>Cela suggère qu'en plus d'assumer la responsabilité des flux de travail DevOps traditionnels, de nombreuses équipes de plateformes sont désormais également invitées à garantir que les meilleures pratiques DevSecOps sont suivies à un moment où les exigences de sécurité des applications deviennent plus strictes, a noté Sandilands.</p>""}, {'', '<p>Plus des trois quarts (76 %) ont également indiqué avoir déployé deux ou plusieurs portails libre-service, et 27 % en avoir déployé cinq ou plus.</p>'}, {'', ""<p>Cependant, seulement 22 % ont déclaré avoir déployé des applications sur Kubernetes dans un environnement de production, et un peu moins de la moitié (46 %) ont déclaré qu'ils n'avaient pas actuellement l'intention de déployer une plate-forme informatique cloud native, qui est largement considérée comme difficile à déployer et à maintenir.</p>""}, {'', '<p>Dans l’ensemble, l’enquête montre clairement que l’objectif principal est de garantir que les plateformes continuent d’évoluer au rythme des besoins des développeurs, a déclaré Sandilands. Les équipes de plateformes ne cherchent pas nécessairement à dicter les outils à utiliser ; elles trouvent plutôt des moyens d’automatiser les processus qui augmentent le niveau de friction auquel les développeurs sont confrontés, a-t-il ajouté.</p>'}, {'', '<p>À plus long terme, les progrès de l’intelligence artificielle (IA) permettront d’atteindre plus facilement cet objectif en utilisant des interfaces en langage naturel que les équipes DevOps peuvent invoquer plus facilement, a noté Sandilands.</p>'}, {'', '<p>Chaque organisation devra décider elle-même si l’ingénierie de plateforme est une méthodologie judicieuse pour gérer DevOps à grande échelle, mais à mesure que le développement et le déploiement des applications continuent d’évoluer, des approches plus nuancées seront nécessaires. En général, les organisations tentent de trouver un équilibre entre l’autonomisation des développeurs et la nécessité de rationaliser les processus back-end pour augmenter la productivité tout en simplifiant la conformité et en améliorant la sécurité.</p>'}, {'', '<p>Le défi, bien sûr, est d’inciter les développeurs à adhérer à ce concept plutôt que d’utiliser leur expertise pour résister aux équipes de plateforme qui pourraient, de leur point de vue, limiter leurs prérogatives d’une manière qui, à terme, étouffe l’innovation.</p>'}]"
L'enquête révèle un manque de progrès significatif en matière d'observabilité,"[{'', '<p>Une enquête mondiale menée auprès de 500 professionnels de l’informatique suggère que les organisations ne progressent pas beaucoup dans leur capacité à observer véritablement les environnements applicatifs, d’autant plus qu’ils deviennent chaque jour plus complexes.</p>'}, {'', ""<p>L'enquête, menée par Logz.io, un fournisseur d'une plateforme d'observabilité, a révélé que seulement un répondant sur dix a déclaré disposer d'une observabilité complète de ses environnements d'application.</p>""}, {'', '<p>Asaf Yigal, directeur technique de Logz.io, a déclaré que même si de plus en plus d’équipes DevOps collectent des logs, des mesures et des traces, la plupart d’entre elles n’ont pas encore déterminé comment exploiter toutes les données collectées. Au fur et à mesure que les données sont ingérées, elles ne doivent pas seulement être stockées ; elles doivent également être corrélées aux différents services qui composent une application, a-t-il noté.</p>'}, {'', ""<p>Plus les entreprises déploient des applications cloud natives dans des environnements de production, plus ce problème devient urgent. Tous les microservices qui composent ces applications génèrent désormais une quantité massive de données de télémétrie qui génèrent plus d'alertes que jamais.</p>""}, {'', '<p>Sans surprise, les plus grands défis rencontrés par les organisations lors de la gestion des clusters Kubernetes dans des environnements de production sont la surveillance/le dépannage (40 %), suivis de près par la sécurité (37 %) et la mise en réseau (33 %).</p>'}, {'', '<p>De nombreuses entreprises ne disposent tout simplement pas des compétences nécessaires pour gérer des applications cloud natives. Près de la moitié des répondants (48 %) ont spécifiquement cité le manque de connaissances comme le plus grand défi qu’ils ont rencontré lorsqu’ils ont essayé d’observer ces types d’applications. Du point de vue de la gestion informatique, la plupart des microservices utilisent tous le même modèle de base, il est donc difficile pour les équipes DevOps d’identifier les microservices susceptibles d’avoir le plus grand impact sur les objectifs de niveau de service (SLO) et les accords de niveau de service (SLA) en cas de perturbation, a noté Yigal.</p>'}, {'', '<p>En l’absence de capacité à déterminer la cause réelle d’un problème, les alertes se multiplient et la fatigue s’accroît, ce qui finit par entraîner des niveaux d’épuisement plus élevés au sein de l’équipe DevOps, a noté Yigal. En fait, 82 % des personnes interrogées ont déclaré que leur délai moyen de résolution (MTTR) lors des incidents de production était supérieur à une heure.</p>'}, {'', ""<p>L'enquête a également révélé que plus de la moitié des répondants (52 %) travaillaient pour des organisations qui tentaient simultanément de maîtriser les coûts de surveillance. Plus des trois quarts (76 %) des répondants ont également indiqué que les outils OpenTelemetry (OTEL) ou centrés sur OTEL étaient au moins quelque peu importants pour leur stratégie globale d'observabilité.</p>""}, {'', '<p>En outre, 87 % des personnes interrogées ont déclaré que leur organisation utilise déjà une certaine forme d’ingénierie de plateforme pour gérer les flux de travail DevOps à grande échelle.</p>'}, {'', '<p>On ne sait pas exactement à quelle vitesse les entreprises adoptent les outils et les plateformes d’observabilité, mais trop d’équipes DevOps n’ont pas la visibilité nécessaire pour identifier la cause profonde d’un problème. Par conséquent, des variantes d’un même problème continuent souvent de se manifester parce que les efforts de correction précédents n’ont tout simplement pas été assez approfondis pour résoudre le problème principal.</p>'}, {'', '<p>Bien sûr, il se peut qu’un jour les algorithmes d’apprentissage automatique, ainsi que d’autres formes d’intelligence artificielle (IA), permettent de faire apparaître plus facilement ces problèmes. Le défi, en attendant, est de poser dès aujourd’hui les bases de l’observabilité pour donner accès aux données qui seront nécessaires à l’entraînement de ces modèles d’IA.</p>'}]"
Oracle cherche à accélérer le rythme de l'innovation Java,"[{'', '<p>Avec la dernière version de Java, il devrait devenir plus simple pour les équipes DevOps qui créent et déploient des applications avec le langage de programmation le plus utilisé dans l’entreprise d’innover plus rapidement.</p>'}, {'', ""<p>Georges Saab, vice-président senior d'Oracle Java Platform, a déclaré que Java 22 est unique dans l'histoire de Java dans le sens où il est spécifiquement conçu pour permettre aux développeurs d'invoquer des fonctionnalités disponibles en version bêta et en préversion tout en continuant à travailler avec les autres fonctions prises en charge dans le vénérable langage de programmation. L'objectif global est d'accélérer le rythme de l'innovation dans les environnements basés sur les applications Java, a-t-il ajouté.</p>""}, {'', ""<p>La dernière édition du Java Development Kit (JDK), par exemple, donne accès en avant-première aux fonctionnalités développées dans le cadre d'une initiative du projet Amber qui simplifie l'expression de chaînes qui incluent des valeurs calculées lors de l'exécution tout en améliorant la sécurité des programmes qui composent des chaînes à partir de valeurs fournies par l'utilisateur et les transmettent à d'autres systèmes.</p>""}, {'', ""<p>Parallèlement, une initiative du projet Loom, également disponible en version préliminaire, rationalise la gestion et l'annulation des erreurs d'une manière qui améliore l'observabilité à l'aide d'une interface de programmation d'application (API) qui permet une concurrence structurée, tandis qu'une capacité Scoped Values \u200b\u200boffre un moyen de partager des données immuables au sein et entre les threads.</p>""}, {'', ""<p>Une initiative du projet Panama ajoute une API vectorielle qui promet de compiler des instructions au moment de l'exécution lorsque, par exemple, on utilise des techniques de génération augmentée de récupération (RAG) pour étendre les modèles d'intelligence artificielle (IA).</p>""}, {'', ""<p>En termes de fonctionnalités supplémentaires désormais généralement prises en charge, Java 22 ajoute une API pour permettre aux programmes Java d'interagir en toute sécurité avec le code et les données exécutés en mémoire en dehors de l'environnement d'exécution Java.</p>""}, {'', ""<p>Plus important encore, du point de vue des performances des applications, la dernière itération du langage rationalise les processus de collecte des déchets en mémoire pour réduire le nombre de fois où une application Java pourrait autrement avoir besoin d'être mise en pause.</p>""}, {'', ""<p>Malgré le nombre de langages de programmation alternatifs existants, la majorité des applications d'entreprise continuent d'être développées en Java. Si de nombreux développeurs connaissent au moins plusieurs langages de programmation, la plupart ont tendance à privilégier un seul langage de programmation principal. Les développeurs Java, en particulier, ont montré peu d'envie d'abandonner ce langage, même si d'autres langages de programmation, tels que JavaScript, sont apparus et sont soit plus simples à utiliser, soit, comme dans le cas de Rust, peuvent offrir des capacités de sécurité plus robustes.</p>""}, {'', '<p>Il est moins évident de savoir à quel rythme les développeurs mettent à niveau leurs applications vers la dernière version. Dans certains cas, les développeurs utilisent encore la version 8, 11 ou, plus récemment, 17. Il faudra peut-être un certain temps avant que la majorité des développeurs Java soient prêts à passer à Java 22. En outre, les équipes DevOps devront alors décider quand la masse critique sera suffisante pour justifier la mise à niveau des environnements d’exécution afin de prendre en charge une autre itération de Java.</p>'}, {'', '<p>D’une manière ou d’une autre, que ce soit via une édition de Java fournie par Oracle ou un autre fournisseur, le langage de programmation n’est pas prêt d’être supplanté. Le défi consiste désormais à fournir un support non seulement pour les différentes versions de Java qui peuvent être exécutées, mais aussi, inévitablement, pour tous les autres artefacts logiciels écrits dans différents langages qui circulent désormais également régulièrement dans les pipelines DevOps.</p>'}]"
Du chaos à la clarté : rationaliser DevSecOps à l'ère numérique,"[{'', '<p>De nos jours, sécuriser le cycle de vie du développement logiciel de bout en bout nécessite que les organisations déploient, maintiennent et maîtrisent un mélange cacophonique d’outils qui contribuent souvent davantage à créer des discordes qu’à apporter de l’harmonie aux processus DevSecOps. La raison en est simple : chaque outil utilisé pour sécuriser une chaîne d’approvisionnement logicielle exécute indépendamment des analyses et génère des alertes qui manquent de contexte, sont souvent redondantes ou se contredisent totalement.</p>'}, {'', '<p>Les développeurs et les ingénieurs logiciels qui prennent en charge ces applications doivent bien entendu corréler les flux constants d’alertes et traiter les vulnérabilités afin d’atteindre les seuils de sécurité requis pour propager la version dans un environnement en amont. La réalité est que toutes les vulnérabilités signalées comme élevées et critiques ne nécessitent pas d’être corrigées, et qu’il n’est pas non plus possible pour une équipe de traiter toutes les vulnérabilités. En moyenne, une équipe de développement a la capacité de traiter 10 % de son backlog de vulnérabilités au cours d’un mois donné. Il est donc impératif de prioriser le backlog de vulnérabilités en fonction de l’impact plutôt que de la gravité si nous voulons donner à ces équipes de développement une chance de réussir à améliorer leur posture de sécurité.</p>'}, {'', ""<p>Les alertes constantes provoquent également de la fatigue et introduisent un risque réel de faux négatifs. Celui qui s'est échappé !</p>""}, {'', ""<p>Les flux de travail DevSecOps d'aujourd'hui ne sont guère plus que des sections d'un orchestre jouant sans partition avec l'espoir qu'une méthodologie permettant de verrouiller systématiquement une chaîne d'approvisionnement logicielle se manifestera comme par magie.</p>""}, {'', '<p>Ce qui manque évidemment, c’est un cadre d’orchestration qui déclenche le flux de travail de sécurité en réponse aux changements au sein du SDLC. Idéalement, il fonctionne de manière transparente avec les flux de travail CI/CD, mais sans en dépendre, créant ainsi une symphonie capable de fournir une meilleure posture de sécurité pour l’organisation.</p>'}, {'<h3>Le problème avec DevSecOps</h3>', ''}, {'', '<p>Dans les environnements de développement d’applications cloud natifs basés sur des microservices, les équipes peuvent développer à l’aide de plusieurs technologies, chacune optimisée pour fournir un service spécifique. Chaque technologie peut nécessiter un outil d’analyse spécialisé, et il ne s’agit là que du code. Lorsque nous ajoutons à l’ensemble le binaire, le pipeline d’infrastructure, les données et l’identité, nous nous retrouvons avec une vaste gamme d’outils et un énorme ensemble de politiques par rapport auxquelles chaque changement doit être évalué.</p>'}, {'', '<p>Oui, vous pouvez analyser les conteneurs régulièrement avant de les déployer en production et cela peut être acceptable si vous déployez une version par mois. Cependant, même dans ce cas, vous pourriez affirmer qu’au moment où les vulnérabilités sont détectées, il est trop tard et vous avez déjà perdu la bataille de l’efficacité.</p>'}, {'', '<p>La raison pour laquelle vous avez opté pour une architecture basée sur les microservices est qu’elle est facile à mettre à l’échelle et rapide à créer et à déployer. Idéalement, vous souhaitez publier rapidement des fonctionnalités pour vos clients. Cela implique un degré de changement très élevé, ce qui signifie que l’évaluation ponctuelle que vous avez effectuée ne sera tout simplement pas adaptée à l’objectif en termes de protection de vos actifs. Chaque validation est susceptible d’exposer un nouveau risque, qui peut finir par être découvert trop tard s’il est laissé aux évaluations ponctuelles.</p>'}, {'', ""<p>En revanche, si la sécurité est orchestrée de manière transparente et asynchrone via des interfaces indépendantes des outils dans vos pipelines CI/CD, en fournissant les résultats requis pour sécuriser vos actifs numériques depuis le premier engagement jusqu'aux déploiements de production et au-delà, vous avez véritablement adopté DevOps. Dans ce scénario, chaque changement est évalué en temps réel et son impact est projeté sur le développement, les opérations et la sécurité avec un appel à l'action clair.</p>""}, {'', '<p>Cette approche fournit également un cadre plus extensible et évolutif pour les organisations à mesure qu’elles progressent dans leur parcours d’adoption de DevSecOps.</p>'}, {'', '<h3>Résumé</h3>'}, {'', ""<p>Aucune équipe de développement ne décide délibérément de créer et de déployer une application non sécurisée. La raison pour laquelle les applications présentant des vulnérabilités connues sont déployées si souvent est que la charge cognitive associée à leur découverte et à leur correction est tout simplement trop élevée. Le développeur moyen ne peut consacrer que 10 à 20 % de son temps à la correction des vulnérabilités. Le reste de son temps est consacré soit à l'écriture de nouveau code, soit à la maintenance de l'environnement de développement d'applications utilisé pour écrire ce code. Si les entreprises veulent des applications plus sécurisées, elles doivent trouver des moyens de permettre aux développeurs de corréler, de hiérarchiser et de contextualiser facilement les vulnérabilités au fur et à mesure de leur identification. La plupart du temps, lorsque les développeurs sont informés qu'une vulnérabilité a été découverte dans leur code, ils ont depuis longtemps perdu le contexte.</p>""}, {'', '<p>Les vulnérabilités doivent être immédiatement identifiées au moment de l’écriture du code, de la création des builds et de l’exécution des demandes d’extraction, et identifiées de manière à pouvoir être exploitées. Sinon, cette vulnérabilité risque de se retrouver au sommet de l’énorme pile de dettes techniques que les développeurs espèrent avoir le temps de traiter un jour.</p>'}, {'', '<p>À ce stade, ce n’est qu’une question de temps avant que les gouvernements du monde entier adoptent des lois qui obligeront les organisations à davantage se responsabiliser en matière de sécurité des logiciels qu’elles créent et déploient. Les équipes DevSecOps avisées reconnaissent déjà que les approches existantes de gestion des flux de travail DevSecOps devront être repensées pour répondre à ces exigences. Le défi et l’opportunité à présent sont de mettre en place un cadre d’orchestration de sécurité évolutif qui élimine les frictions dans les flux de travail DevSecOps et améliore l’efficacité grâce à des évaluations en temps réel, garantissant que votre logiciel est sécurisé par défaut.</p>'}]"
Les responsables du projet OpenTelemetry ajoutent des fonctionnalités de profilage de code,"[{'', ""<p>Cette semaine, lors de la conférence KubeCon + CloudNativeCon Europe, les responsables du projet de logiciel d'agent d'observabilité open source OpenTelemetry, développé sous les auspices de la Cloud Native Computing Foundation (CNCF), ont révélé qu'ils avaient ajouté des capacités de profilage pour permettre aux équipes DevOps d'identifier la cause première des problèmes jusqu'à une ligne de code spécifique.</p>""}, {'', ""<p>Austin Parker, directeur de l'open source pour Honeycomb et responsable de la maintenance d'OpenTelemetry, a déclaré que même si plusieurs fournisseurs de plateformes d'observabilité informatique ont ajouté des capacités de profilage de code au cours de l'année écoulée, il est plus logique de fournir cette capacité sous la forme d'une capacité mise en avant par OpenTelemetry d'une manière que n'importe quelle plateforme informatique peut facilement consommer.</p>""}, {'', ""<p>L'objectif global est de réduire le temps nécessaire à la résolution des problèmes informatiques en identifiant non seulement le code en cause, mais également l'équipe de développement qui en est responsable, a noté Parker.</p>""}, {'', ""<p>OpenTelemetry définit un format qui simplifie la gestion centralisée des mesures de collecte, des journaux et des traces à l'aide de logiciels open source. En général, OpenTelemetry continue de gagner du terrain alors que les équipes DevOps cherchent à rationaliser la quantité de logiciels d'agents commerciaux qu'elles doivent déployer, mettre à jour et gérer.</p>""}, {'', '<p>En réalité, OpenTelemetry regroupe des logiciels d’agent qui, en termes d’observabilité, n’apportent aucune valeur différenciée, a déclaré Parker. C’est essentiel car à mesure que les environnements d’application deviennent plus complexes, il est pratiquement impossible de les gérer sans pouvoir appliquer largement le logiciel d’agent requis pour les instrumenter.</p>'}, {'', '<p>À l’avenir, OpenTelemetry devrait ajouter des fonctionnalités supplémentaires telles que le prétraitement, le filtrage ou le routage des données pour rédiger les données sensibles d’une manière qui favorise toujours l’observabilité.</p>'}, {'', '<p>En fin de compte, à mesure que OpenTelemetry se généralise, il devient beaucoup plus facile pour les développeurs de comprendre les performances de leur application, a noté Parker. Au fil du temps, armés de ces informations, les développeurs créeront et livreront des applications de meilleure qualité, a-t-il ajouté.</p>'}, {'', '<p>L’adoption d’OpenTelemetry est encore à ses débuts, mais elle est clairement en passe de devenir une norme de facto pour permettre l’observabilité. Le nombre de développeurs qui ont accès non seulement aux métriques et aux journaux, mais aussi aux traces devrait augmenter régulièrement à mesure que le coût de l’instrumentation du code diminue.</p>'}, {'', '<p>Bien entendu, il sera également crucial de trouver un moyen de collecter ces données à moindre coût pour appliquer l’intelligence artificielle (IA) aux workflows DevOps. Après tout, avant de pouvoir entraîner un modèle d’IA, il doit avoir accès à des données vérifiées.</p>'}, {'', '<p>En attendant, les équipes DevOps pourraient vouloir réfléchir à la vitesse à laquelle elles peuvent remplacer les agents commerciaux existants déployés dans l’entreprise. Une grande partie de ces logiciels d’agents est plus mature qu’OpenTelemetry, mais au fil du temps, les arguments économiques en faveur de l’utilisation de logiciels d’agents open source deviennent convaincants. De nombreux fournisseurs de plateformes d’observabilité ont déjà rendu la prise en charge d’OpenTelemerty disponible par défaut.</p>'}, {'', '<p>Quelle que soit l’approche adoptée, la nécessité d’une observabilité a toujours été évidente. Le problème est qu’il existe un monde de différence entre la surveillance d’un ensemble prédéfini de mesures et la capacité à lancer des requêtes pour identifier la cause profonde d’un problème que les mesures seules ne permettront pas de faire apparaître.</p>'}]"
Il est temps d'étendre la visibilité DevOps à la périphérie du réseau,"[{'', '<p>Les problèmes de performances déroutent régulièrement les équipes DevOps après le déploiement d’une application, malgré le nombre de tests effectués avant le déploiement. Après une étude plus approfondie, le problème le plus souvent négligé est la nature distribuée de l’application elle-même. Les utilisateurs finaux qui accèdent aux applications à partir de plusieurs emplacements ne bénéficieront jamais du même niveau de service Internet. Ainsi, les logiciels qui fonctionnent parfaitement dans des endroits qui sont des pôles technologiques urbains majeurs, tels que New York, San Francisco ou Londres, n’offriront pas nécessairement la même expérience dans des régions plus éloignées, en particulier dans les zones rurales situées à l’autre bout du monde.</p>'}, {'', ""<p>Tous les services Internet ne sont pas égaux. Les problèmes de latence intermittente du réseau peuvent avoir un impact profond sur les performances des applications, simplement en raison des lois de la physique. Plus un utilisateur final est éloigné du centre de données où l'application est hébergée, plus il est probable que la latence du réseau affecte négativement les performances de l'application.</p>""}, {'', '<p>Malheureusement, la plupart des équipes DevOps n’ont aucune visibilité au-delà de l’environnement du centre de données où l’application est exécutée. Naturellement, la première étape pour obtenir ce niveau de visibilité commence à la périphérie du réseau.</p>'}, {'', '<h3>Visibilité de la pile Internet</h3>'}, {'', '<p>Une pile Internet est composée d’une large gamme de services, chacun pouvant avoir un impact négatif sur les performances des applications distribuées. Des réseaux privés virtuels (VPN) aux réseaux de diffusion de contenu (CDN) en passant par les serveurs et commutateurs DNS (Domain Name System), tous ces éléments ajoutent une latence qui, selon la manière dont le trafic réseau est acheminé, peut avoir un impact majeur sur les performances des applications. Chacun de ces services peut subir une panne ou, plus difficile à détecter, des dégradations intermittentes qui ont un impact imprévisible sur les performances des applications. Qu’il s’agisse d’une application SaaS (Software-as-a-Service) accessible dans le cloud ou d’une plateforme IoT (Internet des objets) exécutée à la périphérie du réseau, pour identifier le service Internet qui pourrait être à l’origine d’un problème, il faut pouvoir surveiller et observer de près ce trafic.</p>'}, {'', '<p>Sans visibilité sur la pile Internet, il devient impossible de faire émerger les informations nécessaires pour fournir le contexte nécessaire à l’optimisation et au dépannage d’une application spécifique. Tout le temps et les efforts consacrés à la création d’une application peuvent être vains simplement parce que personne n’a compris l’impact que la latence du réseau aurait sur l’application avant son déploiement. Même après le déploiement d’une application, tout changement dans la façon dont le trafic réseau est acheminé peut avoir des conséquences majeures. Sans aucun moyen de comprendre les changements qui ont pu se produire, il peut s’écouler des semaines ou des mois avant qu’une organisation se rende compte qu’elle doit enregistrer une demande de service ou changer de fournisseur de services.</p>'}, {'', '<p>Toute approche véritablement globale de l’observabilité doit clairement inclure une analyse des services Internet dont les organisations dépendent de manière cruciale pour garantir une expérience applicative cohérente. Selon la structure de l’équipe informatique, ces informations peuvent être intégrées dans une plateforme d’observabilité adoptée par l’organisation ou corrélées à l’aide d’une plateforme de surveillance des performances Internet (IPM).</p>'}, {'', '<h3>DevOps a besoin de NetOps</h3>'}, {'', '<p>Quelle que soit l’approche adoptée, il n’a jamais été aussi crucial pour les équipes d’exploitation réseau (NetOps) et DevOps de pouvoir aligner leurs approches de surveillance des environnements informatiques distribués. De plus en plus d’applications sensibles à la latence sont déployées. Dans un monde idéal, les équipes DevOps travaillant en étroite collaboration avec NetOps devraient être en mesure de provisionner et de modifier par programmation les services réseau selon les besoins. Tous les services Internet ne peuvent pas être ajustés via une interface de programmation d’application (API), mais la plupart des équipes NetOps ont accès à des consoles qui leur permettent de passer à différents niveaux de service si nécessaire. Au minimum, un fournisseur de services travaillera avec elles pour effectuer les modifications nécessaires.</p>'}, {'', '<p>Bien entendu, il devient plus difficile de négocier si l’équipe NetOps ne sait pas quel élément de la pile Internet doit être mis à niveau. Un fournisseur de services qui n’a généralement aucune visibilité sur l’application peut passer des semaines à essayer de déterminer quel élément de la pile Internet qu’il gère est en cause. Les équipes informatiques qui disposent de leurs propres moyens de surveillance d’un ensemble de services Internet seront en mesure de collaborer avec leurs fournisseurs de services pour résoudre ces problèmes beaucoup plus rapidement.</p>'}, {'', '<h3>Résumé</h3>'}, {'', ""<p>Les équipes informatiques les plus avisées sont parfaitement conscientes de la vulnérabilité des services Internet aux pannes et aux baisses de tension. Elles ont donc tendance à s'assurer que le trafic réseau peut être redirigé d'un service à un autre. Cette approche présente également l'avantage supplémentaire de conserver une certaine marge de manœuvre en matière de prix lorsqu'elles négocient les tarifs des services fournis.</p>""}, {'', '<p>Il convient également de garder à l’esprit que les fournisseurs de services réseau qui savent qu’un client est lié à un fournisseur ne sont peut-être pas aussi motivés à s’assurer que les services réseau fonctionnent à une efficacité optimale. La seule façon pour une équipe informatique de savoir quel niveau de service est fourni à un moment donné est, bien sûr, de générer un rapport IPM qui détaille précisément quels services étaient réellement disponibles à quel niveau de performance. Sinon, ils dépendent entièrement du rapport qu’un fournisseur de services présente avec sa facture mensuelle.</p>'}, {'', '<p>Les rapports générés par une plateforme IPM font également ressortir des informations qui, une fois présentées aux développeurs d’applications, créent une opportunité potentielle d’optimisation de leur code. Sans ce niveau d’analyse, chaque développeur supposera que les services réseau sont soit défaillants, soit mal gérés. Le défi et l’opportunité sont désormais de fournir des informations sur les performances Internet d’une manière que chaque partie prenante ne peut pas simplement utiliser mais, tout aussi crucialement, à laquelle elle doit faire implicitement confiance.</p>'}]"
L'informatique de pointe nécessite DevOps à grande échelle,"[{'', '<p>L’essor de l’informatique de pointe est sur le point de favoriser une convergence tant attendue des meilleures pratiques en matière de DevOps, d’ingénierie des données, de sécurité, de réseau, de technologie opérationnelle (OT) et d’opérations d’apprentissage automatique (MLOps), ce qui rendra à terme les équipes informatiques plus réactives que jamais aux besoins de l’entreprise.</p>'}, {'', ""<p>Historiquement, chacune de ces disciplines informatiques a eu tendance à travailler de manière isolée, avec des transferts délibérés entre les différentes équipes chargées de fournir et de maintenir divers services. Le défi est que, à mesure que de plus en plus d'applications sont déployées à la périphérie du réseau, les équipes informatiques doivent travailler main dans la main. Les applications doivent non seulement être déployées dans un environnement informatique beaucoup plus distribué, mais elles doivent également être régulièrement mises à jour et corrigées.</p>""}, {'', '<p>Parallèlement, des volumes croissants de données sont traités et analysés au point de collecte et de consommation. De plus en plus d’applications dynamiques exécutées à la périphérie du réseau transmettent des données d’analyse agrégées aux applications exécutées dans le cloud ou dans un environnement informatique sur site. Cette évolution nécessite une approche fondamentalement différente de la gestion du stockage dans les différents environnements informatiques de périphérie et les différents systèmes back-end fédérés de l’entreprise.</p>'}, {'', '<p>Les entreprises doivent généralement stocker leurs fichiers localement sur des plateformes de calcul en périphérie de réseau, tout en stockant des copies de ces fichiers dans le cloud à l’aide d’un service de stockage compatible S3 qui permet d’accéder à un stockage basé sur des objets peu coûteux pour permettre tout, de la protection des données à la gestion d’applications d’intelligence artificielle (IA). Cette capacité multiprotocole crée une opportunité non seulement de créer de nouvelles applications distribuées avec état, mais également de moderniser les applications existantes, quels que soient les protocoles utilisés pour stocker les données.</p>'}, {'', '<p>La plupart des plateformes informatiques de pointe sont aujourd’hui gérées par des équipes OT qui dépendent directement des dirigeants d’unités commerciales spécifiques. Cependant, à mesure que ces plateformes sont de plus en plus connectées aux réseaux, il devient impératif de les gérer de manière centralisée dans le cadre de tout effort visant à réduire le coût total de l’informatique. Après tout, l’élément le plus coûteux de l’informatique reste le coût de la main-d’œuvre nécessaire à sa gestion.</p>'}, {'', ""<p>Naturellement, le partage de données en temps quasi réel dans un environnement informatique distribué exerce une pression accrue sur les équipes d'exploitation réseau (NetOps). Au lieu de mettre à jour les applications par lots une fois par jour, les données diffusées en continu depuis des milliers de plates-formes informatiques de pointe doivent être synchronisées avec plusieurs applications pour faire apparaître les informations les plus récentes et les plus précises.</p>""}, {'', '<p>Le plus difficile est que la plupart de ces données sont utilisées pour former des modèles d’IA qui nécessitent en fin de compte le déploiement du moteur d’inférence associé créé à la périphérie. Pour traiter des téraoctets de données, la plateforme sous-jacente requise pour prendre en charge ces moteurs d’inférence devra également être mesurée en téraoctets. Au fil du temps, les modèles d’IA ont tendance à dériver ou, dans le cas de l’IA générative, à devenir complètement illusoires. Les data scientists devront travailler en étroite collaboration avec les équipes DevOps pour remplacer ces modèles d’IA car, contrairement à une application traditionnelle, ils ne peuvent pas être mis à jour via un correctif. L’ensemble du modèle d’IA doit être remplacé par un modèle qui est non seulement plus fiable, mais aussi, et tout aussi important, plus sûr.</p>'}, {'', '<p>Enfin, la nécessité de sécuriser les plateformes de calcul en périphérie est tout simplement cruciale. Les cybercriminels ont tendance à considérer ces plateformes comme des passerelles vers le reste de l’entreprise. Chaque nouvelle plateforme de calcul en périphérie déployée étend la taille de la surface d’attaque à défendre. À moins que les équipes de cybersécurité ne soient profondément impliquées dans la création et le déploiement d’applications et de plateformes de calcul en périphérie, la question n’est pas de savoir si une faille se produira, mais plutôt quand elle se produira.</p>'}, {'', '<p>Il faudra peut-être un certain temps avant que ces différents fiefs informatiques ne convergent, mais à ce stade, cela est presque inévitable. En fait, il sera bientôt difficile de faire la distinction entre l’informatique de pointe et le reste de l’environnement informatique, car les applications pilotées par événements sur plusieurs plates-formes deviendront davantage la norme que l’exception.</p>'}, {'', '<p>Les équipes DevOps, dans le cadre de leur engagement incessant en faveur de l’automatisation, devraient naturellement être à l’avant-garde de ces efforts. Plutôt que de limiter les principes DevOps à la manière dont les applications sont créées et déployées, le moment est clairement venu d’automatiser les flux de travail informatiques de bout en bout. La seule façon de gérer l’informatique de pointe à grande échelle par programmation est d’appliquer également les principes DevOps à grande échelle. Également connue sous le nom d’ingénierie de plateforme, cette approche permet aux organisations de gérer des environnements informatiques distribués à grande échelle sans avoir à embaucher une petite armée d’équipes informatiques pour tout gérer. Il y aura toujours un besoin d’expertise spécialisée, mais à mesure que l’informatique continue d’évoluer, les silos qui ralentissent le rythme de l’innovation devront disparaître.</p>'}, {'', '<p>Tous les membres d’une organisation informatique n’ont pas besoin de savoir gérer de manière experte chaque tâche, mais ils doivent être familiarisés avec les dépendances inhérentes qui existent entre les applications, le réseau, le stockage et la cybersécurité pour faire progresser les objectifs des entreprises qui n’ont jamais été aussi dépendantes de l’informatique.</p>'}, {'', '<p>Chaque organisation doit déterminer le rythme de convergence qui correspond le mieux à ses besoins, mais plus elle se produit rapidement, plus le retour sur investissement dans l’edge computing sera important. Le défi et l’opportunité, comme toujours, seront de gérer une nouvelle transition informatique majeure, où les problèmes à résoudre sont autant culturels que techniques.</p>'}]"
Comment l'IA générative permet des plateformes de tests continus unifiées,"[{'', '<p>Il est temps de disposer d’une plateforme de tests continus unifiée.</p>'}, {'', ""<p>Les plateformes et outils de test de logiciels existants sont spécialisés dans les activités de test pour un sous-ensemble des différentes étapes des flux de valeur. Le paysage des outils de test de logiciels comprend une large gamme de solutions, chacune avec ses points forts, axées sur des aspects spécifiques du développement, de la livraison et de l'exploitation des logiciels.</p>""}, {'', ""<p>Par exemple\xa0:• Les outils de test unitaire se concentrent sur les premières étapes du développement, permettant aux développeurs de tester des unités de code individuelles pour en vérifier l'exactitude.• Les outils de test d'intégration visent à tester les interactions entre différents modules ou services au sein d'une application.• Les outils de test système sont conçus pour tester de bout en bout l'ensemble du système avant sa mise en service.• Les outils de test de performance évaluent le comportement de l'application dans des conditions de charge et de stress.• Les outils de test de sécurité se concentrent sur l'identification des vulnérabilités au sein de l'application.• Les outils de test d'acceptation utilisateur (UAT) facilitent la phase de test finale, où les utilisateurs finaux valident la solution par rapport à leurs exigences.</p>""}, {'', ""<p>Le défi consistant à réunir ces différents besoins de test au sein d'une seule plateforme de tests continus est multiple. Une telle plateforme doit s'intégrer de manière transparente à une grande variété d'outils et d'environnements de développement, prendre en charge différentes méthodologies de test et être suffisamment flexible pour s'adapter à différents processus organisationnels et normes de qualité.</p>""}, {'', '<p>Bien qu’il existe des outils d’intégration continue/déploiement continu (CI/CD) qui intègrent plusieurs étapes de test, ils le font souvent en s’intégrant à des outils de test spécialisés plutôt qu’en proposant eux-mêmes une solution de test unifiée. Ces outils CI/CD sont plus proches de l’orchestration des différentes activités de test plutôt que de leur unification sous les capacités d’une seule plateforme.</p>'}, {'', '<p>Cependant, le concept d’une plateforme de tests continus entièrement unifiée, couvrant toutes les étapes, depuis les exigences jusqu’au déploiement et aux tests en production, représente une opportunité significative d’innovation dans le domaine des outils de développement logiciel. Pour y parvenir, il faudrait probablement tirer parti des avancées dans des domaines tels que l’IA générative, comme indiqué dans mon précédent blog Application de l’IA/ML aux tests continus pour créer des processus de test adaptables et intelligents capables de couvrir l’ensemble des besoins de test de manière cohérente.</p>'}, {'', ""<h3>Avantages d'une plateforme de tests continus unifiée</h3>""}, {'', ""<p>Efficacité et rapidité améliorées : une plateforme de test unifiée intègre les tests à toutes les étapes du cycle de vie du développement logiciel (SDLC), ce qui simplifie considérablement le processus de test. Cette intégration facilite les boucles de rétroaction automatisées et continues qui identifient et corrigent rapidement les défauts, permettant aux équipes de développement d'itérer et d'améliorer rapidement. En conséquence, les équipes peuvent publier de nouvelles fonctionnalités et correctifs plus rapidement, ce qui accélère la mise sur le marché du produit et améliore la réactivité aux besoins des clients et aux évolutions du marché.</p>""}, {'', ""<p>Qualité et fiabilité améliorées : en garantissant des tests complets et cohérents sur l'ensemble de l'application, une plateforme de test unifiée joue un rôle crucial dans l'identification et l'atténuation des problèmes au début du cycle de développement. Cette détection précoce permet de maintenir un niveau élevé de qualité et de fiabilité du logiciel, ce qui accroît la satisfaction et la confiance des utilisateurs dans le produit. L'application cohérente des normes de qualité à toutes les phases de test contribue à la robustesse et à la fiabilité du produit logiciel.</p>""}, {'', ""<p>Économies de coûts et maintenance réduite : l'automatisation du processus de test via une plateforme CT unifiée optimise non seulement l'utilisation des ressources, mais réduit également considérablement les coûts associés aux tests manuels et à la correction des défauts à un stade avancé. La détection précoce des défauts se traduit par des coûts de réparation inférieurs et le processus rationalisé réduit le délai global de mise sur le marché. De plus, les logiciels fiables et de haute qualité nécessitent moins de maintenance, ce qui réduit encore les coûts à long terme et libère des ressources pour les efforts d'innovation et de développement.</p>""}, {'', ""<p>Collaboration améliorée entre les équipes : une plateforme CT unifiée favorise une culture de collaboration et de transparence entre les équipes de développement, de test et d'exploitation. En fournissant un cadre et des outils communs pour toutes les activités de test, elle élimine les silos et permet une communication et une collaboration transparentes tout au long du cycle de développement logiciel. Cette collaboration améliorée garantit que les équipes sont alignées sur les objectifs du projet, peuvent partager leurs idées plus efficacement et travailler ensemble pour identifier et résoudre les problèmes plus efficacement, ce qui conduit à de meilleurs résultats.</p>""}, {'', '<p>Ensemble, ces avantages soulignent comment une plateforme de tests continus unifiée peut transformer le processus de développement logiciel, le rendant plus efficace, rentable et collaboratif tout en garantissant la livraison de produits logiciels fiables et de haute qualité.</p>'}, {'', '<p>Bien que de nombreux outils traitent de parties spécifiques du cycle de vie des tests, la vision d’une plateforme unique qui unifie de manière transparente toutes les étapes des tests, de la définition des exigences à la production, reste un objectif ambitieux. Il s’agit d’une lacune dans le paysage technologique actuel qui présente à la fois un défi et une opportunité de développement futur.</p>'}, {'<h3>Défis pour les plateformes de tests continus unifiées</h3>', ''}, {'', ""<p>Créer une plateforme de tests continus qui unifie les activités de test pour toutes les étapes du flux de valeur de bout en bout est un défi complexe en raison de plusieurs facteurs :1. Diversité des technologies et des outils : les environnements de développement de logiciels modernes sont très divers, intégrant divers langages de programmation, cadres et technologies. Créer une plateforme qui s'intègre parfaitement à toutes ces technologies est un défi.2. Points d'intégration complexes : les tests continus doivent s'intégrer à plusieurs étapes du pipeline de développement, notamment le développement, le déploiement et les opérations. Chacune de ces étapes peut utiliser des outils et des processus différents, ce qui rend difficile la création d'une solution unique.3. Variables des mesures de qualité : différentes équipes et différents projets peuvent avoir différentes définitions de la qualité, des critères de réussite et des mesures de performance. Une plateforme de tests unifiée doit être hautement personnalisable pour répondre à ces différents besoins.4. Gestion du changement : l'adoption d'une nouvelle plateforme nécessite des changements dans les processus et les flux de travail de l'organisation. La résistance au changement est courante dans les organisations, et la transition vers une nouvelle façon de tester peut être accueillie avec scepticisme et inertie.5. Évolutivité et performances : garantir que la plateforme puisse évoluer pour répondre aux besoins de test des grandes organisations avec des milliers de tests exécutés simultanément est un défi technique. Les problèmes de performances peuvent devenir un goulot d'étranglement, affectant l'efficacité globale du processus de développement. 6. Sécurité et conformité : l'intégration des tests à toutes les étapes du développement introduit également des défis de sécurité et de conformité. La plateforme doit garantir que les données sensibles sont protégées et que les pratiques de test sont conformes aux exigences réglementaires. 7. Contraintes de coûts et de ressources : le développement, la maintenance et la prise en charge d'une plateforme de tests continus unifiée nécessitent un investissement important. Les organisations peuvent hésiter à engager les ressources nécessaires sans preuve claire du retour sur investissement. 8. Évolution des pratiques : les pratiques et les outils de développement de logiciels évoluent constamment. Maintenir la plateforme à jour avec les dernières pratiques et technologies nécessite des efforts et une innovation continus.</p>""}, {'', ""<p>Malgré ces défis, la valeur des tests continus tout au long du cycle de développement logiciel est de plus en plus reconnue. Certaines entreprises et communautés open source progressent dans cette direction, en créant des solutions de test plus intégrées et plus flexibles. Cependant, la mise en place d'une plateforme entièrement unifiée qui réponde à tous ces défis est un effort continu et représente une opportunité significative d'innovation dans le secteur du développement et des tests de logiciels.</p>""}, {'', ""<h3>L'IA générative peut faciliter une plateforme de tests continus unifiée</h3>""}, {'', ""<p>L’IA générative peut jouer un rôle important pour surmonter les défis associés à la création d’une plateforme de tests continus qui unifie les activités de test pour toutes les étapes du flux de valeur de bout en bout. Voici comment l’IA générative peut relever chacun des défis :1. Diversité des technologies et des outils : l’IA générative peut être formée sur un large éventail de langages de programmation, de cadres et de technologies pour comprendre et générer du code ou des scripts de test. Cette capacité lui permet de s’adapter à différents environnements et de créer des supports de test compatibles avec divers outils et technologies.2. Points d’intégration complexes : l’IA peut analyser le flux de travail des pipelines de développement et suggérer des points d’intégration optimaux pour les tests. En apprenant à partir de différentes configurations CI/CD (intégration continue/déploiement continu), l’IA peut recommander les meilleures pratiques pour intégrer les tests de manière transparente dans les flux de travail existants.3. Différentes mesures de qualité : les modèles d’IA générative peuvent être personnalisés pour comprendre et appliquer différentes mesures de qualité et différents critères de réussite en fonction des exigences spécifiques du projet. En s’entraînant sur divers ensembles de données, ces modèles peuvent s’adapter à diverses définitions de la qualité et générer des tests ou des analyses pertinents.4. Français : Gestion du changement : l'IA peut aider au processus de gestion du changement en simulant les résultats de l'adoption de nouvelles plateformes de test, offrant ainsi des avantages fondés sur des preuves et atténuant la résistance au changement. De plus, les analyses pilotées par l'IA peuvent mettre en évidence les gains d'efficacité et les améliorations de qualité pour soutenir la transition. 5. Évolutivité et performances : l'IA générative peut optimiser les processus de test en identifiant les redondances et en suggérant des améliorations, améliorant ainsi les performances. En outre, l'IA peut allouer dynamiquement des ressources en fonction des besoins de test, garantissant l'évolutivité sans compromettre l'efficacité. 6. Sécurité et conformité : les modèles d'IA peuvent être formés pour identifier et signaler les problèmes potentiels de sécurité et de conformité dans le processus de test. En apprenant en permanence des dernières normes de sécurité et réglementations de conformité, l'IA peut contribuer à garantir que les pratiques de test répondent aux exigences nécessaires. 7. Contraintes de coûts et de ressources : en automatisant la génération et l'optimisation des cas de test, l'IA générative peut réduire considérablement l'effort manuel requis, réduisant ainsi les coûts et les demandes de ressources. L'IA peut également aider à hiérarchiser les efforts de test en fonction de l'évaluation des risques, garantissant que les ressources sont concentrées là où elles sont le plus nécessaires. Évolution des pratiques : les modèles d'IA générative sont intrinsèquement adaptables et peuvent apprendre en permanence des nouvelles pratiques de développement, des nouveaux outils et des nouvelles technologies. Cela garantit que la plateforme de test reste à jour avec les dernières avancées en matière de développement logiciel.</p>""}, {'', '<p>L’IA générative a le potentiel de transformer les tests continus en fournissant des solutions adaptatives, efficaces et intelligentes aux défis complexes de l’unification des activités de test sur l’ensemble du flux de valeur de bout en bout. Cependant, la réalisation de ce potentiel nécessite une conception minutieuse, une formation approfondie des modèles d’IA et une gestion continue pour garantir que les systèmes d’IA restent efficaces et adaptés à l’évolution des besoins en matière de tests.</p>'}, {'', ""<h3>Résumé : Appel à l'action</h3>""}, {'', '<p>La nécessité d’une plateforme de tests continus (CT) unifiée n’a jamais été aussi urgente. Les plateformes de tests actuelles, chacune experte dans son domaine, ne couvrent que des fragments du cycle de vie du développement logiciel (SDLC), ce qui conduit à un processus de test décousu et inefficace. Cette fragmentation ralentit non seulement le développement et les livraisons, mais compromet également la qualité et la sécurité du produit final. Le rêve d’une plateforme unique qui intègre de manière transparente toutes les étapes des tests, des exigences au déploiement et aux tests en production, représente un bond monumental vers l’efficacité, la sécurité et la qualité du développement logiciel.</p>'}, {'', '<p>Les défis liés à la création d’une telle plateforme sont multiples, allant de la diversité des technologies et des outils à la nature évolutive des pratiques de développement logiciel. Chaque défi, de l’intégration de technologies diverses et de la gestion du changement au sein des organisations à la garantie de l’évolutivité et de la conformité, ajoute de la complexité au développement d’une plateforme de test unifiée. Pourtant, les avantages potentiels de surmonter ces obstacles sont immenses, promettant une amélioration significative de la vitesse et de la qualité de la livraison des logiciels. La reconnaissance de ces avantages par l’industrie est de plus en plus grande, comme en témoignent les efforts de certaines entreprises et communautés open source qui évoluent vers des solutions de test plus intégrées et plus flexibles.</p>'}, {'', '<p>L’IA générative apparaît comme une lueur d’espoir dans cette quête, offrant des solutions innovantes aux défis multiformes de l’unification des activités de test. En exploitant la puissance de l’IA générative, l’industrie peut faire face à la diversité des outils, intégrer des étapes de test complexes, s’adapter à des mesures de qualité variables, gérer les changements organisationnels, évoluer efficacement, garantir la sécurité et la conformité et évoluer avec les pratiques de développement logiciel. La voie à suivre nécessite un effort concerté, mais les investissements dans les innovations en matière de tests pilotés par l’IA peuvent concrétiser la vision d’une plateforme de tests complète, unifiée et continue. Il ne s’agit pas seulement d’une opportunité d’amélioration, mais d’un appel à l’action pour que l’industrie redéfinisse l’avenir des plateformes d’ingénierie.</p>'}]"
