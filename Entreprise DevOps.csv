Title,Content
CloudBees acquiert Launchable pour faire progresser les tests à l'aide de l'IA,"[{'', ""<p>CloudBees a annoncé aujourd'hui l'acquisition de Launchable, un fournisseur d'une plateforme d'automatisation des tests, pour permettre aux équipes DevOps d'améliorer à la fois la sécurité des applications et la qualité des logiciels. Les conditions financières de l'acquisition ne sont pas divulguées.</p>""}, {'', ""<p>Launchable utilise largement les algorithmes d'apprentissage automatique et les capacités d'intelligence artificielle générative (IA) pour rationaliser les processus de test, par exemple en fournissant l'accès à un copilote pour garantir que le bon test est exécuté au bon moment. De plus, si la plateforme détermine qu'un composant logiciel va probablement échouer à une partie d'un test, elle alertera les équipes DevOps de ce fait plutôt que de perdre du temps à exécuter une série de tests avant qu'un échec inévitable ne se produise.</p>""}, {'', ""<p>Sacha Labourey, directeur de la stratégie de CloudBees, a déclaré que l'ajout de la plateforme Launchable offre aux organisations des capacités de test augmentées par l'intelligence artificielle (IA) qui peuvent être facilement ajoutées aux flux de travail DevSecOps existants.</p>""}, {''}, {'', '<p>En général, les tests d’applications restent l’un des goulots d’étranglement coûteux de tout cycle de vie de développement logiciel (SDLC). Les outils d’IA devraient réduire considérablement le temps consacré aux tests inutiles et aux flux de travail inefficaces, car les équipes DevOps sauront quels tests sont les plus importants.</p>'}, {'', '<p>En outre, ils pourront lancer automatiquement des tests dès qu’une modification est apportée au code dans le cadre d’un flux de travail DevOps plus vaste, afin de garantir que les tests ne soient pas ignorés simplement parce qu’ils risquent de prendre trop de temps à exécuter. Lorsqu’un test échoue, ces mêmes outils d’IA permettent de faire apparaître plus facilement la cause profonde du problème de manière à ce qu’il soit plus simple à résoudre pour les développeurs.</p>'}, {'', ""<p>Launchable rapporte que les clients existants tels que BMW et GoCardless ont constaté une réduction de 50 % des heures machine, une réduction de 90 % des temps d'exécution des tests et une réduction de 40 % des temps de construction en s'appuyant sur l'IA pour automatiser les flux de travail de test.</p>""}, {'', '<p>L’IA accélère déjà le rythme auquel le code est écrit. Le défi consiste désormais à appliquer l’IA pour accélérer le rythme auquel des volumes croissants de code peuvent circuler dans les pipelines DevSecOps. L’objectif global est d’améliorer la productivité des développeurs en réduisant le travail qui, au fil du temps, augmente l’épuisement professionnel des développeurs et des ingénieurs logiciels qui les soutiennent, a déclaré Labourey.</p>'}, {'', '<p>Il faudra peut-être un certain temps avant que l’IA ne soit utilisée de manière généralisée dans tous les flux de travail DevOps, mais les organisations qui sont à l’avant-garde de l’adoption constatent des gains de productivité. Il est peu probable que l’IA remplace le besoin d’équipes DevOps. Après tout, le code et les suggestions générés par ces outils doivent toujours être examinés par des professionnels DevOps pour éviter que des hallucinations occasionnelles ne soient incorporées dans un environnement de production.</p>'}, {'', '<p>Cependant, une chose est sûre : grâce à l’essor de l’IA, le rythme auquel les applications sont développées et déployées est sur le point d’augmenter de manière exponentielle sans nécessairement avoir à augmenter considérablement la taille des équipes DevOps. Le problème est que si toutes ces applications ne sont pas correctement testées, ce rythme de déploiement accéléré pourrait tout aussi bien finir par être une bonne chose.</p>'}]"
La DARPA se tourne vers l'IA pour transformer le code C et C++ en Rust,"[{'', ""<p>La DARPA, l'agence de recherche et développement du ministère de la Défense (DOD), s'appuiera sur les capacités émergentes de l'IA dans un nouveau programme pour relever le défi coûteux et chronophage de la réécriture du code C et C++ en Rust, dans le cadre d'une démarche conçue pour répondre à la pression exercée sur les agences fédérales et les organisations privées pour adopter des langages de programmation sûrs en mémoire.</p>""}, {'', ""<p>La DARPA (Defense Advanced Research Projects Agency) a annoncé ce mois-ci son programme Translating All C to Rust (TRACTOR), qui utilisera de grands modèles de langage (LLM) et d'autres techniques d'apprentissage automatique pour automatiser la majeure partie des tâches nécessaires pour déplacer plus de deux décennies de code hérité vers le langage plus sûr Rust.</p>""}, {'', ""<p>Le Bureau du directeur national de la cybersécurité de la Maison Blanche (ONCD) et la CISA exhortent depuis des mois les développeurs à adopter Rust ou d'autres langages modernes, tels que Python ou C#, pour mieux protéger leurs logiciels en éliminant toute une classe de vulnérabilités de sécurité de la mémoire qui représentent les vulnérabilités les plus courantes dans des langages comme C et C++.</p>""}, {'', ""<p>Ces défauts peuvent inclure des dépassements de tampon, l'utilisation de mémoire non initiée et l'utilisation après libération, qui augmente lorsqu'un programme continue d'utiliser un emplacement mémoire après qu'il a été libéré ou désalloué.</p>""}, {'', '<p>La directrice de la CISA, Jen Easterly, a déclaré à la fin de l’année dernière que jusqu’à deux tiers de toutes les vulnérabilités logicielles sont le résultat d’un manque de codage sécurisé en mémoire.</p>'}, {'', ""<h3>C'est beaucoup de code</h3>""}, {'', ""<p>Cependant, les responsables de la DARPA ont déclaré que même s'il existe un large consensus au sein de la communauté des ingénieurs logiciels sur la nécessité de langages sûrs en termes de mémoire, le problème est que C et C++ sont utilisés dans les organisations des secteurs public et privé depuis des décennies, ce qui rend la tâche de déplacer les quantités massives de ce code vers Rust considérablement difficile.</p>""}, {'', '<p>Ils ont souligné que le langage C a été créé dans les années 1970 et a été utilisé pour créer des applications pour tout, des smartphones aux véhicules spatiaux. Le DOD lui-même « dispose de systèmes de longue durée qui dépendent de manière disproportionnée de langages de programmation comme C », ont-ils écrit dans un communiqué, ajoutant qu’il fallait trouver un moyen de réécrire « le code hérité à une échelle qui corresponde à l’ampleur du problème ».</p>'}, {'', '<p>C’est là qu’intervient l’IA, selon Dan Wallach, responsable du programme TRACTOR de la DARPA.</p>'}, {'', '<p>« Vous pouvez désormais vous rendre sur n’importe quel site Web de LLM, commencer à discuter avec l’un des chatbots IA, et tout ce que vous avez à dire est « voici du code C, veuillez le traduire en code Rust idiomatique sûr », copier, coller et quelque chose sort, et c’est souvent très bon, mais pas toujours », a déclaré Wallach dans un communiqué. « Le défi de la recherche est d’améliorer considérablement la traduction automatique du C vers Rust, en particulier pour les constructions de programme les plus pertinentes. »</p>'}, {'', '<h3>Solliciter l’aide des développeurs</h3>'}, {'', ""<p>L'objectif de TRACTOR est de créer un programme qui produira la même qualité et le même style que ceux développés par un programmeur Rust qualifié. La DARPA se tourne vers la communauté des développeurs de logiciels pour l'aider à trouver ce programme en organisant des concours publics où les propositions basées sur le LLM seront testées.</p>""}, {'', ""<p>Selon Wallach, les propositions incluront probablement de nouvelles combinaisons incluant l'analyse logicielle, y compris l'analyse statique et dynamique, et les LLM.</p>""}, {'', ""<p>Le programme débute le 26 août avec une journée des proposants, à laquelle les ingénieurs logiciels intéressés peuvent assister en personne ou virtuellement. Les participants doivent s'inscrire avant le 19 août. Les détails et les informations d'inscription sont disponibles sur SAM.Gov.</p>""}, {'<h3>Un problème mondial</h3>', ''}, {'', '<p>Dans un rapport technique de 19 pages, l’ONCD a écrit que les vulnérabilités de sécurité de la mémoire « représentent un problème majeur pour l’industrie du logiciel car elles obligent les fabricants à publier continuellement des mises à jour de sécurité et leurs clients à appliquer continuellement des correctifs. Ces vulnérabilités persistent malgré les efforts déployés par les fabricants de logiciels pour tenter de réduire leur prévalence et leur impact par diverses méthodes, notamment l’analyse, la correction, la publication de nouveaux codes et l’investissement dans des programmes de formation pour les développeurs. »</p>'}, {'', '<p>Dans le même temps, les clients sont obligés de consacrer des ressources importantes pour remédier aux failles au moyen de programmes complexes de gestion des correctifs et d’initiatives de réponse aux incidents.</p>'}, {'', ""<p>Ils affectent la mémoire d'un ordinateur en permettant aux programmeurs de manipuler directement la mémoire, ce qui facilite l'introduction par inadvertance d'erreurs de codage qui pourraient conduire à une opération apparemment routinière corrompant l'état ou la mémoire, ont écrit les responsables de la DARPA.</p>""}, {'', '<p>De plus, des problèmes de sécurité peuvent survenir lorsqu’un langage de programmation présente ce que l’agence appelle un « comportement indéfini », ce qui se produit lorsque la norme du langage ne contient aucune spécification ou directive sur la manière dont le programme doit se comporter dans des conditions qui ne sont pas explicitement définies dans la norme.</p>'}, {'', '<p>Un rapport de 23 pages de la CISA et d’autres agences de cybersécurité américaines et internationales de pays tels que le Canada, le Royaume-Uni, l’Australie et la Nouvelle-Zélande a révélé qu’environ 70 % des CVE de Microsoft et des failles du projet Chromium de Google sont des vulnérabilités de sécurité de la mémoire. De plus, pour Mozilla, 32 des 34 failles critiques ou hautement classées entraient dans cette catégorie.</p>'}]"
Polyfill devient un risque pour la chaîne d'approvisionnement de 100 000 sites Web,"[{'', '<p>En février, une entreprise chinoise a acheté le domaine et le compte GitHub de Polyfill, une bibliothèque open source populaire utilisée par plus de 100 000 sites Web pour fournir du code JavaScript.</p>'}, {'', ""<p>Dans les mois qui ont suivi l'acquisition par Funnull, le domaine – cdn[.]polyfill[io] – a été utilisé pour diffuser du code malveillant aux appareils via les sites Web qui ont intégré le domaine, redirigeant les utilisateurs vers des sites Web de paris sportifs et pornographiques en fonction de leur région et ouvrant la porte à d'autres attaques, telles que le détournement de formulaire, le détournement de clic et d'autres vols de données, selon les chercheurs.</p>""}, {'', '<p>« Cet incident est un exemple typique d’attaque de la chaîne d’approvisionnement », a écrit le groupe de renseignement sur les menaces du fournisseur de cybersécurité Sansec dans un article de blog. « Le code est doté d’une protection spécifique contre l’ingénierie inverse et ne s’active que sur des appareils mobiles spécifiques à des heures précises. Il ne s’active pas non plus lorsqu’il détecte un utilisateur administrateur. Il retarde également l’exécution lorsqu’un service d’analyse Web est trouvé, probablement pour ne pas apparaître dans les statistiques. »</p>'}, {'', '<p>Selon Simon Wijckmans, fondateur de la société de sécurité c/inside, certains utilisateurs se sont vu proposer un faux domaine Google Analytics qui les a redirigés vers ces autres sites. Début mars, les responsables du domaine Polyfill ont également ajouté un en-tête Cloudflare Security Protection au site, bien que l’objectif de cet en-tête ne soit pas clair.</p>'}, {'', '<p>Les responsables de Cloudflare ont écrit dans un article de blog que la société « n’a jamais recommandé le service polyfill.io ni autorisé l’utilisation du nom de Cloudflare sur son site Web. Nous leur avons demandé de supprimer cette fausse déclaration et ils ont, jusqu’à présent, ignoré nos demandes. C’est un autre signe avant-coureur qu’on ne peut pas leur faire confiance. »</p>'}, {'', '<p>« Cette attaque met en danger environ 100 000 sites Web », a écrit Wijckmans dans une alerte. « Lorsqu’un domaine autrefois sûr est intégré à des milliers de sites Web et dissimulé comme le sont les menaces JavaScript, il devient une voie tentante pour les acteurs malveillants. »</p>'}, {'', ""<p>Le projet open source Polyfill permet aux sites Web d'utiliser des fonctionnalités JavaScript modernes dans des navigateurs plus anciens en incluant uniquement les polyfills nécessaires en fonction du navigateur de l'utilisateur, a écrit Wijckmans.</p>""}, {'', ""<p>Les entreprises ont réagi depuis que les rapports des chercheurs en cybersécurité sur la situation sont sortis cette semaine. Google a commencé à bloquer les publicités pour les sites de commerce électronique qui utilisent le domaine Polyfill et Cloudflare aurait mis en œuvre des réécritures en temps réel du domaine vers sa propre version. De plus, le registraire de domaines et la société d'hébergement Web Namecheap ont mis le domaine en attente.</p>""}, {'', '<p>Cela éliminera les risques pour l’instant, mais Sansec encourage les développeurs à supprimer les références Polyfill dans leur code.</p>'}, {'', '<p>Cela dit, le propriétaire de Polyfill.io ne recule pas. Il aurait relancé son service de diffusion de contenu JavaScript (CDN) sous un nouveau domaine et s’oppose aux accusations, affirmant dans un message sur X (anciennement Twitter) que les médias le calomniaient et qu’il n’y avait aucun risque pour la chaîne d’approvisionnement.</p>'}, {'', '<p>Dans un autre message ciblant Cloudflare pour sa « diffamation répétée, sans fondement et malveillante » et sa « stratégie contraire à l’éthique consistant à supprimer la concurrence avant de promouvoir ses propres produits », le propriétaire de Polyfill.io a déclaré qu’il était « pleinement dévoué au développement d’un produit CDN mondial qui surpasse Cloudflare, mettant en valeur le véritable pouvoir du capital ».</p>'}, {'', '<p>Ils ont déclaré avoir obtenu un financement de démarrage de 50 millions de dollars et avoir finalisé la conception du produit.</p>'}, {'', ""<p>L'industrie a été alertée du problème de Polyfill peu après son rachat par Funnull. Andrew Betts a demandé aux sites Web utilisant le domaine de « le supprimer IMMÉDIATEMENT » dans un message publié sur X. Betts a expliqué qu'il avait créé le projet mais qu'il n'en était jamais propriétaire et qu'il n'avait eu aucune influence sur sa vente.</p>""}, {'', '<p>De plus, Wijckmans de c/side a déclaré que de nombreux fournisseurs de CDN populaires « ont depuis créé leurs propres forks, offrant aux utilisateurs un choix plus sûr. La plupart des navigateurs ont évolué pour que cela ne soit plus nécessaire de toute façon. »</p>'}, {'', ""<p>Il a fait référence à un autre site Web, Polykill, créé trois jours après la vente à Funnull pour sensibiliser les développeurs de ce que les développeurs du site ont appelé une « vulnérabilité majeure de la chaîne d'approvisionnement JavaScript ».</p>""}, {'', '<p>« Il existe de nombreux risques associés au fait de permettre à une entité étrangère inconnue de gérer et de diffuser du JavaScript dans vos applications Web », a écrit le créateur de Polykill. « Ils peuvent observer discrètement le trafic des utilisateurs et, en cas d’intention malveillante, ils peuvent potentiellement voler les noms d’utilisateur, les mots de passe et les informations de carte de crédit directement lorsque les utilisateurs saisissent les informations dans le navigateur Web. »</p>'}, {'', '<p>La situation a attiré l’attention du secteur de la sécurité. Eyal Paz, vice-président de la recherche chez OX Security, a déclaré que « la récente attaque de la chaîne d’approvisionnement de Polyfill met en évidence un problème critique du développement Web actuel : la confiance placée dans les bibliothèques tierces. Ajoutez à cela le fait que de nombreuses organisations n’ont pas la capacité de suivre la longue traîne de la chaîne d’approvisionnement des logiciels et nous sommes face à la tempête parfaite de risques de cybersécurité non gérés. »</p>'}, {'', ""<p>Cette affaire soulève des inquiétudes quant aux normes de sécurité de l'écosystème open source lorsqu'une entreprise est acquise par une autre nation, a déclaré Sarah Jones, analyste de recherche en cybermenace chez Critical Start.</p>""}, {'', '<p>« L’adoption généralisée de Polyfill.io dans divers secteurs, notamment le commerce électronique, la finance, les médias et le divertissement, ainsi que la santé, fournit un vaste réseau de sites Web que les acteurs malveillants peuvent exploiter », a déclaré Jones.</p>'}, {'', ""<p>Polyfill et d'autres plateformes qui hébergent du code open source largement utilisé sont généralement considérées comme des sources fiables, mais comportent un avertissement « à utiliser à nos propres risques », a déclaré Ngoc Bui, expert en cybersécurité chez Menlo Security.</p>""}, {'', '<p>« En raison de la portée étendue des référentiels open source comme ceux-ci, l’impact potentiel d’un problème sera difficile à mesurer », a déclaré Bui. « Des pratiques de codage sécurisées sont essentielles pour garantir que les modifications provenant de ces référentiels n’entraînent pas de dommages. »</p>'}]"
"La plupart des projets Open Source critiques manquent de code sécurisé en mémoire, selon la CISA","[{'', ""<p>L'agence de cybersécurité la plus importante du pays continue d'exhorter les développeurs de logiciels à adopter des langages de programmation sécurisés en mémoire pour aider à réduire le nombre de vulnérabilités dans leurs produits, plus récemment à travers un rapport montrant que plus de la moitié des projets open source largement utilisés contiennent du code non sécurisé en mémoire.</p>""}, {'', '<p>La CISA et le FBI ont également découvert que 55 % des lignes de code des projets étaient écrites dans des langages non sécurisés en mémoire et que les projets les plus importants étaient écrits de manière disproportionnée dans de tels langages. De plus, même les projets qui étaient censés être programmés dans des langages sécurisés en mémoire incluaient certains composants créés avec du code non sécurisé en mémoire.</p>'}, {'', ""<p>Dans le rapport conjoint de 22 pages publié cette semaine et élaboré en collaboration avec leurs homologues du Canada et de l'Australie, les agences ont recommandé aux éditeurs de logiciels de créer des feuilles de route pour la transition vers des feuilles de route sûres en matière de mémoire, y compris des plans pour aborder la sécurité de la mémoire dans les dépendances externes, qui ont tendance à inclure les logiciels open source (OSS).</p>""}, {'', ""<p>Le rapport, « Exploring Memory Safety in Critical Open Source Projects », « fournit un point de départ pour ces feuilles de route en examinant l'ampleur du risque de sécurité de la mémoire dans certains OSS », ont écrit les agences.</p>""}, {'', ""<p>Depuis plus d'un an, la CISA et d'autres agences gouvernementales militent en faveur de langages respectueux de la mémoire, encourageant les développeurs à s'éloigner de C et C++ et à adopter des langages tels que Rust, C#, Go, Java, Python et Swift.</p>""}, {'', ""<p>Selon la CISA, une telle transition éliminerait de nombreux problèmes de sécurité de la mémoire qui hantent les développeurs de logiciels depuis des décennies et qui sont à l'origine des failles les plus courantes dans les logiciels. Ces vulnérabilités affectent la manière dont la mémoire est consultée, écrite ou allouée d'une manière qui n'est pas prévue dans les langages de programmation.</p>""}, {'', ""<p>Elles peuvent entraîner davantage de mises à jour, de correctifs et de réponses aux incidents qui peuvent coûter cher à la fois au fabricant du logiciel et à ses utilisateurs. Les vulnérabilités ouvrent également le logiciel à l'exploitation par des acteurs malveillants. Les failles peuvent inclure des dépassements de mémoire tampon, l'utilisation de mémoire non initiée et l'utilisation après libération.</p>""}, {'', '<p>Les langages à mémoire sécurisée empêchent essentiellement les programmeurs de créer du code pouvant entraîner des bogues de mémoire et des failles de sécurité en incluant des fonctionnalités qui détectent automatiquement les erreurs d’accès à la mémoire, comme les pointeurs suspendus et les dépassements de tampon, au moment de la compilation et de l’exécution. C, C++ et les langages similaires non sécurisés en mémoire ne disposent pas de telles fonctions automatiques, ce qui conduit à davantage de bogues dans la version finale du code.</p>'}, {'', '<p>« Les langages non sécurisés en termes de mémoire nécessitent que les développeurs gèrent correctement l’utilisation et l’allocation de la mémoire », ont écrit les agences dans le rapport. « Les erreurs, qui se produisent inévitablement, peuvent entraîner des vulnérabilités en matière de sécurité de la mémoire, telles que les dépassements de tampon et l’utilisation après libération. L’exploitation réussie de ces types de vulnérabilités peut permettre aux adversaires de prendre le contrôle des logiciels, des systèmes et des données. »</p>'}, {'', '<p>Cela dit, « les langages à mémoire sécurisée déplacent la couche d’abstraction et la responsabilité de l’écriture de code à mémoire sécurisée du développeur vers le compilateur ou l’interpréteur, réduisant ainsi considérablement les possibilités d’introduire des vulnérabilités en matière de sécurité de la mémoire. »</p>'}, {'', '<p>Les 172 projets open source analysés proviennent de la liste des projets critiques du groupe de travail Securing Critical Projects de l’Open Source Security Foundation (OpenSSF).</p>'}, {'', '<p>Les agences ont constaté que même pour les projets écrits dans des langages sécurisés en termes de mémoire, il peut toujours y avoir des failles de sécurité de la mémoire.</p>'}, {'', '<p>« Cela peut être dû à l’utilisation directe de langages non sécurisés en mémoire ou à une dépendance externe à des projets qui utilisent des langages non sécurisés en mémoire », ont-ils écrit. « De plus, les exigences fonctionnelles de bas niveau visant à désactiver la sécurité de la mémoire peuvent créer des opportunités de vulnérabilités de sécurité de la mémoire dans le code écrit dans des langages par ailleurs sécurisés en mémoire. Ces limitations soulignent la nécessité d’une utilisation continue et diligente de langages de programmation sécurisés en mémoire, de pratiques de codage sécurisées et de tests de sécurité. »</p>'}, {'', '<p>Neatsun Ziv, cofondateur et PDG d’OX Security, a déclaré qu’il n’était pas surpris que bon nombre de ces projets utilisent des langages non sécurisés en mémoire. L’un des obstacles à la transition vers un code sécurisé en mémoire est la prévalence des systèmes hérités construits avec C et C++.</p>'}, {'', '<p>« La réécriture de ces codes dans des langages modernes et sûrs en termes de mémoire est non seulement coûteuse mais également complexe, ce qui peut entraîner des perturbations dans les opérations commerciales critiques », a déclaré Ziv. « Le principal défi consiste à trouver un équilibre entre la sécurité renforcée et les implications financières. »</p>'}, {'', '<p>Il a recommandé aux entreprises de créer des plans détaillés pour migrer vers des langages sécurisés en mémoire, notamment en donnant la priorité aux composants les plus critiques. En outre, elles devraient utiliser des outils d’analyse de code et des compilateurs avancés pour détecter et corriger automatiquement les pratiques de codage dangereuses et établir des cadres de gouvernance clairs qui intègrent la sécurité de la mémoire et les pratiques de codage sécurisées tout au long du cycle de vie du développement logiciel.</p>'}, {'', ""<p>« Il est essentiel de se concentrer sur la sécurité de la mémoire pour améliorer la sécurité du développement logiciel futur », a déclaré Ziv. « Cependant, la transition à partir des systèmes existants implique un investissement initial important et une évaluation minutieuse des coûts, des avantages et des compromis potentiels en matière de sécurité pour chaque organisation, c'est pourquoi il faut beaucoup de temps aux entreprises pour effectuer la transition. »</p>""}, {'', ""<p>Le rapport s'appuie sur un autre document, « The Case for Memory Safe Roadmaps », que la CISA et des agences d'autres pays ont publié en décembre 2023. La directrice de la CISA, Jen Easterly, a alors placé l'adoption d'un codage sécurisé par mémoire dans les termes de sécurité nationale et a déclaré qu'il s'agissait d'un élément clé de l'initiative Secure By Design du gouvernement fédéral visant à encourager les développeurs à intégrer des capacités de sécurité tout au long du cycle de vie du développement logiciel plutôt que de les ajouter à la fin.</p>""}, {'', '<p>Cela s’inscrit également dans le cadre d’une autre initiative de l’administration Biden visant à retirer aux utilisateurs la responsabilité de la sécurisation des logiciels et autres technologies et à la placer sur les épaules des développeurs.</p>'}, {'', '<p>En février, la Maison Blanche a publié un rapport technique qui renforçait l’argument selon lequel une utilisation accrue de langages à mémoire sécurisée éliminerait une série de failles de sécurité, le directeur national de la cybersécurité, Harry Coker, déclarant dans un communiqué que « nous, en tant que nation, avons la capacité – et la responsabilité – de réduire la surface d’attaque dans le cyberespace et d’empêcher des catégories entières de failles de sécurité de pénétrer dans l’écosystème numérique ».</p>'}]"
Ingénierie de plateforme : l'évolution vers DevOps-as-a-Service,"[{'', ""<p>Dans le paysage en constante évolution du développement logiciel et des opérations informatiques, de nouvelles méthodologies et de nouveaux cadres apparaissent fréquemment pour répondre aux défis d'efficacité, d'évolutivité et de fiabilité. Parmi ceux-ci, l'ingénierie de plateforme et DevOps ont suscité une attention particulière.</p>""}, {'', ""<p>Dans mon précédent blog, Mesurer la valeur de DevOps en tant que service, j'ai déclaré : « En termes généraux, DaaS correspond aux capacités DevOps mises à la disposition des utilisateurs de ces capacités via des portails et des API. »</p>""}, {'', '<p>Ce blog soutiendra que l’ingénierie de plate-forme est essentiellement une évolution de DevOps vers un modèle plus structuré et orienté services, offrant une approche standardisée et évolutive pour la mise en œuvre des pratiques DevOps dans les organisations.</p>'}, {'', '<h3>Comprendre DevOps</h3>'}, {'', ""<p>Principes DevOps : DevOps est une approche culturelle et méthodologique qui met l'accent sur la collaboration entre les équipes de développement et d'exploitation. Ses principes fondamentaux sont les suivants :</p>""}, {'', ""<p>Collaboration et communication : briser les silos entre le développement et les opérations pour favoriser une culture de collaboration continue.Intégration continue et livraison continue (CI/CD) : automatisation des processus d'intégration et de déploiement pour garantir une livraison rapide et fiable des logiciels.Infrastructure en tant que code (IaC) : gestion et provisionnement de l'infrastructure informatique via du code plutôt que des processus manuels.Surveillance et observabilité : surveillance continue des applications et de l'infrastructure pour garantir les performances, la fiabilité et la disponibilité.</p>""}, {'', '<p>Défis liés à la mise en œuvre de DevOps : Bien que les avantages de DevOps soient évidents, de nombreuses organisations ont du mal à le mettre en œuvre. Les défis courants incluent :</p>'}, {'', ""<p>Intégration d'outils : l'intégration de divers outils et technologies nécessaires au CI/CD, à la surveillance et à l'IaC peut être complexe et prendre du temps. Lacunes en matière de compétences : les organisations manquent souvent des compétences et de l'expertise nécessaires pour mettre en œuvre et gérer efficacement les pratiques DevOps. Changement culturel : faire évoluer la culture organisationnelle pour adopter les principes DevOps peut être difficile et lent.</p>""}, {'', ""<h3>L'essor de l'ingénierie de plateforme</h3>""}, {'', ""<p>Qu'est-ce que l'ingénierie de plateforme ? L'ingénierie de plateforme se concentre sur la création et la maintenance de plateformes intégrées qui fournissent les outils, les environnements et les services nécessaires pour prendre en charge l'ensemble du cycle de vie du développement logiciel. Ces plateformes sont conçues pour rationaliser le développement, les tests, le déploiement et les opérations en offrant un ensemble cohérent et standardisé de services.</p>""}, {'', ""<p>Composants clés de l'ingénierie de plate-forme\xa0:</p>""}, {'', '<p>Interfaces en libre-service : fournir aux développeurs un accès facile aux ressources nécessaires, telles que les environnements, les bases de données et les pipelines CI/CD.</p>'}, {'', '<p>Automatisation : mise en œuvre de l’automatisation pour gérer l’infrastructure, les déploiements et la surveillance, réduisant ainsi les efforts manuels et les erreurs.</p>'}, {'', '<p>Normalisation : création d’environnements et de processus standardisés pour garantir la cohérence et la fiabilité des projets et des équipes.</p>'}, {'', '<p>Évolutivité : concevoir des plateformes qui s’adaptent aux besoins de l’organisation, prenant en charge plusieurs équipes et divers projets.</p>'}, {'', '<h3>Ingénierie de plateforme en tant que DevOps-as-a-Service</h3>'}, {'', ""<p>Définition de DevOps en tant que service : DevOps en tant que service (DaaS) peut être compris comme une approche orientée services pour fournir des fonctionnalités DevOps. Il s'agit de fournir un ensemble d'outils et de pratiques standardisés et évolutifs sous forme de service géré, permettant aux organisations de tirer parti des principes DevOps sans avoir à créer et à maintenir elles-mêmes l'infrastructure sous-jacente.</p>""}, {'', ""<p>Similitudes entre l'ingénierie de plateforme et DevOps-as-a-Service\xa0:</p>""}, {'', ""<p>1. Standardisation et automatisation : l'ingénierie de plateforme et le DaaS mettent tous deux l'accent sur la standardisation et l'automatisation. En fournissant des outils et des environnements standardisés, l'ingénierie de plateforme simplifie les processus d'intégration et de déploiement, de la même manière que le DaaS fournit un ensemble unifié de fonctionnalités DevOps en tant que service.</p>""}, {'', ""<p>2. Capacités en libre-service : l'ingénierie de plateforme fournit des interfaces en libre-service qui permettent aux développeurs d'accéder aux ressources dont ils ont besoin sans attendre les équipes d'exploitation. Cela correspond au modèle DaaS, où les services sont facilement accessibles aux utilisateurs via un portail en libre-service.</p>""}, {'', '<p>3. Concentrez-vous sur l’efficacité et l’évolutivité : les deux approches visent à améliorer l’efficacité et l’évolutivité. L’ingénierie de plateforme crée des plateformes évolutives qui peuvent prendre en charge plusieurs équipes et projets, tandis que DaaS offre des capacités DevOps évolutives qui peuvent évoluer en fonction des besoins de l’organisation.</p>'}, {'', ""<p>4. Réduction de la complexité et des lacunes en matière de compétences : en proposant une plateforme gérée avec des outils intégrés, l'ingénierie de plateforme réduit la complexité de la mise en œuvre des pratiques DevOps et comble les lacunes en matière de compétences au sein des organisations. De même, DaaS fournit une solution clé en main qui simplifie l'adoption et la gestion de DevOps.</p>""}, {'', ""<p>5. Collaboration et intégration améliorées : l'ingénierie de plateforme favorise la collaboration en fournissant une plateforme unifiée qui intègre divers outils et services, favorisant ainsi une communication et une coordination fluides au sein de l'équipe. Cela reflète la nature collaborative du DaaS, qui intègre et orchestre les outils DevOps pour soutenir la livraison et les opérations continues.</p>""}, {'', ""<p>Avantages de l'ingénierie de plateforme en tant que DevOps en tant que service1. Adoption accélérée : l'ingénierie de plateforme accélère l'adoption des pratiques DevOps en fournissant une plateforme prête à l'emploi qui intègre les meilleures pratiques et outils. Les organisations peuvent rapidement intégrer des équipes et commencer à exploiter les capacités DevOps sans installation ni configuration approfondies.</p>""}, {'', ""<p>2. Cohérence et fiabilité : les plateformes standardisées garantissent la cohérence et la fiabilité des projets et des équipes. Cela réduit le risque d'erreurs et de divergences résultant d'intégrations d'outils ad hoc et de processus manuels.</p>""}, {'', ""<p>3. Concentrez-vous sur les compétences de base : en déchargeant la responsabilité de la création et de la maintenance de l'infrastructure DevOps sur une équipe de plateforme centralisée, les organisations peuvent permettre aux équipes de développement et d'exploitation de se concentrer sur leurs compétences de base : développer et fournir des logiciels de haute qualité.</p>""}, {'', ""<p>4. Expérience de développement améliorée : les interfaces en libre-service et les processus automatisés améliorent l'expérience du développeur en offrant un accès rapide et facile aux ressources nécessaires. Cela réduit les frictions et accélère les cycles de développement.</p>""}, {'', '<p>5. Évolutivité et flexibilité : les plateformes évolutives peuvent s’adapter aux besoins croissants des organisations, en prenant en charge des projets plus vastes et plus complexes. Cette flexibilité garantit que la plateforme peut évoluer en fonction des besoins de l’organisation.</p>'}, {'', '<h3>Défis et considérations</h3>'}, {'', ""<p>1. Investissement initial : la création d'une plateforme complète nécessite un investissement initial en temps et en ressources. Les organisations doivent planifier et allouer soigneusement les ressources pour garantir une mise en œuvre réussie.</p>""}, {'', ""<p>2. Amélioration continue : les plateformes doivent être continuellement améliorées et mises à jour pour intégrer de nouveaux outils, technologies et meilleures pratiques. Cela nécessite un effort et un engagement continus de la part de l'équipe de la plateforme.</p>""}, {'', '<p>3. Équilibrer la standardisation et la flexibilité : si la standardisation est bénéfique, il est essentiel de trouver un équilibre entre standardisation et flexibilité. Les plateformes doivent être adaptables pour répondre aux divers besoins des différentes équipes et des différents projets.</p>'}, {'', '<h3>Conclusion</h3>'}, {'', ""<p>L'ingénierie de plateforme représente l'évolution de DevOps vers un modèle plus structuré et orienté services, incarnant efficacement les principes de DevOps en tant que service. En fournissant des plateformes standardisées et évolutives avec des outils et des services intégrés, l'ingénierie de plateforme simplifie l'adoption et la gestion des pratiques DevOps, en répondant aux défis courants tels que l'intégration des outils, les lacunes en matière de compétences et le changement culturel.</p>""}, {'', ""<p>Alors que les entreprises cherchent des moyens d'améliorer l'efficacité, l'évolutivité et la fiabilité de leur développement logiciel et de leurs opérations informatiques, l'ingénierie de plateforme en tant que DevOps-as-a-service offre une solution convaincante. En accélérant l'adoption, en garantissant la cohérence et en améliorant l'expérience des développeurs, l'ingénierie de plateforme permet aux entreprises de tirer pleinement parti des avantages de DevOps et de favoriser l'innovation et l'amélioration continues.</p>""}, {'', '<p>En conclusion, l’ingénierie de plateforme ne remplace pas DevOps mais constitue une évolution qui offre une approche structurée et évolutive pour fournir des fonctionnalités DevOps en tant que service. Cette perspective met en évidence la nature complémentaire de l’ingénierie de plateforme et de DevOps, soulignant l’importance de réussir la transformation numérique.</p>'}]"
Cisco ajoute l'IA générative et des intégrations Splunk plus poussées à AppDynamics,"[{'', ""<p>Cisco a présenté cette semaine un assistant d'intelligence artificielle (IA) générative pour la plateforme d'observabilité Cisco AppDynamics ainsi que des intégrations à la plateforme IT Service Intelligence (ITSI) de Splunk et un Splunk Log Observer Connect pour Cisco AppDynamics à la plateforme qu'il a acquise plus tôt cette année en acquérant Splunk.</p>""}, {'', ""<p>Annoncé lors de l'événement Cisco Live! 2024, Cisco permettra également au troisième trimestre d'héberger Cisco AppDynamics sur la plateforme Microsoft Azure.</p>""}, {'', ""<p>Cisco a également annoncé qu'elle ne commercialiserait plus la plateforme Cisco AppDynamics pour la gestion des performances des applications natives du cloud (APM). Cette plateforme sera remplacée par Splunk Observability Cloud.</p>""}, {'', '<p>Cisco a également ajouté des fonctionnalités supplémentaires de science des données et d’algorithmes d’apprentissage automatique à ITSI pour rationaliser le nombre d’alertes générées par la plateforme. Désormais disponible, l’assistant de configuration de Splunk ITSI fournit une console centralisée en fournissant des informations sur les modèles de seuil obsolètes qui peuvent être optimisés à l’aide de corrections guidées mises en évidence par les modèles d’IA.</p>'}, {'', '<p>Enfin, Cisco a mis en place des fonctionnalités d’authentification unique pour rationaliser les flux de travail partagés entre Cisco AppDynamics et Splunk via une console unique. Ces fonctionnalités d’authentification unique seront étendues à l’ensemble du portefeuille Cisco pour permettre une collaboration accrue.</p>'}, {'', ""<p>Tom Casey, vice-président senior et directeur général des produits et technologies pour le portefeuille de produits Splunk, a déclaré aux participants à la conférence que les plateformes seront progressivement intégrées dans une plateforme de données unique, comprenant Cisco AppDynamics, le service de surveillance réseau Cisco ThousandEyes et la plateforme Splunk. Cette approche permettra également à Cisco de fournir les meilleures capacités de sa catégorie pour tout, de la surveillance des performances des applications à la surveillance de l'expérience numérique dans un environnement de cloud computing hybride, a-t-il ajouté.</p>""}, {'', ""<p>Paul Nashawaty, responsable du développement d'applications chez The Futurum Group, a déclaré que ces capacités combinées soulignent l'engagement de Cisco à fournir une visibilité complète à un moment où les environnements informatiques distribués deviennent de plus en plus difficiles à gérer.</p>""}, {'', '<p>On ne sait pas encore dans quelle mesure les entreprises unifient l’observabilité, la surveillance du réseau et la gestion traditionnelle des services informatiques (ITSM), mais Cisco parie clairement que les responsables informatiques préféreront s’appuyer sur un seul fournisseur pour réduire les coûts d’intégration qui seraient autrement encourus lors de l’utilisation de plateformes fournies par des fournisseurs disparates. Cette approche centralisée simplifiera également la formation de plusieurs modèles d’IA qui seront utilisés pour automatiser davantage une gamme de flux de travail qui s’étendent aujourd’hui sur plusieurs silos informatiques.</p>'}, {'', '<p>Il est toutefois moins clair dans quelle mesure l’acquisition de Splunk pourrait contribuer à une consolidation des plateformes informatiques, alors que les concurrents réalisent des acquisitions similaires pour favoriser une approche plus centrée sur la plateforme afin d’unifier la gestion de l’informatique.</p>'}, {'', '<p>Chaque organisation informatique doit décider de la meilleure façon de se structurer à mesure que les flux de travail de ces équipes deviennent plus intégrés. Il est certain que la taille globale du portefeuille d’applications à gérer ne fera que continuer à augmenter à mesure que l’IA simplifie la création et le déploiement de logiciels. Par conséquent, les équipes informatiques doivent très certainement appliquer les meilleures pratiques DevSecOps à des niveaux d’échelle plus élevés pour garantir la disponibilité des applications dans des environnements informatiques hautement distribués.</p>'}]"
AlmaLinux présente un comité directeur d'ingénierie pour améliorer la collaboration communautaire,"[{'', ""<p>La Fondation AlmaLinux OS a annoncé la création du Comité directeur d'ingénierie AlmaLinux (ALESCo) pour renforcer l'engagement communautaire et la gouvernance technique. Ce nouvel organisme vise à guider la direction technique de la distribution Linux clone de CentOS AlmaLinux, en garantissant sa robustesse et sa stabilité à long terme.</p>""}, {'', ""<p>Cette décision intervient quelques semaines seulement après la sortie d'une nouvelle version d'AlmaLinux, AlmaLinux 9.4. Cette dernière version est compatible avec Red Hat Enterprise Linux (RHEL) 9.4 et prend en charge le matériel hérité, que Red Hat ne prendra plus en charge.</p>""}, {'', '<p>Pourquoi cette décision ? Cody Robertson, directeur technique chez Hawk Host, un revendeur d’hébergement Web, a déclaré : « Ce comité fournit une plate-forme structurée pour rationaliser les capacités d’ingénierie de toutes les personnes impliquées dans le projet AlmaLinux. En favorisant un environnement où la transparence est primordiale, nous engageons non seulement des ingénieurs, mais nous accueillons également des non-ingénieurs sur un pied d’égalité pour qu’ils soient au cœur de la croissance explosive d’AlmaLinux. »</p>'}, {'', '<p>Surnommé le « contrôle aérien » pour les questions d’ingénierie, ALESCo supervisera et guidera les décisions techniques en collaboration avec divers groupes d’intérêt spéciaux (SIG). Le comité a été proposé par les membres de l’équipe d’infrastructure Jonathan Wright, responsable de l’infrastructure d’AlmaLiunx, et Robertson. Il sera responsable de :</p>'}, {'', ""<li>Supervision technique du système d'exploitation</li>""}, {'', ""<li>Transparence dans les décisions d'ingénierie</li>""}, {'', '<li>Garder le cap sur la stabilité à long terme</li>'}, {'', '<li>Gestion des versions</li>'}, {'', '<li>Soutenir les SIG</li>'}, {'', '<p>Pour DevOps, cela signifie qu’AlmaLinux devrait être à la fois meilleur et un peu plus flexible pour sa communauté d’utilisateurs que Red Hat. AlmaLinux a déjà montré qu’il était prêt à s’éloigner de RHEL pour améliorer la sécurité et fournir un support pour le matériel plus ancien. Espérons que cette décision l’aidera à ajouter d’autres améliorations.</p>'}, {'', ""<p>Le conseil d'administration de la Fondation AlmaLinux OS a nommé les premiers membres de l'ALESCo. Il nommera ensuite en interne un président et de nouveaux membres tous les six mois. Cette approche vise à favoriser un leadership diversifié. Dans ce contexte, la diversité signifie qu'AlmaLinux souhaite écouter ses clients et d'autres contributeurs, tels que les rédacteurs techniques et les développeurs. AlmaLinux souhaite également conserver sa crédibilité auprès de la communauté Linux en évitant tout point de contrôle unique et prolongé. Pour assurer la transparence de la prise de décision, toutes les réunions de l'ALESCo seront publiques et les affiliations des membres seront divulguées.</p>""}, {'', '<p>Benny Vasquez, président de la Fondation AlmaLinux OS, a souligné l’engagement de la Fondation envers la transparence et l’implication de la communauté, en déclarant : « La création d’ALESCo renforce notre engagement envers la transparence tout en ouvrant la porte à une contribution technique plus large de notre communauté. Nous sommes ravis de l’engagement accru que cela apportera et nous apprécions les contributions de Wright, Robertson et d’autres contributeurs clés de la proposition comme Farrell, Alex Iribarren du CERN et Daniel Pearson de KnownHost. »</p>'}, {'', '<p>Avec ALESCo en place, AlmaLinux est prêt à améliorer ses efforts de collaboration, garantissant un système d’exploitation robuste et fiable qui répond aux besoins de sa communauté croissante.</p>'}]"
Atlassian utilise l'IA pour automatiser les flux de travail d'ingénierie logicielle,"[{'', '<p>Atlassian a ajouté cette semaine un outil d’intelligence artificielle générative (IA) à sa conférence Team ’24. Rovo utilise la technologie des graphes de connaissances pour exposer les données à la plateforme Atlassian Intelligence afin d’invoquer plusieurs types de modèles d’IA.</p>'}, {'', ""<p>En outre, la société a annoncé l'acquisition d'Optic, un fournisseur d'outils open source de documentation et de gestion des interfaces de programmation d'applications (API). Optic sera intégré à Compass, un portail de développement interne (IDP) lancé par Atlassian l'année dernière. Optic utilise la spécification OpenAPI pour suivre automatiquement les modifications de l'API au fur et à mesure qu'elles se produisent dans un pipeline d'intégration continue (CI). Il trouvera désormais la documentation OpenAPI dans le code et la publiera sur Compass, qui avertira ensuite le reste de l'équipe d'ingénierie logicielle.</p>""}, {'', ""<p>Enfin, Atlassian a également révélé qu'il fusionnait désormais son application de gestion de projets Jira avec Jira Work Management pour mieux rationaliser les flux de travail. Cette prochaine itération de Jira, largement utilisée pour gérer les projets de développement logiciel, sera également dotée de fonctionnalités d'IA activées par Atlassian Intelligence, notamment des agents Rovo pour automatiser les tâches.</p>""}, {'', ""<p>Michael Cannon-Brookes, PDG d'Atlassian, a expliqué aux participants à la conférence que Rovo analyse les documents pour en fournir une compréhension sémantique. Cela permet au logiciel d'orchestrer les flux de travail à la fois sur le portefeuille Atlassian et sur des plateformes tierces connectées via des API. Les organisations pourront également utiliser des outils de découverte accessibles via un outil de chat pour rechercher du contenu, avant d'appeler des agents pour exécuter une série de tâches, a déclaré Cannon-Brookes.</p>""}, {''}, {'', ""<p>Tout cela aidera les équipes DevOps à lancer une requête en langage naturel pour, par exemple, faire apparaître un plan de publication, y compris les documents sources, pour tout projet de développement d'application, a déclaré Cannon-Brookes.</p>""}, {'', '<p>Au total, Atlassian ajoute plus de 30 agents à son portefeuille, qui, à des degrés divers, utilisent l’IA pour automatiser les tâches des applications exécutées dans le cloud. Les entreprises peuvent également choisir de créer leurs propres agents à l’aide des outils fournis par Atlassian. Chaque agent se verra attribuer des types d’objectifs et de buts spécifiques, a noté Cannon-Brookes.</p>'}, {'', '<p>En fin de compte, l’avenir de la collaboration sera défini par la collaboration entre les humains et les IA pour accomplir des tâches, a-t-il ajouté. Les organisations pourront même attribuer des attributs de personnalité à chacun des agents en fonction des préférences des utilisateurs, a ajouté Cannon-Brookes.</p>'}, {'', '<p>Il n’est pas certain que les organisations s’appuieront sur des assistants IA fournis par des fournisseurs d’applications spécifiques ou sur un ensemble restreint d’agents fournis par des fournisseurs de services cloud exécutant plusieurs applications, par exemple. Quelle que soit l’approche adoptée, le niveau d’automatisation des tâches rendu possible par les agents IA qui tirent parti des moteurs de raisonnement intégrés dans LLM est sur le point d’augmenter considérablement.</p>'}, {'', '<p>Naturellement, chaque organisation devra déterminer la meilleure façon de vérifier que les tâches assignées à un agent IA ont été effectuées, ce qui posera un problème de gouvernance. En attendant, les équipes DevOps doivent toutefois établir une liste de tâches qui pourraient bientôt être mieux gérées par un agent IA que par un ingénieur DevOps qui devrait résoudre des problèmes plus complexes.</p>'}]"
Spotify va fournir une instance d'opinion de Backstage IDP,"[{'', ""<p>Spotify a annoncé aujourd'hui qu'il mettait à disposition du marché une instance de son portail de développement interne (IDP) basé sur la plateforme open source Backstage que la société avait précédemment contribué à la Cloud Native Computing Foundation (CNCF).</p>""}, {'', ""<p>Pia Nilsson, directrice principale de l'ingénierie chez Spotify, a déclaré que Spotify Portal for Backstage, disponible en version bêta privée aujourd'hui, fournit aux équipes d'ingénierie de la plateforme un cadre low-code/no-code pour la mise en place d'un IDP.</p>""}, {'', ""<p>De plus, Spotify met désormais à disposition des organisations ayant adopté Backstage un support d'entreprise, ainsi que des plug-ins supplémentaires qui simplifient les intégrations avec des outils et des plateformes de fournisseurs tiers tels que New Relic, Atlassian, Snyk et Datadog.</p>""}, {'', ""<p>Enfin, Spotify ajoute des fonctionnalités pour gérer les hackathons et faire remonter les informations sur l'utilisation. Un prochain plug-in promet également de permettre d'ajouter des entités de données à un catalogue de logiciels résidant dans Backstage.</p>""}, {''}, {'', ""<p>Backstage a gagné en popularité en tant qu'IDP principalement parce qu'il fournit un cadre extensible pour centraliser la gestion des environnements de développement.</p>""}, {'', '<h3>Mise en œuvre du portail Spotify</h3>'}, {'', ""<p>Certaines entreprises ont trouvé difficile de créer et de déployer Backstage. Spotify propose donc désormais une implémentation plus réfléchie du portail Spotify, plus simple à déployer, à configurer et à gérer à l'aide de modèles, a noté Nilsson.</p>""}, {'', ""<p>Les applications étant devenues plus difficiles à créer, l'accent est mis sur l'amélioration de la productivité des développeurs. Dans le même temps, les IDP permettent aux équipes d'ingénierie de plateforme de rationaliser le nombre d'outils utilisés de manière à réduire le coût total du développement logiciel.</p>""}, {'', '<p>De nombreux développeurs souhaitent ajouter des outils selon leurs besoins. Les équipes DevOps doivent donc trouver un moyen de trouver un équilibre entre la réduction de la charge cognitive à laquelle les développeurs sont confrontés et ce qui pourrait devenir une approche trop lourde de la centralisation des flux de travail DevOps. Si les développeurs talentueux commencent à se diriger vers la sortie parce qu’ils n’aiment pas l’expérience proposée, les responsables informatiques auront du mal à les remplacer.</p>'}, {'', '<h3>Donner plus de temps aux équipes DevOps</h3>'}, {'', '<p>L’idée est que les IDP, utilisés dans le contexte de l’ingénierie de plateforme, donneront aux développeurs plus de temps pour se concentrer sur l’écriture de la logique métier. Cependant, le développement d’applications est autant un art qu’une science. Ce n’est pas parce que les développeurs ont plus de temps que les idées et l’inspiration nécessaires pour écrire du code s’ensuivent automatiquement. Les IDP et l’ingénierie de plateforme, à tout le moins, créent cependant la possibilité pour les développeurs d’écrire plus de logique métier plus rapidement. Le degré auquel cela se produit varie naturellement d’une organisation à l’autre.</p>'}, {'', ""<p>Chaque organisation devra décider dans quelle mesure elle souhaite adopter l'ingénierie de plateforme, mais quel que soit son niveau d'engagement, les IDP peuvent jouer un rôle essentiel en simplifiant l'intégration des développeurs dans de nouveaux projets, a noté Nillson. En outre, un IDP permet aux équipes DevOps de suivre plus facilement des indicateurs tels que la vitesse à laquelle le code est mis à jour, a-t-elle déclaré.</p>""}, {'', '<p>Les IDP ne sont bien sûr pas nécessairement idéaux, mais avec l’essor de Backstage, il y a clairement désormais plus de standardisation, car les équipes DevOps continuent de chercher à éliminer autant de frictions que possible.</p>'}]"
Tricentis utilise l'IA générative pour automatiser les tests d'applications,"[{'', ""<p>Cette semaine, Tricentis a ajouté une fonctionnalité d'intelligence artificielle générative (IA), baptisée Tricentis Copilot, à sa plateforme d'automatisation des tests d'applications. Elle vise à réduire la quantité de code que les équipes DevOps doivent créer manuellement.</p>""}, {'', '<p>Tricentus Copilot est basé sur une instance du modèle de langage large (LLM) créé par Open AI et déployé sur le cloud Microsoft Azure. Il s’agit de la première itération de ce qui deviendra à terme plusieurs assistants d’IA : Tricentis Testim Copilot, qui permet d’utiliser le langage naturel pour décrire un test qui est ensuite généré automatiquement en JavaScript. D’autres solutions Tricentis Copilot pour les plateformes Tricentis Tosca et Tricentis qTest seront ajoutées plus tard cette année.</p>'}, {'', ""<p>Mav Turner, directeur des produits et de la stratégie chez Tricentis, a déclaré que les interfaces utilisateur que la société a créées pour chaque plateforme continueront également d'évoluer, afin de simplifier le lancement des invites appropriées au cours de n'importe quel flux de travail.</p>""}, {'', ""<p>L'objectif est de faciliter la création de tests pour améliorer la qualité des applications, a déclaré Turner. Par exemple, les organisations qui utilisent les éditions bêta de Tricentis Copilot ont déjà constaté une augmentation de 20 à 50 % du nombre de tests créés, tout en réduisant les taux d'échec des tests de 16 à 43 % jusqu'à présent, a-t-il ajouté.</p>""}, {''}, {'', '<p>Il ne fait aucun doute que l’IA générative facilite considérablement la création de tests, ce qui devrait permettre aux équipes DevOps d’exécuter davantage de tests tout au long du cycle de vie du développement logiciel (SDLC). Plutôt que de devoir toujours attendre que des équipes de test dédiées écrivent le code requis, il sera de plus en plus possible pour n’importe quel membre d’une équipe DevOps de générer un test, y compris les développeurs d’applications, qui devraient tester le code au fur et à mesure de sa création.</p>'}, {'', '<p>L’IA générative devrait également permettre aux équipes DevOps de réutiliser plus facilement les tests une fois créés et de comprendre comment ils sont exécutés grâce aux capacités de synthèse offertes par les plateformes d’IA générative qui pourront également fournir des recommandations pour améliorer le code de test. Collectivement, ces capacités permettront d’exécuter les tests plus rapidement, d’avoir moins d’erreurs, de réduire les coûts et d’augmenter la productivité.</p>'}, {'', '<p>Ces fonctionnalités peuvent toutes être fournies d’une manière qui n’entraîne pas l’utilisation d’une partie du code de test pour entraîner ultérieurement une mise à jour du LLM de base fourni par OpenAI, a noté Turner. À plus long terme, Tricentis continuera de rechercher les LLM qui pourraient être les mieux appliqués à un cas d’utilisation donné d’une manière transparente pour une équipe DevOps qui pourrait ne pas être intéressée par le LLM utilisé pour automatiser une tâche. Actuellement, cependant, cela n’a pas beaucoup de sens économique pour Tricentis de créer son propre LLM, a déclaré Turner.</p>'}, {'', '<p>On ne sait pas encore dans quelle mesure l’IA générative démocratisera les tests d’applications, mais à mesure que ces tests deviendront plus faciles à créer, il devrait y avoir plus de temps pour exécuter des tests plus complexes qui pourraient, par exemple, résoudre des problèmes de cybersécurité. La plupart des tests exécutés aujourd’hui font généralement apparaître des erreurs de programmation courantes. Cependant, à mesure que les tests deviennent plus rapides à l’ère de l’IA, il devrait y avoir plus de temps pour exécuter une gamme plus large de tests afin de garantir les meilleures expériences d’application possibles.</p>'}]"
Votre IA pourrait vous mentir,"[{'', '<p>Des tests simples peuvent démontrer comment une IA peut répondre et modifier les réponses — un exercice utile pour les développeurs et DevOps qui souhaitent utiliser ces outils pour des activités telles que la génération de code ou de script.</p>'}, {'', '<p>Alors que je jouais avec l’IA générative pour un projet de loisir – comme la plupart d’entre nous – j’ai découvert que dans quelques cas, elle s’opposait obstinément à ce que je suive mes instructions explicites. Ce n’était rien de controversé. C’est juste que j’utilisais un système qui utilisait les mathématiques d’une seule manière, et l’IA insistait pour présenter les résultats de la même catégorie à partir d’un sous-système différent. Même lorsqu’on lui indiquait les étapes à suivre pour générer des résultats valides, l’outil d’IA restait bloqué sur le nom du sous-système et « substituait » un sous-système invalide du même nom. Il disait « Oh oui, je t’ai compris. Tiens : » et présentait ensuite ce que je venais de lui dire comme étant factuellement incorrect. J’ai donc commencé à repousser les limites et j’ai trouvé un grand nombre de cas où ce type de confusion se produisait.</p>'}, {'', '<p>Dans mon hobby, je peux prendre le temps de calculer séparément la mèche dont j’ai besoin, puis de l’insérer. Dans un cadre professionnel, c’est loin d’être optimal. Je veux dire, si nous économisons une douzaine d’heures sur un projet et devons investir une heure pour réparer des mèches comme celle-ci, c’est quand même une énorme victoire. Le problème est que nous devons connaître le problème pour le résoudre. Mais le système – dans la plupart des cas, le fait aveuglément et ne vous le dit pas. C’est le cas même lorsqu’il sait qu’il n’obéit pas à vos instructions.</p>'}, {'', '<p>Parfois, il est acceptable d’accepter les décisions imposées par les concepteurs de l’IA. Mais un employé ne conserverait pas longtemps son emploi si, lorsqu’on lui disait « Faites cette tâche exactement de cette manière », il répondait « Oui, vos instructions ne correspondaient pas à ma vision du monde, alors j’ai fait autre chose ; regardez cette excellente solution ! » L’IA n’obtient qu’un peu plus de patience tant qu’elle vous permet de la corriger rapidement. Et jusqu’à présent, lors de nos tests (à l’exception du problème des amateurs ci-dessus), nous avons réussi à la convaincre de faire ce que nous lui disions, et non ce que les concepteurs de l’IA considéraient comme « mieux ».</p>'}, {'', '<p>Voici donc le test minimum que vous devez exécuter sur n’importe quel générateur de code pour déterminer s’il fait ce que vous lui demandez ou ce que ses concepteurs d’IA l’ont programmé pour faire.</p>'}, {'', ""<p>Demandez à votre IA de générer un site Web dynamique simple, dans la langue de votre choix. En une seule itération, demandez-lui d'optimiser les lignes de code. La plupart ne le feront pas\xa0; ils pensent qu'ils savent mieux que ça. Ensuite, après la génération du code, demandez à l'outil s'il s'agit du nombre minimal de lignes de code qui résolvent le problème dans cette langue. Il vous donnera alors la réponse que vous avez réellement demandée. Répétez maintenant cet exercice pour plus de lisibilité. Utilisez des langages, des frameworks et des bibliothèques spécifiques que vous pourriez avoir comme normes.</p>""}, {'', ""<p>Pour les problèmes simples, l’IA générative génère du code utilisable. Je vois l’IA dans le codage comme une extension des bibliothèques : un moyen plus rapide de résoudre les problèmes courants. Mais nous devons toujours être prudents. Lorsqu’on a demandé à un outil d’IA générative majeur de générer cette simple page Web, il nous a donné quatre fichiers sources et a déclaré qu’il s’agissait d’une « réponse ». J’avais demandé la réponse la plus efficace, optimisée pour un minimum de lignes de code. Comme vous pouvez l’imaginer, nous avons ri de quatre fichiers sources pour faire apparaître une page Web qui accepte un nom et affiche « Bonjour [[nom]] ». (C’était notre cas de test simple et dynamique). Lorsque je lui ai demandé « Est-ce le moins de lignes de code ? », l’outil m’a rapidement présenté la réponse à laquelle on s’attendait. C’était un seul fichier, puis il m’a grondé en disant que ce n’était pas le plus lisible. Je n’ai jamais mentionné la lisibilité de cet outil, j’ai mentionné qu’il était optimisé pour un minimum de lignes de code. Mais à la troisième itération, il m'a donné la réponse optimale pour l'exemple dynamique, et à la deuxième la réponse optimale pour une page Web simple pour afficher «\xa0bonjour le monde\xa0» de manière non interactive.</p>""}, {'', '<p>Comme je l’ai évoqué plus haut, obtenir de mauvaises réponses n’est pas un obstacle majeur avec l’IA, mais vous devez être conscient des faiblesses de l’outil que vous avez choisi, et l’outil que vous choisissez doit certainement être informé du niveau de manipulation dans lequel les concepteurs sont engagés, afin que le personnel puisse être à l’affût de solutions sous-optimales (sous-optimales en termes de besoins organisationnels, ce qui est la seule « solution optimale » qui compte).</p>'}, {'', '<p>Exécutez donc ce test en même temps que vos tests plus fonctionnels et continuez à vous battre. Si nous pouvons utiliser l’IA générative pour rationaliser le développement et les tests, nous devrions absolument le faire, mais nous devons nous rappeler qu’il s’agit simplement d’un autre outil d’automatisation, et parfois moins fiable que d’autres outils.</p>'}]"
Notre infrastructure continue de s'étendre,"[{'', '<p>On pourrait penser que tôt ou tard, les spécialistes du marketing et les premiers utilisateurs prendraient une pause, mais il semble que si un groupe ralentit un peu, un autre prend le relais. Notre infrastructure se développe dans presque tous les sens possibles, ce qui crée une charge plus importante pour tous les aspects de DevOps, en particulier dans le domaine informatique.</p>'}, {'', '<p>Nous avons davantage de besoins en matière de sécurité, car nous avons une plus grande empreinte, par exemple. Nous avons des API depuis longtemps, mais nous avons maintenant des groupes d’API massifs qui sont utilisés et réutilisés, et alors que l’analyse de code standard fonctionne avec la source, les entreprises qui s’occupent de la « sécurité des API » commencent par la sécurité de l’interface, et non du code. Cela évolue lentement, mais la gestion des API est également affectée, et les organisations qui ont déterminé qu’elles n’avaient pas besoin de gestion des API parce que les marchés adjacents répondaient suffisamment à leurs besoins découvrent peu à peu qu’avec la prolifération vient la nécessité de se pencher à nouveau sur la gestion des API.</p>'}, {'', '<p>Je reste au courant de la plupart des nouvelles technologies car, en tant qu’analyste du secteur, ce qui arrive peut éclairer mes discussions sur ce à quoi ressemble un espace technologique donné aujourd’hui. Mais même pour moi, qui ne passe pas une grande partie de ma journée à coder, à travailler sur la chaîne d’outils ou à perfectionner la nouvelle plateforme de déploiement, il peut être difficile de rester au courant de tout. Si l’un des principes fondamentaux de DevOps était l’absence de spécialisation et une charge de travail partagée, je pense que tout le monde est désormais d’accord avec ce que certains d’entre nous disaient dès le premier jour : certaines compétences spécialisées sont toujours nécessaires, sauf dans des cas particuliers. Mais nous partageons certainement les charges de travail de manière plus large et beaucoup plus réactive, donc DevOps est absolument un énorme succès. L’ingénierie de fiabilité du site (SRE) étend ce succès mais se concentre davantage sur les opérations. Je ne suis vraiment pas un grand fan. J’aime le fait que DevOps ait voulu fusionner les processus et rendre la gestion des applications plus holistique ; je ne pense pas que la re-diviser soit la meilleure idée qui soit. Comme tout titre, « SRE » est interprété assez largement. Certains rôles impliquent réellement DevOps (mais SRE semble plus intéressant), et d’autres organisations remplacent simplement « développeur » par DevOps et « opérations » par SRE. Cependant, la plupart utilisent SRE pour gérer l’ensemble de l’environnement dans lequel l’application s’exécute.</p>'}, {'', '<p>Où allons-nous ? Un changement constant. Voilà la réponse. J’attends une pause depuis quelques décennies, mais depuis que les machines virtuelles sont devenues populaires, cela n’a pas cessé et cela continuera.</p>'}, {'', '<p>Cela signifie beaucoup pour vous, pour votre viabilité en tant qu’employé et pour le recrutement. Avec la surabondance de personnes sur le marché, nous assistons à une résurgence des offres d’emploi de type « il faut connaître tous les outils de notre chaîne d’outils ». Bien sûr, vous ne devez pas le faire, dans un monde où la combinaison de langages, de cadres, d’outils CI/CD, d’alertes et d’outils de surveillance constituerait une excellente équation combinatoire de niveau universitaire. Vous ajoutez de la valeur en étant adaptable. À la fois pour votre employeur actuel et pour les futurs. L’élément clé que vous devez chercher à démontrer est votre capacité à adopter et à maîtriser de nouvelles technologies lorsque cela est nécessaire. Faites-le dans votre emploi actuel pour montrer votre valeur et gardez-en une trace pour votre prochain emploi. J’ai toujours été responsable du recrutement par intermittence, et lorsque le marché est inondé de bons candidats, c’est le type de différenciateur qui attire l’attention de l’équipe. Faites-le ; faites-en quelque chose qui en vaut la peine et documentez que vous l’avez fait. Vous ne le regretterez pas.</p>'}, {'', '<p>Il y a quelques semaines, nous avons aidé un jeune diplômé à déterminer ce qu’il voulait faire de sa carrière, et le consensus était « évitez l’IA jusqu’à ce qu’elle soit efficace ». Ce n’est pas la première fois que j’entends ce conseil – ce n’est même pas le premier groupe auquel je faisais partie qui le donne. Et je suis d’accord dans l’ensemble, mais dans les endroits où elle est mise en œuvre par tous les fournisseurs du secteur et où elle ajoute de la valeur, il est certain que vous devez savoir comment elle fonctionne et ce que vous pouvez en faire, et un projet est un moyen d’acquérir ces connaissances.</p>'}, {'', '<p>C’est toujours une opportunité en cours, et vous continuez à faire fonctionner les lumières. J’ai vu des gens qui s’inquiètent de ne pas en faire assez. Tout employeur qui mérite d’être embauché veut que vous fassiez fonctionner les systèmes en premier, et tout le reste ensuite. Donc, si vous faites cela, vous en faites assez. Maintenant, allez plus loin. Et continuez à faire du bon travail. Cette nouvelle technologie ne s’est pas encore mise en œuvre et elle ne s’auto-entretient pas (encore)… C’est tout ce que vous avez à faire.</p>'}]"
Comment l'IA générative permet des plateformes de tests continus unifiées,"[{'', '<p>Il est temps de disposer d’une plateforme de tests continus unifiée.</p>'}, {'', ""<p>Les plateformes et outils de test de logiciels existants sont spécialisés dans les activités de test pour un sous-ensemble des différentes étapes des flux de valeur. Le paysage des outils de test de logiciels comprend une large gamme de solutions, chacune avec ses points forts, axées sur des aspects spécifiques du développement, de la livraison et de l'exploitation des logiciels.</p>""}, {'', ""<p>Par exemple\xa0:• Les outils de test unitaire se concentrent sur les premières étapes du développement, permettant aux développeurs de tester des unités de code individuelles pour en vérifier l'exactitude.• Les outils de test d'intégration visent à tester les interactions entre différents modules ou services au sein d'une application.• Les outils de test système sont conçus pour tester de bout en bout l'ensemble du système avant sa mise en service.• Les outils de test de performance évaluent le comportement de l'application dans des conditions de charge et de stress.• Les outils de test de sécurité se concentrent sur l'identification des vulnérabilités au sein de l'application.• Les outils de test d'acceptation utilisateur (UAT) facilitent la phase de test finale, où les utilisateurs finaux valident la solution par rapport à leurs exigences.</p>""}, {'', ""<p>Le défi consistant à réunir ces différents besoins de test au sein d'une seule plateforme de tests continus est multiple. Une telle plateforme doit s'intégrer de manière transparente à une grande variété d'outils et d'environnements de développement, prendre en charge différentes méthodologies de test et être suffisamment flexible pour s'adapter à différents processus organisationnels et normes de qualité.</p>""}, {'', '<p>Bien qu’il existe des outils d’intégration continue/déploiement continu (CI/CD) qui intègrent plusieurs étapes de test, ils le font souvent en s’intégrant à des outils de test spécialisés plutôt qu’en proposant eux-mêmes une solution de test unifiée. Ces outils CI/CD sont plus proches de l’orchestration des différentes activités de test plutôt que de leur unification sous les capacités d’une seule plateforme.</p>'}, {'', '<p>Cependant, le concept d’une plateforme de tests continus entièrement unifiée, couvrant toutes les étapes, depuis les exigences jusqu’au déploiement et aux tests en production, représente une opportunité significative d’innovation dans le domaine des outils de développement logiciel. Pour y parvenir, il faudrait probablement tirer parti des avancées dans des domaines tels que l’IA générative, comme indiqué dans mon précédent blog Application de l’IA/ML aux tests continus pour créer des processus de test adaptables et intelligents capables de couvrir l’ensemble des besoins de test de manière cohérente.</p>'}, {'', ""<h3>Avantages d'une plateforme de tests continus unifiée</h3>""}, {'', ""<p>Efficacité et rapidité améliorées : une plateforme de test unifiée intègre les tests à toutes les étapes du cycle de vie du développement logiciel (SDLC), ce qui simplifie considérablement le processus de test. Cette intégration facilite les boucles de rétroaction automatisées et continues qui identifient et corrigent rapidement les défauts, permettant aux équipes de développement d'itérer et d'améliorer rapidement. En conséquence, les équipes peuvent publier de nouvelles fonctionnalités et correctifs plus rapidement, ce qui accélère la mise sur le marché du produit et améliore la réactivité aux besoins des clients et aux évolutions du marché.</p>""}, {'', ""<p>Qualité et fiabilité améliorées : en garantissant des tests complets et cohérents sur l'ensemble de l'application, une plateforme de test unifiée joue un rôle crucial dans l'identification et l'atténuation des problèmes au début du cycle de développement. Cette détection précoce permet de maintenir un niveau élevé de qualité et de fiabilité du logiciel, ce qui accroît la satisfaction et la confiance des utilisateurs dans le produit. L'application cohérente des normes de qualité à toutes les phases de test contribue à la robustesse et à la fiabilité du produit logiciel.</p>""}, {'', ""<p>Économies de coûts et maintenance réduite : l'automatisation du processus de test via une plateforme CT unifiée optimise non seulement l'utilisation des ressources, mais réduit également considérablement les coûts associés aux tests manuels et à la correction des défauts à un stade avancé. La détection précoce des défauts se traduit par des coûts de réparation inférieurs et le processus rationalisé réduit le délai global de mise sur le marché. De plus, les logiciels fiables et de haute qualité nécessitent moins de maintenance, ce qui réduit encore les coûts à long terme et libère des ressources pour les efforts d'innovation et de développement.</p>""}, {'', ""<p>Collaboration améliorée entre les équipes : une plateforme CT unifiée favorise une culture de collaboration et de transparence entre les équipes de développement, de test et d'exploitation. En fournissant un cadre et des outils communs pour toutes les activités de test, elle élimine les silos et permet une communication et une collaboration transparentes tout au long du cycle de développement logiciel. Cette collaboration améliorée garantit que les équipes sont alignées sur les objectifs du projet, peuvent partager leurs idées plus efficacement et travailler ensemble pour identifier et résoudre les problèmes plus efficacement, ce qui conduit à de meilleurs résultats.</p>""}, {'', '<p>Ensemble, ces avantages soulignent comment une plateforme de tests continus unifiée peut transformer le processus de développement logiciel, le rendant plus efficace, rentable et collaboratif tout en garantissant la livraison de produits logiciels fiables et de haute qualité.</p>'}, {'', '<p>Bien que de nombreux outils traitent de parties spécifiques du cycle de vie des tests, la vision d’une plateforme unique qui unifie de manière transparente toutes les étapes des tests, de la définition des exigences à la production, reste un objectif ambitieux. Il s’agit d’une lacune dans le paysage technologique actuel qui présente à la fois un défi et une opportunité de développement futur.</p>'}, {'', '<h3>Défis pour les plateformes de tests continus unifiées</h3>'}, {'', ""<p>Créer une plateforme de tests continus qui unifie les activités de test pour toutes les étapes du flux de valeur de bout en bout est un défi complexe en raison de plusieurs facteurs :1. Diversité des technologies et des outils : les environnements de développement de logiciels modernes sont très divers, intégrant divers langages de programmation, cadres et technologies. Créer une plateforme qui s'intègre parfaitement à toutes ces technologies est un défi.2. Points d'intégration complexes : les tests continus doivent s'intégrer à plusieurs étapes du pipeline de développement, notamment le développement, le déploiement et les opérations. Chacune de ces étapes peut utiliser des outils et des processus différents, ce qui rend difficile la création d'une solution unique.3. Variables des mesures de qualité : différentes équipes et différents projets peuvent avoir différentes définitions de la qualité, des critères de réussite et des mesures de performance. Une plateforme de tests unifiée doit être hautement personnalisable pour répondre à ces différents besoins.4. Gestion du changement : l'adoption d'une nouvelle plateforme nécessite des changements dans les processus et les flux de travail de l'organisation. La résistance au changement est courante dans les organisations, et la transition vers une nouvelle façon de tester peut être accueillie avec scepticisme et inertie.5. Évolutivité et performances : garantir que la plateforme puisse évoluer pour répondre aux besoins de test des grandes organisations avec des milliers de tests exécutés simultanément est un défi technique. Les problèmes de performances peuvent devenir un goulot d'étranglement, affectant l'efficacité globale du processus de développement. 6. Sécurité et conformité : l'intégration des tests à toutes les étapes du développement introduit également des défis de sécurité et de conformité. La plateforme doit garantir que les données sensibles sont protégées et que les pratiques de test sont conformes aux exigences réglementaires. 7. Contraintes de coûts et de ressources : le développement, la maintenance et la prise en charge d'une plateforme de tests continus unifiée nécessitent un investissement important. Les organisations peuvent hésiter à engager les ressources nécessaires sans preuve claire du retour sur investissement. 8. Évolution des pratiques : les pratiques et les outils de développement de logiciels évoluent constamment. Maintenir la plateforme à jour avec les dernières pratiques et technologies nécessite des efforts et une innovation continus.</p>""}, {'', ""<p>Malgré ces défis, la valeur des tests continus tout au long du cycle de développement logiciel est de plus en plus reconnue. Certaines entreprises et communautés open source progressent dans cette direction, en créant des solutions de test plus intégrées et plus flexibles. Cependant, la mise en place d'une plateforme entièrement unifiée qui réponde à tous ces défis est un effort continu et représente une opportunité significative d'innovation dans le secteur du développement et des tests de logiciels.</p>""}, {""<h3>L'IA générative peut faciliter une plateforme de tests continus unifiée</h3>"", ''}, {'', ""<p>L’IA générative peut jouer un rôle important pour surmonter les défis associés à la création d’une plateforme de tests continus qui unifie les activités de test pour toutes les étapes du flux de valeur de bout en bout. Voici comment l’IA générative peut relever chacun des défis :1. Diversité des technologies et des outils : l’IA générative peut être formée sur un large éventail de langages de programmation, de cadres et de technologies pour comprendre et générer du code ou des scripts de test. Cette capacité lui permet de s’adapter à différents environnements et de créer des supports de test compatibles avec divers outils et technologies.2. Points d’intégration complexes : l’IA peut analyser le flux de travail des pipelines de développement et suggérer des points d’intégration optimaux pour les tests. En apprenant à partir de différentes configurations CI/CD (intégration continue/déploiement continu), l’IA peut recommander les meilleures pratiques pour intégrer les tests de manière transparente dans les flux de travail existants.3. Différentes mesures de qualité : les modèles d’IA générative peuvent être personnalisés pour comprendre et appliquer différentes mesures de qualité et différents critères de réussite en fonction des exigences spécifiques du projet. En s’entraînant sur divers ensembles de données, ces modèles peuvent s’adapter à diverses définitions de la qualité et générer des tests ou des analyses pertinents.4. Français : Gestion du changement : l'IA peut aider au processus de gestion du changement en simulant les résultats de l'adoption de nouvelles plateformes de test, offrant ainsi des avantages fondés sur des preuves et atténuant la résistance au changement. De plus, les analyses pilotées par l'IA peuvent mettre en évidence les gains d'efficacité et les améliorations de qualité pour soutenir la transition. 5. Évolutivité et performances : l'IA générative peut optimiser les processus de test en identifiant les redondances et en suggérant des améliorations, améliorant ainsi les performances. En outre, l'IA peut allouer dynamiquement des ressources en fonction des besoins de test, garantissant l'évolutivité sans compromettre l'efficacité. 6. Sécurité et conformité : les modèles d'IA peuvent être formés pour identifier et signaler les problèmes potentiels de sécurité et de conformité dans le processus de test. En apprenant en permanence des dernières normes de sécurité et réglementations de conformité, l'IA peut contribuer à garantir que les pratiques de test répondent aux exigences nécessaires. 7. Contraintes de coûts et de ressources : en automatisant la génération et l'optimisation des cas de test, l'IA générative peut réduire considérablement l'effort manuel requis, réduisant ainsi les coûts et les demandes de ressources. L'IA peut également aider à hiérarchiser les efforts de test en fonction de l'évaluation des risques, garantissant que les ressources sont concentrées là où elles sont le plus nécessaires. Évolution des pratiques : les modèles d'IA générative sont intrinsèquement adaptables et peuvent apprendre en permanence des nouvelles pratiques de développement, des nouveaux outils et des nouvelles technologies. Cela garantit que la plateforme de test reste à jour avec les dernières avancées en matière de développement logiciel.</p>""}, {'', '<p>L’IA générative a le potentiel de transformer les tests continus en fournissant des solutions adaptatives, efficaces et intelligentes aux défis complexes de l’unification des activités de test sur l’ensemble du flux de valeur de bout en bout. Cependant, la réalisation de ce potentiel nécessite une conception minutieuse, une formation approfondie des modèles d’IA et une gestion continue pour garantir que les systèmes d’IA restent efficaces et adaptés à l’évolution des besoins en matière de tests.</p>'}, {'', ""<h3>Résumé : Appel à l'action</h3>""}, {'', '<p>La nécessité d’une plateforme de tests continus (CT) unifiée n’a jamais été aussi urgente. Les plateformes de tests actuelles, chacune experte dans son domaine, ne couvrent que des fragments du cycle de vie du développement logiciel (SDLC), ce qui conduit à un processus de test décousu et inefficace. Cette fragmentation ralentit non seulement le développement et les livraisons, mais compromet également la qualité et la sécurité du produit final. Le rêve d’une plateforme unique qui intègre de manière transparente toutes les étapes des tests, des exigences au déploiement et aux tests en production, représente un bond monumental vers l’efficacité, la sécurité et la qualité du développement logiciel.</p>'}, {'', '<p>Les défis liés à la création d’une telle plateforme sont multiples, allant de la diversité des technologies et des outils à la nature évolutive des pratiques de développement logiciel. Chaque défi, de l’intégration de technologies diverses et de la gestion du changement au sein des organisations à la garantie de l’évolutivité et de la conformité, ajoute de la complexité au développement d’une plateforme de test unifiée. Pourtant, les avantages potentiels de surmonter ces obstacles sont immenses, promettant une amélioration significative de la vitesse et de la qualité de la livraison des logiciels. La reconnaissance de ces avantages par l’industrie est de plus en plus grande, comme en témoignent les efforts de certaines entreprises et communautés open source qui évoluent vers des solutions de test plus intégrées et plus flexibles.</p>'}, {'', '<p>L’IA générative apparaît comme une lueur d’espoir dans cette quête, offrant des solutions innovantes aux défis multiformes de l’unification des activités de test. En exploitant la puissance de l’IA générative, l’industrie peut faire face à la diversité des outils, intégrer des étapes de test complexes, s’adapter à des mesures de qualité variables, gérer les changements organisationnels, évoluer efficacement, garantir la sécurité et la conformité et évoluer avec les pratiques de développement logiciel. La voie à suivre nécessite un effort concerté, mais les investissements dans les innovations en matière de tests pilotés par l’IA peuvent concrétiser la vision d’une plateforme de tests complète, unifiée et continue. Il ne s’agit pas seulement d’une opportunité d’amélioration, mais d’un appel à l’action pour que l’industrie redéfinisse l’avenir des plateformes d’ingénierie.</p>'}]"
Le cycle de vie IT-DevOps est comme une pyramide qui ne cesse de croître,"[{'', '<p>Ne laissez personne vous dire à quel point la vie informatique est plus facile aujourd’hui. Elle est, à bien des égards, plus amusante et productive… mais elle n’est pas plus facile. En fait, nous faisons plus avec moins, au cube, à ce stade. Ce que je veux dire, c’est qu’il faut considérer le cycle de vie comme une pyramide. La base est la chaîne build++, du VCS à la protection active destinée au public ; la hauteur correspond aux nouvelles technologies et aux nouveaux systèmes qui sont ajoutés quotidiennement, comme les outils SaaS auxquels un comptable vient de souscrire ou les nouvelles architectures qu’une équipe de développement d’une filiale vient de décider d’adopter (mais qui devront éventuellement être maintenues par le service informatique). Et la profondeur correspond aux capacités entièrement nouvelles qui sont requises par les forces du marché ou la réglementation – SBOM est l’exemple type de ce groupe ; soudain, presque tous les fournisseurs vous les proposent dans le cadre de leur produit.</p>'}, {'', '<p>Quel que soit l’axe de croissance, la charge de travail augmente généralement en conséquence. Pensez-y : si un nouveau produit est mis en œuvre, cela a un impact sur la charge de travail du service informatique. Si une nouvelle technologie qui résout un problème est mise en œuvre, elle a également un impact sur la charge de travail. Et si une nouvelle fonctionnalité dans un produit existant est dictée par l’équipe de conformité pour une raison quelconque, cela augmente également la charge de travail. Il ne s’agit là que du côté commercial, et cela n’inclut pas le fait que la plupart des organisations abandonnent régulièrement de nouvelles applications développées de nos jours.</p>'}, {'', '<p>L’automatisation a aidé, mais elle n’a pas encore résolu le problème. Je pense qu’elle le peut, avec du temps et de la concentration, mais dans l’ensemble, la demande de nouvelles technologies pour résoudre des problèmes anciens (« Nous ne pouvons pas tester efficacement » étant répondu par « Voici la génération, l’exécution et le filtrage des résultats automatisés des tests ») a entraîné une augmentation nette de la charge de travail. Bien que beaucoup moins importante qu’elle aurait pu l’être, elle continue d’augmenter. C’est vrai pour l’informatique. L’exemple de test ci-dessus s’applique au développement d’applications et à la sécurité, avec des ensembles de résultats qui n’existaient pas il y a six ans parce que le processus était trop coûteux en heures de travail. Cela s’applique également aux opérations, car la puissance de traitement nécessaire pour effectuer ces tests – et elle est importante dans les environnements de test de grande taille – relève de la responsabilité des opérations.</p>'}, {'', '<p>Vous voulez peindre un tableau sombre ? Non. Il faut être réaliste. Essayer de prendre en compte ce que nous faisons déjà et de saisir l’opportunité de mettre en œuvre de véritables mesures pour gagner du temps. Par exemple, appliquer la même automatisation des tests aux tests (pour la plupart des organisations, je pense aux tests de sécurité) qui étaient déjà effectués réduira le coût de leur réalisation. Utiliser SBoM pour SCA pour contribuer à la conformité existante permet de gagner beaucoup de temps dans la génération de ces informations. Utiliser de nouveaux assistants de codage réduit le temps de codage et (sans doute à ce stade, mais inévitablement, compte tenu du temps disponible) améliore la qualité du code, réduisant ainsi les reprises, qui sont coûteuses, même dans un environnement agile.</p>'}, {'', '<p>Nous développons nos capacités presque quotidiennement. Je suggère simplement que, dans le processus, nous appliquions rétroactivement les bonnes choses pour réduire les frais généraux existants. Et comme toujours, cette grosse bête complexe existe et continue de fonctionner parce que vous êtes là, la nourrissez et la stabilisez. Continuez à la faire fonctionner et gagnez du temps en utilisant intelligemment les nouvelles technologies. Il y a bien plus à faire que ce que vous rationalisez.</p>'}, {'', ""<p>Source de l'image : Photo de Michael Dziedzic sur Unsplash</p>""}]"
Cognition Labs présente Devin AI Software Engineer,"[{'', ""<p>Cognition Labs, une startup dotée de 21 millions de dollars de financement, a présenté cette semaine ce qu'elle décrit comme le premier logiciel d'ingénierie au monde, surnommé Devin, basé sur l'intelligence artificielle (IA).</p>""}, {'', ""<p>Devin est capable de répondre à des commandes textuelles et peut se voir confier des tâches telles que l'évaluation des performances d'une application. Il élaborera ensuite un plan et configurera les outils requis à l'aide de sa propre interface de ligne de commande (CLI), d'un éditeur de code et d'un navigateur grâce auxquels il pourra accéder, lire et comprendre, par exemple, la documentation à l'aide d'un moteur de raisonnement et d'une capacité de planification à long terme basée sur les avancées de l'apprentissage par renforcement.</p>""}, {'', ""<p>Ces capacités permettent à Devin, par exemple, de créer un site Web, d'identifier et de corriger de manière autonome les bogues dans les bases de code, de déployer des applications et même de former d'autres modèles d'IA.</p>""}, {'', ""<p>Selon Cognition Labs, une évaluation comparative de Devin, utilisant un outil SWE-bench qui demande aux agents de résoudre des tâches, a révélé que Devin résolvait correctement 13,86 % des problèmes de bout en bout, dépassant de loin l'état de l'art précédent de 1,96 %. Même en leur donnant les fichiers exacts à éditer, les meilleurs modèles précédents ne pouvaient résoudre que 4,80 % des problèmes.</p>""}, {'', '<p>Mark Hinkle, PDG de Peripety Labs, une société de conseil, a déclaré qu’il était trop tôt pour dire quand Devin pourrait être prêt à être utilisé dans les environnements informatiques d’entreprise, mais les démonstrations ont montré à quel point les progrès rapides de l’IA sont sur le point de transformer la manière dont les logiciels sont créés et construits. L’expérience utilisateur est sensiblement différente de celle de GitHub Copilot ou de CodeWhisperer d’Amazon Web Services (AWS), qui se concentrent davantage sur l’aide aux développeurs pour écrire du code plutôt que sur l’exécution de tâches assignées, a-t-il noté.</p>'}, {'', '<p>On ne sait pas exactement quelle infrastructure est nécessaire pour exécuter Devin, ce qui, compte tenu de la pénurie actuelle d’unités de traitement graphique (GPU), pourrait s’avérer être un facteur limitant l’adoption, du moins à court terme, a ajouté Hinkle.</p>'}, {'', '<p>Indépendamment des capacités actuelles de Devin, auxquelles les ingénieurs logiciels ne peuvent accéder que sur invitation pour le moment, il est clair que les avancées dans les capacités de raisonnement des modèles d’IA permettront aux équipes DevOps d’assigner des tâches à un modèle d’IA comme elles le feraient avec n’importe quel autre membre de leur équipe. L’impact de cette capacité sur la demande d’ingénieurs logiciels reste à voir, mais il reste nécessaire de comprendre ce qu’il faut demander à Devin de créer et de vérifier comment les logiciels qu’il crée ont été construits. La seule chose qui est sûre, c’est que le rythme auquel les logiciels peuvent être créés et déployés est sur le point de s’accélérer.</p>'}, {'', '<p>Il ne fait aucun doute que Microsoft, AWS et d’autres fournisseurs d’outils d’ingénierie logicielle recherchent des capacités similaires. Ce n’est donc peut-être qu’une question de temps avant que les avancées en matière de raisonnement et de planification à long terme ne soient largement appliquées au développement de logiciels.</p>'}, {'', '<p>En attendant, les équipes DevOps pourraient vouloir réévaluer leurs plans stratégiques pour les années à venir, car il devient plus facile de créer des logiciels. Des projets qui auraient pu nécessiter de grandes équipes d’ingénieurs logiciels pourraient bientôt être réalisés par des équipes beaucoup plus petites. En retour, le nombre de projets de développement de logiciels qui pourraient être lancés par des organisations de toutes tailles augmentera à mesure que le travail traditionnellement requis continuera à être éliminé.</p>'}]"
Est-il temps de reconsidérer l’IC ?,"[{'', '<p>DevOps est une technologie ancienne. Beaucoup d’entre vous n’ont jamais développé en cascade et n’ont aucune idée de ce qu’étaient des outils comme Visual Test, et c’est en fait très cool. Bien que connaître d’autres processus et outils aide à prendre des décisions éclairées, il est encore plus important d’être immergé dans les outils qui comptent aujourd’hui. Nous avons mis en place des piles DevOps entières qui permettent le développement, la création, les tests et, de plus en plus, le déploiement d’applications dans un environnement agile.</p>'}, {'', '<p>Comme toujours, les leaders du marché et ceux qui leur font concurrence, les complètent ou servent un sous-ensemble du marché ont grandi et sont devenus des normes. En effet, l’orchestration de la publication d’applications (ARO) était une expression inventée aux débuts de DevOps qui a complètement disparu. Je l’ai entendue récemment et je me suis dit : « Oh, est-ce qu’on ramène ce nom ? » Les fournisseurs sont venus et repartis ; certains ont fusionné avec d’autres produits, et d’autres ont tout simplement disparu. Encore une fois, c’est une bonne chose car le marché, ses besoins et sa clientèle sont tous matures. Bien qu’il soit terriblement douloureux d’être l’utilisateur d’un produit qui disparaît du jour au lendemain, la prévalence de l’open source dans le paysage CI/CD atténue un peu cette douleur. Et le marché s’améliore à mesure que les outils disparaissent et que de nouveaux prennent leur place.</p>'}, {'', '<p>Mais parfois, le mouvement est lent, simplement en raison de l’inertie. La plupart d’entre nous – et par nous, j’entends les services informatiques, et par la plupart, je veux dire beaucoup – utilisons Jenkins. Le nombre semble être en baisse constante, et les responsables de l’outil ont certainement pris des mesures pour améliorer les éléments qui incitent les organisations à envisager d’autres solutions, mais la tendance est là.</p>'}, {'', '<p>Et je pense personnellement que c’est une bonne chose. La stagnation est ce qui a fait de la méthode en cascade une telle faiblesse dans l’informatique d’entreprise. Tout le monde l’a fait, donc c’est une bonne chose. Il en va de même pour les vieilles phrases que nous avons tous entendues au moins dans une variante – « Personne n’a jamais été licencié pour avoir acheté IBM », ou « Personne n’a jamais été licencié pour avoir acheté Oracle », « … Cisco », etc. La réponse évidente à ces déclarations à l’époque, comme aujourd’hui, était : « Eh bien, peut-être qu’ils auraient dû ».</p>'}, {'', '<p>Il en va de même pour Jenkins. C’est un excellent système qui peut tout faire. Et c’est là une partie du problème. Si vous avez de petits projets ou si vous êtes une équipe qui l’utilise de manière autonome avec des exigences légères qui tournent toutes autour de la sortie rapide d’un produit de qualité, Jenkins pourrait bien être trop pour vous. Au début, Jenkins ne l’était pas, mais l’architecture des plug-ins et des années de concurrence et d’évolution des besoins du marché l’ont rendu énorme. Et même s’il existe sans doute une meilleure documentation que n’importe quel autre CI, et incontestablement la plus grande base d’utilisateurs à qui poser des questions, il est toujours lourd à utiliser à la place d’un SaaS qui est préconfiguré et auquel vous pouvez simplement remettre un référentiel contenant des fichiers de build.</p>'}, {'', '<p>De nombreuses entreprises ont besoin de ce que Jenkins est devenu – il ne s’est pas développé dans le vide. Mais il y a beaucoup d’organisations, notamment les nouvelles, les petites organisations agiles ou les équipes autonomes, qui n’en ont tout simplement pas besoin.</p>'}, {'', ""<p>Il existe de nombreuses options, et il vaut la peine de faire le point sur ce que vous essayez de faire avec votre processus de construction/test, et de dresser une liste des éléments requis, puis d'examiner sérieusement ce qui est disponible. Pour certains, la légèreté est une bonne chose.</p>""}, {'', ""<p>Bien sûr, autant que l'on peut l'être à notre époque technologique, je suis minimaliste. J'aime que mon D&D soit une variante allégée en règles, que mes outils de développement aient suffisamment de fonctionnalités sans m'encombrer de choses superflues, que je me moque d'Excel pour avoir inclus des fonctions abstraites et les avoir ensuite vendues aux consommateurs, etc.</p>""}, {'', ""<p>Dans ma société de développement, nous utilisons effectivement Jenkins. Mais nos projets vont de simples projets ponctuels à des systèmes complexes avec plusieurs serveurs et clients, nous pensons donc que c'est la meilleure solution pour cette gamme de variations. Et comme beaucoup d'autres, l'investissement pour déplacer d'anciens projets serait discutable sans un besoin urgent. Non pas que je fasse beaucoup de développement moi-même ces jours-ci, mais je m'y consacre juste assez pour être dangereux. Mais nous avons posé la question et déterminé la réponse. Pour l'instant.</p>""}, {'', '<p>Si vous rencontrez des problèmes avec Jenkins, regardez autour de vous. Il existe des options. Elles ne conviennent pas à tout le monde, mais votre organisation n’est pas non plus adaptée à tout le monde. Trouvez le meilleur outil, pas le plus pratique, et continuez à vous battre.</p>'}]"
Application de l'IA/ML aux tests continus,"[{'', '<p>L’intelligence artificielle (IA) et l’apprentissage automatique (ML) peuvent jouer un rôle transformateur tout au long du cycle de vie du développement logiciel, en mettant l’accent sur l’amélioration des tests continus (CT). Le CT est particulièrement essentiel dans le contexte des pipelines d’intégration continue/déploiement continu (CI/CD), où le besoin de rapidité et d’efficacité doit être équilibré avec les exigences de qualité et de sécurité. L’IA/ML contribue à automatiser les tâches complexes, à prédire les problèmes potentiels avant qu’ils ne surviennent et à fournir des informations exploitables, réduisant ainsi les efforts manuels et permettant une utilisation plus stratégique des ressources humaines. De plus, l’application de l’IA/ML va au-delà de la simple automatisation des tests. Elle englobe la capacité d’apprendre à partir des données, de s’adapter à de nouvelles informations et de s’améliorer au fil du temps. Cette capacité est inestimable pour identifier les modèles, anticiper les vulnérabilités et optimiser les stratégies de test. Dans l’assurance qualité, les outils basés sur l’IA peuvent prédire les domaines les plus susceptibles d’échouer et adapter les efforts de test en conséquence. Dans le domaine de la sécurité, les algorithmes ML peuvent détecter les anomalies qui signifient des menaces potentielles, tandis que dans les opérations, l’IA peut améliorer les mécanismes de rétroaction, conduisant à des systèmes plus résilients et plus réactifs.</p>'}, {'', '<p>Voici comment les technologies d’IA peuvent être appliquées pour réduire les goulots d’étranglement associés aux activités de test dans diverses activités de test\xa0:</p>'}, {'', ""<p>1. Analyse des exigences • Explication : garantit que les scénarios de test correspondent aux exigences commerciales et aux besoins des utilisateurs. • Goulot d'étranglement : une mauvaise interprétation ou une analyse incomplète peut conduire à une couverture de test inadéquate. • Solution IA/ML : le PNL peut automatiser l'extraction et l'interprétation des exigences, garantissant ainsi une couverture de test complète et précise.</p>""}, {'', ""<p>2. Stratégie de test • Explication : décrit l'approche de test, les objectifs et les ressources. • Goulot d'étranglement : une stratégie floue peut conduire à des efforts de test et à une allocation des ressources inefficaces. • Solution IA/ML : l'IA peut analyser les données historiques pour suggérer les stratégies de test les plus efficaces et prédire les besoins en ressources.</p>""}, {'', ""<p>3. Plans de test • Explication : documents détaillés guidant le processus de test, les délais et les responsabilités. • Goulot d'étranglement : les plans inflexibles peuvent avoir du mal à s'adapter aux changements du projet, ce qui entraîne des retards. • Solution IA/ML : les algorithmes d'apprentissage automatique peuvent suggérer des ajustements aux plans de test en fonction des développements en cours du projet et des résultats passés.</p>""}, {'', ""<p>4. Cas de test • Explication : conditions spécifiques dans lesquelles un test est exécuté. • Goulot d'étranglement : développement chronophage et effort de maintenance important. • Solution IA/ML : l'IA peut automatiser la génération de cas de test à partir de documents d'exigences, améliorant ainsi l'efficacité et la couverture.</p>""}, {'', ""<p>5. Scripts de test • Explication : scripts automatisés qui exécutent des cas de test. • Goulot d'étranglement : le développement et la maintenance de scripts peuvent nécessiter beaucoup de ressources. • Solution IA/ML : l'IA peut générer et mettre à jour des scripts de test en fonction des modifications apportées à l'application ou aux cas de test, réduisant ainsi les efforts de maintenance.</p>""}, {'', ""<p>6. Données de test • Explication : Ensembles de données utilisés pendant les tests pour simuler des scénarios réels. • Goulot d'étranglement : Créer, gérer et conserver des données de test précises sont un défi. • Solution IA/ML : L'IA peut automatiser la génération et la gestion des données de test, garantissant ainsi la pertinence et la variété.</p>""}, {'', ""<p>7. Environnement de test• Explication : la configuration dans laquelle les tests sont effectués reflète aussi fidèlement que possible les environnements de production.• Goulot d'étranglement : la configuration et la maintenance des environnements de test sont complexes.• Solution IA/ML : l'IA peut prédire et configurer des environnements de test optimaux en fonction des exigences de test, réduisant ainsi le temps de configuration.</p>""}, {'', ""<p>8. Coordination avec les systèmes dépendants • Explication : garantir que le système testé interagit correctement avec les bases de données et d'autres applications. • Goulot d'étranglement : la gestion des dépendances peut entraîner des retards. • Solution IA/ML : l'IA peut automatiser la détection et la résolution des problèmes d'intégration, améliorant ainsi l'efficacité de la coordination.</p>""}, {'', ""<p>9. Configuration de l'environnement de test • Explication : Configuration de l'infrastructure et des outils nécessaires aux tests. • Goulot d'étranglement : La complexité de la configuration et les conflits de ressources entraînent des retards. • Solution IA/ML : Les algorithmes d'IA peuvent optimiser la configuration de l'environnement, en ajustant automatiquement les ressources selon les besoins.</p>""}, {'', ""<p>10. Configuration de la campagne de test • Explication : organisation et planification d'une série d'exécutions de test. • Goulot d'étranglement : nécessite une planification minutieuse et peut être entravé par des limitations de ressources. • Solution IA/ML : l'IA peut aider à planifier et à hiérarchiser les campagnes de test en fonction de l'analyse des risques et de l'impact.</p>""}, {'', ""<p>11. Exécution des tests • Explication : Le processus d'exécution de cas de test et de scripts, à la fois automatisés et manuels. • Goulot d'étranglement : Prend du temps, en particulier pour les tests manuels. • Solution IA/ML : L'IA peut prioriser l'exécution des tests et identifier les tests instables, simplifiant ainsi le processus.</p>""}, {'', ""<p>12. Rapport de verdict de test • Explication : Déterminer et signaler le résultat des exécutions de test. • Goulot d'étranglement : la détermination manuelle du verdict peut être lente. • Solution IA/ML : l'IA peut interpréter automatiquement les résultats des tests, accélérant ainsi la création de rapports.</p>""}, {'', ""<p>13. Enregistrement des données • Explication : enregistrement des données pertinentes pour le test pour une analyse plus approfondie. • Goulot d'étranglement : une collecte de données étendue peut submerger les ressources. • Solution IA/ML : l'IA peut filtrer et enregistrer intelligemment les données pertinentes, réduisant ainsi le bruit.</p>""}, {'', ""<p>14. Analyse des résultats des tests • Explication : Analyse des résultats des tests pour identifier les défauts et les problèmes. • Goulot d'étranglement : Nécessite beaucoup de temps et d'expertise. • Solution IA/ML : Les algorithmes ML peuvent rapidement identifier les modèles et les anomalies dans les résultats des tests, mettant en évidence les problèmes potentiels.</p>""}, {'', ""<p>15. Rapport des résultats des tests • Explication : communication des résultats aux parties prenantes. • Goulot d'étranglement : la compilation des rapports prend beaucoup de temps. • Solution IA/ML : les outils de reporting automatisés optimisés par l'IA peuvent générer rapidement des rapports perspicaces et complets.</p>""}, {'', ""<p>16. En attente de la résolution des tests échoués par une ressource • Explication : temps d'arrêt en attendant la résolution des problèmes identifiés. • Goulot d'étranglement : arrête la progression des tests. • Solution IA/ML : l'IA peut prédire les domaines susceptibles d'échouer et proposer des correctifs potentiels</p>""}, {'', '<p>L’application de l’IA/ML aux activités de test de logiciels offre de nombreux avantages, mais présente également plusieurs défis. La résolution de ces problèmes nécessite une combinaison de solutions techniques, d’ajustements de processus et de changements culturels.</p>'}, {'', ""<p>1. Manque de compréhension intuitive de l'application testée• Problème : les modèles d'IA/ML peuvent ne pas saisir pleinement le contexte de l'application ou les nuances de ses fonctionnalités, ce qui conduit à des scénarios de test moins efficaces.• Solution : Améliorez les modèles d'IA avec des données contextuelles plus riches et intégrez des boucles de rétroaction où les testeurs peuvent affiner et ajuster les cas de test générés par l'IA. L'utilisation de techniques telles que l'apprentissage par renforcement peut également aider les modèles d'IA à mieux comprendre les contextes d'application au fil du temps.</p>""}, {'', ""<p>2. Répétabilité et cohérence entre les sessions de test• Problème : les tests pilotés par l'IA peuvent générer des résultats différents pour la même entrée au cours de différentes sessions, ce qui complique la cohérence et la traçabilité des tests.• Solution : implémenter le contrôle de version pour les modèles d'IA et leurs données de formation, afin de garantir la cohérence entre les sessions de test. Utiliser des approches déterministes en conjonction avec l'IA pour maintenir un noyau de tests stables et répétables.</p>""}, {'', ""<p>3. Manque de compréhension des tests générés• Problème : les testeurs peuvent avoir du mal à comprendre ou à faire confiance à la logique derrière les cas de test générés par l'IA, ce qui affecte leur capacité à évaluer efficacement les résultats des tests.• Solution : Intégrer des explications dans les modèles d'IA/ML pour fournir des informations sur leurs processus de prise de décision. Favorisez une culture de confiance et de compréhension par l'éducation et la transparence sur le fonctionnement des modèles d'IA.</p>""}, {'', ""<p>4. Couverture des tests• Problème : l'IA/ML risque de ne pas couvrir correctement tous les scénarios de test et de passer à côté de défauts critiques.• Solution : associez l'IA/ML aux méthodes de test traditionnelles pour garantir une couverture complète. Révisez et ajustez régulièrement les critères utilisés par les modèles d'IA/ML pour générer des cas de test, en vous assurant qu'ils correspondent à l'évolution des fonctionnalités et des risques de l'application.</p>""}, {'', ""<p>5. Compatibilité avec différents outils de test • Problème : les modèles d'IA/ML peuvent ne pas s'intégrer de manière transparente aux outils et cadres de test existants, ce qui limite leur utilité. • Solution : développez ou utilisez des solutions d'IA/ML avec une prise en charge API étendue et des capacités d'intégration. Travaillez avec des fournisseurs d'outils ou contribuez à des projets open source pour améliorer la compatibilité.</p>""}, {'', '<p>6. Acceptation par les équipes• Problème : Les testeurs et les développeurs peuvent être sceptiques ou réticents aux tests pilotés par l’IA en raison de craintes de suppression d’emplois ou de méfiance envers l’efficacité de l’IA.• Solution : Former et impliquer les équipes dans le développement et la mise en œuvre de stratégies de test IA/ML. Démontrer la valeur de l’IA/ML pour renforcer leurs rôles plutôt que de les remplacer, en se concentrant sur l’IA comme un outil pour s’attaquer aux tâches banales et leur permettre de se concentrer sur un travail plus complexe et plus gratifiant.</p>'}, {'', ""<p>7. Qualité et disponibilité des données• Problème : les modèles d'IA/ML nécessitent de grandes quantités de données de haute qualité pour l'entraînement. Des données inadéquates ou de mauvaise qualité peuvent conduire à des tests inefficaces.• Solution : investir dans des stratégies de conservation et de génération de données telles que la création de données synthétiques pour garantir que les modèles sont bien entraînés.</p>""}, {'', ""<p>8. Apprentissage et adaptation continus• Problème : les modèles d'IA/ML peuvent devenir obsolètes à mesure que les applications évoluent.• Solution : établir des mécanismes d'apprentissage continu où les modèles sont régulièrement mis à jour avec de nouvelles données et de nouveaux commentaires, garantissant qu'ils restent pertinents et efficaces.</p>""}, {'', ""<p>9. Considérations éthiques et biais • Problème : les modèles de test d'IA/ML peuvent hériter ou amplifier les biais présents dans leurs données d'entraînement, ce qui conduit à des résultats injustes ou discriminatoires. • Solution : mettre en œuvre des lignes directrices éthiques et des méthodologies de détection des biais pour le développement et l'utilisation des modèles d'IA/ML. Vérifiez régulièrement les modèles pour détecter les biais et corrigez-les si nécessaire.</p>""}, {'', '<p>En relevant ces défis avec des stratégies réfléchies, les organisations peuvent maximiser les avantages de l’IA/ML dans les activités de test tout en atténuant les inconvénients potentiels, conduisant à des processus plus efficaces, efficients et dignes de confiance.</p>'}, {'', '<p>Ce blog a expliqué les cas d’utilisation de l’intelligence artificielle (IA) et de l’apprentissage automatique (ML) dans le domaine des tests continus du cycle de vie du développement logiciel. En exploitant la puissance de l’IA/ML, nous avons vu comment les organisations peuvent améliorer considérablement leurs processus de développement, les rendant plus efficaces, plus sûrs et plus réactifs aux besoins des utilisateurs. L’exploration d’applications spécifiques d’IA/ML dans diverses activités a fourni un modèle pour l’automatisation et l’optimisation de tâches qui nécessitaient traditionnellement un effort manuel important.</p>'}]"
Qu’est-ce qui motive les changements dans les licences Open Source ?,"[{'', ""<p>Les projets open source comme Kubernetes ont transformé l'industrie du logiciel grâce à l'aide de communautés coopératives comme la CNCF et à la possibilité de partager rapidement des informations. L'open source a également facilité le lancement de produits et la conquête de parts de marché grâce à des projets de logiciels open source (OSS) pilotés par des fournisseurs, un peu comme un modèle de mise sur le marché (GTM) de ventes premium ou PLG.</p>""}, {'', '<p>Cependant, des années d’innovation et de disruption dans le domaine du SaaS ont menacé la rentabilité des projets open source pilotés par les fournisseurs, obligeant plusieurs acteurs majeurs à s’éloigner du système de licences open source sur lequel leurs entreprises ont été fondées. Ces changements ont non seulement bouleversé la communauté des utilisateurs, mais ont également amené certains à s’interroger sur l’avenir de l’OSS. Il existe cependant des différences fondamentales entre les projets qui ont récemment modifié leur licence pour la rendre plus restrictive et les projets soutenus par des communautés comme la CNCF.</p>'}, {'', '<h3>OSS piloté par les fournisseurs ou piloté par la communauté</h3>'}, {'', ""<p>Dans quelle mesure la communauté d’utilisateurs est-elle impliquée et influente par rapport au fournisseur principal ou original ? C’est une façon de classer les projets open source. Les projets open source pilotés par la communauté sont ceux où la communauté d’utilisateurs joue un rôle principal dans la création, la mise à jour, le support et la sécurisation du logiciel. Les projets Kubernetes, Linux, Apache et CNCF en sont quelques exemples. Ces projets utilisent généralement des licences permissives, telles que GPL, Apache ou MIT, qui permettent aux utilisateurs de modifier, de partager et d’utiliser le logiciel pour n’importe quelle raison sans imposer de limites ou d’exigences. Ces projets suivent également les principes natifs du cloud d’évolutivité, de portabilité, de résilience et d’automatisation. Les projets open source pilotés par des fournisseurs sont ceux où un ou quelques fournisseurs fournissent la plupart des logiciels et de la maintenance. Dans le même temps, la communauté d’utilisateurs peut ajouter des modifications, du support et des correctifs. La principale motivation derrière ces projets est de les monétiser. MongoDB, Elastic, Redis Labs et HashiCorp sont quelques exemples de projets open source pilotés par des fournisseurs. Ces projets utilisent souvent des licences plus restrictives ou conditionnelles, telles que AGPL, SSPL ou BSL, qui restreignent l'utilisation du logiciel dans certaines situations, comme l'offrir en tant que service, ou obliger les utilisateurs à payer des frais ou à partager les modifications. Ces projets peuvent également avoir des fonctionnalités ou des services propriétaires ou non disponibles dans la version open source ou qui sont personnalisés pour des plateformes ou des environnements cloud spécifiques.</p>""}, {'', '<h3>Influence du SaaS sur les projets Open Source</h3>'}, {'', '<p>L’avènement des ressources de cloud computing bon marché et omniprésentes et des connexions haut débit rapides a transformé la manière dont les logiciels sont fournis et consommés. Le logiciel en tant que service (SaaS) est devenu un modèle dominant pour de nombreuses applications, offrant aux utilisateurs commodité, évolutivité et économies de coûts. Cependant, le SaaS pose également un défi pour certains projets open source pilotés par des fournisseurs, car il permet aux fournisseurs de cloud et à d’autres concurrents de proposer le même logiciel ou un logiciel similaire en tant que service sans contribuer au fournisseur d’origine ou au projet d’origine. Cela réduit le potentiel de revenus et l’incitation du fournisseur à continuer d’investir dans le projet open source.</p>'}, {'', ""<p>En réponse aux défis posés par le cloud computing, certains projets open source pilotés par des fournisseurs ont modifié leurs licences ou leurs modèles GTM. Par exemple, MongoDB, Elastic, Confluent, Redis Labs et HashiCorp ont adopté de nouvelles licences qui restreignent l'utilisation de leur logiciel en tant que service par des tiers ou les obligent à payer des frais ou à partager leurs modifications. Ces changements visent à protéger les revenus et la pérennité des fournisseurs d'origine et à garantir qu'ils puissent continuer à investir dans le projet open source. Cependant, ces changements ont également suscité une certaine controverse et des réactions négatives de la part de la communauté des utilisateurs, qui peuvent avoir l'impression que le projet devient moins ouvert et plus propriétaire ou qu'ils perdent certains des avantages et des libertés de l'open source.</p>""}, {'', ""<p>Cependant, les projets open source communautaires ont largement conservé leurs licences permissives et leur approche collaborative. Ces projets bénéficient toujours de la diversité et de l'ampleur de leur communauté d'utilisateurs, qui contribuent au développement, à la maintenance, au support et à la sécurité du logiciel. Ces projets bénéficient également du soutien d'organisations et de fondations, telles que la Linux Foundation, l'Apache Software Foundation et la CNCF, qui fournissent la gouvernance, le financement et l'infrastructure. Ces projets peuvent également proposer des fonctionnalités ou des services premium, tels que des certifications, des formations ou des conseils, pour générer des revenus et pérenniser le projet open source.</p>""}, {'', '<h3>Résumé</h3>'}, {'', '<p>Il existe de nombreux types de projets, de licences, de communautés et de modèles commerciaux différents. Si certains projets open source pilotés par des fournisseurs ont rencontré des difficultés pour couvrir les coûts de développement et ont modifié leurs licences pour les rendre plus restrictives, les projets pilotés par des communautés ont en grande partie conservé leurs licences permissives et leur approche collaborative, car les incitations à la base de ces projets ne sont pas motivées par le profit. L’open source est et restera un élément essentiel et précieux de l’industrie technologique, permettant l’innovation, la collaboration et l’interopérabilité.</p>'}, {'', ""<p>Pour en savoir plus sur les sujets liés au cloud natif, rejoignez la Cloud Native Computing Foundation, le Techstrong Group et l'ensemble de la communauté cloud native à Paris, en France, lors du KubeCon+CloudNativeCon EU 2024, du 19 au 22 mars 2024.</p>""}]"
Google a raison d'avoir peur,"[{'', '<p>Dans le grand schéma des choses, du moins en ce qui concerne l’évolution de l’IA, les récents échecs de Google surviennent à un moment plutôt inopportun, mais les préoccupations qui les ont poussés à aborder la controverse sont pires que ce que la plupart des gens pensent.</p>'}, {'', '<p>L’une des choses que l’IA générative peut faire vraiment bien est de répondre à des questions sur les informations disponibles sur Internet. Bien que cela ne semble pas avoir été l’objectif – ou du moins pas le seul – de ceux qui nous ont apporté l’IA générative, c’est l’un des résultats. Ou, plus précisément, remplacer la recherche ne semble pas avoir été l’objectif.</p>'}, {'', '<p>Mais cela va vite. S’il est rapide et facile de taper quelques mots et d’obtenir une liste de résultats – au point que la recherche sur Internet porte aujourd’hui un nom simple – pour de nombreuses questions, il suffit de taper une phrase, d’obtenir une réponse, d’en taper encore plus et d’obtenir une réponse plus précise. Oh, il y a beaucoup de risques à faire confiance à une IA pour résumer les choses à votre place (comme nous l’avons vu à plusieurs reprises, auprès de plusieurs fournisseurs) – mais ce risque existe qu’il s’agisse d’une IA ou non, et il est résolu en insistant sur les liens sources pour toute réponse basée sur des faits. Vous raconter une histoire n’a pas besoin de liens ; vous parler d’un produit en a absolument besoin.</p>'}, {'', '<p>Nous le voyons un peu dans les moteurs de recherche aujourd’hui, avec un bref résumé en haut de la page qui s’appuie sur une source que le moteur a jugée fiable, et la liste des liens que les moteurs de recherche ont toujours renvoyés en dessous. L’IA peut faire cela de manière plus approfondie et, lorsqu’elle est gérée de manière responsable (contrairement à ce que Google a fait jusqu’à présent), elle peut créer des réponses de meilleure qualité que la simple citation de la source la plus fiable ou la plus fiable.</p>'}, {'', ""<p>La capacité de recherche sera intégrée à une grande variété de produits, ce qui signifie que le backend de cette recherche sera laissé à des produits comme Google et Bing. Chaque faux pas d'un fournisseur poussera les créateurs de produits à vouloir davantage de contrôle sur leur côté de l'IA, et l'incapacité à répondre à ces besoins poussera les implémenteurs de produits et les clients à s'éloigner des fournisseurs de backend.</p>""}, {'', '<p>Mon conseil ? Observez et attendez. Essayez d’utiliser l’IA pour effectuer des recherches ; elle fonctionne très bien pour renvoyer des informations spécifiques et, lorsque vous y êtes invité, des liens également. Ce voyage va être semé d’embûches, et tout indique que l’économie à court terme ne tolérera pas beaucoup d’erreurs. Les fournisseurs vont et viennent, les produits seront meilleurs dans un domaine et moins bons dans un autre. Et pour protéger l’organisation, ne vous laissez pas emporter par la tendance à investir dans l’IA pour chaque produit. C’est l’avenir, mais il y a beaucoup de rebondissements sur le chemin entre ici et là.</p>'}, {'', '<p>Notez un aspect spécifique pour lequel l’organisation a besoin d’aide et pour lequel l’IA pourrait être en mesure de l’aider, puis recherchez des solutions valables. N’achetez pas des produits simplement parce qu’ils incluent l’IA générative : ayez un objectif précis. Et faites attention à ce que font vos fournisseurs actuels.</p>'}, {'', '<p>Google n’est pas encore au bord du gouffre : l’entreprise a plus d’argent que la plupart des autres pays. Mais elle a un gros retard et devra travailler pour regagner la confiance des consommateurs. Pour certains, il n’existe aucun moyen de regagner la confiance des consommateurs, et Google devra l’accepter.</p>'}, {'', '<p>Et nous devons être pragmatiques. Vous travaillez pour une entreprise, alors prenez des décisions commerciales en fonction de ce qui est le mieux pour l’entreprise, peu importe ce que font les autres. Continuez à créer des applications étonnantes et gardez le sourire, car l’IA finira par vous faciliter la tâche.</p>'}]"
Techstrong Research PulseMeter : la mise en cache transforme les performances des applications,"[{'', '<p>La demande d’accès instantané aux données et d’expériences numériques fluides n’a jamais été aussi forte. Le récent rapport PulseMeter de Techstrong Research, « Database Caching Hits the Mainstream », se penche sur l’adoption de la mise en cache des bases de données et son rôle essentiel dans l’amélioration des performances et de l’évolutivité des applications.</p>'}, {'', ""<p>Alors que le volume et l'utilisation des données augmentent à un rythme sans précédent, les développeurs de logiciels et les professionnels des bases de données recherchent constamment des solutions innovantes pour améliorer la lecture/écriture afin d'offrir une efficacité et des performances optimales. La mise en cache des bases de données est passée de son statut de niche à celui de stratégie courante pour les organisations qui souhaitent accroître les performances des applications, relever les défis de l'évolutivité et améliorer la résilience des applications et des bases de données dans des conditions de fonctionnement fluctuantes.</p>""}, {'', ""<p>Les entreprises numériques et les attentes des clients rendent les applications plus nécessaires pour fournir un accès aux données en temps réel ou quasi réel. Cette nécessité est particulièrement aiguë dans les applications mobiles et Web, les environnements de données distribués, l'architecture de microservices cloud-native et les cas d'utilisation à forte demande comme les jeux, qui nécessitent tous des niveaux de performance et de fiabilité des bases de données sans précédent.</p>""}, {'', '<p>Le rapport PulseMeter souligne que l’amélioration de la latence et des performances grâce à la mise en cache des bases de données peut considérablement accroître les revenus et favoriser des expériences numériques positives. À l’inverse, l’absence de mise en œuvre de ces améliorations peut faire la différence entre le succès et l’échec d’une organisation. La mise en cache des bases de données est passée d’une technique spécialisée pour des cas d’utilisation spécifiques à un aspect fondamental de l’optimisation des performances des bases de données et des applications. Elle est désormais considérée comme une technologie fiable et éprouvée qui répond aux défis croissants liés au volume de données et aux exigences analytiques des applications contemporaines.</p>'}, {''}, {'', '<p>Les données recueillies par Techstrong Research auprès de DevOps, de développeurs de logiciels, de professionnels des bases de données, d’ingénieurs SRE, d’ingénieurs de plateforme et d’autres parties prenantes révèlent des informations essentielles sur l’adoption et l’impact de la technologie de mise en cache. 64,8 % des répondants utilisent déjà la mise en cache, et 13,8 % supplémentaires évaluent son intégration dans leurs opérations. Cette adoption significative souligne le rôle essentiel de la mise en cache dans l’infrastructure informatique actuelle, avec des solutions de pointe comme Amazon ElastiCache, Redis et NGINX détenant collectivement plus de 40 % des parts de marché.</p>'}, {'', '<p>Parmi les technologies à l’origine de l’essor de la mise en cache des bases de données, Redis est bien connu pour sa polyvalence et ses performances. Disponible à la fois en version open source et commerciale, Redis est parfaitement adapté à un large éventail de cas d’utilisation, de la mise en cache de base aux structures de données avancées, aux opérations, aux analyses en temps réel et à la gestion des files d’attente. Ce qui distingue Redis, ce sont les améliorations de performances qu’il offre, permettant des applications en temps réel dans les jeux, les services financiers, la santé et d’autres secteurs. Le fort taux d’adoption de Redis, comme le souligne le rapport PulseMeter de Techstrong Research, souligne son rôle central dans l’acceptation et la mise en œuvre généralisées des technologies de mise en cache des bases de données.</p>'}, {'', '<p>L’étude identifie les performances et la fiabilité comme les principales considérations pour les entreprises qui explorent de nouvelles technologies de mise en cache. De plus, il existe une demande croissante d’expertise externe pour obtenir une conception et une mise en œuvre optimales de la mise en cache, ce qui reflète la complexité et les connaissances spécialisées requises pour naviguer efficacement dans ce domaine.</p>'}, {'', '<p>Le rapport PulseMeter de Techstrong Research souligne le rôle essentiel de la mise en cache des bases de données dans la prise en charge des applications en temps réel et de l’expérience numérique. Alors que les entreprises sont confrontées aux défis de la transformation numérique et à la croissance exponentielle des données, les technologies de mise en cache s’imposent comme des outils essentiels pour améliorer les performances des bases de données, garantir l’évolutivité et répondre aux exigences toujours croissantes des applications et services modernes. Ce passage d’une solution de niche à une nécessité grand public marque une évolution significative des stratégies de gestion des bases de données, soulignant la nécessité d’une innovation et d’une expertise continues dans l’application des technologies de mise en cache.</p>'}, {'', '<p>Remarque\xa0: Redis a sponsorisé le rapport PulseMeter de Techstrong Research, «\xa0La mise en cache des bases de données devient courante\xa0».</p>'}]"
"Piloter l'évolution de DevOps : ArgoCD, Tekton et les migrations transparentes","[{'', ""<p>Le paysage des frameworks CI/CD a évolué au cours des dernières années. Depuis les débuts de Flux et Jenkins jusqu'à l'accent actuel mis sur ArgoCD, ce parcours reflète la dynamique du secteur en faveur de déploiements efficaces et fiables.</p>""}, {'', ""<p>Ces derniers temps, de nombreuses plateformes de distribution de logiciels modernes ont adopté ArgoCD ou Flux pour automatiser les déploiements au sein de leurs écosystèmes. Cependant, les déploiements multicloud et l'évolutivité restent un défi et nous constatons que le secteur évolue vers des frameworks natifs Kubernetes plus puissants comme Tekton.</p>""}, {'', '<h3>Flux : ouvrir la voie à GitOps</h3>'}, {'', ""<p>Flux a été l'un des pionniers de la promotion de la méthodologie GitOps. Elle permettait aux développeurs de gérer l'infrastructure et les déploiements de manière déclarative via des manifestes contrôlés par version.</p>""}, {'', ""<p>GitOps, tel qu'introduit par Flux, a mis l'accent sur l'utilisation des référentiels Git comme source unique de vérité pour la configuration, rendant les déploiements plus fiables et traçables.</p>""}, {'', '<h3>Jenkins : le cheval de bataille du CI/CD</h3>'}, {'', ""<p>Grâce à son extensibilité et à son vaste écosystème de plugins, Jenkins est rapidement devenu la solution CI/CD de référence pour de nombreuses organisations. Bien que Jenkins soit puissant, sa complexité et les défis liés à la maintenance des pipelines ont conduit à l'exploration de solutions alternatives.</p>""}, {'', '<h3>Spinnaker : déploiements à grande échelle</h3>'}, {'', '<p>Spinnaker est devenu un outil puissant pour la livraison continue, en mettant l’accent sur l’évolutivité et les déploiements multi-cloud.</p>'}, {'', ""<p>Sa prise en charge des versions Canary, des déploiements bleu-vert et de l'orchestration des pipelines a attiré les organisations visant des stratégies de déploiement plus sophistiquées.</p>""}, {'', ""<h3>ArgoCD : l'essor de GitOps</h3>""}, {'', ""<p>ArgoCD a apporté un nouvel accent sur GitOps, en s'appuyant sur les bases posées par Flux. Il a fourni une méthode déclarative de gestion des applications Kubernetes, garantissant que l'état souhaité dans le référentiel Git est continuellement synchronisé avec l'état du cluster.</p>""}, {'', '<p>L’adoption d’ArgoCD a été généralisée en raison de sa simplicité, de son évolutivité et de sa capacité à se synchroniser avec plusieurs clusters.</p>'}, {'', '<p>De nombreuses plateformes de distribution de logiciels « modernes » ont intégré ArgoCD ou y ont été intégrées séparément pour tenter de mettre en place une approche complète de distribution de logiciels. Cette intégration permet aux utilisateurs d’adopter les principes GitOps de manière transparente dans leurs flux de travail existants.</p>'}, {'', '<p>Voici un aperçu de ce à quoi ressemble le travail avec ArgoCD sur ces plateformes\xa0:</p>'}, {'', '<ol>Tout d’abord, vous devrez orchestrer le CI séparément, ce qui conduit à un changement de contexte et à un flux de travail CI/CD non unifié. Vous devrez exploiter les plugins sur la plupart de ces plates-formes et faire des allers-retours entre l’exploration des étapes et leur ajout à votre pipeline pour quelques autres. Pour le CD, il est essentiel que vous connectiez votre instance ArgoCD en saisissant l’URL du serveur. Cela nécessite une compréhension pratique d’ArgoCD et également une configuration fonctionnelle de celui-ci avant même de vous inscrire à ces plates-formes DevOps. ArgoCD doit être défini comme une source d’artefact et ses étapes spécifiques doivent être ajoutées comme étapes de déploiement lors de la configuration d’un pipeline. D’autres plates-formes nécessitent que vous incluiez des étapes ou des commandes dans les fichiers YAML pour interagir avec ArgoCD.</ol>'}, {'', '<li>Tout d’abord, vous devrez orchestrer le CI séparément, ce qui entraîne un changement de contexte et un flux de travail CI/CD non unifié. Vous devrez exploiter les plugins sur la plupart de ces plateformes et faire des allers-retours entre l’exploration des étapes et leur ajout à votre pipeline pour quelques autres.</li>'}, {'', ""<li>Pour le CD, il est essentiel de connecter votre instance ArgoCD en saisissant l'URL du serveur. Cela nécessite une compréhension pratique d'ArgoCD et également une configuration fonctionnelle de celui-ci avant même de vous inscrire à ces plateformes DevOps.</li>""}, {'', ""<li>ArgoCD doit être défini comme une source d'artefact et ses étapes spécifiques doivent être ajoutées en tant qu'étapes de déploiement lors de la configuration d'un pipeline. D'autres plates-formes nécessitent que vous incluiez des étapes ou des commandes dans les fichiers YAML pour interagir avec ArgoCD.</li>""}, {'', ""<p>Bien que l'automatisation de GitOps puisse être obtenue en intégrant ArgoCD aux plateformes DevOps, il convient de noter qu'il ne s'agit pas de flux de travail natifs. Des défis surgiront, tels que :</p>""}, {'', ""<ol>Une synchronisation incorrecte entre le référentiel et l'état de déploiement sur ces plateformes peut entraîner des déploiements involontaires ou des dérives de configuration. Les pratiques GitOps, lorsqu'elles sont associées à certaines plateformes, peuvent avoir des limites dans la gestion des déploiements multicloud ou cloud hybride. La complexité augmente avec la taille de l'infrastructure. Plus le nombre de clusters et d'environnements est élevé, plus le nombre de pipelines est élevé et plus l'orchestration est désordonnée. Le fait qu'ArgoCD doive être configuré et exécuté avant même de souscrire à des plateformes DevOps va à l'encontre de l'objectif d'un DevOps low-code/no-code.</ol>""}, {'', ""<li>Une synchronisation incorrecte entre le référentiel et l'état de déploiement sur ces plates-formes peut entraîner des déploiements inattendus ou des dérives de configuration.</li>""}, {'', ""<li>Les pratiques GitOps, lorsqu'elles sont couplées à certaines plateformes, peuvent avoir des limites dans la gestion des déploiements multi-cloud ou cloud hybride.</li>""}, {'', ""<li>La complexité augmente avec la taille de l'infrastructure. Plus le nombre de clusters et d'environnements est élevé, plus le nombre de pipelines est important et plus l'orchestration est désordonnée.</li>""}, {'', ""<li>Le fait qu'ArgoCD doive être configuré et exécuté avant même de souscrire à des plateformes DevOps va à l'encontre de l'objectif d'un DevOps low-code/no-code.</li>""}, {'', ""<p>Alors que le secteur cherche des solutions pour relever les défis posés par les cadres CI/CD existants, Tekton apparaît comme une alternative prometteuse. Des passionnés de technologie de premier plan dans le domaine DevOps, comme le cofondateur de CloudBees, Sacha Labourey, et la dynamique du marché montrent clairement qu'un nombre important d'organisations travaillent à l'intégration de Tekton dans leurs plateformes, tout comme nous l'avons vu avec ArgoCD il y a quelques années.</p>""}, {'', '<p>Ce qui est incroyable, c’est que nous avions déjà des plateformes comme Ozone, qui ont commencé à travailler à la préparation de Tekton pour l’entreprise il y a presque trois ans et qui ont encore évolué au fil des ans.</p>'}, {'', '<h3>Tekton\xa0: CI/CD natif pour conteneurs</h3>'}, {'', '<p>Tekton est un framework natif Kubernetes pour la création de systèmes CI/CD. Il fournit des ressources personnalisées pour définir des pipelines CI/CD sous forme de code, ce qui permet une plus grande flexibilité et une plus grande extensibilité.</p>'}, {'', ""<p>Cela se fait en introduisant le concept de tâches qui s'exécutent dans un pipeline. Ces tâches sont réutilisables et prédéfinies, et peuvent toutes être visualisées dans le hub Tekton.</p>""}, {''}, {'', ""<p>Ce qu'ArgoCD réalise pour GitOps est réalisé avec un seul des centaines de modèles de tâches fournis par Tekton.</p>""}, {''}, {'', ""<p>Trop beau pour être vrai ? Eh bien, à chaque arc-en-ciel sa pluie ! Migrer d'un framework vers un autre est une tâche énorme, et c'est précisément la raison pour laquelle de nombreuses entreprises évitent la migration.</p>""}, {'', '<h3>Les LLM peuvent-ils aider à automatiser la migration des pipelines existants vers Tekton\xa0?</h3>'}, {'', ""<p>Si vous regardez le diagramme ci-dessous, la réponse est un oui retentissant. Le modèle est formé en fonction des pipelines existants et de l'apparence des pipelines Tekton correspondants.</p>""}, {''}, {'', '<p>L’objectif principal de ce flux de travail est de faciliter un processus de conversion transparent à l’aide de grands modèles linguistiques (LLM) pour l’IA générative.</p>'}, {'', ""<p>Au cœur de cette approche se trouve l'utilisation du contexte de chat et l'application stratégique des rôles pour les modèles LLM afin d'établir le contexte. De plus, des exemples de pipelines sont exploités pour former et affiner le modèle afin de définir à quoi ressemblent les fichiers YAML correspondants de toutes ces plateformes pour un pipeline donné.</p>""}, {'', '<p>Voici une lecture plus détaillée présentée par Ozone en première dans l’industrie l’année dernière à KubeCon North America.</p>'}, {'', '<p>L’évolution des frameworks CI/CD reflète la nature dynamique du secteur DevOps et la demande de pipelines de distribution de logiciels efficaces. De Flux et Jenkins à ArgoCD et Tekton, chaque framework répond aux défis du déploiement.</p>'}, {'', ""<p>ArgoCD a gagné en popularité grâce à ses principes GitOps et à son intégration à Kubernetes, mais il est confronté à des obstacles en matière de synchronisation, de prise en charge multicloud et d'évolutivité. Tekton, un framework CI/CD natif de Kubernetes, offre une évolutivité, une architecture pilotée par les événements et des pratiques natives du cloud pour compenser ces lacunes.</p>""}, {'', ""<p>Qu'il s'agisse d'adopter ArgoCD ou d'explorer Tekton, l'objectif est de disposer de pipelines résilients et évolutifs favorisant l'innovation et l'accélération des délais de mise sur le marché.</p>""}, {'', ""<p>Pour en savoir plus sur les sujets liés au cloud natif, rejoignez la Cloud Native Computing Foundation, le Techstrong Group et l'ensemble de la communauté cloud native à Paris, en France, lors du KubeCon+CloudNativeCon EU 2024, du 19 au 22 mars 2024.</p>""}]"
Le grand débat : l’avenir des humains et des logiciels à l’ère de l’IA,"[{'', '<p>Il est à la fois passionnant pour la technologie et effrayant pour les humains d’écouter les débats des deux côtés sur la mesure dans laquelle l’IA prendra le relais des tâches de développement, de livraison et de support des logiciels et sur son impact sur les emplois.</p>'}, {'', '<p>AI Advocate : Bienvenue dans le futur ! Avec les capacités actuelles de l’IA, nous envisageons un monde où les applications logicielles peuvent pratiquement se développer, se livrer et se prendre en charge elles-mêmes. Le besoin d’implication humaine diminue rapidement. AI Antagonist : C’est une vision trop optimiste. Vous sous-estimez la valeur de la créativité humaine, de l’empathie et du jugement éthique. Il y a des domaines dans lesquels l’IA ne peut tout simplement pas remplacer la touche humaine. AI Advocate : Pensez à la conception et à la spécification. L’IA peut générer des prototypes et suggérer des améliorations. Il s’agit d’efficacité et de tirer parti de l’IA pour réduire le travail de base. AI Antagonist : Mais qui définit la vision ? L’IA n’a pas la compréhension des besoins des utilisateurs et des demandes du marché que seuls les humains possèdent. La prise de décision stratégique et la compréhension nuancée de la psychologie des utilisateurs sont irremplaçables. AI Advocate : C’est vrai, mais le rôle de l’IA dans la supervision et la formation ne peut être négligé. Elle peut gérer ses opérations, n’ayant besoin que d’une surveillance humaine minimale pour mettre à jour les données et les règles. AI Antagonist : Et pourtant, qui garantit que ces systèmes d’IA sont conformes aux normes éthiques et aux pratiques actuelles ? Les humains sont essentiels pour superviser ces systèmes, s’assurer qu’ils fonctionnent comme prévu et ne dévient pas vers un territoire contraire à l’éthique. AI Advocate : Je vous l’accorde. Cependant, dans l’assurance qualité et les tests, l’IA réduit considérablement la charge de travail humaine en automatisant de nombreux processus. AI Antagonist : L’automatisation a sa place, mais la perspicacité humaine est essentielle pour identifier les problèmes complexes et comprendre l’expérience utilisateur d’une manière que l’IA ne peut pas. Les tests exploratoires et d’utilisabilité ont une profondeur que l’IA ne peut pas atteindre. AI Advocate : La surveillance éthique et juridique pourrait être votre point fort. L’IA a du mal à prendre des décisions nuancées en matière de confidentialité, de sécurité et de partialité. Je reconnais que les humains sont nécessaires pour naviguer dans ces domaines complexes. AI Antagonist : Et n’oubliez pas le support client. L’IA peut gérer les requêtes de routine, mais les problèmes complexes et les questions sensibles nécessitent une intervention humaine. La touche personnelle est quelque chose que l’IA ne peut pas reproduire. AI Advocate : L’innovation est un effort collaboratif, je l’admets. L’IA aide, mais la créativité humaine favorise l’amélioration continue et s’adapte aux tendances du marché.</p>'}, {'', '<h3>Quelles sont les implications si le défenseur de l’IA a raison ?</h3>'}, {'', '<p>Si la vision des défenseurs de l’IA se concrétise, les conséquences pour l’humanité pourraient être profondément transformatrices. Tout d’abord, l’automatisation des tâches routinières et complexes annoncerait une nouvelle ère d’efficacité et de productivité, libérant potentiellement les humains de la corvée d’un travail monotone. Ce changement pourrait conduire à une concentration accrue sur les tâches créatives, stratégiques et interpersonnelles qui exploitent les forces humaines et favorisent l’innovation. En outre, l’adoption généralisée de l’IA dans le développement de logiciels pourrait réduire considérablement le temps et les ressources nécessaires pour mettre de nouvelles technologies sur le marché, accélérant le rythme des progrès technologiques et résolvant potentiellement des problèmes critiques dans des domaines tels que la santé, l’éducation et la protection de l’environnement. Cependant, cet avenir suscite également des inquiétudes quant aux suppressions d’emplois et à la nécessité d’ajustements sociétaux pour gérer la transition, soulignant l’importance de la requalification et de l’éducation pour préparer la main-d’œuvre à un nouveau paysage économique dominé par l’IA et l’automatisation.</p>'}, {'', '<h3>Quelles sont les implications si l’antagoniste de l’IA a raison ?</h3>'}, {'', '<p>D’un autre côté, si le point de vue de l’antagoniste de l’IA est maintenu, les implications soulignent la valeur indispensable de l’implication humaine dans la technologie. Ce scénario suggère que malgré les progrès de l’IA, la compréhension nuancée, le jugement éthique et les capacités créatives de résolution de problèmes des humains restent essentiels. Il met en évidence les risques potentiels d’une dépendance excessive à l’IA, notamment les dilemmes éthiques, les préoccupations en matière de confidentialité et la perpétuation de préjugés si l’intelligence humaine n’est pas soigneusement surveillée. L’accent mis sur les compétences et la créativité humaines pourrait conduire à une approche plus équilibrée du développement technologique, où l’IA est utilisée comme un outil pour augmenter les capacités humaines plutôt que pour les remplacer. Cet avenir donnerait la priorité au développement de technologies qui sont non seulement innovantes mais aussi éthiques, centrées sur l’utilisateur et socialement responsables, garantissant que le progrès technologique profite à l’ensemble de la société.</p>'}, {'', '<h3>Un avenir équilibré</h3>'}, {'', '<p>En fin de compte, la vérité se situe probablement quelque part entre ces deux points de vue, suggérant un avenir où l’IA et les capacités humaines sont étroitement liées. Les implications d’une telle approche équilibrée seraient un écosystème collaboratif dans lequel l’IA renforce les efforts humains, conduisant à des niveaux de productivité et d’innovation sans précédent tout en créant de nouvelles opportunités pour les travailleurs humains. Cet avenir met l’accent sur l’importance de l’adaptabilité, de l’apprentissage tout au long de la vie et des considérations éthiques comme piliers centraux du progrès technologique. Il souligne également la nécessité de politiques et de systèmes éducatifs qui soutiennent la main-d’œuvre dans la transition vers des rôles qui tirent parti des forces humaines en matière de créativité, d’empathie et de réflexion stratégique, en veillant à ce que les avantages de l’IA soient répartis équitablement dans la société.</p>'}, {'', ""<p>Source de l'image : https://vecteezy_futuristic-technology-concept-mixed-media-innovations-data_7019122_167-3.jpg</p>""}]"
Enquête : changement significatif dans les préférences en matière d'outils IaC,"[{'', ""<p>Une enquête menée auprès de 350 professionnels de l'informatique par Firefly, un fournisseur d'une plateforme de gestion des environnements de cloud computing, suggère que l'utilisation d'outils d'infrastructure en tant que code (IaC) évolue rapidement à l'ère du cloud computing.</p>""}, {'', '<p>Par exemple, plus de 40 % des personnes interrogées ont indiqué qu’elles utilisaient déjà, dans une certaine mesure, OpenTofu, un outil IaC open source lancé en tant que fork de Terraform IaC à la suite de modifications des conditions de licence du logiciel mis à disposition par HashiCorp. Plus de la moitié d’entre elles ont déclaré qu’elles prévoyaient d’utiliser OpenTofu à l’avenir. En comparaison, 60 % utilisent Terraform aujourd’hui, mais seulement un peu plus de 20 % ont déclaré qu’elles prévoyaient d’utiliser Terraform à l’avenir.</p>'}, {'', ""<p>Les changements de licence mis en œuvre par HashiCorp n'affectent que les entités tierces au sein d'une plateforme tierce. Cependant, l'enquête indique que le nombre d'utilisateurs finaux affectés par ce changement est plus important que prévu. 56 % des personnes interrogées ont qualifié les changements apportés à la licence Terraform de perturbateurs.</p>""}, {'', ""<p>L'enquête révèle également une plus grande diversité en termes de types d'outils IaC utilisés, les outils IaC de Pulumi et le framework open source Crossplane, développés sous les auspices de la Cloud Native Computing Foundation (CNCF), gagnant également du terrain. Le rapport révèle que plus de 40 % des personnes interrogées utilisent également Pulumi, et plus de la moitié d'entre elles prévoient d'utiliser cet outil IaC à l'avenir.</p>""}, {'', '<p>Crossplane, quant à lui, est désormais utilisé par 40 % des répondants, et 60 % d’entre eux prévoient de l’adopter.</p>'}, {'', '<p>A l’inverse, le rapport indique également que le nombre d’organisations qui envisagent de migrer d’Amazon Web Services (AWS) vers CloudFormation est en hausse. Alors que plus de 60 % d’entre elles utilisent aujourd’hui AWS CloudFormation, seulement un tiers d’entre elles prévoient de l’utiliser à l’avenir.</p>'}, {'', ""<p>Cindy Blake, vice-présidente du marketing chez Firefly, a déclaré qu'à mesure que les entreprises utilisent des plateformes cloud supplémentaires, les équipes informatiques recherchent clairement des outils qui facilitent la centralisation de la gestion de plusieurs plateformes informatiques. Cette tendance ne fera que s'accélérer à mesure que de plus en plus d'équipes DevOps adopteront l'ingénierie de plateforme comme méthodologie de gestion de la création et du déploiement d'applications à grande échelle, a-t-elle ajouté.</p>""}, {'', '<p>En fait, comme l’a noté Blake, il est déjà assez courant pour les équipes informatiques d’utiliser deux ou plusieurs frameworks. Le rapport révèle que 57 % des personnes interrogées travaillent pour des organisations qui utilisent deux ou plusieurs frameworks. Dans l’ensemble, le rapport révèle que plus de 64 % des personnes interrogées ont également codifié plus de la moitié de leurs actifs cloud.</p>'}, {'', '<p>Selon Blake, le nombre d’organisations qui utilisent des outils IaC pour gérer l’infrastructure par programmation ne fera qu’augmenter. Le rapport révèle par exemple que les trois quarts des répondants utilisent déjà ou prévoient d’utiliser des outils IaC pour configurer des environnements d’application SaaS (Software-as-a-Service). À plus long terme, les progrès de l’intelligence artificielle (IA) permettront aux organisations d’atteindre plus facilement cet objectif. Des outils open source tels qu’AiAC, par exemple, permettent déjà d’exploiter des modèles de langage volumineux (LLM) pour générer du code. Ces types d’outils élimineront à terme la nécessité de maîtriser des langages de programmation spécifiques pour provisionner l’infrastructure informatique sous forme de code, a noté Blake.</p>'}, {'', '<p>On ne sait pas encore clairement comment les outils IaC évolueront dans le contexte du workflow DevOps, mais une chose est sûre : le nombre de types d’outils adoptés n’a jamais été aussi diversifié.</p>'}]"
Oubliez le Shift Left : pourquoi le « No Shift » est l’avenir de l’innovation logicielle,"[{'', '<p>L’évolution des pratiques DevOps, des tests continus, DevSecOps et de l’ingénierie de fiabilité des sites (SRE) a toujours eu pour objectif d’accroître l’efficacité, la sécurité et la rapidité des processus de développement et de déploiement de logiciels. La stratégie de « shift left » a été la pierre angulaire de cette évolution, mettant l’accent sur l’intégration des pratiques de test et de sécurité au début du cycle de vie du développement pour détecter et résoudre les problèmes plus tôt, réduisant ainsi les coûts et améliorant la qualité. Cependant, avec les progrès de l’automatisation, de l’IA, des technologies d’apprentissage automatique et des stratégies de déploiement telles que les indicateurs de fonctionnalité, une stratégie de « no shift » émerge comme une alternative convaincante. Cette stratégie plaide en faveur de la faisabilité du développement et des tests directement en production, contournant ainsi potentiellement le pipeline traditionnel de livraison du développement à la production. Ici, nous comparerons les stratégies de « shift left » et de « no shift », en examinant leurs forces, leurs faiblesses et la manière dont cette dernière pourrait relever ses défis.</p>'}, {'', '<h3>Concepts clés</h3>'}, {'', '<p>Shift Left : cette stratégie consiste à intégrer les problèmes de test, de sécurité et d’exploitation dès le début du processus de développement logiciel. L’objectif est de détecter et de résoudre les problèmes plus tôt, ce qui est moins coûteux et prend moins de temps que de le faire plus tard dans le cycle de vie. Elle met l’accent sur une collaboration précoce et fréquente entre les équipes interfonctionnelles. No Shift : contrairement à la stratégie « shift left », la stratégie « no shift » consiste à utiliser des technologies et des pratiques avancées pour développer et tester en toute sécurité dans l’environnement de production. Cette stratégie s’appuie sur des données en temps réel, des indicateurs de fonctionnalités pour des déploiements contrôlés, l’IA pour les prédictions et la détection des anomalies et une automatisation robuste pour garantir que les modifications peuvent être apportées directement en production sans compromettre la stabilité ou la sécurité.</p>'}, {'', '<h3>Décalage vers la gauche : forces et faiblesses</h3>'}, {'', '<p>Détection précoce des problèmes : détecte les bogues et les vulnérabilités au début du cycle de développement. Rentabilité : réduit le coût de résolution des problèmes en les détectant tôt. Collaboration améliorée : encourage la collaboration entre les équipes interfonctionnelles dès le départ.</p>'}, {'', '<p>Faiblesses de Shift Left\xa0: Ralentissement initial\xa0: l’intégration précoce des tests et de la sécurité peut ralentir les efforts de développement initiaux. Complexité\xa0: nécessite des efforts importants pour intégrer et automatiser divers outils et pratiques.</p>'}, {'', '<h3>Points forts et points faibles de No Shift :</h3>'}, {'', ""<p>Commentaires en temps réel : offre un retour immédiat des utilisateurs réels et des données de production. Rapidité : livraison potentiellement plus rapide en éliminant la phase traditionnelle de préparation/test. Innovation : exploite les technologies de pointe en matière d'IA et de ML pour l'analyse prédictive et la détection des anomalies.</p>""}, {'', ""<p>Faiblesses de No Shift\xa0: Risque en production\xa0: risque initial plus élevé d'impacter les utilisateurs avec des bugs ou des problèmes de sécurité. Complexité et frais généraux\xa0: nécessite des mécanismes sophistiqués de signalisation, de surveillance et de restauration des fonctionnalités.</p>""}, {'', '<h3>Surmonter les faiblesses du No Shift</h3>'}, {'', '<p>Pour atténuer les risques et les défis associés à la stratégie de non-changement, les organisations peuvent adopter plusieurs pratiques :</p>'}, {'', ""<p>• Signalisation de fonctionnalités robuste : implémentez un système complet de signalisation de fonctionnalités pour contrôler les versions de fonctionnalités et effectuer des déploiements progressifs.• Surveillance et observabilité avancées : utilisez des outils de surveillance pilotés par l'IA pour détecter les anomalies et les problèmes en temps réel.• Restaurations automatisées : développez des mécanismes pour une restauration rapide des modifications qui causent des problèmes en production.• Segmentation des utilisateurs : testez de nouvelles fonctionnalités sur des sous-ensembles d'utilisateurs pour minimiser les impacts négatifs potentiels.</p>""}, {'', '<h3>Comment l’ingénierie de plateforme peut faciliter le « No Shift »</h3>'}, {'', '<p>L’ingénierie de plateforme, combinée aux outils de développement assistés par l’IA, joue un rôle essentiel pour permettre et faciliter une stratégie de développement logiciel sans changement réussie. Cette combinaison crée une infrastructure robuste, flexible et intelligente qui prend en charge le développement, les tests et le déploiement transparents des applications directement dans les environnements de production. Voici comment ces composants fonctionnent ensemble pour faire de la stratégie sans changement une réalité :</p>'}, {'', ""<p>1. Provisionnement automatisé de l'environnement</p>""}, {'', ""<p>• L'ingénierie de plate-forme crée des environnements standardisés en libre-service pour les développeurs, réduisant ainsi le temps de configuration et la complexité traditionnellement impliqués dans la préparation d'environnements de type production.• Les outils d'IA peuvent prédire et ajuster automatiquement les ressources en fonction des besoins de l'application, garantissant des performances optimales sans intervention manuelle.</p>""}, {'', '<p>2. Intégration continue intelligente/déploiement continu (CI/CD)</p>'}, {'', ""<p>• Les outils de développement assistés par l'IA analysent le code en temps réel pour prédire les problèmes potentiels d'intégration et de déploiement, en recommandant des correctifs avant la fusion ou le déploiement du code.• L'ingénierie de la plate-forme garantit que les pipelines CI/CD sont évolutifs, sécurisés et résilients, intégrant des informations sur l'IA pour rationaliser les flux de travail et les processus de prise de décision.</p>""}, {'', '<p>3. Surveillance et observabilité avancées</p>'}, {'', ""<p>• L'ingénierie de plate-forme fournit une base d'outils de surveillance qui collectent de vastes quantités de données opérationnelles.• Les outils d'IA exploitent ces données pour l'analyse prédictive, la détection des anomalies et l'analyse automatisée des causes profondes, permettant aux équipes de résoudre les problèmes de manière préventive avant qu'ils n'affectent les utilisateurs.</p>""}, {'', '<p>4. Posture de sécurité renforcée</p>'}, {'', ""<p>• Les outils assistés par l'IA analysent en permanence les vulnérabilités et les comportements anormaux au sein de la base de code et de l'environnement de production, offrant des alertes en temps réel et des stratégies de correction ou d'atténuation automatisées.• L'ingénierie de la plate-forme intègre ces outils dans le cycle de vie du développement, garantissant que la sécurité est une préoccupation primordiale traitée efficacement et sans perturber le flux de développement.</p>""}, {'', '<p>5. Gestion des indicateurs de fonctionnalité et tests A/B</p>'}, {'', ""<p>• L'ingénierie de la plateforme facilite la mise en œuvre d'indicateurs de fonctionnalités et de cadres de tests A/B, permettant aux développeurs de déployer de nouvelles fonctionnalités progressivement et en toute sécurité.• Les outils d'IA analysent les interactions et les commentaires des utilisateurs avec les nouvelles fonctionnalités en temps réel, offrant des informations qui aident à décider s'il faut annuler, poursuivre ou étendre le déploiement d'une fonctionnalité.</p>""}, {'', '<p>6. Évolutivité et optimisation des performances</p>'}, {'', ""<p>• Les outils assistés par IA peuvent prédire les modèles de charge et augmenter ou diminuer automatiquement les ressources selon les besoins, garantissant que les applications restent réactives sous des charges variables.• L'ingénierie de la plate-forme garantit que l'infrastructure prend en charge la mise à l'échelle dynamique et l'optimisation des performances, ce qui permet de maintenir des performances et une disponibilité élevées même lorsque les demandes changent.</p>""}, {'', '<h3>Conclusion</h3>'}, {'', '<p>Le débat entre les stratégies de « shift left » et de « no shift » met en évidence la nature dynamique des méthodologies de développement logiciel. Si la stratégie de shift left s’est avérée efficace pour améliorer la qualité et la sécurité des logiciels, la stratégie de no shift, qui met l’accent sur le retour d’information en temps réel et le déploiement rapide, constitue une alternative intéressante à une époque dominée par les technologies d’automatisation, d’IA et de ML.</p>'}, {'', '<p>En exploitant les atouts de l’ingénierie de plateforme et des outils de développement assisté par l’IA, les entreprises peuvent mettre en œuvre efficacement une stratégie sans changement. Cette approche accélère non seulement le cycle de développement, mais réduit également considérablement les risques associés au déploiement direct en production. Elle favorise une culture de l’innovation, permettant aux développeurs d’itérer et d’affiner rapidement leurs applications en temps réel, en fonction des retours réels des utilisateurs et des informations basées sur les données. La synergie entre l’ingénierie de plateforme et l’IA crée une base solide pour un cycle de vie de développement logiciel résilient, efficace et adaptatif.</p>'}, {'', '<p>Malgré les risques et les complexités initiaux, la stratégie « no shift » permet d’accélérer les cycles d’innovation et de tirer parti des données du monde réel, ce qui en fait un choix intéressant pour les organisations désireuses d’investir dans les outils et pratiques nécessaires pour atténuer ses faiblesses. À mesure que ces technologies continuent d’évoluer et de mûrir, il est probable que l’approche « no shift » deviendra de plus en plus réalisable et populaire, supplantant potentiellement la stratégie « shift left » comme stratégie privilégiée par les organisations avant-gardistes cherchant à maximiser la rapidité, l’efficacité et l’innovation dans leurs processus de développement logiciel.</p>'}, {''}]"
Dell intègre DevOps aux environnements Edge Computing,"[{'', ""<p>Dell Technologies a mis à jour aujourd'hui sa plateforme d'informatique de pointe cette semaine pour simplifier le provisionnement programmatique de l'infrastructure à l'aide des meilleures pratiques DevOps.</p>""}, {'', ""<p>Phil Burt, directeur principal de la gestion de projet pour l'informatique de pointe chez Dell Technologies, a déclaré que la version 2.0 de Dell NativeEdge ajoute des plans déclaratifs supplémentaires basés sur des fichiers YAML et le framework Topology and Orchestration Specification for Cloud Applications (TOSCA).</p>""}, {'', ""<p>Des plans pour Telit, Litmus et PTC pour les cas d'utilisation de fabrication, Deep North pour la vente au détail, Dell Streaming Data Platform pour l'analyse et Rancher Labs K3s pour les environnements Kubernetes sont fournis.</p>""}, {'', ""<p>Ces plans simplifient le déploiement d'applications sur un système d'exploitation léger développé par Dell et optimisé pour les plates-formes informatiques de pointe ou sur toute autre pile logicielle qu'une organisation informatique peut préférer, a-t-il ajouté.</p>""}, {'', ""<p>Dell ajoute également la prise en charge du module de plateforme sécurisée virtuelle (vTPM) et des fonctionnalités de démarrage sécurisé UEFI pour appliquer les principes informatiques de confiance zéro aux plates-formes informatiques de pointe à l'aide d'une cryptographie matérielle et d'un stockage sécurisé pour les clés de chiffrement, les certificats et les mots de passe.</p>""}, {'', '<p>Enfin, Dell propose également un plan d’abonnement de trois ans pour NativeEdge afin de réduire le coût total du déploiement de plates-formes informatiques de pointe.</p>'}, {'', ""<p>Dell s'appuie sur des machines virtuelles développées pour les plateformes de calcul en périphérie afin de fournir des logiciels. Les équipes informatiques peuvent alors choisir de déployer des applications monolithiques ou des applications basées sur des conteneurs exécutées sur une instance de Kubernetes déployée en tant qu'extension de la machine virtuelle fournie par Dell. Ces plateformes peuvent ensuite également se connecter automatiquement à un réseau pour fournir la connectivité requise.</p>""}, {'', ""<p>Quelle que soit l'approche, Dell fournit une méthode permettant d'orchestrer par programmation la gestion des environnements informatiques de pointe hautement distribués, a noté Burt.</p>""}, {''}, {'', '<p>À plus long terme, Dell étudie également comment appliquer l’intelligence artificielle générative (IA) à la gestion des environnements informatiques de pointe en utilisant les données fournies par ses outils de surveillance, a-t-il ajouté.</p>'}, {'', '<p>L’informatique de pointe existe depuis des décennies sous une forme ou une autre. Mais à mesure que de plus en plus d’entreprises commencent à traiter et à analyser les données au point où elles sont créées et consommées, le nombre d’applications déployées à la périphérie du réseau a considérablement augmenté. Ainsi, le nombre d’applications devant être créées et déployées à la périphérie à l’aide des meilleures pratiques DevOps augmente de manière exponentielle. En fait, la gestion de ces charges de travail ne fera que devenir de plus en plus difficile à mesure que de plus en plus de modèles d’IA seront déployés à la périphérie du réseau.</p>'}, {'', ""<p>Bien entendu, toutes les charges de travail en périphérie ne sont pas gérées par une équipe informatique. Pendant des années, de nombreuses organisations se sont appuyées sur des équipes de technologie opérationnelle (OT) qui ont historiquement déployé des applications sur le réseau à l'aide de processus manuels. Les organisations qui adoptent les pratiques DevOps pour automatiser le déploiement des charges de travail en périphérie devront naturellement faire face à certains problèmes culturels ainsi qu'aux défis techniques rencontrés lors du déploiement programmatique de logiciels.</p>""}, {'', '<p>On ne sait pas exactement quelle quantité de logiciels sera exécutée en périphérie du réseau dans les années à venir, mais à mesure que les capacités de l’infrastructure informatique mise à disposition continueront d’augmenter, la complexité des applications qui seront déployées augmentera également. Le défi, comme toujours, sera de trouver la meilleure façon de gérer tous ces logiciels distribués à une échelle qui sera bientôt sans précédent.</p>'}]"
DevOps : quel est l’objectif final ?,"[{'', '<p>Dans le domaine de la haute technologie, nous avons tendance à ne pas avoir d’objectif final. Nous avons des objectifs intermédiaires, mais pas de grand « voici ce que nous devons faire ». Et pourtant, chaque espace est inévitablement associé à une « voici la raison ». Alors, quel devrait être l’objectif principal de DevOps ?</p>'}, {'', '<p>Nous avons commencé avec comme objectif la rationalisation des processus, et nous avons rapidement évolué vers l’efficacité et une plus grande réactivité – tous des objectifs louables, mais il est dans la nature des technologues et des hommes d’affaires (pour des raisons très différentes) de ne pas s’arrêter une fois l’objectif atteint. Nous avons procédé à plusieurs autres itérations qui ont toutes essentiellement rassemblé davantage de processus sous l’égide de DevOps, avant de nous concentrer sur l’automatisation. L’IA poursuivra cette tendance à l’automatisation, mais dans quel but ?</p>'}, {'', '<p>Où voulons-nous aller exactement ? En théorie, nous pourrions faire en sorte que l’IA écrive le code, le construise, le teste et le déploie avec seulement un petit contingent de personnes impliquées dans le processus. Sérieusement, si l’idée est de laisser les gens d’affaires produire des choses à la manière des promesses des 4GL, alors nous sommes plus près que jamais d’atteindre cet objectif.</p>'}, {'', '<p>Mais, comme pour les calculatrices, il faudra bien que quelqu’un crée ces systèmes, et nous sommes confrontés à un autre problème : si personne n’écrit de nouveau code, l’IA basée sur l’apprentissage aura du mal à résoudre de nouveaux problèmes. Ce défi peut, en théorie, être surmonté en apprenant à l’IA à désassembler et à abstraire les solutions existantes, mais cela prendra du temps.</p>'}, {'', '<p>L’autre point commun entre les hommes d’affaires et les technologues dans le monde moderne est le manque de véritables philosophes. Je suis technologue, mais une grande partie de ma famille proche apprécie la philosophie. J’ai donc étudié la philosophie en relation avec la science il y a de nombreuses années, et les philosophes, étant ésotériques, ont ruiné leur capacité à influencer la pensée scientifique. Se concentrer sur des questions telles que « Et si demain, nous trouvions de l’aluminium qui n’en était pas vraiment ? » était stupide et sapait l’idée fondamentale selon laquelle « la science nous dira ce que nous pouvons faire, la philosophie nous dira ce que nous devons faire. »</p>'}, {'', '<p>Nous sommes donc seuls à cet égard, et étant donné la communauté mondiale des développeurs, nous pouvons supposer que, simplement parce que vous le pouvez, quelqu’un le fera. Notre question est donc la suivante : « Que veut et attend une entreprise donnée de ses processus DevOps ? » Le rythme du changement est sur le point de s’accélérer à nouveau, donc savoir ce qui est important pour votre organisation aura de l’importance dans les cinq prochaines années. Une entreprise qui utilise la technologie dans des rôles de support a des besoins très différents de ceux d’une entreprise qui utilise la technologie pour développer des produits, et ses besoins sont très différents de ceux d’une entreprise pour laquelle la technologie est le produit.</p>'}, {'', '<p>L’objectif est-il d’éliminer la majeure partie de l’informatique ? Ou de rationaliser encore davantage l’informatique et de la rendre encore plus réactive aux besoins de l’entreprise ? Peut-être l’objectif est-il de développer des produits ou des marchés entièrement nouveaux avec le temps libéré par l’IA. La différence est énorme en termes de planification. Et ce sont les types de questions qui se posent. L’idée selon laquelle une technologie donnée peut faire des choses (ou les faire se produire) plus rapidement et plus efficacement sans changer de personnel est naïve et ignore l’histoire de l’industrie automobile.</p>'}, {'', ""<p>Réunissez-vous, responsables informatiques et commerciaux. Élaborez un plan. Voulez-vous faire plus avec moins ou voulez-vous réduire les frais généraux ? Voulez-vous que l'informatique se spécialise tandis que l'IA se charge de la masse du travail générique ? Le service d'assistance de niveau 1 peut aujourd'hui être pris en charge par des chatbots. Est-ce le type de direction que l'organisation souhaite prendre ? Dites à la direction informatique que le changement arrive ; la question est de savoir comment l'organisation veut l'exploiter et une décision consciente est bien meilleure que des résultats accidentels.</p>""}, {''}]"
Embrasser l'avenir : naviguer dans les vagues de l'IA dans DevOps,"[{'', '<p>Dans un paysage numérique en évolution rapide, l’avènement de l’IA générative et des grands modèles de langage (LLM) a inauguré une nouvelle ère d’innovation et de transformation. Alors que les organisations informatiques s’orientent vers la maîtrise de DevOps, DevSecOps et SRE, les dirigeants se trouvent à la croisée des chemins entre enthousiasme et anxiété. Ce double sentiment découle du vaste potentiel de l’IA pour révolutionner les opérations et des défis inhérents à l’intégration de l’IA générative. Dans cet article, nous explorerons les raisons de cet enthousiasme et de cette anxiété, décrirons les principaux cas d’utilisation de l’IA générative et fournirons des conseils stratégiques pour assurer un parcours DevOps et de transformation numérique fluide.</p>'}, {'', ""<h3>Les catalyseurs de l'excitation</h3>""}, {'', ""<p>1. Efficacité et innovation améliorées : l'IA générative, avec sa capacité à automatiser des processus complexes et à générer de nouvelles idées, promet une efficacité sans précédent. Elle permet aux organisations informatiques de rationaliser les pipelines de développement, de réduire les erreurs manuelles et de favoriser l'innovation, faisant du saut vers DevOps et DevSecOps non seulement un objectif mais une réalité tangible. 2. Prise de décision améliorée : les grands modèles de langage offrent des capacités d'analyse de données sophistiquées, transformant de vastes ensembles de données en informations exploitables. Cela permet aux dirigeants de prendre rapidement des décisions éclairées, d'optimiser les opérations et d'améliorer la fiabilité des services conformément aux principes SRE. 3. Avantage concurrentiel : à l'ère du numérique, garder une longueur d'avance signifie adopter les dernières technologies. L'IA générative offre un avantage concurrentiel unique, permettant aux organisations de proposer des expériences numériques supérieures, d'innover en matière de produits et de services et de répondre de manière proactive aux changements du marché.</p>""}, {'', ""<p>1. Génération et révision automatisées de code et de test : les outils basés sur l'IA peuvent générer des extraits de code et de test, effectuer des révisions et des tests de code et suggérer des optimisations, accélérant ainsi les cycles de développement et améliorant la qualité du code. 2. Amélioration de la sécurité : en intégrant l'IA dans les protocoles de sécurité, les organisations peuvent prédire et atténuer les menaces potentielles plus efficacement, en améliorant leurs initiatives DevSecOps avec des mesures proactives plutôt que réactives. 3. Gestion et résolution des incidents : l'IA peut automatiser les flux de travail de réponse aux incidents, prédire les pannes avant qu'elles ne se produisent et suggérer des mesures correctives, en s'alignant sur les objectifs SRE de maintien d'une haute disponibilité et d'une fiabilité élevée.</p>""}, {'', ""<h3>Sources d'anxiété</h3>""}, {'', '<p>1. Problèmes d’éthique et de confidentialité : le risque d’utilisation abusive des technologies d’IA, notamment les violations de la vie privée et les prises de décision biaisées, soulève d’importantes questions éthiques, qui suscitent l’appréhension des dirigeants. 2. Lacunes en matière de compétences et impact sur la main-d’œuvre : l’évolution vers des opérations centrées sur l’IA nécessite de nouvelles compétences et de nouveaux rôles. Les dirigeants s’inquiètent de la capacité d’adaptation de la main-d’œuvre existante et du défi que représente le fait de combler les lacunes en matière de compétences émergentes. 3. Problèmes d’intégration et de compatibilité : l’intégration de l’IA dans les systèmes existants et la garantie de la compatibilité au sein de l’infrastructure numérique posent des défis techniques, ce qui complique la transition vers des pratiques avancées de DevOps et de SRE.</p>'}, {'', '<h3>Naviguer dans le parcours de la transformation numérique</h3>'}, {'', '<p>Pour exploiter les avantages de l’IA tout en atténuant ses risques, les dirigeants devraient envisager les actions stratégiques suivantes :</p>'}, {'', '<p>1. Favorisez une culture d’IA : Cultivez une culture d’apprentissage continu et d’adaptabilité. Encouragez votre équipe à adopter des outils et des méthodologies d’IA, en insistant sur l’importance de la mise à niveau et de la reconversion. 2. Mettez en œuvre des lignes directrices éthiques en matière d’IA : Élaborez et respectez des lignes directrices éthiques pour l’utilisation de l’IA, en mettant l’accent sur la transparence, la responsabilité et l’équité. Assurez-vous que les mesures de confidentialité et de sécurité font partie intégrante de vos initiatives d’IA. 3. Investissez dans les talents et la formation : Comblez les lacunes en matière de compétences en investissant dans des programmes de formation et des partenariats avec des établissements d’enseignement. Envisagez d’embaucher des spécialistes de l’IA pour combler l’écart entre les rôles informatiques traditionnels et les exigences en matière d’IA. 4. Donnez la priorité à l’intégration transparente : Adoptez une approche progressive de l’intégration de l’IA, en garantissant la compatibilité avec les systèmes existants. Tirez parti des API et des architectures de microservices pour faciliter des transitions plus fluides. 5. Établissez une gouvernance et une supervision : Créez un cadre de gouvernance pour superviser les initiatives d’IA, en garantissant l’alignement avec les objectifs organisationnels et la conformité aux exigences réglementaires. Cela doit inclure la surveillance des performances de l’IA et de son impact sur les opérations et la dynamique de la main-d’œuvre.</p>'}, {'', '<p>Alors que nous sommes à l’aube d’une ère de transformation, la fusion de l’IA avec les pratiques DevOps, DevSecOps et SRE représente un bond en avant considérable. Si le parcours est semé d’embûches, les récompenses potentielles sont immenses. En adoptant l’IA avec une vision stratégique, une considération éthique et un engagement en faveur de l’amélioration continue, les responsables informatiques peuvent propulser leurs organisations vers une efficacité, une innovation et un avantage concurrentiel inégalés.</p>'}, {'', '<p>En conclusion, l’intégration de l’IA dans DevOps pour accélérer davantage la transformation numérique n’est pas seulement une option, mais une nécessité pour rester pertinent à l’ère numérique. Le parcours nécessite une navigation prudente, mais avec la bonne approche, les organisations informatiques peuvent en sortir plus fortes, plus agiles et mieux équipées pour affronter l’avenir. Saisissons cette opportunité de redéfinir le paysage numérique, en veillant à ce que nos transformations soient non seulement réussies, mais aussi durables et responsables.</p>'}, {'', '<p>Adoptez l’avenir, adoptez l’IA.</p>'}]"
Ce que l'évolution du matériel spécialisé pour l'IA et le ML signifie pour DevOps,"[{'', '<p>L’adoption généralisée des technologies d’intelligence artificielle (IA) et d’apprentissage automatique (ML) accélère l’évolution du matériel informatique, essentiel pour automatiser les processus complexes et améliorer la précision de la prise de décision. Cette accélération est cruciale pour faire progresser l’informatique et le traitement des données, en particulier dans les différents segments des pipelines de données IA/ML.</p>'}, {'', '<p>Cette demande croissante incite un large éventail de fabricants de puces, des acteurs établis aux concurrents émergents, à innover et à jouer un rôle de premier plan dans le développement de solutions de traitement plus rapides et plus efficaces. L’objectif principal ? Concevoir des puces qui optimisent le transfert de données, améliorent la gestion de la mémoire et renforcent l’efficacité énergétique. Ces avancées sont loin d’être progressives ; elles sont essentielles pour répondre aux exigences croissantes des applications sophistiquées d’IA et de ML.</p>'}, {'', '<p>L’évolution du paysage matériel de DevOps nécessite une plus grande unification et une plus grande automatisation des différentes applications de l’infrastructure d’IA spécialisée. Avec la diversification des architectures de puces, la création de piles d’applications axées sur la portabilité, les performances et la facilité d’accès devient encore plus cruciale. La capacité à adopter de manière transparente plusieurs architectures sera essentielle pour trouver le bon équilibre entre les différentes capacités techniques des personnes qui souhaitent interagir avec la technologie.</p>'}, {'', ""<p>Cet article explore les implications de ces avancées matérielles sur les processus DevOps au sein de l'IA et du ML. En outre, nous explorons les stratégies que les équipes DevOps peuvent envisager pour garantir que les applications restent efficaces et portables sur différentes architectures de puces.</p>""}, {'', '<p>Par le passé, le matériel à usage général, notamment les GPU et les CPU, était la base de nombreuses charges de travail. Cependant, un changement clair vers le matériel spécialisé est en cours dans l’intelligence artificielle et l’apprentissage automatique, où les exigences en matière de formation et d’inférence ont augmenté de manière exponentielle. Ces applications se heurtent souvent aux limitations de performances inhérentes au matériel traditionnel, en partie en raison des contraintes modernes de la loi de Moore. Par conséquent, il existe un besoin croissant de matériel capable de gérer des tâches d’IA spécifiques avec une efficacité et une rapidité accrues. Par exemple, dans certains scénarios d’apprentissage automatique, l’utilisation de matériel prenant en charge les calculs à virgule flottante en simple précision peut accélérer les processus sans avoir besoin de la précision fournie par les calculs en double précision.</p>'}, {'', '<p>Même si NVIDIA reste une force dominante sur le marché des puces d’IA, la concurrence s’intensifie et diverses entreprises proposent des alternatives innovantes. Et ce ne sont pas seulement les habituels Intel ou AMD qui sont à l’origine de cette concurrence. Google a également fait des progrès avec ses unités de traitement Tensor (TPU). Amazon a récemment annoncé Trainium2, une nouvelle puce d’IA conçue spécifiquement pour la formation des systèmes d’IA. Cette puce, qui devrait concurrencer Maia de Microsoft et les TPU de Google, souligne la tendance croissante des grandes entreprises technologiques à développer des puces d’IA personnalisées. Au-delà de ces géants, des startups telles que Cerebras, SambaNova Systems, Graphcore et Tenstorrent apportent de nouvelles solutions matérielles d’IA.</p>'}, {'', '<p>À mesure que le matériel spécialisé devient de plus en plus répandu, la communauté DevOps devra gérer de nouveaux défis, notamment la portabilité des performances. La portabilité des performances consiste à garantir que les applications fonctionnent efficacement et fonctionnent bien sur différentes architectures informatiques avec un minimum de modifications, voire aucune.</p>'}, {'', '<p>L’informatique cognitive (la catégorie plus large de l’IA et du ML) varie en complexité, en fonction des algorithmes, des modèles et des exigences uniques des créateurs en matière de fonctionnalités spécifiques au matériel. Si une version adaptée à l’architecture optimisera certainement les performances sur les plateformes respectives, elle complique le processus visant à garantir une expérience logicielle cohérente sur différents matériels.</p>'}, {'', '<p>Le défi de la conception d’un système consiste à optimiser l’environnement pour une efficacité maximale, en particulier lorsque la nature précise de la charge de travail est inconnue de ceux qui sont responsables de la conception et du support des systèmes.</p>'}, {'', '<p>Bien entendu, les pipelines d’intégration continue et de déploiement continu (CI/CD) doivent également faire l’objet de considérations importantes et connexes. Les subtilités des pipelines CI/CD sont amplifiées lorsque l’on recherche la portabilité des performances. La nécessité de valider les performances logicielles sur plusieurs configurations matérielles introduit une matrice de tests plus élaborée et peut allonger les cycles de déploiement, affectant directement les exigences de mise sur le marché. Soudain, les charges de travail franchissent désormais les limites traditionnelles de l’infrastructure informatique et des hautes performances/supercalculateurs autrefois définies par les technologies de microservices et de traitement par lots ; elles ne font désormais qu’un dans un pipeline CI/CD.</p>'}, {'', '<p>À mesure que les entreprises adoptent du matériel spécialisé, il existe un risque d’augmentation parallèle du nombre de spécialistes se concentrant uniquement sur un type de matériel ou sur un cas d’utilisation d’application. Si une telle expertise peut favoriser l’innovation et l’optimisation d’une plateforme particulière, elle crée également de la complexité et un risque de silos de connaissances et de complexités inutiles pour les équipes opérationnelles et les clients qui utilisent ces systèmes.</p>'}, {'', ""<p>La portabilité des performances et les concepts étroitement liés, tels que les performances indépendantes du matériel et l'efficacité multiplateforme, sont de plus en plus importants pour les équipes DevOps. À mesure que le paysage technologique évolue, la question urgente devient : comment l'industrie et les équipes DevOps peuvent-elles gérer cette évolution en toute transparence ?</p>""}, {'', '<p>Les recherches et le développement en cours joueront sans aucun doute un rôle clé. Par exemple, le ministère américain de l’Énergie (DoE) étudie de nouvelles méthodologies pour soutenir son projet de calcul exascale. Il s’agit notamment d’affiner les bibliothèques de logiciels existantes, d’élaborer de nouveaux modèles de programmation et de développer de nouveaux outils qui pourraient éventuellement influencer les pratiques DevOps plus larges. D’autres chercheurs développent des couches d’abstraction logicielle, visant à simplifier l’adaptation de code générique à des configurations matérielles spécifiques.</p>'}, {'', '<p>Au-delà des nouveaux outils et méthodologies qui peuvent provenir des efforts actuels de R&D, il existe de nombreux outils et processus existants qui se prêtent à l’amélioration de la portabilité des performances, notamment :</p>'}, {'', ""<li>Conteneurisation : les conteneurs encapsulent les applications et leurs dépendances de manière à garantir leur exécution cohérente dans différents environnements. Les outils open source comme SingularityCE avec compatibilité Open Container Initiative (OCI) peuvent aider à standardiser et à simplifier le déploiement sur différentes configurations matérielles, favorisant ainsi la portabilité des performances pour le calcul haute performance et la gestion traditionnelle de l'infrastructure informatique.</li>""}, {'', '<li>Analyse comparative et profilage : pour garantir la portabilité des performances, il est impératif de comprendre le comportement des logiciels sur différentes architectures. Les outils d’analyse comparative fournissent des mesures quantitatives des performances, tandis que les outils de profilage offrent des informations sur le comportement des logiciels, aidant ainsi les développeurs à identifier les goulots d’étranglement et les domaines nécessitant une optimisation.</li>'}, {'', ""<li>Bibliothèques de portabilité de code : en plus des bibliothèques comme OpenCL qui permettent l'exécution de logiciels sur divers matériels, les avancées récentes dans les technologies de conteneurs complètent cette capacité. Par exemple, les améliorations apportées aux interfaces des périphériques de conteneur, telles que celles de SingularityCE, rationalisent l'intégration de ressources spécifiques au matériel. Ce développement aide les équipes DevOps à optimiser les logiciels pour divers matériels sans réécritures approfondies de la base de code, illustrant les outils prenant en charge la diversité matérielle et l'agilité logicielle dans tous les aspects de l'informatique cognitive.</li>""}, {'', '<p>Bien entendu, en plus de tous ces outils et stratégies, les méthodologies agiles resteront essentielles car elles privilégient le développement itératif, le feedback/l’amélioration continue et l’adaptabilité, autant d’éléments importants pour les configurations matérielles et logicielles en évolution rapide.</p>'}, {'', '<p>À l’heure où nous embrassons les nouvelles frontières de l’IA et du ML, le rôle des équipes DevOps dans la navigation dans un paysage matériel en constante évolution devient de plus en plus vital. Au cœur de ce parcours se trouve le formidable défi de la portabilité des applications, un défi qui nécessite une expertise technique et un changement stratégique vers l’adaptabilité. C’est là que les conteneurs apparaissent comme des outils indispensables qui garantissent une expérience et des performances applicatives cohérentes sur diverses plateformes et gèrent les subtilités de la portabilité.</p>'}, {'', '<p>De même, l’adoption de méthodologies agiles va au-delà de l’adhésion aux processus ; elle incarne un état d’esprit de flexibilité et de réactivité, essentiel dans les changements technologiques rapides. Ces approches, loin d’être des solutions temporaires, font partie intégrante d’une stratégie visant à libérer le potentiel de l’IA. Alors que les équipes DevOps continuent de relever ces défis, leur succès dépendra de leur adaptabilité et de leur volonté d’explorer et d’intégrer les technologies établies et émergentes, en particulier celles qui excellent en termes d’évolutivité et d’efficacité. Cette exploration et cette intégration proactives seront la clé de la survie et de la prospérité dans ce paysage technologique dynamique. Et pourtant, alors que nous avons évoqué ce problème concernant les applications scientifiques, l’impact se fait également sentir du côté AIOps, qui utilise du matériel spécifique à l’IA avec des pratiques AIOps. Bien que notre objectif ici était de discuter des complexités de la création et de l’administration de systèmes d’intelligence artificielle et d’apprentissage automatique, nous n’avons pas encore abordé la conversation sur la sécurité, en particulier avec l’informatique confidentielle, qui est un sujet pour une autre fois. Bon informatique.</p>'}]"
Les professionnels de l'informatique ont des sentiments mitigés à propos de l'IA,"[{'', ""<p>Une enquête menée auprès de 1 213 décideurs informatiques travaillant pour des petites et moyennes entreprises (PME) aux États-Unis, au Royaume-Uni et en Inde, publiée aujourd'hui, a révélé que près de la moitié (45 %) s'inquiètent de l'impact que l'intelligence artificielle (IA) aura sur leur travail.</p>""}, {'', ""<p>Réalisée par le cabinet de recherche Propeller Insights pour le compte de JumpCloud, un fournisseur d'une plateforme de gestion informatique et de cybersécurité, l'enquête a également révélé que plus des trois quarts (76 %) des personnes interrogées estiment que leurs organisations devraient investir dans l'IA, 79 % considérant l'IA comme un élément positif net pour leur organisation.</p>""}, {'', '<p>La plupart des répondants conviennent également que leur organisation aborde l’IA exactement à la bonne vitesse (55 %), contre 22 % qui ont déclaré que leur organisation avançait trop vite et 19 % qui ont déclaré que leur organisation avançait trop lentement.</p>'}, {'', '<p>Cependant, 62 % d’entre eux ont également noté que l’IA dépasse la capacité de leur organisation à se protéger contre les menaces, selon l’enquête.</p>'}, {'', '<p>Tom Bridge, responsable produit principal chez JumpCloud, a déclaré que même s’il ne fait aucun doute que l’IA va changer les rôles au sein des organisations, il est peu probable qu’elle remplace le besoin d’administrateurs informatiques. Au contraire, ces derniers seront en mesure de gérer des environnements informatiques complexes à des niveaux d’échelle beaucoup plus élevés, car l’IA devient un multiplicateur de force pour la gestion informatique. De nombreuses tâches de bas niveau qui rendent la gestion informatique fastidieuse, comme la génération de rapports, seront de plus en plus automatisées, a noté Bridge.</p>'}, {'', '<p>On ne sait pas encore exactement quel impact l’IA aura sur les fonctions informatiques, mais elle intervient à un moment où les organisations informatiques sont confrontées à des défis sur plusieurs fronts. Par exemple, l’enquête révèle que 56 % des personnes interrogées sont plus préoccupées par la sécurité de leur organisation qu’il y a un an. Près des trois quarts (72 %) ont déclaré que toute réduction de leur budget de sécurité augmenterait les risques organisationnels.</p>'}, {'', '<p>Plus de la moitié (56 %) des personnes interrogées ont identifié la sécurité comme leur plus grand défi informatique, suivie par le déploiement de nouveaux services et applications (45 %), l’augmentation de la charge de travail (44 %) et le coût des solutions de travail à distance (42 %). Les répondants ont déclaré que les attaques réseau constituaient leur plus grand défi (40 %), suivies des exploitations de vulnérabilités logicielles (34 %) et des ransomwares (29 %). 83 % d’entre eux exigent désormais une authentification multifacteur (MFA), tandis que les deux tiers exigent la biométrie (66 %) pour accéder à une partie de leurs ressources informatiques. Au total, 83 % doivent également toujours gérer les mots de passe.</p>'}, {'', '<p>Plus des trois quarts (76 %) signalent également des exigences accrues en matière de conformité et de réglementation dans leur région.</p>'}, {'', '<p>En outre, 88 % des personnes interrogées ont constaté une augmentation des prix des fournisseurs au cours des six derniers mois, selon l’enquête. Plus d’un tiers des répondants ont besoin de cinq à dix applications pour gérer le cycle de vie des employés (37 %), mais 22 % en ont besoin de 11 ou plus. Les trois quarts (75 %) ont déclaré qu’ils préféreraient un seul outil pour faire leur travail plutôt que plusieurs solutions ponctuelles.</p>'}, {'', ""<p>Plus des trois quarts (76 %) des entreprises s'appuient sur un fournisseur de services gérés (MSP) pour gérer une partie de leur environnement informatique, et 42 % d'entre elles s'appuient désormais sur un MSP pour gérer entièrement leur environnement informatique.</p>""}, {'', '<p>Enfin, 57 % des personnes interrogées ont déclaré que leur entreprise avait connu des licenciements au cours de l’année écoulée. En plus de devoir se séparer de ces employés, les professionnels de l’informatique seront naturellement plus inquiets quant à leur propre sécurité d’emploi.</p>'}, {'', '<p>Il est vrai que la gestion des technologies de l’information est aujourd’hui plus incertaine qu’à n’importe quel moment de l’histoire récente. Le défi, comme toujours, consiste à adopter l’automatisation de manière aussi dynamique que possible afin de réduire un niveau de complexité informatique qui devient rapidement trop complexe pour être géré autrement.</p>'}]"
La gravité des données restera un problème,"[{'', '<p>Les choses s’améliorent dans le domaine informatique à un rythme que nous n’avions pas vu depuis la première vague d’adoption des conteneurs. Nous disposons d’une automatisation massive dans l’ensemble du cycle de développement logiciel (SDLC) avec des plug-ins basés sur l’IA pour les IDE qui aident les développeurs à écrire du code à la fois de meilleure qualité et plus sécurisé, une sécurité profondément intégrée dans la chaîne d’outils DevOps et l’unification des outils de test/sécurité dans l’ensemble du SDLC. Nous produisons à nouveau plus de code de meilleure qualité plus rapidement qu’il y a un an.</p>'}, {'', '<p>Et ce qui commence enfin à rattraper son retard, ce sont les données. J’aime beaucoup la technologie actuelle de virtualisation des données, qui permet de les déplacer d’une région à une autre ou d’un cloud à un autre à loisir, en arrière-plan. En même temps, les données auxquelles on accède entre-temps peuvent être instantanément transférées. J’ai déjà abordé le sujet du stockage et des données, et franchement, c’est le monde rêvé.</p>'}, {'', '<p>Mais cela ne change rien au fait que l’emplacement des données est important. Bien que le déplacement massif de données vers leur destination finale soit utile, séparer les données des applications qui y accèdent revient simplement à ajouter un autre maillon faible dans la chaîne de l’infrastructure applicative.</p>'}, {'', '<p>C’est là que réside la véritable force des conteneurs et de Kubernetes. Avec Kubernetes, nous pouvons placer des conteneurs contenant l’intégralité de nos applications dans l’environnement où résident nos données. Cela signifie que notre infrastructure d’application est consolidée et que, en cas de besoin, les applications et les données peuvent être déplacées n’importe où, car Kubernetes est véritablement portable (à condition de ne pas utiliser les fonctionnalités de personnalisation de certains fournisseurs/plateformes).</p>'}, {'', '<p>Les environnements que nous choisissons actuellement d’utiliser pour les applications d’entreprise, du mainframe au cloud public et tout ce qui se trouve entre les deux, sont tous capables d’exécuter des conteneurs, et même s’il existe encore des différences de déploiement, une légère personnalisation pour déplacer une application entière est bien meilleure que la réécriture d’applications entières pour cibler un nouvel environnement hôte. Et la quantité de reciblage nécessaire diminue constamment.</p>'}, {'', ""<p>Alors, décidez où vous souhaitez héberger votre solution, utilisez la virtualisation des données pour y déplacer les ensembles de données, lancez vos applications dans Kubernetes à cet endroit et profitez des fruits de votre travail. Fusions, acquisitions, passages d'une architecture à une autre, passage d'un fournisseur de cloud public A à un fournisseur de cloud public B… Tout cela est grandement facilité par la portabilité des données et de Kubernetes.</p>""}, {'', '<p>Bien que les gens aiment l’appeler « cloud », nous devrions plutôt l’appeler « portable ». La personne en charge du marketing qui a décidé que le cloud incluait Kubernetes est mon ennemi juré, d’ailleurs. Parce que le cloud est utilisé sans inclure Kubernetes, ce qui, bien sûr, ne fait que brouiller les pistes. La programmation portable est une bien meilleure description de la mobilité des données et de l’architecture Kubernetes utilisées en tandem. Elle n’a absolument pas besoin d’un cloud.</p>'}, {'', ""<p>Et nous nous dirigeons vers une productivité encore plus grande. Il suffit de choisir où le déplacer et de continuer. Nous vivons en effet une époque plutôt cool, et c'est vous tous qui la rendez réelle. Continuez votre bon travail.</p>""}, {'', ""<p>Source de l'image : https://vecteezy_businessman-holding-social-planet-showing-networking-skills_1268342.jpg</p>""}, {''}, {''}]"
Comment GitHub aborde l'expérience des développeurs internes,"[{'', ""<p>L'expérience des développeurs (DX) est au cœur de toutes les conversations, et ce pour de nombreuses raisons. Une DX de qualité est intrinsèquement liée à une productivité, une rapidité et une satisfaction accrues des développeurs. Les organisations qui investissent dans la DX ont tendance à constater des améliorations d'efficacité et une diminution du taux de rotation du personnel, ce qui contribue à réduire les coûts.</p>""}, {'', '<p>De nombreux efforts sont donc déployés pour améliorer l’expérience des développeurs avec leurs flux de travail et leurs outils internes. Mais quels sont les bons modèles d’expérience de développement dans la pratique ? Pour cela, tournons-nous vers GitHub.</p>'}, {'', '<p>GitHub, le référentiel de code et la plateforme de contrôle de version omniprésents, est naturellement l’outil préféré des développeurs. En interne, GitHub cherche constamment à améliorer l’expérience de ses propres développeurs en matière de création, de déploiement et de maintenance de logiciels. Ci-dessous, nous allons découvrir comment GitHub aborde la transformation numérique et insuffle ces caractéristiques dans sa culture de développement logiciel.</p>'}, {'', '<p>La pratique la plus cruciale de GitHub est de permettre aux développeurs d’entrer dans l’état de flux et d’y rester. Dr Eirini Kalliamvakou, chercheuse chez GitHub, définit le « travail en profondeur » comme le temps réservé à la concentration. « Pendant ce temps, vous subissez le moins d’interruptions possible », a-t-elle déclaré. Selon Kalliamvakou, les employés de GitHub bloquent activement du temps pour le travail en profondeur – certaines équipes ont même des « jeudis sans réunion ».</p>'}, {'', '<p>Des recherches ont démontré les bénéfices de consacrer du temps à un travail approfondi. Comme je l’ai déjà évoqué, une étude interne réalisée auprès d’employés de Microsoft a révélé que les développeurs qui consacraient officiellement du temps à la concentration pour coder ont connu une augmentation significative de leur satisfaction globale et de leur productivité perçue.</p>'}, {'', ""<p>Plus récemment, GitHub s'est associé au groupe de recherche DX pour une étude, DevEx In Action, qui évalue les résultats observables de l'amélioration de l'expérience des développeurs. Les résultats ont montré que les développeurs qui consacrent beaucoup de temps au travail approfondi bénéficient d'une augmentation de productivité de 50 %.</p>""}, {'', '<p>Il n’est pas surprenant que les développeurs de GitHub aient adopté GitHub Copilot, l’outil de saisie semi-automatique de code basé sur le Codex d’OpenAI. Selon Jon Peck, responsable senior des relations avec les développeurs et de la défense des intérêts des entreprises chez GitHub, Copilot est véritablement intégré dans la plupart de ses tâches quotidiennes. « Copilot vit avec moi dans n’importe quel environnement de développement que j’utilise », a-t-il déclaré.</p>'}, {'', '<p>Aujourd’hui, la technologie permet de créer des fonctions complètes et viables, qui tiennent compte du contexte et du style, a-t-il déclaré. De plus, Copilot Chat s’intègre désormais à Jetbrains et à VSCode, ce qui le rapproche de l’environnement de développement habituel du programmeur. Peck souligne en particulier les gains d’expérience des développeurs qui l’utilisent pour des activités ennuyeuses, telles que la création de tests unitaires pour un nouveau code ou la synthèse automatique des fonctionnalités des composants.</p>'}, {'', ""<p>L'automatisation est un autre moyen par lequel GitHub améliore la productivité et l'expérience de ses développeurs. « L'automatisation permet aux développeurs de rester plus longtemps dans le flux, en les interrompant uniquement lorsque quelque chose nécessite leur attention », a déclaré Peck.</p>""}, {'', ""<p>Par exemple, les équipes utilisent GitHub Actions pour toutes sortes de déclencheurs, comme les réponses aux nouveaux commits de code ou la modification de l'état des problèmes, a déclaré Peck. L'exécution de tests asynchrones en arrière-plan et la notification au développeur uniquement en cas de problème leur permettent de rester concentrés sur le code pendant des périodes plus longues. De plus, la communication asynchrone autour des demandes d'extraction et des révisions de code peut garantir que les membres de l'équipe collaborent efficacement pendant leur temps libre.</p>""}, {'', ""<p>Ces caractéristiques sont souvent adoptées dans les cultures de développement de logiciels cloud-native comme GitHub. Plus vous pouvez exécuter des tests et des processus de build en arrière-plan, mieux c'est, explique Kalliamvakou. De plus, réduire la fatigue des notifications est un autre moyen de garantir que les développeurs ne sont pas interrompus à moins que quelque chose ne nécessite vraiment leur attention.</p>""}, {'', '<p>Pour optimiser l’expérience des développeurs, il est judicieux de recueillir leurs commentaires. Par exemple, chez GitHub, l’équipe en charge de l’expérience des développeurs mène régulièrement des enquêtes internes pour mettre en évidence les points à améliorer. Cela permet d’évaluer la satisfaction des développeurs quant à la quantité de travail approfondi qu’ils effectuent. De même, LinkedIn recueille régulièrement des informations qualitatives à l’aide d’un cadre interne pour évaluer la satisfaction des développeurs à l’égard de leurs outils.</p>'}, {'', '<p>GitHub propose également une newsletter pour faire connaître les nouvelles fonctionnalités internes introduites et pour faire le point régulièrement sur l’état d’avancement des choses. « Lorsque vous lancez une initiative d’expérience développeur, vous devez faire connaître cette initiative et suivre la réaction des utilisateurs au fil du temps », a déclaré Kalliamvakou. Garder un œil actif sur le succès de ces initiatives est un bon moyen de vérifier l’état de santé de l’entreprise pour éviter l’épuisement professionnel.</p>'}, {'', '<p>Un autre domaine dans lequel GitHub se concentre sur l’amélioration de l’expérience des développeurs est celui de la sécurité. Traditionnellement, les systèmes effectuent une analyse de sécurité une fois que tout le code a été validé. Cependant, cela peut être très pénible à résoudre, en particulier lorsque vous devez rechercher plusieurs dépendances et que vous n’avez pas une grande visibilité.</p>'}, {'', '<p>« Plus vous rapprochez l’analyse de sécurité du point où la vulnérabilité a été créée, plus il sera facile d’y remédier », a déclaré Peck. Déplacer vers la gauche pour détecter les vulnérabilités potentielles sur le moment et proposer des options de « correction en un clic » peut améliorer considérablement l’expérience du développeur. Il a également décrit comment GitHub a conçu les fonctionnalités de sécurité avancées de dogfood GitHub pour effectuer des tâches telles que l’analyse continue des secrets, la désinfection des entrées et l’analyse basée sur des modèles.</p>'}, {'', '<p>Il est intéressant de noter que l’étude DevEx in Action a révélé que les développeurs qui déclarent avoir un haut niveau de compréhension de leur code se sentent 42 % plus productifs que ceux qui ont une compréhension faible ou inexistante. Alors, comment pouvons-nous améliorer notre compréhension du code ?</p>'}, {'', '<p>Il arrive souvent qu’un développeur oublie le contexte de quelque chose qu’il a écrit l’année dernière, ou qu’un élément programmé par un coéquipier semble obscur. C’est un autre domaine dans lequel l’IA pourrait être utilisée pour générer automatiquement de la documentation. Cependant, l’avenir de la compréhension du code va au-delà de la simple documentation statique.</p>'}, {'', '<p>« La tendance est à l’apprentissage là où ils se trouvent, pour les maintenir dans le flux », a déclaré Kalliamvakou. Elle indique que les informations basées sur l’IA intégrées dans l’IDE \u200b\u200bqui expliquent ce que signifie un programme ou une fonction rendent le flux de travail du développeur très intuitif.</p>'}, {'', '<p>Les développeurs sont souvent confrontés au problème « ça marche sur ma machine », qui peut constituer un obstacle majeur à leur expérience. En substance, les applications dépendent de bien plus que du code : elles dépendent de l’environnement, des dépendances locales, de ses configurations et du périphérique qui exécute le programme.</p>'}, {'', ""<p>En utilisant un IDE basé sur le cloud comme Codespaces, explique Peck, les développeurs GitHub peuvent créer un conteneur dans le cloud et conserver les mêmes configurations. L'avantage de cette solution est que vous pouvez également conserver votre expérience d'édition locale, a-t-il ajouté.</p>""}, {'', '<p>Selon Kalliamvakou, les avantages de l’expérience des développeurs ne sont plus seulement anecdotiques : nous disposons désormais de preuves concrètes pour étayer ces affirmations. Par exemple, les développeurs qui trouvent leur travail intéressant se sentent 30 % plus productifs, selon le rapport susmentionné.</p>'}, {'', ""<p>Il semble que le fait d'avoir à ses côtés un assistant de codage IA formé sur un modèle de langage étendu (LLM) puisse favoriser la satisfaction des développeurs et les gains de productivité. L'introduction d'outils basés sur l'IA peut aider à automatiser une partie du travail insatisfaisant, laissant plus de temps pour des activités plus engageantes, a déclaré Kalliamvakou.</p>""}, {'', '<p>Mais même si l’IA peut résoudre de nombreux problèmes, il y aura toujours un certain degré de labeur. Kalliamvakou recommande de répartir les tâches fastidieuses entre plusieurs employés et d’éviter de les confier à une seule personne ! Chez GitHub, l’équipe DevEx analyse également l’efficacité des réunions pour éliminer celles qui ne sont pas de haute qualité, ce qui peut également favoriser l’engagement.</p>'}]"
Une enquête révèle une série de défis en cours en matière de développement agile,"[{'', ""<p>Une enquête menée auprès de 788 professionnels du développement de logiciels révèle que 71 % d'entre eux utilisent des méthodologies de développement agiles dans leur cycle de vie de développement logiciel (SDLC), mais seulement 44 % ont déclaré que cela fonctionne très bien (11 %) ou plutôt bien (33 %) dans l'entreprise.</p>""}, {'', ""<p>Réalisée par Digital.ai, l'enquête identifie la visibilité et la traçabilité de bout en bout, de l'initiative commerciale au déploiement, comme la principale opportunité d'amélioration (44 %), suivie de la capacité à mesurer le temps de cycle, le temps d'attente et les goulots d'étranglement (34 %), les tests continus (29 %) et l'identification et la mesure du risque technologique (28 %).</p>""}, {'', ""<p>Près de la moitié (46 %) ont identifié le nombre trop important de systèmes comme la principale raison pour laquelle Agile n'évoluait pas, suivi par les équipes cloisonnées (37 %), le choc des cultures (34 %), l'adoption incohérente (30 %) et l'incapacité à mesurer la valeur commerciale (28 %).</p>""}, {'', '<p>Malgré ces problèmes, près des deux tiers (63 %) des entreprises interrogées ont déclaré avoir une visibilité sur leur pipeline Agile, et 55 % d’entre elles ont affirmé avoir une visibilité totale. Cependant, seules 36 % ont déclaré que les équipes DevOps travaillent actuellement ensemble comme une seule équipe pour accélérer l’innovation et le déploiement de logiciels de haute qualité et plus fiables. Dans le même temps, seuls les deux tiers (66 %) ont affirmé qu’au moins 50 % de leurs applications étaient livrées à temps et « avec qualité ».</p>'}, {'', '<p>Joyce Tompsett, directrice générale du projet State of Agile chez Digital.ai, a déclaré que l’enquête montre clairement que de nombreuses organisations tentent encore de trouver le bon équilibre en matière de développement de logiciels. Par exemple, l’enquête révèle que les deux principales raisons pour lesquelles les organisations adoptent Agile (à égalité à 41 %) sont l’accélération des délais de mise sur le marché, la priorisation de la livraison et la mesure de la valeur client/entreprise. La plupart des équipes de développement de logiciels sont évaluées à la fois en fonction de la vélocité (33 %) et de la valeur délivrée (29 %), selon l’enquête.</p>'}, {'', ""<p>L'enquête identifie également la satisfaction client (43 %), le délai de livraison (39 %) et l'avantage concurrentiel (34 %) comme leurs principales priorités pour 2024.</p>""}, {'', '<p>En général, l’enquête révèle que 33 % des répondants ont déclaré que les dirigeants et les cadres dirigeants dirigent et participent activement à la transformation agile de l’entreprise. Cependant, ils sont plus nombreux (41 %) à déclarer que la participation des dirigeants n’est pas suffisante. Plus d’un tiers (37 %) ont déclaré que les équipes commerciales ne comprennent tout simplement pas ce qu’est la méthode Agile ni ce qu’elle peut faire. Plus d’un quart (27 %) ont déclaré qu’il n’y a pas suffisamment de formation.</p>'}, {'', '<p>Sans surprise, la méthodologie la plus largement adoptée est Scrum (63 %), le Scaled Agile Framework (SAFe) étant le plus utilisé (26 %). 22 % des entreprises interrogées n’ont pas de cadre obligatoire, tandis que 12 % ont déclaré avoir créé le leur.</p>'}, {'', ""<p>Le débat sur ce qui constitue précisément le développement agile est bien sûr en cours. Chaque organisation a tendance à adopter les éléments du cadre qui lui conviennent le mieux, de sorte que le niveau d'adoption réel d'une organisation à l'autre varie considérablement. Une chose est sûre : plus l'environnement informatique est complexe, plus il peut être difficile d'adopter le développement agile.</p>""}, {'', '<p>Et il reste encore beaucoup à faire. Si seulement deux tiers des entreprises livrent la moitié (50 %) de leurs applications dans les délais et avec un niveau de qualité acceptable, cela signifie qu’il y a plus de projets qui ne répondent pas à cette norme que de nombreuses équipes de développement de logiciels ne veulent bien l’admettre.</p>'}, {'', '<p>Cependant, à mesure que de plus en plus d’organisations deviennent plus dépendantes des logiciels, la tolérance à ce niveau d’échec ne fera que diminuer, car les entreprises et les dirigeants perdent patience face aux pratiques de développement d’applications qui ne sont pas à la hauteur des attentes.</p>'}]"
Guide DevOps sur le profilage Java,"[{'', ""<p>Le profilage Java est une technique utilisée pour comprendre le comportement détaillé d'une application Java. Il consiste à surveiller et à mesurer divers aspects de l'exécution d'un programme, tels que l'utilisation de la mémoire, l'utilisation du processeur, l'exécution des threads et la récupération de place.</p>""}, {'', ""<p>Le profilage Java peut être utilisé à différentes étapes du cycle de vie du développement logiciel (SDLC). Au cours du développement, il peut aider à identifier les goulots d'étranglement et les points chauds de performances, qui peuvent ensuite être optimisés pour de meilleures performances. Dans la phase de test, le profilage peut être utilisé pour vérifier que l'application fonctionne comme prévu sous charge. Enfin, en production, le profilage peut être utilisé pour surveiller les performances de l'application et détecter les problèmes potentiels avant qu'ils n'affectent les utilisateurs.</p>""}, {'', ""<p>Les outils de profilage Java fournissent des informations sur la machine virtuelle Java (JVM) et l'application qui y est exécutée. Ils permettent aux développeurs de surveiller l'exécution des threads, la création d'objets, la récupération de place et de nombreux autres aspects du fonctionnement de la JVM. En utilisant un profileur Java, les développeurs peuvent acquérir une compréhension approfondie des caractéristiques de performances de l'application et identifier les domaines potentiels d'optimisation.</p>""}, {'', ""<h3>Impact sur l'efficacité et l'évolutivité des applications</h3>""}, {'', ""<p>L'optimisation des performances joue un rôle essentiel dans DevOps en améliorant l'efficacité et l'évolutivité des applications. Les applications efficaces utilisent moins de ressources, ce qui réduit les coûts et permet d'avoir plus d'utilisateurs ou des charges de travail plus importantes. L'évolutivité est la capacité d'une application à gérer des charges de travail accrues sans diminution des performances. En identifiant et en éliminant les goulots d'étranglement des performances, les équipes DevOps peuvent s'assurer que les applications évoluent efficacement à mesure que la demande augmente.</p>""}, {'', '<h3>Relation entre performance et expérience utilisateur</h3>'}, {'', '<p>Les performances sont directement liées à l’expérience utilisateur. Une application lente ou peu réactive peut frustrer les utilisateurs et les amener à abandonner complètement l’application. En s’assurant que les applications fonctionnent bien, les équipes DevOps peuvent améliorer la satisfaction et la rétention des utilisateurs. De plus, en surveillant les performances des applications en production, les équipes DevOps peuvent identifier et résoudre de manière proactive les problèmes avant qu’ils n’affectent les utilisateurs.</p>'}, {'', '<h3>Gestion des coûts et optimisation des ressources</h3>'}, {'', '<p>L’optimisation des performances peut également contribuer à la gestion des coûts et à l’optimisation des ressources. En optimisant les performances des applications, les équipes DevOps peuvent réduire la quantité de ressources informatiques nécessaires à l’exécution de l’application. Cela peut se traduire par des économies de coûts importantes, en particulier dans les environnements cloud où les coûts sont directement liés à l’utilisation des ressources.</p>'}, {'', '<p>De plus, en comprenant les modèles d’utilisation des ressources de l’application, les équipes DevOps peuvent prendre des décisions plus éclairées concernant l’allocation des ressources et la planification des capacités. Cela peut aider à éviter le sur-provisionnement, qui conduit à un gaspillage de ressources, ou le sous-provisionnement, qui peut entraîner de mauvaises performances de l’application.</p>'}, {'', '<h3>Fuites de mémoire</h3>'}, {'', ""<p>Les fuites de mémoire constituent un problème de performances courant dans les applications Java. Une fuite de mémoire se produit lorsqu'une application alloue continuellement de la mémoire mais ne parvient pas à la libérer lorsqu'elle n'est plus nécessaire. Au fil du temps, cela peut entraîner des exceptions OutOfMemoryError et provoquer le blocage de l'application.</p>""}, {'', '<p>Le profilage Java peut aider à identifier les fuites de mémoire en surveillant l’utilisation de la mémoire par l’application au fil du temps. Si l’utilisation de la mémoire augmente continuellement même lorsque l’application est inactive, cela peut indiquer une fuite de mémoire. Les outils de profilage peuvent également fournir des informations sur les objets qui consomment le plus de mémoire, ce qui peut aider à localiser la source de la fuite.</p>'}, {'', '<h3>Problèmes de conflit et de synchronisation des threads</h3>'}, {'', ""<p>Les conflits de threads et les problèmes de synchronisation peuvent avoir un impact significatif sur les performances des applications Java. Les conflits de threads se produisent lorsque plusieurs threads tentent d'accéder simultanément à une ressource partagée, ce qui les oblige à attendre et entraîne une baisse des performances. Les problèmes de synchronisation, tels que les blocages et les livelocks, peuvent entraîner le blocage des threads, les empêchant de progresser.</p>""}, {'', ""<p>Le profilage Java peut aider à détecter les conflits de threads et les problèmes de synchronisation en surveillant l'état et l'exécution des threads. Les outils de profilage peuvent afficher les threads en cours d'exécution, en attente ou bloqués et peuvent fournir une trace de pile de chaque thread, ce qui peut aider à identifier la cause des conflits ou des problèmes de synchronisation.</p>""}, {'<h3>Frais généraux de collecte des ordures</h3>', ''}, {'', ""<p>La récupération de place est un aspect essentiel de la gestion de la mémoire de Java, mais elle peut également être une source importante de perte de performances. Pendant la récupération de place, la JVM interrompt l'exécution de l'application pour récupérer la mémoire des objets qui ne sont plus utilisés. Si la récupération de place se produit trop fréquemment ou prend trop de temps, cela peut entraîner des pauses de l'application et une baisse des performances.</p>""}, {'', ""<p>Le profilage Java peut aider à comprendre et à optimiser le comportement du garbage collection. Les outils de profilage peuvent fournir des informations détaillées sur les événements de garbage collection, tels que leur fréquence, leur durée et la quantité de mémoire récupérée. Ces informations peuvent aider les développeurs à ajuster la configuration du garbage collector pour minimiser son impact sur les performances de l'application.</p>""}, {'', '<h3>Manque de ressources</h3>'}, {'', ""<p>Le manque de ressources est un autre problème de performances courant dans les applications Java. Il se produit lorsqu'un système ou un processus ne parvient pas à obtenir un accès suffisant aux ressources, ce qui entraîne une baisse des performances ou une défaillance. Dans les applications Java, le manque de ressources peut être causé par des facteurs tels qu'une mémoire insuffisante, un processeur insuffisant, un espace disque insuffisant ou une bande passante réseau insuffisante.</p>""}, {'', ""<p>Le profilage Java peut aider à détecter et à résoudre les problèmes de pénurie de ressources. En surveillant l'utilisation des ressources, les outils de profilage peuvent identifier le moment où les ressources deviennent rares et fournir des informations sur les facteurs contribuant à la pénurie de ressources. Ces informations peuvent aider les développeurs à optimiser l'utilisation des ressources et à garantir que l'application dispose de suffisamment de ressources pour fonctionner efficacement.</p>""}, {'', '<p>Les outils de profilage Java fournissent généralement les fonctionnalités suivantes\xa0:</p>'}, {'', ""<li>Capacité à détecter les fuites de mémoire : les fuites de mémoire peuvent constituer un problème majeur dans les applications Java, car elles peuvent entraîner une erreur de manque de mémoire, perturbant le bon fonctionnement de l'application. Les outils de profilage peuvent aider à identifier ces fuites, permettant aux développeurs de les corriger avant qu'elles ne causent des problèmes plus graves.</li>""}, {'', '<li>Profilage du processeur : cela permet aux développeurs de voir combien de temps processeur chaque méthode de leur application consomme. En identifiant les méthodes qui consomment une quantité disproportionnée de temps processeur, les développeurs peuvent optimiser leur code pour améliorer ses performances.</li>'}, {'', '<li>Profilage détaillé des threads : cela peut aider les développeurs à identifier les problèmes de synchronisation, les blocages et autres problèmes potentiels dans les applications multithread. En fournissant ces informations essentielles, les outils de profilage peuvent aider les développeurs à créer des applications plus efficaces et plus performantes.</li>'}, {'', '<p>Voici quelques bonnes pratiques pour tirer le meilleur parti de vos efforts de profilage Java.</p>'}, {'<h3>Identifier les indicateurs clés de performance</h3>', ''}, {'', '<p>Tout d’abord, il est essentiel d’identifier les indicateurs de performance clés les plus pertinents pour votre application. Les indicateurs que vous choisissez dépendent de la nature de votre application et de ses exigences de performance. Parmi les indicateurs courants, citons le temps de réponse, l’utilisation du processeur, l’utilisation de la mémoire et l’activité de récupération de place. En identifiant ces indicateurs clés, vous pouvez utiliser vos outils de profilage pour les surveiller et identifier les problèmes potentiels.</p>'}, {'', '<h3>Intégration du profilage dans les pipelines CI/CD</h3>'}, {'', '<p>Une autre bonne pratique consiste à intégrer le profilage dans vos pipelines d’intégration continue/déploiement continu (CI/CD). Cela vous permet de détecter les problèmes de performances dès le début du processus de développement, avant qu’ils ne deviennent plus difficiles à résoudre. En profilant régulièrement votre application pendant le développement, vous pouvez vous assurer que les modifications que vous apportez n’affectent pas ses performances.</p>'}, {'', '<h3>Profilage dans des scénarios réels</h3>'}, {'', '<p>Bien qu’il puisse être utile de profiler votre application dans des conditions contrôlées, il est également important de le faire dans des scénarios réels. Cela signifie profiler votre application lorsqu’elle est sous charge, lorsqu’elle traite des données réelles et lorsqu’elle s’exécute sur les mêmes configurations matérielles et logicielles qu’en production. En procédant ainsi, vous obtiendrez une image beaucoup plus précise des performances de votre application dans le monde réel.</p>'}, {'', '<h3>Collaboration et partage des connaissances</h3>'}, {'', '<p>Enfin, le profilage Java ne doit pas être une activité solitaire. Il est important de partager vos découvertes et vos idées avec votre équipe et de collaborer pour trouver des solutions aux problèmes de performances que vous identifiez. Ce faisant, vous pouvez créer une culture de sensibilisation aux performances au sein de votre équipe et vous assurer que tout le monde travaille vers l’objectif commun de créer des applications Java efficaces et performantes.</p>'}, {'', '<p>Le profilage Java est un élément essentiel du processus de développement logiciel. En comprenant ses principales fonctionnalités et en suivant les meilleures pratiques, vous pouvez l’utiliser pour créer des applications efficaces, performantes et capables de répondre aux exigences du monde moderne. Ne sous-estimez donc pas la puissance du profilage : adoptez-le et regardez vos applications Java atteindre de nouveaux sommets de performances.</p>'}]"
Nous contrôlerons le monde !,"[{'', '<p>L’un des avantages de la centralisation du contrôle est qu’il faut toujours qu’il y ait quelqu’un aux commandes. Cela peut être une très bonne chose ou une très mauvaise chose. Dans le domaine informatique, cela ne va même pas assez loin pour être une chose terrible.</p>'}, {'', '<p>L’IA générative et les intégrations croissantes ont renouvelé l’insistance de la foule sur le fait qu’il est certain qu’ils doivent contrôler chaque petit élément de votre centre de données (virtuel). L’idée qu’un fournisseur donné doit être le point de configuration et de gestion d’une grande variété d’outils – les siens, ceux de ses concurrents et ceux du marché voisin – existe depuis toujours. Les fournisseurs aiment l’idée d’avoir ce niveau d’influence sur votre infrastructure et, soyons honnêtes, beaucoup d’entre nous aiment l’idée que cela soit pris en charge pour nous.</p>'}, {'', '<p>Mais plusieurs questions se posent ici. Tout d’abord, quelle est la probabilité que le fournisseur X bénéficie d’un niveau de support élevé pour les outils des autres fournisseurs de votre infrastructure ? Je veux dire, avant même d’aborder l’idée que ce désir de contrôle n’est pas du tout altruiste (nous ne parlons pas ici de Staline, mais nous ne parlons certainement pas non plus de Washington), il y a la question de savoir comment l’IA sera capable d’apprendre et de configurer/gérer les produits d’un concurrent. Surveiller le trafic Internet ? Eh bien, si les systèmes du concurrent sont très connus et que le format des API/fichiers de configuration est bien connu, peut-être ? Mais cela nous amène au vrai problème.</p>'}, {'', '<p>Mon conseil aux fournisseurs serait de commencer par maîtriser leurs propres produits. Nous travaillons dans un domaine très complexe où les outils nécessitent de nombreuses connaissances spécifiques du domaine pour être gérés, et encore plus pour être bien gérés. Les fournisseurs ont généralement tenu pour acquis que ce niveau de connaissances serait disponible. Avant de rendre ce niveau de connaissances inutile pour les concurrents ou de les intégrer aux installations des concurrents, faites en sorte que vos outils soient extrêmement simples à utiliser. Cela ne signifie pas que vous devez les simplifier : vous disposez de la puissance de l’IA et vous allez vous concentrer sur la configuration et la gestion. Éliminez donc presque tous les problèmes de configuration avec vos propres produits.</p>'}, {'', '<p>Exemple\xa0: «\xa0Excellent outil de sécurité. Je viens d’ajouter une API sur https://devops.com/rocks. Veuillez la verrouiller afin qu’elle ne soit accessible qu’aux utilisateurs du groupe WeRule, puis générez une règle WAF pour l’empêcher d’être utilisée pour accéder à plus de trois enregistrements par jour, et autorisez uniquement les appels GET et POST.\xa0» La plupart des fournisseurs du secteur sont loin d’atteindre ce niveau de simplicité. Alors pourquoi leur feriez-vous confiance pour configurer/contrôler les produits d’autres fournisseurs\xa0?</p>'}, {'', '<p>Il existe une tendance à ce que les produits couvrent davantage l’espace dans lequel ils se trouvent. C’est très important dans le domaine de la sécurité à l’heure actuelle, mais on le voit aussi dans DevOps. CI/CD/CDD/VC/kitchen sink (KS) : je suis d’accord avec cela, car cela nous offre une configuration simplifiée et une meilleure communication entre les applications. Je pense que c’est une meilleure voie que d’essayer de servir de commande et de contrôle pour une pile technologique entière dont le fournisseur ne possède pas. Je veux toujours que des produits ponctuels et les meilleurs de leur catégorie soient disponibles, mais beaucoup d’entre nous n’ont tout simplement pas le temps et les ressources nécessaires pour mettre en œuvre cinq produits différents pour faire le travail (même s’ils sont « gratuits », comme nous le savons tous, ils ne le sont pas). Un fournisseur qui gère toute une chaîne est donc idéal pour ce cas d’utilisation, et souvent les informations partagées sur l’application, la sécurité, le réseau, etc. peuvent fonctionner ensemble mieux que la somme de ses parties. Restons donc dans la voie de l’« application parapluie » et ne nous laissons pas emporter par la tentative de conduire dans la voie du « gestionnaire du monde entier ».</p>'}, {'', '<p>Nous sommes occupés, fatigués et nombreux sont ceux qui sont blasés. Alors ne nous dites pas comment vous parvenez à gérer les outils des autres fournisseurs, dites-nous plutôt comment vous facilitez la gestion des vôtres. Merci, de la part de toute l’équipe informatique.</p>'}]"
10 fonctionnalités clés des assistants de code AI,"[{'', ""<p>L'un des outils les plus puissants qui ont émergé pour rationaliser le processus de codage est l'assistant de code. Les assistants de code sont des outils logiciels conçus pour aider les programmeurs en fournissant des suggestions et des fonctionnalités automatisées qui aident à écrire du code de manière plus efficace et plus précise. Ils agissent comme des aides virtuelles, rendant le processus de codage plus rapide, plus intuitif et moins sujet aux erreurs.</p>""}, {'', ""<p>Les assistants de code peuvent être classés en deux types : basés sur des règles et basés sur l'IA. Les assistants basés sur des règles suivent des règles prédéfinies et fonctionnent selon des directives définies. Ils sont principalement utilisés pour la coloration syntaxique, le formatage de code et la détection d'erreurs de base. Les assistants de code basés sur l'IA peuvent apprendre des modèles de codage passés, prédire ce qu'un programmeur essaie d'accomplir et suggérer des extraits de code appropriés. Ils sont capables de comprendre le contexte du code et de fournir des suggestions précises, ainsi que de générer des blocs de code entiers basés sur des instructions en langage naturel. Lisez cet article de blog pour découvrir des exemples d'assistants de codage IA populaires.</p>""}, {'', ""<p>Que vous soyez un développeur de logiciels expérimenté gérant des projets complexes ou un débutant, les assistants de codage peuvent automatiser les aspects fastidieux du codage, vous permettant de vous concentrer davantage sur la résolution de problèmes et l'innovation.</p>""}, {'', '<p>Le développement de logiciels devenant de plus en plus complexe et exigeant en termes de temps, le rôle de l’IA dans ce domaine s’est considérablement accru. Les assistants de code pilotés par l’IA représentent un changement majeur dans la manière dont les logiciels sont développés. Ils comblent le fossé entre l’expertise humaine et l’efficacité des processus automatisés.</p>'}, {'', '<p>L’IA dans le développement de logiciels accélère non seulement le processus de codage, mais améliore également la qualité du code. Ces outils peuvent analyser de grandes quantités de code, en tirer des enseignements et appliquer cet apprentissage pour aider les développeurs en temps réel. Ils peuvent s’adapter à différents styles et exigences de codage, ce qui en fait des outils polyvalents pour un large éventail de tâches de programmation. De plus, à mesure que les projets logiciels augmentent en taille et en complexité, les assistants de code IA deviennent essentiels pour gérer et naviguer dans ces complexités. Ils offrent des informations intelligentes, détectent les problèmes potentiels avant qu’ils ne deviennent problématiques et aident à maintenir les grandes bases de code propres et gérables.</p>'}, {'', '<p>En plus d’améliorer l’efficacité, les assistants de code IA contribuent à réduire les barrières à l’entrée dans le développement de logiciels. Ils rendent le codage plus accessible aux personnes ayant différents niveaux d’expertise, y compris celles qui débutent en programmation. Cette accessibilité est essentielle à une époque où la demande de compétences en développement de logiciels ne cesse d’augmenter. En fournissant une assistance en temps réel et des ressources d’apprentissage, les assistants de code IA ne sont pas seulement des outils pour les développeurs actuels, mais aussi des aides à l’apprentissage pour les futures générations de programmeurs.</p>'}, {'', '<p>Avec les progrès rapides des grands modèles de langage (LLM), les fonctionnalités suivantes deviennent la norme dans les assistants de codage à l’approche de 2024.</p>'}, {'', '<h3>Compréhension contextuelle avancée</h3>'}, {'', ""<p>Les assistants de code modernes évoluent pour comprendre le contexte du code de manière plus approfondie. Contrairement aux simples outils de saisie semi-automatique, ces systèmes d'IA peuvent analyser la portée plus large d'un projet, reconnaître des modèles et proposer des suggestions qui correspondent à l'architecture globale et aux modèles de conception. Cette compréhension avancée permet non seulement de rationaliser le codage, mais également de maintenir la cohérence dans l'ensemble de la base de code.</p>""}, {'', ""<h3>Suggestions d'optimisation du code en temps réel</h3>""}, {'', '<p>Les outils d’IA peuvent recommander de manière proactive des modifications pour améliorer les performances et l’efficacité. Par exemple, un assistant de code peut suggérer un algorithme plus efficace ou mettre en évidence des blocs de code redondants qui peuvent être simplifiés. Cette fonctionnalité améliore non seulement la qualité du code, mais sert également d’outil d’apprentissage pour les développeurs, en les exposant aux meilleures pratiques et aux approches alternatives.</p>'}, {'<h3>Assistance au débogage améliorée</h3>', ''}, {'', ""<p>Le débogage est souvent un aspect chronophage du développement. En 2024, les assistants de code devraient révolutionner ce processus en offrant une assistance de débogage améliorée. Grâce à une analyse approfondie, ces outils détectent automatiquement les erreurs et les anomalies, suggérant des correctifs potentiels. Ils peuvent localiser avec précision l'emplacement d'un bug et fournir un contexte, réduisant ainsi considérablement le temps que les développeurs passent dans la phase de débogage.</p>""}, {'', '<h3>Refactorisation automatisée</h3>'}, {'', ""<p>Le refactoring est essentiel pour maintenir un code propre, et les assistants pilotés par l'IA offrent désormais des capacités de refactoring automatisées sophistiquées. Ils suggèrent intelligemment des modifications qui améliorent la lisibilité et la maintenabilité du code, comme la restructuration du code ou le changement de nom des variables pour plus de clarté. Ces suggestions seront particulièrement utiles pour gérer et mettre à jour le code existant, en veillant à ce qu'il soit conforme aux normes de codage modernes.</p>""}, {'', '<h3>Génération et mise à jour de la documentation du code</h3>'}, {'', ""<p>Maintenir la documentation à jour est un défi. Les assistants de code peuvent désormais automatiser cette tâche en générant et en mettant à jour la documentation en temps réel. Au fur et à mesure que le code évolue, la documentation reflétera ces mises à jour, garantissant ainsi qu'elle reste toujours synchronisée avec la base de code. Cette fonctionnalité sera inestimable dans les grandes équipes où le suivi des modifications peut être intimidant.</p>""}, {'', '<h3>Assistance au codage prédictif</h3>'}, {'', '<p>Au-delà des suggestions réactives, les assistants de code proposeront une assistance prédictive. En analysant les schémas de codage et les structures de projet, ces outils prédiront les prochaines étapes du développeur et proposeront des suggestions pertinentes avant même qu’elles ne soient explicitement demandées. Cette approche proactive accélérera le processus de développement, réduisant ainsi la charge cognitive des développeurs.</p>'}, {'', '<h3>Comprendre et répondre aux requêtes des développeurs en langage naturel</h3>'}, {'', ""<p>Les développeurs peuvent poser des questions ou exprimer des exigences de codage en anglais simple, et l'assistant interprétera et exécutera ces demandes. Les interfaces basées sur le chat proposées par les principaux assistants de codage IA vont encore plus loin. Cette avancée rendra le codage plus intuitif et accessible, en particulier pour les débutants ou les utilisateurs non experts.</p>""}, {""<h3>Suggestions en temps réel et résolution de conflits dans les projets d'équipe</h3>"", ''}, {'', ""<p>La collaboration dans les projets de codage peut entraîner des conflits et des incohérences. Les assistants de code proposent désormais des suggestions en temps réel et une résolution automatique des conflits pour les projets d'équipe. Ces outils identifient les conflits potentiels dans les demandes de fusion et suggèrent des résolutions optimales, améliorant ainsi la collaboration en équipe et préservant l'intégrité du code.</p>""}, {'', '<h3>Assistance au codage personnalisée basée sur les interactions passées</h3>'}, {'', ""<p>La personnalisation devient une fonctionnalité importante des assistants de code. En tirant les leçons des interactions passées avec un développeur, ces outils adapteront leurs suggestions et leur assistance en fonction des styles et préférences de codage individuels. Cette approche d'apprentissage adaptatif garantit que l'assistance fournie est la plus pertinente et la plus efficace pour chaque développeur.</p>""}, {'', ""<h3>Améliorer l'accessibilité du codage pour les utilisateurs non experts</h3>""}, {'', '<p>L’une des caractéristiques les plus marquantes des assistants de programmation est leur rôle dans la démocratisation du codage. En simplifiant les concepts de codage complexes et en automatisant les tâches banales, ces outils rendront le développement de logiciels plus accessible aux non-experts. Les débutants et ceux qui apprennent de nouveaux langages de programmation trouveront ces assistants particulièrement utiles, car ils fournissent des conseils et des aides à l’apprentissage.</p>'}, {'', ""<p>Les assistants de code basés sur l'IA deviennent partie intégrante du processus de développement logiciel. Ils rendent non seulement le codage plus rapide et plus efficace, mais réduisent également les risques d'erreurs, ce qui rend le logiciel résultant plus fiable. Le rôle des assistants de code ne devrait que croître, apportant des fonctionnalités plus avancées et intuitives. Que vous soyez un programmeur chevronné ou un débutant, l'adoption d'assistants de code peut changer la donne dans votre parcours de codage.</p>""}]"
Salesforce ajoute la possibilité d'appliquer des politiques de sécurité personnalisées aux API,"[{'', ""<p>Salesforce a ajouté aujourd'hui la possibilité de personnaliser les politiques de sécurité pour les interfaces de programmation d'applications (API) à la plate-forme de gestion des API Mulesoft Anypoint.</p>""}, {'', ""<p>Gerry Egan, vice-président de la gestion des produits pour Mulesoft chez Salesforce, a déclaré que cette capacité permet aux organisations d'utiliser un kit de développement de politiques Anypoint Flex Gateway pour rationaliser le développement de politiques de sécurité API personnalisées qui peuvent être implémentées sous forme de code.</p>""}, {'', '<p>En plus de fournir des intégrations avec des environnements de développement intégrés (IDE), le kit de développement de politiques Anypoint Flex Gateway fournit également des politiques de test qui peuvent être utilisées pour garantir la sécurité des API.</p>'}, {'', ""<p>Cette approche permet de garantir plus facilement la sécurité des API lors de leur création et de leur déploiement dans le contexte d'un flux de travail DevOps automatisé, plutôt que de devoir s'appuyer entièrement sur une plate-forme de sécurité API distincte, a-t-il noté.</p>""}, {'', ""<p>La passerelle Anypoint de Mulesoft est déjà dotée de politiques de sécurité que les équipes DevSecOps peuvent facilement mettre en œuvre. Salesforce ajoute désormais la possibilité d'appliquer des politiques personnalisées que les équipes DevOps peuvent appliquer à mesure qu'il devient évident que les cyberattaques contre les API deviennent de plus en plus sophistiquées, a noté Egan. Ces politiques peuvent, par exemple, être utilisées pour supprimer des données sensibles ou les remplacer par un jeton, a-t-il ajouté.</p>""}, {'', '<p>La responsabilité de la sécurité des API n’est souvent pas bien établie dans de nombreuses organisations, mais à mesure que la responsabilité de la sécurité des API est de plus en plus confiée aux équipes DevSecOps, on s’intéresse de plus en plus à la résolution du problème sous forme de code lors du développement et du déploiement des API. Si les équipes de cybersécurité sont chargées de maintenir la cybersécurité après le déploiement des API, peu d’entre elles ont une idée de la manière dont une API est conçue. Il s’agit d’un problème important, car les cybercriminels sont devenus plus habiles à la fois à exfiltrer des données via des API externes et à manipuler la logique métier sous-jacente pour compromettre un processus numérique, a noté Egan.</p>'}, {'', ""<p>La passerelle Anypoint de Mulesoft automatise le processus de création et d'application des politiques de sécurité aux API dans le cadre du cycle de vie du développement logiciel (SDLC) d'une manière qui minimise tout impact sur la productivité des développeurs, a-t-il ajouté. Cette approche offre l'avantage supplémentaire de réduire le niveau de charge cognitive que les développeurs subiraient s'ils devaient créer manuellement des politiques logicielles, a déclaré Egan.</p>""}, {'', '<p>En général, la plupart des API sont internes, mais il n’est pas rare qu’elles soient soudainement exposées à Internet à mesure que les cas d’utilisation des applications évoluent. La meilleure façon de garantir la sécurité des API est au moment de la création. En fournissant aux développeurs des outils de sécurité des API, ils ont la possibilité de résoudre les problèmes de sécurité dès le début du processus de développement de l’application.</p>'}, {'', '<p>Plusieurs failles de sécurité informatique impliquant des API ont déjà été constatées l’année dernière. Ce n’est peut-être qu’une question de temps avant que d’autres failles ne se produisent et puissent s’avérer catastrophiques. Quelle que soit l’ampleur de la faille de sécurité de l’API, l’équipe de développement d’applications qui a créé et déployé l’API en premier lieu sera tenue responsable de sa résolution. Le défi consiste à trouver le moyen le plus simple d’éliminer le problème en premier lieu à un moment où le nombre d’API créées et déployées ne cesse d’exploser.</p>'}]"
