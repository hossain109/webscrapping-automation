Title,Content
Une enquête révèle une adoption accrue de DevOps dans les environnements mainframe,"[{'', ""<p>Une enquête menée auprès de 1\xa0223 professionnels de l'informatique travaillant dans des environnements mainframe révèle que 62 % d'entre eux travaillent pour des organisations qui ont adopté les pratiques DevOps, et 35 % s'appuient sur les workflows DevOps pour créer et déployer des applications sur des mainframes et des systèmes informatiques distribués.</p>""}, {'', ""<p>Réalisée par BMC, l'enquête révèle également que 70 % des personnes interrogées travaillent dans des organisations qui ont adopté l'intelligence artificielle (IA) pour les opérations informatiques (AIOps), 33 % d'entre elles l'utilisant à la fois sur les mainframes et dans les environnements informatiques distribués. L'AIOps est désormais la troisième priorité la plus importante (45 %) citée, après la conformité/sécurité (64 %) et l'optimisation des coûts (49 %).</p>""}, {'', '<p>De plus, plus des trois quarts (76 %) travaillent pour des organisations qui utilisent à des degrés divers l’IA générative, et 40 % d’entre eux constatent déjà des avantages tels qu’une automatisation accrue qui réduit les tâches répétitives (37 %), l’analyse du code et des fichiers pour détecter les vulnérabilités (36 %) et des informations commerciales (34 %).</p>'}, {'', ""<p>Enfin, l'enquête révèle que les mainframes exécutent des charges de travail plus diversifiées, 64 % des personnes interrogées indiquant que les nouvelles applications mainframe sont écrites en Java. Au total, 43 % ont ajouté que leur base d'applications Java existante s'élargit, tandis que 55 % réécrivent des applications existantes en Java.</p>""}, {'', ""<p>John McKenny, vice-président senior et directeur général de l'optimisation et de la transformation de l'Intelligent Z chez BMC, a déclaré que l'enquête montre clairement que les organisations informatiques dotées de mainframes sont à l'avant-garde de plusieurs vagues d'innovation technologique. Cette vague se poursuivra l'année prochaine, car IBM proposera des plates-formes de nouvelle génération qui utilisent un processeur IBM Telum II à 5,5 GHz avec huit cœurs, mieux optimisé pour exécuter des moteurs d'inférence d'intelligence artificielle (IA) ainsi qu'une puce IBM Spyre Accelerator qui simplifie l'invocation de plusieurs modèles d'IA.</p>""}, {'', ""<p>Ces types de fonctionnalités permettent aux entreprises qui ont investi dans des mainframes de continuer à voir la plateforme évoluer de manière à pouvoir, par exemple, exécuter efficacement des applications Python, a ajouté M. McKenny. En fait, 94 % des personnes interrogées continuent d'avoir une opinion positive du mainframe, a-t-il noté.</p>""}, {'', '<h3>Rendre les mainframes plus accessibles</h3>'}, {'', '<p>L’objectif est désormais de rendre les mainframes plus accessibles à un plus large éventail de professionnels de l’informatique, à un moment où de nombreuses personnes qui possèdent actuellement cette expertise s’apprêtent à prendre leur retraite, a souligné M. McKenny. Plutôt que d’exiger que la prochaine génération d’équipes informatiques apprenne à maîtriser les nuances du mainframe, les outils nécessaires à la gestion des mainframes sont modernisés pour l’ère de l’IA, a-t-il ajouté.</p>'}, {'', '<p>Bien entendu, le coût d’acquisition d’un mainframe reste plus élevé que celui des autres plateformes, mais compte tenu de la manière dont IBM facture aux entreprises l’utilisation des mainframes, la plateforme continue au fil du temps de fournir en fin de compte un coût total de possession (TCO) inférieur, par exemple en permettant à certaines charges de travail exécutées sur LinuxOne de fonctionner sans coût supplémentaire. Le défi consiste désormais à s’assurer que les entreprises disposent toujours d’une expertise mainframe suffisante pour atteindre cet objectif.</p>'}, {'', '<p>Chaque organisation devra finalement décider quel type de charge de travail est le plus logique à exécuter sur quel type de plateforme. Cependant, le mainframe continue d’exécuter certaines classes d’applications de traitement des transactions en ligne (OLTP) et les analyses associées à des niveaux de performances que les autres plateformes ne peuvent toujours pas égaler.</p>'}]"
"Une enquête révèle des perspectives positives pour DevOps, qui stimulent les investissements","[{'', ""<p>Une enquête menée auprès de plus de 500 praticiens DevOps révèle qu'environ 60 % des répondants prévoient d'augmenter leurs investissements dans les outils et plateformes DevOps au cours des deux prochaines années, et 20 % prévoient des augmentations budgétaires significatives.</p>""}, {'', ""<p>L'enquête a été menée par Techstrong Research, une branche de Techstrong Group, qui est également la société mère de DevOps.com. Les répondants ont indiqué que les domaines dans lesquels les organisations investissent le plus au cours des deux prochaines années sont les outils et plateformes DevOps de base, notamment les tests, les référentiels d'artefacts de code, l'observabilité, les plateformes de déploiement automatisées, les portails de développeurs internes (IDP) et les outils d'infrastructure en tant que code (IaC).</p>""}, {'', '<p>Plus précisément, un peu moins d’un tiers (32 %) des entreprises prévoient de remplacer ou de mettre à niveau leur plateforme d’intégration continue/livraison continue (CI/CD) dans les 12 à 18 prochains mois, tandis que 23 % évaluent actuellement leurs besoins futurs. Cependant, seulement 16 % prévoient de passer à une plateforme DevOps intégrée dans les 12 à 18 prochains mois, contre 37 % qui l’ont déjà fait. En outre, seulement 20 % ont adopté l’ingénierie de plateforme comme méthodologie de gestion de DevOps à grande échelle.</p>'}, {'', ""<p>Dans l'ensemble, l'enquête révèle que 88 % des personnes interrogées considèrent l'avenir de DevOps comme globalement positif, la moitié (50 %) le décrivant comme très positif. Seuls 1,5 % des répondants ont une vision négative de DevOps.</p>""}, {'', '<p>De plus, 62,5 % des personnes interrogées ont déclaré considérer l’intelligence artificielle (IA) comme un ajout extrêmement ou très précieux aux flux de travail DevOps. Par rapport à 10,1 % qui considèrent l’IA comme peu ou pas du tout utile. De nombreuses organisations utilisent déjà l’IA dans le développement aujourd’hui (32,7 %), et 41,5 % envisagent de l’utiliser. Seuls 9 % ont déclaré que l’IA était entièrement intégrée à leurs flux de travail DevOps.</p>'}, {'', '<p>Selon Mitch Ashley, analyste principal chez Techstrong Research, l’enquête montre clairement que même si les modes s’estompent et que d’autres tendances disparaissent, DevOps continue d’apporter de la valeur aux équipes de développement logiciel et aux organisations informatiques. DevOps a également un long chemin à parcourir en matière d’investissements, les investissements dans l’automatisation, l’IA et la sécurité des applications entraînant une vague d’investissements supplémentaires, a-t-il ajouté.</p>'}, {'', '<p>En fait, près des trois quarts (72,7 %) des répondants appliquent DevOps sur 50 % ou plus de leurs projets, et 18,7 % des répondants appliquent DevOps sur tous les projets. Les phases de développement et de test du cycle de vie du développement logiciel sont les domaines qui font le plus appel aux pratiques DevOps, avec respectivement 78,6 % et 72,4 %.</p>'}, {'', '<p>81 % des personnes interrogées ont identifié l’augmentation de la vitesse à laquelle leur entreprise délivre des logiciels dans des environnements de production comme l’un des principaux avantages de DevOps, 45,6 % d’entre elles décrivant ces augmentations comme étant élevées. Un pourcentage similaire (80 %) a déclaré que DevOps permet également à leur entreprise d’obtenir des délais de mise sur le marché plus rapides.</p>'}, {'', ""<p>L'enquête révèle qu'un total de 81 % des personnes interrogées ont également déclaré que DevOps améliore la productivité des développeurs, 35 % d'entre elles décrivant ces gains comme élevés. Les équipes DevOps publient du code plus fréquemment, les logiciels étant livrés en production sur une base quotidienne (11,8 %), hebdomadaire (34,1 %) et mensuelle (29 %) respectivement.</p>""}, {'', ""<p>Malgré ces avantages, l'enquête montre également clairement que des améliorations supplémentaires des flux de travail DevOps sont nécessaires, les personnes interrogées estimant que l'impact de DevOps est faible dans la gestion de la complexité (16,3 %), l'augmentation de la collaboration entre les silos (14,8 %) et l'amélioration de la sécurité des logiciels (13 %).</p>""}, {'', '<p>En général, l’enquête révèle que plus de la moitié des personnes interrogées (55,8 %) ont atteint des niveaux de maturité DevOps plus élevés. Au sein de ce groupe, 34,4 % d’entre elles normalisent DevOps sur les projets de leur organisation, tandis que 21,5 % ont déclaré avoir atteint un niveau de maîtrise des meilleures pratiques DevOps.</p>'}, {'', ""<p>En comparaison, 18,1 % ont indiqué qu'ils testaient DevOps sur un sous-ensemble de projets, tandis que 21,8 % appliquaient les enseignements DevOps à plusieurs projets.</p>""}, {'', '<p>L’enquête montre clairement que les entreprises continuent d’adopter les meilleures pratiques DevOps selon leurs besoins. Une chose est sûre, cependant, à des degrés divers, DevOps est désormais plus indispensable que jamais.</p>'}, {'', ""<p>Pour plus d'informations, téléchargez une copie du rapport DevOps Next ici.</p>""}]"
Optimisation des tests ETL pour une meilleure qualité et fiabilité des données,"[{'', '<p>Avez-vous déjà pris une décision commerciale sur la base de données inexactes ou incomplètes ? Dans le monde actuel axé sur les données, la qualité et la fiabilité de vos données peuvent faire la différence entre votre stratégie commerciale et la vôtre. Les entreprises s’appuyant de plus en plus sur les données pour guider leurs décisions, il est plus important que jamais de s’assurer que ces données sont exactes, cohérentes et fiables. C’est là qu’entrent en jeu les tests ETL (Extract, Transform, Load).</p>'}, {'', ""<p>Les processus ETL sont essentiels pour déplacer des données provenant de diverses sources vers un système unifié, mais sans tests rigoureux, même de petites erreurs dans ces processus peuvent entraîner des problèmes de qualité des données importants. Ce blog met en évidence les meilleures pratiques, les défis courants et les solutions innovantes pour optimiser les tests ETL, vous aidant à préserver l'intégrité de vos données et à prendre des décisions en toute confiance.</p>""}, {'', '<h3>Libérer la puissance des tests ETL</h3>'}, {'', ""<p>Dans le domaine de la gestion des données, les tests ETL constituent la pierre angulaire de l'assurance qualité des données. Leur importance ne peut être surestimée, car ils servent de gardien de l'intégrité des données tout au long du pipeline de données. Voici quelques raisons pour lesquelles les tests ETL sont essentiels :</p>""}, {'', ""<li>Validation de l'exactitude des données : vérifie méticuleusement que la transformation des données est exécutée correctement, garantissant que les données chargées dans les systèmes cibles sont exactes et fiables.</li>""}, {'', ""<li>Conformité aux règles métier : confirme que les données adhèrent aux règles et normes métier prédéfinies, préservant ainsi la cohérence au sein de l'organisation.</li>""}, {'', ""<li>Assurance de l'intégration des données : valide l'intégration transparente des données provenant de sources disparates, essentielle pour une veille économique et des analyses complètes.</li>""}, {'', '<li>Détection et prévention des erreurs : en identifiant les problèmes au début du pipeline de données, les tests ETL empêchent la propagation d’erreurs pouvant conduire à des décisions commerciales coûteuses.</li>'}, {'', '<li>Support de gouvernance des données : il joue un rôle essentiel dans le maintien des politiques de gouvernance des données en garantissant la qualité, la sécurité et la conformité des données tout au long du processus ETL.</li>'}, {'', '<p>« D’ici 2028, les outils basés sur GenAI seront capables d’écrire 70 % des tests logiciels, réduisant ainsi le besoin de tests manuels et améliorant la couverture des tests, la convivialité des logiciels et la qualité du code ». IDC.</p>'}, {'', '<h3>Défis pour atteindre la qualité des données</h3>'}, {'', ""<p>Bien que la recherche de données de haute qualité grâce aux tests ETL soit cruciale, les organisations se retrouvent souvent confrontées à un paysage complexe rempli d'obstacles. Ces défis découlent de la nature complexe des écosystèmes de données modernes, du rythme rapide des changements technologiques et des demandes toujours croissantes d'informations basées sur les données. Comprendre ces obstacles est la première étape vers l'élaboration de stratégies robustes pour les surmonter et atteindre le niveau de qualité des données souhaité. Les principaux défis des tests ETL auxquels les organisations sont confrontées sont les suivants :</p>""}, {'', '<li>Volume et diversité des données : les méthodes de test traditionnelles ont souvent du mal à gérer des téraoctets ou des pétaoctets de données, ce qui entraîne des cycles de test prolongés et des contraintes de ressources. De plus, les données se présentent sous différents formats structurés, semi-structurés et non structurés, chacun nécessitant des approches de traitement et de validation différentes.</li>'}, {'', ""<li>Transformations complexes : les transformations de données complexes sont difficiles à tester de manière approfondie, en particulier lorsqu'elles impliquent plusieurs règles métier et cas limites. Les transformations conditionnelles complexes créent une multitude de chemins de données possibles, ce qui rend difficile le test complet de tous les scénarios.</li>""}, {'', '<li>Contraintes de temps : la demande croissante de traitement des données en temps réel ou quasi réel exerce une pression sur les équipes de test pour valider la qualité des données à grande vitesse. Équilibrer le temps consacré au développement et aux tests conduit souvent à des compromis dans la couverture des tests.</li>'}, {'', ""<li>Différences d'environnement : les variations de puissance de traitement, de mémoire ou de stockage entre les environnements de test et de production peuvent masquer des problèmes de qualité des données liés aux performances. De même, les incohérences dans les versions ou les configurations logicielles entre les environnements peuvent entraîner un comportement inattendu en production.</li>""}, {'', ""<li>Manque de visibilité de bout en bout : il peut être extrêmement difficile de suivre les données via des processus ETL complexes pour identifier la cause profonde des problèmes de qualité. En outre, une surveillance inadéquate de l'ensemble du pipeline de données peut entraîner des angles morts où les problèmes de qualité des données passent inaperçus.</li>""}, {'', ""<li>Évolution du paysage des données : l'ajout fréquent de nouvelles sources de données nécessite des mises à jour constantes des processus ETL et des cas de test correspondants. L'évolution des réglementations en matière de confidentialité et de conformité des données nécessite des ajustements continus des procédures de traitement et de test des données.</li>""}, {'', '<h3>Stratégies pour obtenir des données de type production</h3>'}, {'', '<p>Pour garantir l’efficacité des tests ETL, il est essentiel de travailler avec des données qui ressemblent étroitement aux données de production. Voici quelques stratégies pour obtenir efficacement des données de type production\xa0:</p>'}, {'', ""<li>Sous-ensemble de données : créez des sous-ensembles représentatifs de données de production qui conservent les caractéristiques et les complexités de l'ensemble de données complet.</li>""}, {'', '<li>Masquage des données : mettez en œuvre des techniques de masquage des données robustes pour protéger les informations sensibles tout en préservant les propriétés statistiques des données.</li>'}, {'', '<li>Génération de données synthétiques : utilisez des algorithmes avancés pour générer des données synthétiques qui reflètent les modèles et les distributions des données de production.</li>'}, {'', '<li>Actualisation incrémentielle des données : mettez à jour les données de test avec de nouvelles données de production pour garantir la pertinence et capturer de nouveaux modèles de données.</li>'}, {'', '<li>Copies de données virtuelles : exploitez les technologies de virtualisation de bases de données pour créer des copies légères et à jour des données de production à des fins de test.</li>'}, {'', '<li>Profilage et analyse des données : effectuez un profilage approfondi des données pour comprendre les caractéristiques des données de production et les reproduire dans des environnements de test.</li>'}, {'<h3>Bonnes pratiques pour les tests ETL</h3>', ''}, {'', '<li>Établissez des objectifs de test clairs : définissez des objectifs spécifiques et mesurables pour chaque phase de test afin de garantir une couverture complète.</li>'}, {'', '<li>Implémenter le contrôle de version : utilisez des systèmes de contrôle de version pour suivre les modifications dans les processus ETL et les cas de test, facilitant ainsi le dépannage et les restaurations.</li>'}, {'', ""<li>Automatisez les tests répétitifs : exploitez les outils d'automatisation des tests pour exécuter des tests de routine, libérant ainsi des ressources pour des scénarios de test plus complexes.</li>""}, {'', ""<li>Priorisez les cas de test : concentrez-vous sur les éléments de données critiques et les zones à haut risque pour maximiser l'impact des efforts de test.</li>""}, {'', '<li>Mettre en œuvre des tests continus : intégrez les tests tout au long du cycle de développement pour découvrir et résoudre les problèmes le plus tôt possible.</li>'}, {'', '<li>Documentez minutieusement : conservez une documentation détaillée des cas de test, des résultats et de tout problème de qualité des données découvert pendant les tests.</li>'}, {'', '<li>Collaborer entre les équipes : Favorisez une collaboration étroite entre les ingénieurs de données, les testeurs et</li>'}, {'', '<h3>Types de tests ETL</h3>'}, {'', ""<p>Les tests ETL sont essentiels pour garantir l'exactitude et l'intégrité des données lors de leur transfert de leur source d'origine à leur destination. Ce processus implique une série de contrôles et de validations pour détecter les erreurs, les incohérences et d'autres problèmes lors des étapes d'extraction, de transformation et de chargement des données. Pour gérer efficacement cela, les tests ETL sont classés en différents types, chacun ciblant des aspects spécifiques du pipeline de données.</p>""}, {''}, {'', '<h3>Avantages des tests ETL automatisés</h3>'}, {'', ""<p>Les tests ETL automatisés sont devenus une véritable révolution pour les entreprises. Ils offrent une solution puissante aux défis liés à la garantie de la qualité des données à grande échelle. Alors que les entreprises sont confrontées à des volumes de données croissants, à des transformations complexes et à la nécessité d'obtenir des informations rapides, l'automatisation des tests se distingue par son efficacité et sa fiabilité. En exploitant des outils et des technologies de pointe, les tests ETL automatisés accélèrent non seulement le processus de test, mais améliorent également sa précision et son exhaustivité. Grâce à l'automatisation, les entreprises peuvent bénéficier des avantages suivants :</p>""}, {'', '<li>Couverture de test accrue : permet des tests plus complets sur une plus large gamme de scénarios et de variations de données.</li>'}, {'', '<li>Exécution plus rapide : les tests automatisés peuvent être exécutés rapidement et fréquemment, permettant une identification rapide des problèmes.</li>'}, {'', '<li>Cohérence et fiabilité : élimine les erreurs humaines et garantit une exécution cohérente des cas de test.</li>'}, {'', ""<li>Évolutivité : offre la possibilité d'augmenter ou de diminuer la capacité sans effort, ce qui est particulièrement avantageux pour gérer les charges de pointe ou les exigences de test fluctuantes.</li>""}, {'', ""<li>Efficacité et flexibilité améliorées : en automatisant les tâches répétitives, les testeurs peuvent se concentrer sur des activités de test plus complexes et à forte valeur ajoutée. Cela permet le déploiement et la gestion dynamiques des machines virtuelles, du stockage et des réseaux, accélérant ainsi le processus de test et facilitant l'itération et l'expérimentation rapides.</li>""}, {'', ""<li>Détection précoce des problèmes : des tests automatisés continus tout au long du processus de développement permettent d'identifier les problèmes plus tôt, réduisant ainsi le coût des correctifs.</li>""}, {'', '<li>Rapports améliorés : les outils de test automatisés fournissent souvent des rapports et des analyses détaillés, offrant des informations plus approfondies sur les résultats et les tendances des tests.</li>'}, {'', '<li>Maintenance plus facile : les tests automatisés bien conçus sont plus faciles à mettre à jour et à entretenir à mesure que les processus ETL évoluent.</li>'}, {'', ""<h3>Assurez l'avenir de votre stratégie de données avec un partenaire de test ETL compétent</h3>""}, {'', '<p>L’optimisation des tests ETL n’est pas seulement une nécessité technique ; c’est un impératif stratégique pour toute organisation qui s’appuie sur les données pour prendre des décisions. AgreeYa est l’une de ces options qui aident les organisations en proposant des stratégies de test robustes, en surmontant les défis courants et en tirant parti de l’automatisation.</p>'}]"
Observe ajoute des agents d'IA génératifs à sa plateforme d'observabilité,"[{'', ""<p>Observe, Inc. a lancé aujourd'hui une mise à jour du projet Voyager qui ajoute des agents d'intelligence artificielle (IA) générative à sa plateforme d'observabilité éponyme.</p>""}, {'', ""<p>En outre, la société ajoute la prise en charge de la gestion des performances des applications (APM) open source OpenTelemetry, une instance d'APM basée sur le logiciel agent open source développé sous les auspices de la Cloud Native Computing Foundation (CNCF). Observez l'utilisation antérieure d'OpenTelemetry pour collecter des données.</p>""}, {'', '<p>Enfin, Observe propose désormais également une intégration avec le lac de données Snowflake pour le stockage des données de télémétrie. Cette fonctionnalité permet aux équipes informatiques de lancer des requêtes sur ces données sans avoir à les déplacer en dehors du lac de données Snowflake.</p>'}, {'', ""<p>Après avoir levé 145 millions de dollars supplémentaires, le PDG d'Observe, Jeremy Burton, a déclaré que la société avait développé AI Investigator, un ensemble d'agents IA génératifs qui ont été formés pour effectuer des tâches spécifiques telles que l'accès aux dossiers d'exécution ou aux incidents antérieurs, la compréhension des plateformes Kubernetes, Amazon Web Service (AWS) ou GitHub, ou la génération de requêtes d'observabilité. Une fois les problèmes résolus, des résumés seront générés et pourront être utilisés pour former davantage ou ajouter des agents IA supplémentaires.</p>""}, {'', '<p>Les agents IA sont orchestrés par un « planificateur IA » principal qui gère le flux de travail de dépannage. En fait, il s’agit d’un compagnon ou d’un assistant numérique conçu pour les ingénieurs DevOps et les administrateurs informatiques. En conséquence, les équipes DevOps et informatiques se retrouveront à travailler en collaboration aux côtés d’assistants numériques formés pour effectuer des tâches qui auparavant auraient nécessité un effort manuel, a noté Burton.</p>'}, {'', ""<p>Auparavant, Observe avait ajouté un copilote d'IA générative qui fournit, par exemple, des résumés d'incidents informatiques. Les agents d'IA font passer l'IA générative au niveau supérieur en exploitant les moteurs de raisonnement intégrés dans plusieurs grands modèles de langage (LLM) pour automatiser des tâches spécifiques.</p>""}, {''}, {'', ""<p>Observe affirme avoir désormais près de 100 clients, dont Capital One et Commonwealth Bank of Australia. Le chiffre d'affaires annuel récurrent (ARR) a augmenté de plus de 200 % par rapport à l'année précédente, le chiffre d'affaires net global ayant augmenté de 190 % à la fin du premier semestre de son exercice 2025, selon la société.</p>""}, {'', '<p>En général, la manière dont l’informatique a été historiquement gérée est en train de changer fondamentalement à l’ère de l’IA générative. Chaque organisation devra déterminer dans quelle mesure elle fera confiance aux agents d’IA pour effectuer des tâches spécifiques, mais à mesure que de nouvelles avancées en matière de LLM seront réalisées, les capacités de raisonnement de ces agents d’IA ne feront que devenir plus robustes. Un grand nombre de tâches autrefois effectuées manuellement par les professionnels de l’informatique seront de plus en plus automatisées.</p>'}, {'', '<p>Il est moins évident de savoir dans quelle mesure ce niveau d’automatisation peut conduire à une réorganisation des équipes informatiques, certaines tâches qui nécessitaient auparavant des spécialistes étant désormais effectuées par des agents d’IA. Cela ne signifie pas pour autant que les humains ne seront plus nécessaires pour gérer les flux de travail informatiques. Cependant, la nature fondamentale des emplois dans le domaine informatique est en train de changer. Dans la plupart des cas, cela devrait se traduire par moins de travail et de stress pour toutes les personnes concernées, mais il ne fait aucun doute que des équipes plus petites seront également en mesure de gérer l’informatique à des niveaux d’échelle qui auraient semblé inimaginables il n’y a pas si longtemps.</p>'}]"
L'agent AWS AI pour le développement de logiciels prend en charge des tâches plus complexes,"[{'', ""<p>Amazon Web Services (AWS) a publié une mise à jour de son agent Amazon Q Developer pour le développement de logiciels qui, selon les tests d'évaluation, peut résoudre 51 % de tâches supplémentaires.</p>""}, {'', ""<p>En utilisant un benchmark, baptisé SWE-bench, créé par OpenAI qui évalue la capacité d'une plateforme d'intelligence artificielle (IA) à résoudre les problèmes de développement logiciel qu'un développeur Python pourrait rencontrer, le score de l'agent Amazon Q Developer a augmenté depuis sa première mise à disposition, passant de 25,6 % de tâches résolues à 38,8 % sur l'ensemble de données vérifié et de 13,82 % à 19,75 % sur l'ensemble de données SWE-bench complet.</p>""}, {'', ""<p>Neha Goswami, directrice de l'ingénierie pour Amazon Q Developer, a déclaré que ces résultats montrent qu'au fil du temps, les agents d'IA tels qu'Amazon Q Developer continuent d'évoluer de manière à, par exemple, tirer parti des avancées dans les capacités de raisonnement permises par les grands modèles linguistiques (LLM) pour résoudre des tâches de plus en plus complexes.</p>""}, {'', ""<p>Entre-temps, de nombreux développeurs utilisent déjà l'interface en langage naturel qu'Amazon Q Developer expose pour analyser les bases de code existantes et exécuter des modifications de code en quelques minutes, a noté Goswami.</p>""}, {'', ""<p>Cette capacité, à son tour, permet aux organisations de rester plus facilement au courant des dernières générations de mises à jour des langages de programmation, par exemple en réduisant le niveau de travail requis pour mettre à jour vers la dernière version de Java, a-t-elle ajouté. À plus long terme, les outils d'IA générative tels qu'Amazon Q Developer faciliteront la conversion du code écrit dans un langage de programmation vers un autre, a déclaré Goswami.</p>""}, {'', ""<p>En général, les agents IA sont formés pour effectuer une gamme beaucoup plus large de tâches complexes. L'agent Amazon Q Developer peut ouvrir, créer et fermer des fichiers, sélectionner et désélectionner des segments de code, rechercher et remplacer du code et annuler les modifications si nécessaire.</p>""}, {'', ""<p>La réponse des outils invoqués est ensuite incorporée dans une invite mise à jour qui est renvoyée au LLM pour décider de ses prochaines actions. L'agent décidera de manière autonome qu'il a généré les modifications appropriées pour répondre à une demande qui est ensuite partagée avec un développeur pour examen. L'agent Q Developer est également doté d'une logique pour l'empêcher de rester bloqué dans des chemins improductifs.</p>""}, {'', ""<p>AWS a également développé un framework de code texte pour l'agent Amazon Q Developer qui utilise des jetons pour créer des représentations de code, de fichiers et d'espaces de travail, ce qui permet à un LLM de découvrir plus facilement les éléments d'un environnement de développement logiciel.</p>""}, {'', '<p>On ne sait pas exactement combien de développeurs ont adopté l’IA générative non seulement pour écrire du code mais aussi pour gérer des tâches, mais à mesure que les LLM continuent d’évoluer, le rythme auquel les applications sont créées et déployées devrait s’accélérer considérablement. Aujourd’hui, la plupart des avantages tirés de l’IA générative se traduisent par une écriture plus rapide du code, mais à mesure que les capacités de raisonnement des LLM s’améliorent, les flux de travail DevOps utilisés pour déployer les applications devraient également devenir plus automatisés. La quantité de logiciels qui pourrait être déployée dans les prochaines années pourrait dépasser de loin ce qui a été déployé au cours de la dernière décennie.</p>'}, {'', '<p>Le défi consiste à comprendre quelles tâches les agents d’IA générative sont capables d’effectuer correctement aujourd’hui plutôt qu’à planifier une prochaine vague d’agents plus avancés qui piloteront la prochaine ère de DevOps.</p>'}]"
Automatiser les tests de sécurité des applications Web pour lutter contre les cybermenaces,"[{'', '<p>Pour la plupart des professionnels de la cybersécurité, les tests de sécurité peuvent sembler une évidence, presque comme un pari risqué. Pourtant, rien n’est plus faux. Malgré les centaines d’applications Web et d’API exposées dans nos surfaces d’attaque, de nombreux actifs restent dangereusement non testés et vulnérables aux cyberattaques. Avec l’essor de l’IA, ce nombre ne fera qu’augmenter.</p>'}, {'', '<p>Cela n’a rien de surprenant. Une enquête récente menée auprès de plus de 100 professionnels de la cybersécurité au Royaume-Uni a révélé que les menaces pesant sur leurs applications Web étaient très préoccupantes. Pourtant, la plupart des équipes de sécurité ne parviennent à tester ces applications qu’une fois par mois, ce qui laisse une part importante d’entre elles vulnérables, ce qui met en évidence une lacune critique dans nos programmes de cybersécurité.</p>'}, {'', '<p>Alors pourquoi n’arrivons-nous pas à réaliser des tests corrects ?</p>'}, {'', '<h3>Les surfaces d’attaque deviennent ingérables</h3>'}, {'', ""<p>Les surfaces d'attaque ont toujours été des cibles mouvantes. Elles fluctuent à mesure que les organisations développent leurs piles technologiques et s'intègrent aux systèmes d'autres clients et partenaires. Mais à long terme, elles ne font que croître, ce qui rend difficile de suivre le rythme.</p>""}, {'', ""<p>Les mêmes professionnels de la cybersécurité britanniques ont révélé que leurs organisations ont du mal à faire face au volume considérable et à la nature dynamique des applications Web. En fait, 54,2 % des répondants ont admis que le nombre d'applications Web dans leur environnement est trop important pour permettre des tests adéquats.</p>""}, {'', '<p>D’autres obstacles importants incluent le nombre d’API à tester et le temps nécessaire pour tester chaque application Web, cités respectivement par 59,8 % et 55,1 % des répondants.</p>'}, {'', ""<p>Le sondage a également révélé un fait choquant : ces organisations subissent chaque trimestre des événements de sécurité importants liés à leur application Web, dont la résolution peut prendre jusqu'à huit heures.</p>""}, {'', '<h3>Alors, où sont les tests ?</h3>'}, {'', '<p>Les organisations utilisent diverses méthodes, notamment DAST, IAST et les tests de pénétration, pour identifier les vulnérabilités, les erreurs de configuration et autres faiblesses des applications Web.</p>'}, {'', '<p>Pourtant, plus d’un quart des personnes interrogées ont admis ne pas disposer d’un processus formel pour tester la sécurité de leurs applications Web. Près de la moitié d’entre elles ont déclaré qu’elles utilisaient rarement des outils ou des méthodes de test de sécurité pour découvrir les vulnérabilités de leurs applications Web.</p>'}, {'', '<p>Raisons des tests peu fréquents et de la couverture limitée :</p>'}, {'', ""<li>Trop d'applications et d'API : le nombre d'applications et d'API à tester peut être écrasant</li>""}, {'', '<li>Manque de temps : les contraintes de temps empêchent des tests approfondis et fréquents</li>'}, {'', ""<li>Mises à jour et modifications fréquentes des applications\xa0: les mises à jour et les modifications constantes des applications rendent difficile le maintien d'un calendrier de tests cohérent</li>""}, {'', '<li>Insuffisance de personnel : manque de personnel qualifié pour effectuer des tests approfondis</li>'}, {'', '<li>Limitations budgétaires : les contraintes financières limitent la capacité à investir dans des outils et des ressources de test complets.</li>'}, {'', ""<h3>S'attaquer au problème</h3>""}, {'', ""<p>Au-delà des contraintes de temps et de ressources, l'amélioration de la fréquence et de l'efficacité des tests n'est pas négociable. Voici quelques bonnes pratiques :</p>""}, {'', ""<li>Surveillance continue : la visibilité continue sur la surface d'attaque permet aux organisations d'être proactives et de guider efficacement les activités de correction. La surveillance continue permet d'identifier les vulnérabilités à un stade précoce, réduisant ainsi le risque d'attaques réussies.</li>""}, {'', ""<li>Automatisation : près des trois quarts des dirigeants britanniques interrogés ont déclaré qu'ils prévoyaient d'accroître l'automatisation de leurs flux de tests de sécurité des applications Web. L'automatisation peut permettre d'économiser du temps, de l'argent et des efforts, à condition qu'elle ne crée pas de problèmes supplémentaires tels que la génération de faux positifs. Elle peut rationaliser le processus de test, permettant des évaluations plus fréquentes et plus complètes.</li>""}, {'', ""<li>Tests de production : les tests effectués dans l'environnement de production, plutôt que dans des sandbox ou hors ligne, garantissent que tous les éléments affectant une application Web, y compris les bases de données, les bibliothèques open source et les mécanismes d'authentification, sont pris en compte. Cette approche fournit une représentation plus précise des vulnérabilités potentielles et de leurs impacts.</li>""}, {'', ""<li>Investissement dans DevSecOps : pour accélérer les cycles de développement et améliorer les délais de mise sur le marché, les entreprises ont investi dans des logiciels DevOps pour publier du code plus rapidement. Cependant, elles n'ont pas investi dans des logiciels de sécurité (DevSecOps). L'intégration de la sécurité dans le pipeline DevOps est essentielle pour garantir qu'un développement rapide ne se fasse pas au détriment d'une sécurité compromise.</li>""}, {'', '<h3>Prendre du recul</h3>'}, {'', '<p>Le message principal ici est que les organisations sont de plus en plus exposées et que les méthodes de test sont inadéquates pour sécuriser les environnements.</p>'}, {'', '<p>Les méthodes de test automatisées ne sont pas une mince affaire. Un changement rapide peut fournir une couverture plus complète, une identification plus rapide des vulnérabilités et un processus de correction plus rapide.</p>'}, {'', ""<p>Il peut rationaliser les processus manuels à forte intensité de main-d'œuvre en effectuant des tests continus ou fréquents de toutes les applications Web et des API associées dans l'environnement, en identifiant avec précision les risques et en filtrant les problèmes ou événements de faible priorité.</p>""}, {'', '<p>Non seulement cela améliorera considérablement la posture de sécurité d’une organisation, mais cela soulagera également une partie de la pression sur l’équipe de sécurité.</p>'}]"
Une étude révèle que l'IA et l'automatisation accélèrent le rythme du développement de logiciels,"[{'', ""<p>Une enquête menée auprès de 555 dirigeants d'entreprises de logiciels et publiée cette semaine révèle que 75 % d'entre eux ont constaté une réduction allant jusqu'à 50 % du temps de développement grâce à la mise en œuvre de diverses technologies d'intelligence artificielle (IA) et d'automatisation.</p>""}, {'', ""<p>Réalisée par la branche CIO Dive de studioID pour le compte d'Outsystems, fournisseur d'une plateforme low-code, et de KPMG, l'enquête révèle que plus de la moitié (56 %) des personnes interrogées ont déclaré avoir bénéficié ou s'attendre à bénéficier d'applications de meilleure qualité, avec moins de bugs et des performances améliorées. Un tiers des répondants (33 %) ont déclaré avoir un arriéré de 150 à 800 cas d'utilisation pour l'IA générative en particulier.</p>""}, {'', '<p>Ces cas d’utilisation de l’IA générative incluent l’optimisation DevOps (59 %), la génération de code (58 %), la documentation (56 %) et la conception d’interfaces utilisateur (50 %).</p>'}, {'', '<p>Rodrigo Coutinho, responsable produit IA chez OutSystems, a déclaré que la plupart des entreprises utilisent aujourd’hui l’IA générative pour écrire du code et réaliser des tests, et qu’il faudra peut-être encore un certain temps avant qu’elle ne soit appliquée à l’ensemble du cycle de vie du développement logiciel (SDLC). En fait, 38 % des dirigeants ont cité les difficultés d’intégration de l’IA générative dans les flux de travail existants comme leur principal obstacle à l’adoption. Au total, 39 % ont également noté qu’il y avait encore un manque d’expertise en IA au sein de leurs équipes de développement logiciel.</p>'}, {'', '<p>En outre, d’autres défis subsistent, tels que la confidentialité des données et la sécurité (56 %) ainsi que les défis réglementaires et de conformité (42 %).</p>'}, {'', '<p>Néanmoins, presque tous les répondants (93 %) prévoient d’augmenter leur investissement dans des outils augmentés par l’IA au cours des deux prochaines années, 71 % prévoyant d’intégrer l’IA dans le développement d’applications et les flux de travail de gestion du SDLC.</p>'}, {'', '<p>La plupart des avantages de l’IA ont été constatés par les développeurs professionnels, mais avec le temps, l’IA devrait élargir les rangs des développeurs citoyens, a déclaré Coutinho. Près de la moitié des personnes interrogées (47 %) ont déclaré qu’elles s’attendaient à l’émergence d’un nouveau type de développeur d’applications doté de compétences spécialisées en IA, telles que l’ingénierie rapide, tandis que 43 % ont déclaré qu’elles s’attendaient à ce que la responsabilité des développeurs s’élargisse.</p>'}, {'', '<p>En général, les entreprises doivent adopter l’IA générative avec prudence. Les recommandations formulées sont probabilistes, les équipes DevOps doivent donc comprendre que certaines des réponses proposées sont très créatives, a noté Coutinho. Cependant, au fil du temps, à mesure que des modèles de langage plus étendus (LLM) spécifiques à un domaine seront formés, la qualité globale des suggestions formulées deviendra plus précise, a-t-il ajouté.</p>'}, {'', '<p>Une chose est sûre : le rythme auquel les logiciels sont créés et déployés ne fera qu’augmenter à l’ère de l’IA générative. La qualité du code utilisé pour créer ces logiciels ne s’améliorera peut-être pas à court terme par rapport à ce que les humains écrivent aujourd’hui, mais avec le temps, la qualité des applications devrait s’améliorer, car, par exemple, moins de vulnérabilités seront créées.</p>'}, {'', '<p>En attendant, les entreprises devraient plutôt se lancer dans l’IA plutôt que de se lancer dans l’opérationnalisation de celle-ci de manière réfléchie, a déclaré Coutinho. La combinaison gagnante à l’avenir comprendra toujours un mélange d’humains et de machines qui garantissent le déploiement d’applications de haute qualité, a-t-il ajouté.</p>'}, {'', '<p>Le défi, bien sûr, est que l’enthousiasme pour l’IA dépasse de loin notre capacité collective à gérer les flux de travail SDLC existants. Ainsi, malgré les meilleures intentions, l’IA pourrait bientôt se révéler être une trop bonne chose.</p>'}]"
Naviguer dans les eaux agiles : pourquoi l'intégration de Copilot exige des ajustements méthodologiques,"[{'', ""<p>Le Congrès américain a récemment interdit à son personnel d’utiliser l’IA Copilot de Microsoft, un chatbot intégré à grand modèle de langage qui permet l’automatisation des produits Microsoft tels que Word, Excel, PowerPoint, Outlook et Teams, en invoquant des problèmes de sécurité. Et ils ne sont pas les seuls à penser ainsi, car de nombreux professionnels oscillent entre enthousiasme et crainte lorsqu’il est question de l’IA. En attendant, Microsoft a vanté avec assurance Copilot, promettant que la puissance de son IA réduira le travail quotidien de gestion d’une entreprise. L’entreprise est passée du simple discours sur l’IA à l’intégration de celle-ci dans toutes les couches de sa pile technologique. Son introduction récente de Copilot Runtime permet même aux développeurs d’utiliser l’IA dans leurs propres programmes, il n’est donc pas surprenant que la plupart des directeurs des systèmes d’information expérimentent Copilot. Ses promesses de productivité et l’élargissement du champ d’application et les économies de coûts qui en résultent pour un réinvestissement potentiel sont trop alléchantes pour être ignorées. Microsoft montre certainement l’exemple, et la seule question est de savoir comment les autres devraient suivre. Au cours de la dernière décennie, les organisations agiles ont surpassé les autres en prenant et en gérant les décisions plus rapidement. L'adoption de l'agilité dans le domaine informatique a entraîné des changements dans la manière dont les infrastructures, les applications, les données et les compétences sont produites, consommées ou les deux. Les éléments fondamentaux de l'agilité, tels que la collaboration, l'automatisation et les améliorations continues, sont les principales sources d'innovation pertinentes pour les applications, le développement et le déploiement.</p>""}, {'', '<p>Les événements récents ont perturbé la progression agile dans le domaine informatique. L’ère de flexibilité induite par la COVID-19 dans la manière, le moment et le lieu de travail remet en question le statu quo dans nos modes de collaboration. En outre, l’IA générative et les complexités associées à la gouvernance informatique ont également bouleversé la progression agile. Par conséquent, une économie incrémentale a été créée qui oblige chaque entreprise à prendre les opportunités et les défis plus au sérieux.</p>'}, {'', ""<p>L'agilité distribuée est déjà en pratique depuis un certain temps. Les réalités du travail à distance et hybride ne sont que des extensions de ce que nous avons déjà vu dans les équipes distribuées. Cependant, les promesses de productivité d'outils tels que Copilot sont nouvelles, donc supposer que les pratiques Agile actuelles fonctionneront avec les pratiques GenAI est une erreur. Alors que pouvons-nous faire ?</p>""}, {'', '<p>Voici quelques réflexions sur la manière d’intégrer la méthodologie agile dans le cadre de l’adoption de Copilot.</p>'}, {'', '<h3>Étendre DevOps pour inclure la représentation de DataOps et MLOps</h3>'}, {'', '<p>Étant donné l’importance des données ainsi que des modèles d’IA et d’apprentissage automatique, les équipes DevOps doivent inclure des représentants des équipes DataOps et MLOps (ModelOps est un sous-ensemble de MLOps). Ce n’est qu’à ce moment-là que l’objectif de rapprocher la « production » et les « opérations » peut être atteint. À première vue, le remplacement peut sembler être la plus grande menace de l’IA, mais son premier acte sera plutôt de révéler et d’approfondir les fissures dans la collaboration.</p>'}, {'', '<h3>L’intelligence logicielle est plus importante que jamais</h3>'}, {'', '<p>Ne pas comprendre les systèmes d’application de manière globale avant de produire automatiquement le code en production sera désastreux. L’informatique d’entreprise est un mélange d’applications et d’actifs informatiques IA et non IA. De plus, l’« explicabilité » du code ne peut être obtenue que lorsque l’intelligence logicielle sur le code produit par GenAI est atteinte. En fin de compte, ce n’est pas l’exactitude fonctionnelle, mais l’adéquation architecturale qui compte le plus pour débloquer des améliorations de productivité. L’IA évolue rapidement, mais laisser la vitesse prendre trop de priorité ouvre la voie à l’échec.</p>'}, {'', '<h3>La conformité continue et la sécurité continue sont tout aussi importantes</h3>'}, {'', '<p>L’une des principales préoccupations des outils GenAI concerne les vulnérabilités que le code généré automatiquement peut introduire dans l’informatique des entreprises, ce qui est la principale raison pour laquelle le Congrès américain a interdit l’utilisation de Copilot. Il est important de procéder aux ajustements appropriés au niveau des modules pour la conformité et la sécurité, afin qu’ils soient conçus et livrés en continu, plutôt que d’être vérifiés et assurés périodiquement. La réglementation est connue pour être à l’origine de l’innovation, et les entreprises doivent délibérément anticiper les difficultés futures.</p>'}, {'', '<h3>Augmenter les portes de qualité dans votre pipeline CI/CD pour les assistants IA</h3>'}, {'', ""<p>Les principes fondateurs de l'open source (transparence, inspection et adaptation) peuvent être étendus aux produits GenAI. L'« inspection » ne doit pas seulement couvrir les aspects qualité, performance, sécurité et UX du code fourni par les outils, mais également l'adéquation architecturale au sein de l'informatique de l'entreprise.</p>""}, {'', '<h3>Mesurez le succès et soyez transparent sur vos lacunes</h3>'}, {'', '<p>L’impact de l’IA peut être flou, mais certains résultats doivent être mesurables pour justifier son adoption. L’élaboration d’indicateurs de performance clés spécifiques à l’IA peut contribuer à consolider le rôle du copilote au sein de l’équipe. Trouver les bons indicateurs à mesurer est de la plus haute importance et constitue en soi un défi de taille.</p>'}, {'', '<p>Dans l’analyse détaillée de l’impact de l’IA, il est également important d’accepter ouvertement les défauts. Le système est, bien sûr, imparfait par nature, et ces imperfections doivent être suivies et corrigées. L’IA évoluant si rapidement, de nombreux problèmes seront probablement résolus à court terme. Notez les défauts et faites-en un examen régulier.</p>'}, {'', '<h3>L’inadéquation des compétences aura un impact sur les promesses de productivité</h3>'}, {'', ""<p>Les outils ne sont efficaces que si ceux qui les utilisent le savent. Un développeur expérimenté peut démontrer un niveau de productivité supérieur à la moyenne avec un assistant IA, mais un développeur inexpérimenté peut rapidement créer plus de problèmes que de solutions. Former les communautés de développement et d'assurance qualité à maîtriser les outils et les directives de gouvernance du code et des tests nécessite d'ajuster le modèle opérationnel Agile.</p>""}, {'', '<p>N’oubliez pas que les équipes DevOps sont bien plus que des développeurs, les équipes DataOps bien plus que des ingénieurs de données et les équipes ModelOps bien plus que des data scientists. Les compétences interdisciplinaires des équipes DevOps vont considérablement évoluer lorsque l’IA fera partie de la conversation. À mesure que les frontières entre ces disciplines s’estomperont, ceux qui seront prêts à s’adapter se hisseront au sommet.</p>'}, {'', '<p>L’adaptation de la méthodologie aux défis perçus ne doit pas limiter les avantages potentiels que GenAI peut produire. S’il est utilisé correctement, GenAI peut contribuer à l’hyper-automatisation des tâches de développement et d’assurance qualité, à l’évaluation des options de conception grâce au prototypage rapide, à la simplification du processus de documentation, à la surveillance de l’environnement de production pour prévoir les goulots d’étranglement des performances, etc.</p>'}, {'', '<p>Si nous n’adaptons pas nos méthodes agiles pour répondre à ces nouvelles réalités et dégager de la valeur plus rapidement, le « time to market » et les avantages en termes de coûts associés seront mal perçus. Les changements dans la méthodologie agile sont inévitables, car GenAI et Agile offrent de réels avantages concurrentiels. Ne vous laissez pas aller.</p>'}]"
Recherche sur la frontière homme/machine : exploiter l'IA générative dans l'ingénierie logicielle,"[{'', '<p>Dans le paysage technologique actuel, l’IA générative n’est pas seulement un mot à la mode : c’est une force transformatrice qui redéfinit la manière dont les logiciels sont conçus, développés et livrés. ArchAITecture Research Collaborative (A²RC), une initiative de recherche à but non lucratif dirigée par la MITRE Corporation, se concentre sur l’intégration de l’IA générative dans le cycle de vie du développement logiciel (SDLC). Ce projet va au-delà de l’exploration technique ; il plonge dans l’avenir de la collaboration homme/machine. Il est important de noter qu’A²RC partagera les résultats de cette recherche avec la communauté au sens large pour favoriser l’innovation et la prise de décision éclairée.</p>'}, {'', ""<h3>IA générative : la nouvelle frontière de l'ingénierie logicielle</h3>""}, {'', ""<p>L'IA générative (GAI) révolutionne chaque phase du cycle de vie du développement logiciel, de l'idéation et de la conception au codage, aux tests et au déploiement. Contrairement à l'IA traditionnelle, qui s'appuie sur des règles et des ensembles de données prédéfinis, l'IA générative peut créer de nouveaux contenus, de nouvelles solutions et même du code à partir de zéro. Cette capacité a un potentiel révolutionnaire pour l'ingénierie logicielle.</p>""}, {'', '<p>Mais la véritable question est la suivante : comment pouvons-nous combiner l’intelligence artificielle et l’expertise humaine pour créer un processus d’ingénierie logicielle plus puissant et plus innovant ? Cette recherche explore la manière dont les humains et les machines peuvent collaborer, l’intelligence artificielle agissant comme un partenaire créatif plutôt que comme un simple outil.</p>'}, {'', '<h3>Au-delà de l’automatisation : le potentiel créatif de l’IA générative</h3>'}, {'', '<p>Le potentiel de l’IAG va au-delà de l’automatisation. Il offre une nouvelle dimension de créativité et de résolution de problèmes. Si l’IAG peut générer du code, concevoir des interfaces et même proposer des solutions architecturales, les humains apportent des éléments essentiels comme l’intuition, la créativité et la compréhension contextuelle.</p>'}, {'', '<p>Cette recherche vise à harmoniser l’intelligence humaine et l’IAG pour repousser les limites du possible en matière de développement logiciel. Notre objectif est de guider le futur SDLC afin qu’il soit non seulement efficace, mais aussi plus innovant et optimisé pour les complexités des besoins logiciels modernes.</p>'}, {'<h3>Le rôle du projet collaboratif de recherche ArchAITecture (A²RC)</h3>', ''}, {'', ""<p>A²RC est à l'avant-garde de cette exploration. Notre équipe comprend des chercheurs, des ingénieurs et des leaders d'opinion qui se consacrent à faire progresser l'intégration sûre et sécurisée de l'IAG dans l'ingénierie logicielle. La recherche est guidée par une vision d'un avenir où l'IAG et la créativité humaine travaillent main dans la main pour fournir des logiciels de qualité à la vitesse de la pertinence.</p>""}, {'', ""<p>L'accent n'est pas uniquement mis sur les aspects techniques de l'IA, mais aussi sur la manière dont elle peut améliorer l'expérience humaine dans l'ingénierie logicielle. Notre équipe étudie des questions telles que : Comment pouvons-nous intégrer l'IA dans les flux de travail existants sans perturber la créativité humaine ? Quelles sont les principales pratiques en matière de collaboration homme/IA dans le développement de logiciels ? Comment pouvons-nous maintenir la transparence, l'éthique et la responsabilité dans les résultats générés par l'IA ?</p>""}, {'', '<p>En outre, A²RC explore les discussions cruciales qui pourraient redéfinir l’avenir de l’ingénierie logicielle :</p>'}, {'', '<li>D’outil à coéquipier : imaginez GAI évoluant d’un simple outil vers un agent autonome, remodelant la dynamique d’équipe et les processus de prise de décision.</li>'}, {'', '<li>Plateformes numériques pilotées par l’IA : Considérez le potentiel des plateformes centrées sur l’IA qui pourraient révolutionner la distribution de logiciels, en transformant les rôles et les flux de travail à l’image des plateformes low-code et no-code d’aujourd’hui.</li>'}, {'', ""<h3>Une invitation à participer : façonner l'avenir avec l'IA générative</h3>""}, {'', ""<p>GAI est sur le point de révolutionner l'ingénierie logicielle. Il est essentiel de disposer de perspectives et d'idées diverses issues des nombreux rôles du SDLC pour comprendre l'évolution du paysage.</p>""}, {'', ""<p>Nous menons une enquête pour recueillir des informations auprès de toutes les personnes associées à la conception et à la distribution de logiciels dans le monde entier. Cette enquête joue un rôle crucial pour comprendre comment l'IA générative est utilisée dans des scénarios réels et comment elle impacte les pratiques logicielles modernes et les humains.</p>""}, {'', '<p>Cette enquête se concentre sur l’intersection dynamique de la collaboration entre l’homme et la machine, dans le but de :</p>'}, {'', '<li>Identifier les facteurs clés pour un logiciel humain-IA efficace et efficient</li>'}, {'', '<li>Comprendre les cas d’utilisation, les risques et les récompenses associés à l’intégration des outils GAI dans le SDLC.</li>'}, {'', ""<li>Découvrez l'impact de l'IA générative sur le cycle de vie du développement logiciel</li>""}, {'', '<p>En participant, vous contribuerez à élaborer un rapport complet qui reflète l’état actuel de la pratique et favorise les innovations futures.</p>'}, {'', ""<p>Nous avons conçu l'enquête anonyme pour recueillir un large éventail d'expériences et de points de vue. Les informations que nous recueillerons éclaireront nos recherches en cours et nous aideront à développer de nouveaux outils, cadres et meilleures pratiques pour intégrer GAI dans le SDLC. Pour participer à l'enquête, veuillez consulter ce lien.</p>""}, {'', '<p>En plus de l’enquête, nous invitons les organisations à participer à l’élaboration d’études de cas. Ces études de cas incluront l’utilisation d’un outil d’aide à la décision léger conçu pour aider les équipes et les organisations à définir leurs objectifs et à mesurer efficacement leur réussite. Cette approche pratique offre une occasion unique d’explorer les avantages et les défis de la collaboration homme/machine avec l’IA générative.</p>'}, {'', '<h3>Pourquoi la participation est importante</h3>'}, {'', '<p>Le succès de cette recherche dépend de la participation d’individus et d’organisations passionnés par la technologie et son avenir. Que vous soyez architecte logiciel, développeur ou leader technologique, votre contribution est inestimable pour façonner l’avenir de l’IA générative en ingénierie logicielle.</p>'}, {'', '<p>En participant à l’enquête ou en menant une étude de cas, vous jouerez un rôle essentiel dans l’exploration du potentiel de l’IA générative. Les informations que vous fournirez auront un impact direct sur le développement de systèmes d’IA non seulement innovants, mais également éthiques, transparents et alignés sur les valeurs humaines.</p>'}, {'', ""<h3>Participez à la révolution de l'IA générative</h3>""}, {'', '<p>L’avenir de l’ingénierie logicielle se dessine aujourd’hui, avec l’IA générative comme élément central. Cette recherche offre l’opportunité de participer à cette transformation, en contribuant à définir la manière dont l’IA et la créativité humaine collaboreront dans les années à venir.</p>'}, {'', '<p>A²RC estime que l’avenir de l’ingénierie logicielle réside dans l’intégration transparente de l’IA générative dans chaque phase du SDLC. Cette recherche marque une étape passionnante vers cet avenir, mais nous avons besoin de la participation de la communauté technologique mondiale pour réussir.</p>'}, {'', ""<h3>Participez à l'enquête, menez une étude de cas, façonnez l'avenir</h3>""}, {'', '<p>Que vous choisissiez de participer à cette recherche révolutionnaire en répondant à l’enquête ou en menant une étude de cas, vos connaissances contribueront à façonner l’avenir de l’ingénierie logicielle, en garantissant que l’IA générative améliore, plutôt que de remplacer, la créativité et l’expertise humaines.</p>'}, {'', '<p>Cette recherche ne porte pas uniquement sur la technologie : elle porte sur l’avenir du travail, de la créativité et du potentiel humain. Rejoignez A²RC dans cette importante exploration et contribuez à façonner l’avenir de l’ingénierie logicielle grâce à l’IA générative.</p>'}, {'', ""<p>Pour toute demande de renseignements ou pour vous impliquer, contactez Trac Bannon à [email\xa0protected] ou l'ArchAITecture Research Collaborative à [email\xa0protected].</p>""}]"
L’enthousiasme des développeurs pour l’IA générative augmente : enquête,"[{'', '<p>Les développeurs sont de plus en plus positifs quant aux effets de l’IA générative sur leur travail et leurs entreprises en général, un changement dans une communauté qui, par le passé, semblait le craindre, selon un rapport publié cette semaine par le développeur d’API cloud Kong.</p>'}, {'', '<p>En outre, ils affirment que les API joueront un rôle de plus en plus important dans le développement continu de l’IA, l’intégration avec l’IA étant l’aspect le plus critique de la gestion des API au cours des un à deux ans à venir.</p>'}, {'', '<p>« Pour les entreprises, l’intégration de l’IA dans la gestion des API est essentielle pour automatiser et optimiser les opérations, renforcer la sécurité et offrir des expériences utilisateur plus personnalisées », écrivent les auteurs du « API Impact Report 2024 » de Kong. « Elle peut permettre la détection des anomalies en temps réel, l’ajustement dynamique des ressources pour des performances optimales et le développement de services innovants. »</p>'}, {'', ""<p>Dans une enquête menée auprès de 747 professionnels de l'informatique et chefs d'entreprise, 92 % des répondants ont déclaré que l'IA était une priorité pour leur entreprise et 83 % ont déclaré que les investissements en IA dans leur organisation ont ouvert des opportunités pour de nouveaux produits ou services au cours de l'année écoulée.</p>""}, {'', '<p>Cela dit, la tendance des utilisateurs à contourner les politiques de leur organisation en matière d’utilisation de l’IA exerce une pression sur les entreprises pour qu’elles mettent en place des outils tels que des passerelles d’IA et des solutions de prévention des pertes de données (DLP) afin de garantir la gouvernance des données et la conformité réglementaire, d’autant plus que les gouvernements mettent de plus en plus en place de nouvelles lois pour rendre l’utilisation et l’innovation de l’IA générative plus sûres, selon Kong.</p>'}, {'', '<h3>Les directives d’utilisation de l’IA sont importantes</h3>'}, {'', '<p>Les auteurs du rapport ont noté que les organisations et les individus adoptent l’IA à un rythme beaucoup plus rapide que lors des précédentes avancées technologiques, « ce qui signifie que les organisations doivent évoluer plus vite que jamais pour rester à la pointe de la vague d’innovation en matière d’IA ». Environ 80 % des développeurs interrogés ont déclaré que leurs organisations avaient mis en place des directives ou des restrictions en matière d’utilisation de l’IA, et environ 20 % n’en avaient aucune. Environ 2 % ont déclaré que leurs organisations interdisaient purement et simplement les outils d’IA générative.</p>'}, {'', '<p>Près de 60 % des personnes interrogées ont déclaré qu’elles parvenaient à contourner les restrictions pour accéder aux outils d’IA dont elles ont besoin. Cependant, la confidentialité, la sécurité et la gouvernance des données figurent également en tête de liste (près de 60 %) des défis rencontrés lors de l’intégration des services d’IA aux infrastructures de microservices existantes.</p>'}, {'', '<p>« Nous avons atteint le moment où l’adoption de l’IA est impérative. Cependant, le succès à long terme dépendra de la stratégie adoptée », a déclaré Marco Palladino, cofondateur et directeur technique de Kong. « Notre rapport souligne la nécessité d’une infrastructure sous-jacente aux applications d’IA qui assure une gouvernance solide tout en améliorant les performances de l’IA. »</p>'}, {'', ""<h3>Se familiariser avec l'IA générative</h3>""}, {'', '<p>Le rapport de l’entreprise fait écho aux conclusions d’autres fournisseurs en matière de développeurs et d’IA. Une enquête réalisée à la fin de l’année dernière par Kobiton a révélé que la plupart des développeurs et testeurs d’applications mobiles utilisent des outils d’IA générative pour diverses tâches et pour relever les défis des opérations de développement et d’assurance qualité. En outre, ils utilisent cette technologie lorsqu’ils disposent de ressources financières limitées ou qu’ils ne disposent pas de suffisamment de programmeurs qualifiés.</p>'}, {'', ""<p>En avril, Docker a déclaré dans un rapport que les ingénieurs logiciels s'appuient de plus en plus sur l'IA pour créer leurs applications, avec des cas d'utilisation allant du codage et de la documentation à la recherche, au dépannage et au débogage.</p>""}, {'', '<p>De même, l’enquête de Kong a révélé que les développeurs s’intéressent de plus en plus à l’IA générative, 60 % d’entre eux déclarant qu’ils étaient passionnés par le travail avec cette technologie émergente et 57 % affirmant qu’elle faciliterait leur travail.</p>'}, {'', '<p>D’autres s’inquiètent néanmoins de cette situation : 35 % des sondés estiment que l’adoption de l’IA progresse trop vite. Ils sont également inquiets de l’avenir : 18 % pensent que l’utilisation d’outils d’IA entraînera des licenciements dans leur entreprise. Près d’un sur dix déclare ne pas vouloir travailler avec l’IA, certains estimant que l’IA dévalorisera leur travail ou augmentera leur charge de travail.</p>'}, {'', '<p>« En passant de la perspective personnelle à l’opinion sur l’impact sur l’entreprise, la moitié des personnes interrogées estiment que l’IA améliorera la productivité et l’innovation », écrivent les auteurs. « 26 % d’entre elles estiment que l’IA créera de nouveaux postes ou de nouvelles opportunités dans leurs organisations. »</p>'}, {'', '<h3>Les API joueront un rôle central</h3>'}, {'', '<p>Selon Kong, les API vont devenir de plus en plus importantes dans le développement et l’utilisation de l’IA. L’entreprise a noté que les API en général sont largement utilisées, avec 85 % des entreprises du Fortune 100 les utilisant comme élément central de leurs opérations informatiques et près de la moitié des développeurs et des gestionnaires déclarant que leurs entreprises doivent être plus conscientes de la valeur commerciale que les API apportent.</p>'}, {'', '<p>Elles joueront également un rôle central dans l’IA, les auteurs du rapport écrivant que « les API permettent la communication entre les humains et les systèmes d’IA, ainsi qu’entre les systèmes d’IA et d’autres outils numériques. Les API servent de mains, d’yeux et d’oreilles à l’IA. Et à mesure que l’utilisation de l’IA augmente, le nombre d’API qui les rendent possibles augmentera également. »</p>'}, {'', '<p>Ils ont souligné une prévision de Gartner selon laquelle d’ici 2026, plus de 30 % de l’augmentation de la demande d’API proviendra de l’IA et des outils utilisant des modèles de langage volumineux (LLM). Au-delà de cela, Kong prédit que la valeur des API permettant l’IA augmentera de 170 % d’ici 2030.</p>'}, {'', '<p>Selon les recherches de Kong, l’impact économique des API aux États-Unis atteindra 3 400 milliards de dollars et l’impact mondial atteindra 17 300 milliards de dollars. En s’appuyant sur les données du Fonds monétaire international sur la croissance prévue du PIB mondial et l’économie numérique mondiale, l’impact économique des API devrait passer de 12,7 % du PIB mondial cette année à 14 % en 2030.</p>'}, {'', '<p>« Un simple calcul mathématique permet de comprendre pourquoi une stratégie API est si essentielle pour les entreprises d’aujourd’hui : dans peu de temps, les consommateurs de nos sites Web ou de nos produits numériques seront plus susceptibles d’être des IA que des humains », écrivent les auteurs. « Que nous utilisions l’IA, que nous la formions ou que nous laissions interagir l’IA avec les API pour effectuer des opérations, une API est impliquée. C’est pourquoi le contrôle de l’utilisation de l’IA au niveau de l’interface est un problème d’API. »</p>'}]"
GitHub oriente Copilot Autofix vers l'œil de la tempête de sécurité de l'IA,"[{'', ""<p>GitHub, société de gestion de versions distribuées et de plateforme de collaboration, a présenté son nouvel outil Copilot Autofix. Ce service logiciel basé sur l'IA s'adresse aux développeurs qui doivent remédier aux vulnérabilités logicielles dans le code destiné aux applications traditionnelles et dans celles imprégnées d'une injection de nouvelles ou existantes races d'IA.</p>""}, {'', '<p>Copilot Autofix fait partie du groupe de produits de la plateforme GitHub Advanced Security (GHAS). Mentionnée pour la première fois au printemps de cette année, la technologie était au stade de la version bêta publique avant de passer ce mois-ci à une version complète.</p>'}, {'', '<p>Ce produit finalisé de première version intègre le moteur d’analyse de code CodeQL de GitHub, un moteur d’analyse de code développé par GitHub pour automatiser les contrôles de sécurité afin que les développeurs puissent analyser et afficher les résultats sous forme d’alertes d’analyse de code. Il intègre également GPT-4o, un modèle de langage multimodal de grande taille qui offre des fonctionnalités de conversation en temps réel et de génération de texte. Copilot Autofix propose également des heuristiques et la technologie dispose de son propre ensemble d’API pour permettre aux équipes d’implémenter son ensemble d’outils et de créer des suggestions de code (et des extraits de code) pour corriger et remédier aux vulnérabilités. Les développeurs peuvent accepter, modifier ou rejeter les suggestions de code proposées.</p>'}, {'', '<h3>Trouvé oui, corrigé peut-être</h3>'}, {'', '<p>« Les outils d’analyse de code détectent les vulnérabilités, mais ils ne s’attaquent pas au problème fondamental [de la réparation des logiciels] : la correction nécessite une expertise en sécurité et du temps, deux ressources précieuses dont on manque cruellement. En d’autres termes, le problème n’est pas de trouver les vulnérabilités, mais de les corriger », a déclaré Mike Hanley, responsable de la sécurité et vice-président senior de l’ingénierie chez GitHub.</p>'}, {'', ""<p>En utilisant son nouveau slogan « Trouvé signifie corrigé » pour aborder ce point précis, Hanley explique que Copilot Autofix analyse les vulnérabilités dans le code, explique pourquoi elles sont importantes… et propose des suggestions de code qui aident les développeurs à corriger les vulnérabilités aussi rapidement qu'elles sont découvertes.</p>""}, {'', '<blockquote>« Au cours de la version bêta publique, nous avons constaté que les développeurs corrigeaient les vulnérabilités du code plus de trois fois plus rapidement que ceux qui le faisaient manuellement, ce qui constitue un exemple frappant de la manière dont les agents d’IA peuvent radicalement simplifier et accélérer le développement de logiciels sécurisés », s’enthousiasme Hanley. « Les développeurs peuvent éviter les nouvelles vulnérabilités dans leur code grâce à Copilot Autofix dans la demande d’extraction, et désormais également réduire l’arriéré de la dette de sécurité en générant des correctifs pour les vulnérabilités existantes. »</blockquote>'}, {'', '<p>« Au cours de la version bêta publique, nous avons constaté que les développeurs corrigeaient les vulnérabilités du code plus de trois fois plus rapidement que ceux qui le faisaient manuellement, ce qui constitue un exemple frappant de la manière dont les agents d’IA peuvent radicalement simplifier et accélérer le développement de logiciels sécurisés », s’enthousiasme Hanley. « Les développeurs peuvent éviter les nouvelles vulnérabilités dans leur code grâce à Copilot Autofix dans la demande d’extraction, et désormais également réduire l’arriéré de la dette de sécurité en générant des correctifs pour les vulnérabilités existantes. »</p>'}, {'', '<p>La division GHAS de GitHub a déclaré qu’elle avait de grands projets pour Copilot Autofix et ses ensembles d’outils de plateforme associés. Elle s’efforce d’améliorer la portée et la précision de « l’analyse secrète » actuelle. L’organisation définit l’analyse secrète comme une fonctionnalité de sécurité qui permet de détecter et d’empêcher l’inclusion accidentelle d’informations sensibles telles que les clés API, les mots de passe, les jetons et autres secrets dans le référentiel de code d’une équipe DevOps. L’approche de GitHub ici signifie que l’analyse secrète analyse les validations de code dans les référentiels pour les types de secrets connus afin que les administrateurs du référentiel puissent être alertés en cas de détection. Hanley de GitHub a également déclaré que l’équipe développe de nouveaux flux de travail qui font évoluer Copilot Autofix pour les organisations ayant un volume élevé de dettes de sécurité, le tout sur des plateformes de développement familières.</p>'}, {'', ""<p>Avec une prise en charge initiale de JavaScript, TypeScript, Java et Python, Copilot Autofix étend désormais également la prise en charge à C#, C/C++, Go, Kotlin, Swift et Ruby. Disponible gratuitement pour les développeurs travaillant sur des projets open source, les clients GitHub Enterprise Cloud payants qui s'abonnent à GHAS trouveront Copilot Autofix activé par défaut dans leurs paramètres GHAS.</p>""}, {'', ""<h3>L'IA pour le bien, pour le bien ?</h3>""}, {'', '<p>Compte tenu de la vitesse à laquelle les équipes DevOps modernes doivent tenter de créer des offres logicielles fonctionnelles et du fait que tous les groupes ne disposent pas d’un gourou de la sécurité, GitHub affirme que le marché est mûr pour un outil alimenté par l’IA capable de remédier à ce niveau. À une époque où les professionnels moins techniques sont « préoccupés par l’impact de l’IA », il s’agit peut-être d’un bon exemple d’intelligence d’automatisation utilisée pour résoudre des problèmes plutôt que pour en créer de nouveaux.</p>'}]"
Automatisation DevOps : un guide détaillé,"[{'', ""<p>L'automatisation DevOps est une plateforme productive où l'assistance humaine est réduite. Les processus qui génèrent des boucles de rétroaction entre les équipes de développement et d'exploitation sont étudiés et travaillés en conséquence grâce à une stratégie structurée. Le déploiement de mises à jour itératives peut être effectué plus rapidement sur les applications en production grâce à l'utilisation tactique des services de tests d'automatisation dans DevOps. Dans cet article, vous obtiendrez une brève compréhension du processus d'intégration de l'automatisation dans DevOps et d'optimisation des performances.</p>""}, {'', ""<h3>Qu'est-ce que l'automatisation DevOps ?</h3>""}, {'', ""<p>Il s'agit d'un processus technologique spécifique dans lequel les meilleures pratiques en matière de technologies de l'information et d'ingénierie logicielle sont conçues pour permettre la livraison et l'automatisation continues de produits logiciels. Les tests, le développement, la surveillance et le déploiement des logiciels sont automatisés par la plateforme d'automatisation. Les processus DevOps sont automatisés par des scripts d'automatisation DevOps, qui sont considérés comme des outils spécifiques.</p>""}, {'', ""<p>Cela permet de garantir que la livraison des logiciels s'effectue à un rythme plus rapide et que les développeurs peuvent se concentrer et travailler sur d'autres tâches de test cruciales. Les scripts d'automatisation garantissent des tests et un déploiement plus rapides des logiciels. Ce processus est exécuté de manière répétable, fiable et cohérente.</p>""}, {'', ""<p>Grâce à la mise en œuvre tactique de l'automatisation DevOps, les organisations peuvent rationaliser leurs activités, améliorer la qualité des logiciels et réduire le temps consacré aux activités marketing. L'accent doit également être mis sur l'exploitation des services de tests automatisés.</p>""}, {'', '<p>Voici les processus clés qui doivent être pris en compte pour optimiser les activités d’automatisation DevOps\xa0:</p>'}, {'', ""<ol>La nécessité d'une plateforme : une plateforme appropriée est nécessaire pour réaliser l'automatisation dans DevOps. Cette plateforme doit permettre aux utilisateurs de surveiller, de définir et d'exécuter des flux de travail automatisés et de garantir que les processus bénéficient de la visibilité requise.</ol>""}, {'', ""<li>La nécessité d'une plateforme : une plateforme appropriée est nécessaire pour réaliser l'automatisation dans DevOps. Cette plateforme doit permettre aux utilisateurs de surveiller, de définir et d'exécuter des flux de travail automatisés et de garantir que les processus bénéficient de la visibilité requise.</li>""}, {''}, {'', ""<ol>Des objectifs atteignables doivent être fixés : des objectifs bien définis doivent être fixés pour le processus d'automatisation. Une feuille de route appropriée peut être suivie par l'équipe pour s'assurer que le processus soit un succès.</ol>""}, {'', ""<li>Des objectifs atteignables doivent être fixés : des objectifs bien définis doivent être fixés pour le processus d'automatisation. Une feuille de route appropriée peut être suivie par l'équipe pour s'assurer que le processus soit un succès.</li>""}, {''}, {'', '<ol>Les processus utilisés doivent être fréquemment automatisés : les opérations spécifiques qui doivent être automatisées en premier lieu doivent être identifiées. Les organisations peuvent être fréquemment impactées par ces processus utilisés</ol>'}, {'', '<li>Les processus utilisés doivent être fréquemment automatisés : les opérations spécifiques qui doivent être automatisées en premier lieu doivent être identifiées. Les organisations peuvent être fréquemment impactées par ces processus utilisés</li>'}, {''}, {'', ""<ol>Les processus automatisés doivent être testés : les tests doivent être effectués par l'équipe pour s'assurer qu'ils fonctionnent correctement. Les tests peuvent être effectués régulièrement pour s'assurer que le processus d'automatisation des tests DevOps fonctionne comme prévu.</ol>""}, {'', ""<li>Les processus automatisés doivent être testés : les tests doivent être effectués par l'équipe pour s'assurer qu'ils fonctionnent correctement. Les tests peuvent être effectués régulièrement pour s'assurer que le processus d'automatisation des tests DevOps fonctionne comme prévu.</li>""}, {''}, {'', '<ol>Les processus automatisés doivent être surveillés et adoptés : les processus doivent être surveillés et les ajustements nécessaires doivent être effectués si nécessaire. Lorsque le processus est exécuté correctement, il garantit des résultats efficaces.</ol>'}, {'', '<li>Les processus automatisés doivent être surveillés et adoptés : les processus doivent être surveillés et les ajustements nécessaires doivent être effectués si nécessaire. Lorsque le processus est exécuté correctement, il garantit des résultats efficaces.</li>'}, {''}, {'', ""<ol>Les processus automatisés doivent être suivis et signalés : la progression du processus d'automatisation doit être correctement suivie et signalée. Le processus bénéficie de la visibilité requise et les domaines à améliorer sont identifiés.</ol>""}, {'', ""<li>Les processus automatisés doivent être suivis et signalés : la progression du processus d'automatisation doit être correctement suivie et signalée. Le processus bénéficie de la visibilité requise et les domaines à améliorer sont identifiés.</li>""}, {''}, {'', ""<ol>La plateforme d'intégration continue (CI) doit être exploitée : L'équipe de développement qui écrit le code est combinée par la plateforme CI en un seul code source principal. Les bugs ou erreurs qui surviennent dans le code sont rapidement identifiés par les développeurs. Il s'agit d'une partie cohérente des services de tests d'automatisation.</ol>""}, {'', ""<li>La plateforme d'intégration continue (CI) doit être exploitée : L'équipe de développement qui écrit le code est combinée par la plateforme CI en un seul code source principal. Les bugs ou erreurs qui surviennent dans le code sont rapidement identifiés par les développeurs. Il s'agit d'une partie cohérente des services de tests d'automatisation.</li>""}, {''}, {'', ""<ol>La plateforme de livraison continue (CD) doit être utilisée de manière stratégique : le logiciel est livré aux clients et aux utilisateurs de manière fiable, rapide et cohérente. Cela permet aux organisations de s'assurer que leur logiciel fonctionne conformément aux exigences spécifiques et est toujours à jour.</ol>""}, {'', ""<li>La plateforme de livraison continue (CD) doit être utilisée de manière stratégique : le logiciel est livré aux clients et aux utilisateurs de manière fiable, rapide et cohérente. Cela permet aux organisations de s'assurer que leur logiciel fonctionne conformément aux exigences spécifiques et est toujours à jour.</li>""}, {''}, {'', ""<ol>L'infrastructure en tant que code (IaC) doit être utilisée : l'IaC est un processus spécifique qui aide les organisations à gérer, configurer et déployer rapidement leur infrastructure informatique. Lorsque les tâches de gestion de l'infrastructure informatique sont mises à jour, les coûts et le temps d'une organisation sont considérablement réduits.</ol>""}, {'', ""<li>L'infrastructure en tant que code (IaC) doit être utilisée : l'IaC est un processus spécifique qui aide les organisations à gérer, configurer et déployer rapidement leur infrastructure informatique. Lorsque les tâches de gestion de l'infrastructure informatique sont mises à jour, les coûts et le temps d'une organisation sont considérablement réduits.</li>""}, {'', ""<p>Lorsque les processus mentionnés ci-dessus sont prioritaires, l'automatisation DevOps peut être réalisée de manière succincte. Cela permet à son tour de livrer rapidement des logiciels de qualité.</p>""}, {""<h3>Lancer le processus d'automatisation DevOps</h3>"", ''}, {'', ""<ol>Identifier l'état actuel de l'environnement DevOps : avant d'automatiser le processus DevOps, le domaine existant doit être clairement compris. Les personnes, les techniques et les outils impliqués dans le flux DevOps actuel doivent être correctement examinés.</ol>""}, {'', ""<li>Identifier l'état actuel de l'environnement DevOps : avant d'automatiser le processus DevOps, le domaine existant doit être clairement compris. Les personnes, les techniques et les outils impliqués dans le flux DevOps actuel doivent être correctement examinés.</li>""}, {''}, {'', '<ol>Mettre en évidence les domaines du processus DevOps qui doivent être automatisés : une fois l’environnement actuel bien compris, il faut identifier les domaines qui peuvent bénéficier de l’automatisation. Les activités qui peuvent être automatisées doivent être prises en compte.</ol>'}, {'', '<li>Mettre en évidence les domaines du processus DevOps qui doivent être automatisés : une fois l’environnement actuel bien compris, il faut identifier les domaines qui peuvent bénéficier de l’automatisation. Les activités qui peuvent être automatisées doivent être prises en compte.</li>'}, {''}, {'', ""<ol>Sélectionnez les bons outils : pour réussir l'automatisation DevOps, vous devez sélectionner des outils d'automatisation appropriés. Le type de tâches à automatiser doit être pris en compte.</ol>""}, {'', ""<li>Sélectionnez les bons outils : pour réussir l'automatisation DevOps, vous devez sélectionner des outils d'automatisation appropriés. Le type de tâches à automatiser doit être pris en compte.</li>""}, {''}, {'', ""<ol>Construire le pipeline d'automatisation : une fois les bons outils sélectionnés, il faut construire le pipeline d'automatisation DevOps. Cela comprend la configuration des appareils, la mise en place de l'infrastructure nécessaire et la création des scripts d'automatisation.</ol>""}, {'', ""<li>Construire le pipeline d'automatisation : une fois les bons outils sélectionnés, il faut construire le pipeline d'automatisation DevOps. Cela comprend la configuration des appareils, la mise en place de l'infrastructure nécessaire et la création des scripts d'automatisation.</li>""}, {''}, {'', ""<ol>Tester et déployer la plateforme d'automatisation : avant de déployer la plateforme d'automatisation, il faut la tester pour s'assurer qu'elle fonctionne comme prévu. Une fois la vérification effectuée (l'équipe doit s'assurer que tout fonctionne correctement), la plateforme d'automatisation peut être déployée en production.</ol>""}, {'', ""<li>Tester et déployer la plateforme d'automatisation : avant de déployer la plateforme d'automatisation, il faut la tester pour s'assurer qu'elle fonctionne comme prévu. Une fois la vérification effectuée (l'équipe doit s'assurer que tout fonctionne correctement), la plateforme d'automatisation peut être déployée en production.</li>""}, {''}, {'', ""<ol>Surveiller et optimiser le processus d'automatisation : une fois le déploiement effectué, il doit être surveillé pour s'assurer qu'il fonctionne efficacement et correctement. Des ajustements peuvent être effectués pour optimiser les performances.</ol>""}, {'', ""<li>Surveiller et optimiser le processus d'automatisation : une fois le déploiement effectué, il doit être surveillé pour s'assurer qu'il fonctionne efficacement et correctement. Des ajustements peuvent être effectués pour optimiser les performances.</li>""}, {''}, {'', '<p>Voici quelques-uns des meilleurs outils d’automatisation DevOps\xa0:</p>'}, {'', ""<ol>Kubernetes\xa0: il s'agit d'une plateforme d'automatisation et d'orchestration de conteneurs, couramment utilisée par les équipes DevOps. Les conteneurs sont gérés à grande échelle et fonctionnent bien avec Docker. La gestion de centaines de conteneurs est automatisée. Les applications conteneurisées peuvent être déployées sur un cluster. Docker\xa0: il s'agit d'un outil open source basé sur Linux. Les environnements conteneurisés sont créés pour les applications, où ils sont rendus plus sûrs et portables, et les conflits sont réduits lors des tests. DevOps est activé par l'outil Docker afin que les applications puissent être créées et exécutées rapidement et efficacement.</ol>""}, {'', ""<li>Kubernetes\xa0: il s'agit d'une plateforme d'automatisation et d'orchestration de conteneurs, très utilisée par les équipes DevOps. Les conteneurs sont gérés à grande échelle et fonctionnent bien avec Docker. La gestion de centaines de conteneurs est automatisée. Les applications conteneurisées peuvent être déployées sur un cluster.</li>""}, {'', ""<li>Docker\xa0: il s'agit d'un outil open source basé sur Linux. Des environnements conteneurisés sont créés pour les applications, ce qui les rend plus sûres et portables, et réduit les conflits lors des tests. DevOps est activé par l'outil Docker afin que les applications puissent être créées et exécutées rapidement et efficacement.</li>""}, {'', ""<p>Les conteneurs sont accessibles via le moteur Docker et les applications peuvent ensuite être exécutées dans un environnement distant. Les organisations qui se concentrent sur la réduction des coûts d'infrastructure peuvent opter pour Docker.</p>""}, {'', ""<ol>Splunk : Grâce à cet outil, les logs ou données générés par la machine sont recherchés, analysés et visualisés en temps réel. L'état de la machine est analysé et les points de défaillance matérielle sont identifiés. Les données de la machine sont transmises à Splunk et toutes les données disponibles sont traitées par l'outil. Ansible : Il s'agit d'un outil de gestion de configuration sans agent qui offre une livraison continue et peut être facilement déployé. De nombreuses tâches répétitives telles que l'orchestration intra-service, le déploiement d'applications et le provisionnement cloud sont automatisées. Les nœuds peuvent être connectés via cet outil et les modules sont transmis depuis un emplacement centralisé.</ol>""}, {'', ""<li>Splunk : Grâce à cet outil, les logs ou données générés par la machine sont recherchés, analysés et visualisés en temps réel. L'état de la machine est analysé et les points de défaillance matérielle sont identifiés. Les données de la machine sont transmises à Splunk et toutes les données disponibles sont traitées par l'outil.</li>""}, {'', ""<li>Ansible : il s'agit d'un outil de gestion de configuration sans agent qui offre une livraison continue et peut être facilement déployé. De nombreuses tâches répétitives telles que l'orchestration intra-service, le déploiement d'applications et le provisionnement cloud sont automatisées. Les nœuds peuvent être connectés via cet outil et les modules sont poussés depuis un emplacement centralisé.</li>""}, {'', '<h3>Conclusion</h3>'}, {'', ""<p>DevOps encourage la collaboration et renforce le facteur culturel au sein des équipes de développement, d'exploitation et autres équipes concernées. L'objectif est d'adopter le modèle agile et de réaliser des processus technologiques à un rythme plus rapide. L'automatisation améliore non seulement l'efficacité de DevOps, mais aide également à effectuer des tâches fastidieuses de manière méticuleuse et facile. Pour des informations plus détaillées et plus perspicaces sur la mise en œuvre et l'exécution tactiques de l'automatisation DevOps, vous pouvez consulter une société de services de tests de logiciels haut de gamme.</p>""}]"
"Bonjour Henrik, le moteur d’intelligence artificielle multi-contextuelle de Neuralogics","[{'', '<p>Neuralogics a suivi la tendance du secteur qui voit les fournisseurs de technologie étiqueter leurs moteurs d’IA avec un nom humain. Avec Salesforce Einstein, IBM Watson et Davis (celui-là, c’est Dynatrace) et même l’inclusion possible de Waldo Search déjà parmi nous, Neuralogics rejoint maintenant la famille avec Henrik.</p>'}, {'', ""<p>Nommé d'après le cofondateur de Neuralogics, Henrik Hofmeister (son partenaire Jacob Laurvigen a eu la générosité de prendre du recul), il s'agit d'un cadre d'IA conçu pour permettre à n'importe quel utilisateur de créer ce que l'entreprise promet être des « applications logicielles entièrement fonctionnelles » à partir d'une simple invite.</p>""}, {'', ""<p>Laurvigen, qui occupe le poste de PDG, et Hofmeister, qui occupe le poste de scientifique en chef et de directeur technique, ont quitté leur précédente société d'intelligence artificielle et de renseignement lors d'une acquisition il y a deux ans. Ils ont ensuite constitué une équipe d'ingénierie logicielle composée de développeurs et de scientifiques des données de Tradeshift, Booking.com et Comcast. L'équipe a passé deux ans à élaborer le cadre qui a rendu possible le lancement d'aujourd'hui.</p>""}, {'', ""<h3>C'est ringard, mais sincère</h3>""}, {'', '<p>« C’est le travail de notre vie », a déclaré Hofmeister. « Henrik.ai repousse les limites de l’IA pour amplifier le potentiel humain et répondre aux besoins dynamiques de notre réalité en constante évolution. Nous nous engageons à mener le changement de paradigme de l’IA, en fournissant des solutions de pointe qui améliorent l’efficacité, stimulent la créativité et relèvent des défis complexes. »</p>'}, {'', '<p>Hofmeister ne se contente pas d’utiliser dans son commentaire toutes les expressions à la mode dans le secteur technologique, mais suggère que ce nouvel outil d’IA peut être utilisé par n’importe quel travailleur, quelle que soit l’étendue de ses connaissances techniques. Il affirme que cette approche simplifiée du développement d’applications peut être utilisée par n’importe qui, qu’il s’agisse d’un chef d’entreprise, d’un professionnel de la santé, d’un éducateur ou d’un entrepreneur.</p>'}, {'', ""<p>« Au cours de mon mandat en tant que directeur principal des produits chez Comcast, j'ai pu constater de visu à quel point les longs délais d'exécution et les retards critiques dans la mise sur le marché de nouvelles solutions clients pouvaient ralentir l'innovation », a déclaré Christian O. Petersen, directeur de la croissance et membre fondateur de l'équipe de Neuralogics.</p>""}, {'', '<h3>Intelligence multi-contextuelle</h3>'}, {'', '<p>Au cœur d’Henrik.ai se trouve un réseau de modèles d’IA spécialement formés, appelés fonctions d’« intelligence multicontextuelle », qui fonctionnent ensemble pour créer des systèmes logiciels complets et conformes à partir de zéro. L’approche native de l’IA garantit que le logiciel créé est non seulement fonctionnel, mais également évolutif, sécurisé et adaptable au paysage technologique.</p>'}, {'', '<blockquote>Comme son nom l’indique, la logique algorithmique qui sous-tend l’intelligence multicontextuelle vise à créer un modèle d’IA capable de fonctionner dans de multiples contextes et de s’adapter à divers environnements et scénarios. Il ne s’agit pas d’IA multimodale au sens d’une intelligence applicable au texte, à la parole, à la vidéo et à d’autres données non structurées (peut-être spatiales) ; et il ne s’agit pas non plus d’IA multimodèle en tant que telle, c’est-à-dire de fonctions d’intelligence tirées d’une sélection de services d’IA qui ont été tissés ensemble pour former une notion plus complexe de compréhension des machines… il s’agit plutôt d’un amalgame des deux approches combinées à ce que Neuralogics appelle sa matrice disciplinaire.</blockquote>'}, {'', '<p>Comme son nom l’indique, la logique algorithmique qui sous-tend l’intelligence multicontextuelle vise à créer un modèle d’IA capable de fonctionner dans de multiples contextes et de s’adapter à divers environnements et scénarios. Il ne s’agit pas d’IA multimodale au sens d’une intelligence applicable au texte, à la parole, à la vidéo et à d’autres données non structurées (peut-être spatiales) ; et il ne s’agit pas non plus d’IA multimodèle en tant que telle, c’est-à-dire de fonctions d’intelligence tirées d’une sélection de services d’IA qui ont été tissés ensemble pour former une notion plus complexe de compréhension des machines… il s’agit plutôt d’un amalgame des deux approches combinées à ce que Neuralogics appelle sa matrice disciplinaire.</p>'}, {'<h3>La matrice disciplinaire, recodée</h3>', ''}, {'', '<p>« La matrice disciplinaire de Neuralogics sert de représentation fondamentale des vérités et des connaissances établies dans divers domaines – informatique, psychologie cognitive, linguistique et ingénierie des systèmes. Elle garantit que nos systèmes d’IA reposent sur un cadre solide de connaissances interdisciplinaires, leur permettant de comprendre et de résoudre des problèmes complexes avec profondeur et précision », note l’entreprise dans sa documentation principale.</p>'}, {'', '<p>Neuralogics affirme avoir également intégré une bonne dose d’apprentissage adaptatif dans sa plateforme afin que ses systèmes d’IA puissent utiliser des algorithmes d’apprentissage avancés qui leur permettent de s’adapter en fonction de nouvelles données, des interactions des utilisateurs et des environnements changeants. En renforçant les garde-fous pour garantir que toutes les « décisions » de l’IA sont logiques, éthiques et transparentes, l’approche multicontextuelle de l’entreprise pourrait faire de cette innovation non seulement une autre innovation en matière de services d’IA, mais aussi une innovation qui imite une nouvelle facette de l’activité comportementale humaine codée.</p>'}, {'', '<p>De plus, il s’agit d’une IA créée par un homme appelé Hofmeister, donc tous les utilisateurs au Royaume-Uni boiront à cela.</p>'}]"
La Maison Blanche va dépenser 11 millions de dollars pour étudier l'utilisation des logiciels open source,"[{'', '<p>Le renforcement de la sécurité autour des logiciels open source est un élément clé des efforts plus vastes de la Maison Blanche en matière de cybersécurité depuis que le président Biden a publié son décret visant à améliorer la posture de sécurité des États-Unis en mai 2021, quelques mois seulement après son entrée en fonction.</p>'}, {'', '<p>L’administration Biden et le ministère de la Sécurité intérieure (DHS) investissent désormais 11 millions de dollars dans le lancement d’un programme visant à évaluer l’utilisation de logiciels open source dans les environnements d’infrastructures critiques et comment mieux les protéger.</p>'}, {'', '<p>S’exprimant lors du salon Def-Con à la fin de la semaine dernière à Las Vegas, le directeur national de la cybersécurité, Harry Coker Jr., a annoncé le projet de lancement de l’initiative gouvernementale pour la prévalence des logiciels open source, à laquelle participent les laboratoires nationaux du pays.</p>'}, {'', '<p>« Nous savons que l’open source est à la base de notre infrastructure numérique, et il est essentiel qu’en tant que gouvernement, nous contribuions à la communauté dans le cadre de nos efforts plus larges en matière d’infrastructure », a déclaré Coker lors de la conférence.</p>'}, {'', '<p>Il a ajouté que même si le gouvernement crée l’initiative et y consacre de l’argent, il aura besoin de la participation de professionnels de la cybersécurité.</p>'}, {'', '<p>« Ces propositions politiques reposent sur le dévouement des chercheurs et sur leur volonté de partager librement leurs découvertes afin de travailler », a déclaré Coker. « Dans nos discussions sur l’élaboration d’un régime de responsabilité logicielle, nous cherchons également de plus en plus à tirer parti de cette communauté unique dans le cadre de solutions politiques novatrices. »</p>'}, {'', '<p>L’un des éléments clés de l’initiative de l’administration Biden en matière de cybersécurité pour améliorer la cybersécurité du pays consiste à transférer la responsabilité des utilisateurs de la technologie vers ses créateurs, par le biais d’efforts tels que son message logiciel Secure By Design. Coker a réitéré cela lors de son discours, en expliquant à l’auditoire d’experts en cybersécurité la nécessité de « faire peser davantage de responsabilités en matière de cybersécurité sur les acteurs les plus compétents de l’écosystème. Cela concerne les producteurs de technologie, oui, et certainement le gouvernement fédéral. Mais cela concerne également vous tous… Je sais que le même ensemble de valeurs qui motive la divulgation responsable des vulnérabilités vous conduira à continuer à intensifier vos efforts pour la protection d’Internet. »</p>'}, {'', '<p>Le pouvoir du gouvernement n’est pas illimité, a-t-il déclaré. Le président ne peut pas résoudre les problèmes en émettant simplement un ordre. Coker a fait remarquer que le gouvernement et l’industrie technologique sont au courant depuis des décennies des failles de sécurité du Border Gateway Protocol, mais qu’une grande partie du trafic Internet américain reste vulnérable au piratage. Il en va de même pour l’utilisation de langages de programmation à mémoire sécurisée comme Rust et Go pour éliminer un pourcentage important de vulnérabilités trouvées dans les logiciels actuels.</p>'}, {'', '<p>« Pourtant, les logiciels essentiels qui sous-tendent notre société sont écrits en C, simplement parce que c’est pratique », a-t-il déclaré. « La « tragédie des biens communs » autour du développement de logiciels open source est un phénomène bien connu ; pourtant, des logiciels vitaux sont maintenus par de minuscules groupes de bénévoles qui travaillent avec un budget très serré. »</p>'}, {'', ""<p>Katie Teitler-Santullo, stratège en cybersécurité chez OX Security, a déclaré que l'efficacité d'un programme comme l'Open Source Software Prevalence Initiative était incertaine.</p>""}, {'', '<p>« D’un côté, des initiatives comme celle-ci émanant de la Maison Blanche et du DHS signalent au secteur privé qu’une surveillance accrue est en vue », a déclaré Teitler-Santullo, dont la société propose une plateforme de gestion de la sécurité des applications. « Étant donné la dépendance croissante aux logiciels open source – et aux composants open source de la plupart des logiciels – les entreprises doivent avoir une meilleure compréhension des logiciels, à toutes les étapes de leur cycle de vie. »</p>'}, {'', '<p>Pourtant, les programmes gouvernementaux de ce type ne sont assortis d’aucune garantie de faisabilité ou d’impact. De nombreuses organisations ont du mal à surveiller et à trier ce qu’elle appelle la « longue traîne des logiciels » et les équipes de sécurité et de développement d’applications ont du mal à suivre le rythme de l’évolution rapide de l’écosystème open source.</p>'}, {'', '<p>« Ce qui est incontestable, c’est l’importance pour les organisations, publiques et privées, de comprendre et d’agir sur les vulnérabilités de l’open source ainsi que sur les logiciels personnalisés construits sur des composants open source », a déclaré Teitler-Santullo. « Comprendre votre code à plusieurs niveaux et tout au long du cycle de vie du développement logiciel… n’est plus une option. Une petite vulnérabilité peut causer des dommages à grande échelle. »</p>'}, {'', '<p>Les remarques de Coker au Def-Con sont intervenues un jour après que la Maison Blanche a publié un résumé des réponses à sa demande d’informations sur ce qui devrait figurer parmi les priorités à long terme du gouvernement en matière de sécurité des logiciels open source.</p>'}, {'', '<p>La création de l’Open-Source Software Prevalence Initiative est l’une des réponses du gouvernement à ce qu’il a entendu de la part de ceux qui ont répondu à la demande d’information.</p>'}, {'', ""<p>Le rapport appelle l'administration à faire appel aux agences fédérales pour accélérer la sécurité des logiciels open source. Il s'agit notamment de développer davantage de listes de composants logiciels ainsi que de créer un bureau du programme open source du gouvernement américain. Selon le résumé, les réponses se sont concentrées sur un certain nombre de mesures, notamment la promotion de l'utilisation élargie des listes de composants logiciels (SBOM), le renforcement de la chaîne d'approvisionnement en logiciels et la sécurisation des référentiels de packages, qui ont été la cible d'acteurs malveillants cherchant à diffuser leurs codes malveillants par le biais d'organisations installant et exécutant par inadvertance ce qui semble être un logiciel légitime.</p>""}, {'', '<p>Ils ont également recommandé au gouvernement d’offrir davantage d’incitations pour utiliser des langues sans danger pour la mémoire.</p>'}, {'', '<p>« Il y a également eu un consensus sur le fait que la mise en œuvre d’une programmation sécurisée en mémoire serait beaucoup plus facile pour les nouveaux projets que pour les projets existants », selon les auteurs du rapport. « Pour ces derniers, de nombreux répondants ont soutenu l’utilisation d’une approche hiérarchisée et hiérarchisée comme moyen d’optimiser les ressources tout en se concentrant sur les projets les plus importants. »</p>'}, {'', '<p>Le lancement récent par la DARPA (Defense Advanced Research Project Agency) de son programme TRACTOR (Translating All C to Rust) pour automatiser de nombreuses tâches nécessaires à la réécriture de code C et C++ en Rust en est un exemple.</p>'}, {'', '<p>D’autres recommandations comprenaient le financement du développement d’outils et de bibliothèques pour sécuriser l’écosystème des logiciels open source, la création de partenariats public-privé au sein de la communauté, l’aide à l’élargissement du vivier de talents des développeurs, l’expansion de la coopération internationale et la recherche sur l’utilisation de l’IA, des modèles à grands langages et des techniques d’apprentissage automatique.</p>'}]"
Dites « non » à « NoOps » : pourquoi nous ne pouvons pas nous permettre de laisser l’IA se déchaîner,"[{'', '<p>Nous avons dépassé le battage médiatique initial autour de l’IA. Aujourd’hui, les secteurs et les métiers de tous horizons reconnaissent la nécessité d’intégrer cette technologie dans leurs flux de travail, ce qui a conduit à une augmentation substantielle de son adoption. L’année dernière, le marché mondial de l’IA était évalué à près de 200 milliards de dollars et il devrait désormais connaître un taux de croissance annuel de 40 % d’ici la fin de la décennie.</p>'}, {'', '<p>Cette croissance est particulièrement marquée parmi les équipes de développement logiciel. Les recherches montrent que 70 % des équipes de développement logiciel ont adopté l’IA et que 30 % ont mis en œuvre une stratégie autour de celle-ci. Ces équipes constatent des gains substantiels en termes de vitesse d’écriture de code grâce à l’IA. Les chiffres en sont la preuve. Les équipes de développement logiciel dotées d’une stratégie d’adoption de l’IA efficace signalent une augmentation de 250 % de la vitesse de développement.</p>'}, {'', '<p>La technologie de l’IA n’est pas parfaite. Elle a des limites et des inconvénients. Par exemple, si l’IA peut accélérer l’écriture de code, elle peut également augmenter le taux de renouvellement du code. Or, deux fois plus de code signifie deux fois plus d’erreurs. La plateforme de développement populaire GitHub recommande de prendre des précautions lors de l’utilisation de code non écrit par un humain. Qu’un code soit écrit par des humains ou par GenAI, il est essentiel de vérifier la qualité du code (c’est-à-dire une approche de code propre).</p>'}, {'', '<p>Malgré cette réalité, certains envisagent un avenir proche de « NoOps », où la communauté des développeurs pourra pleinement compter sur l’IA grâce à une automatisation avancée, des systèmes d’auto-réparation et une surveillance intelligente. Cela pourrait un jour devenir possible dans une certaine mesure à mesure que la technologie continue de progresser et d’évoluer, mais notre environnement actuel n’est pas encore équipé pour cela.</p>'}, {'', '<p>À l’heure actuelle, l’IA n’est pas suffisamment fiable ni précise pour remplacer les développeurs humains et les équipes DevOps. Les enjeux sont trop importants, et les entreprises doivent faire preuve d’esprit critique et de supervision pour commencer à se préparer à l’ère où l’IA prendra le contrôle de toutes les opérations informatiques. Croire le contraire expose votre organisation à des risques d’erreurs, de code et de logiciels de mauvaise qualité et, en fin de compte, à des pertes commerciales.</p>'}, {'', '<p>L’IA remplacera-t-elle un jour complètement les développeurs ? Je penche plutôt pour la réponse « non ». Mais voici quelques éléments à prendre en compte lorsque l’on réfléchit à l’adoption de l’IA et à la « NoOps ».</p>'}, {""<h3>La communauté DevOps ne peut pas manquer l'opportunité de l'IA</h3>"", ''}, {'', '<p>L’IA est le présent et l’avenir. Les équipes de développement ne peuvent pas se permettre de passer à côté de la valeur qu’apporte l’IA ou de prendre du retard dans l’apprentissage de ses différents cas d’utilisation, de ses particularités et de ses inconvénients. Les développeurs et les ingénieurs de tous les rôles et de tous les niveaux de compétence doivent être parfaitement familiarisés avec l’utilisation de ces outils GenAI. S’ils ne le sont pas, ils risquent d’être laissés pour compte.</p>'}, {'', '<p>Cela est particulièrement vrai à l’heure où les logiciels deviennent un actif commercial de plus en plus essentiel. Pensez à l’importance que nous accordons à la technologie dans tous les aspects de notre vie. Les téléphones que nous utilisons, les voitures que nous conduisons, les appareils intelligents que nous installons dans nos maisons : tous ces éléments sont alimentés par des logiciels. Les éditeurs de logiciels qui fournissent ces logiciels doivent donc s’assurer que leur code de base est propre, c’est-à-dire qu’il est sécurisé, fiable, maintenable et de haute qualité.</p>'}, {'', '<p>Les outils d’IA ne sont pas seulement utiles aux développeurs. Utilisés correctement, les assistants de codage IA peuvent avoir un impact considérable sur la création, la gestion et le test des logiciels et de l’infrastructure logicielle, au bénéfice des équipes DevOps. Par exemple, l’IA peut s’avérer précieuse dans le processus CI/CD, en le rationalisant grâce à de meilleures capacités d’automatisation pour la création, le test et le déploiement. L’IA peut également intégrer un nouveau code dans un environnement existant, ce qui accélère le déploiement et réduit le risque d’erreur.</p>'}, {'', '<p>L’automatisation par l’IA améliore également l’agilité et l’efficacité. Elle peut prendre en charge les tâches fastidieuses afin que les équipes n’aient pas besoin de perdre du temps sur des tâches fastidieuses et puissent se concentrer sur des projets prioritaires. Comme l’IA peut gérer des tâches fastidieuses et répétitives, elle réduit les erreurs humaines et permet aux équipes de se concentrer sur des tâches qui bénéficient de la créativité, de l’innovation et de la pensée critique humaines.</p>'}, {'', '<p>Grâce à l’IA, les développeurs peuvent affiner leur code, en garantissant la qualité des parties les plus complexes d’un projet tout en réduisant les erreurs dans les parties de code auxiliaires. Notre dépendance aux logiciels signifie que le code doit être de haute qualité, ce qui signifie également qu’il est sécurisé par conception. La prévention des cyberattaques réduit les incidents coûteux et menaçant la réputation, et l’IA peut contribuer considérablement à cet effort.</p>'}, {'', '<p>Mais n’oubliez pas que l’IA n’est pas parfaite. Elle peut contribuer à rationaliser le DevOps et le processus de développement et continuera de le faire à mesure que la technologie s’améliorera. Cependant, la surveillance humaine est une nécessité indéniable dans le domaine des logiciels. Les entreprises ne peuvent pas se permettre de mettre en œuvre l’IA sans mesures de protection appropriées. Il ne s’agit pas seulement de disposer des bons outils, mais également de s’assurer que vous les utilisez de la bonne manière et de vérifier leur travail comme vous le feriez avec un collègue humain.</p>'}, {'', ""<h3>L'utilisation responsable de l'IA implique une surveillance humaine</h3>""}, {'', '<p>Comme toute technologie émergente, l’IA a besoin de garde-fous. Les équipes et les entreprises doivent s’assurer que les employés utilisent l’IA d’une manière qui ne compromet pas la productivité, la qualité ou les résultats de l’entreprise.</p>'}, {'', '<p>La réalité est que vous ne pouvez pas remplacer la pensée critique humaine par l’apprentissage automatique (ML) – ni maintenant ni dans un avenir proche. Les humains doivent toujours être au cœur de la prise de décision vitale dans les processus de développement et DevOps. Par exemple, l’IA ne dispose pas du contexte d’une base de code ; elle peut ne pas comprendre ce que vous essayez d’accomplir avec un projet. Son résultat ne peut pas simplement être déployé sans examen. Vous avez besoin d’une protection humaine.</p>'}, {'', '<p>En fait, les chercheurs de Vectara, une plateforme GenAI fondée par d’anciens employés de Google, ont quantifié les hallucinations de l’IA pour comprendre les limites et les échecs de cette technologie. Ils ont découvert que les chatbots inventent des choses entre 3 et 27 % du temps. Pour recenser ce genre de problèmes, ils tiennent même un « classement des hallucinations » sur GitHub. Pensez au nombre de fois où vous avez vu des exemples de chatbots et de générateurs d’IA créant du contenu ou des réponses à des questions absurdes : les mêmes préoccupations s’appliquent au développement de logiciels et à DevOps.</p>'}, {'', '<p>Travailler avec l’IA exige également transparence et responsabilité. Vous ne pouvez pas vous fier à l’IA pour extraire des données d’une source irréprochable ; vous devez être en mesure de vérifier l’origine des contributions de l’IA pour garantir leur exactitude. Les équipes humaines doivent également savoir ce que ces outils peuvent faire : leurs limites, leurs règles et leurs cas d’utilisation spécifiques. Cela revient à s’assurer que les équipes bénéficient d’une formation et d’une éducation adéquates.</p>'}, {'', '<p>Il ne suffit pas de tirer les leçons des cas d’utilisation actuels. À mesure que ces outils évoluent et progressent, la manière dont nous les utilisons doit également être évaluée de manière cohérente. Où constatons-nous des inconvénients ? Des améliorations ? Des échecs ? Comment pouvons-nous utiliser au mieux cette technologie dès maintenant ? C’est le genre de conversations ouvertes et honnêtes que les développeurs et les équipes DevOps devraient constamment avoir pour apprendre les uns des autres.</p>'}, {'', '<p>Nous n’avons aucun moyen de prédire l’avenir ou de savoir à quoi ressemblera l’IA dans cinq, dix ou quinze ans. Cela signifie que nous devrons toujours rester agiles et évoluer en même temps qu’elle. Les équipes DevOps devront continuer à peaufiner et à adapter les meilleures pratiques.</p>'}, {""<h3>Un avenir imprévisible, alimenté par l'IA</h3>"", ''}, {'', '<p>L’IA joue désormais un rôle majeur et incontournable dans le paysage logiciel. Alors que nous évoluons vers un avenir axé sur l’IA, nous devons continuer à comprendre ses implications et nous efforcer d’obtenir le meilleur résultat possible.</p>'}, {'', '<p>Alors que nous anticipons la croissance et les progrès de l’IA au cours de la prochaine décennie, il est important de reconnaître que nous ne sommes pas actuellement équipés technologiquement pour accorder à ces outils un contrôle total sur tous les processus. L’idée d’un avenir « NoOps » est intrigante, mais pour l’instant, elle n’est tout simplement pas viable. Tout comme nous ne sommes pas encore au point où les voitures sans conducteur constituent incontestablement des alternatives plus sûres à nos véhicules, ou que nous pouvons faire entièrement confiance aux chatbots pour répondre correctement aux questions de chat en direct, les assistants de codage IA ne peuvent pas simplement fonctionner seuls.</p>'}, {'', '<p>Les développeurs et les équipes DevOps ne peuvent pas ignorer la nécessité de l’intervention humaine dans les logiciels. Comme ses homologues humains, l’IA n’est pas parfaite. Pourtant, une opportunité indéniable se présente à nous. Nous vivons une époque passionnante d’innovation et nous nous apercevons d’une incroyable transformation. Mais ce n’est qu’en faisant preuve d’un certain niveau de prudence lorsque nous approchons de ce précipice que nous pourrons nous assurer d’en récolter les fruits lorsqu’il se produira.</p>'}]"
L'alignement de Microsoft sur GitHub favorise la naissance du développeur d'IA,"[{'', '<p>Microsoft souhaite que les développeurs évoluent vers des développeurs d’IA. C’est une évolution assez prévisible : au début, les développeurs étaient des développeurs mainframe, puis ils sont devenus des développeurs PC, des développeurs mobiles, puis des développeurs cloud. Ensuite, logiquement, les ingénieurs logiciels devront devenir des développeurs d’IA centrés sur l’automatisation, plus proches des modèles et des moteurs qui permettent nos nouveaux services d’intelligence.</p>'}, {'', '<p>Pour faciliter cette transition, Redmond promeut bien sûr l’utilisation des services d’IA Microsoft Azure comme moyen de créer des applications d’IA personnalisées. Pour atteindre la masse critique qu’elle recherche tant, Microsoft s’associe à GitHub pour permettre à ses plus de 100 millions de développeurs de créer des applications d’IA directement sur la plateforme GitHub.</p>'}, {'', ""<p>Bien que Microsoft ait acquis GitHub en 2018 et que la plateforme ait ensuite été renforcée par des outils de pipeline CI/CD et des capacités de sécurité plus larges, elle reste une entité relativement indépendante dans le style d'un Red Hat au sein d'IBM.</p>""}, {'', '<h3>Place de marché des modèles GitHub</h3>'}, {'', '<p>Microsoft a désormais mis à disposition des développeurs les fonctions de sélection de modèles Azure AI Studio via GitHub Models, un portail « marketplace » où les ingénieurs logiciels qui souhaitent développer une application d’IA générative peuvent trouver et expérimenter gratuitement des modèles d’IA avant d’envisager un paiement par jeton une fois l’application passée en production. Les efforts de Microsoft dans ce domaine incluent également la fourniture d’API conçues pour permettre la mise en production d’applications d’IA.</p>'}, {'', '<p>GitHub Marketplace existe en tant que « terrain de jeu gratuit » où les programmeurs peuvent ajuster les paramètres du modèle et soumettre des invites pour voir comment un modèle d’IA réagit. Ce travail comprend également des intégrations avec Codespaces et Microsoft Visual Studio Code. GitHub Codespaces fournit un environnement de développement sécurisé avec une sélection de ressources intégrées prédéfinies qui bénéficient d’une intégration native avec la plateforme GitHub.</p>'}, {'', ""<p>GitHub Models a été créé en août de cette année pour permettre l'essor de l'ingénieur en IA avec accès aux principaux modèles de langage, grands et petits.</p>""}, {'', ""<h3>L'essor de l'ingénieur en IA</h3>""}, {'', '<p>« Alors que l’innovation en matière de modèles d’IA s’accélère, Azure reste déterminé à proposer la meilleure sélection de modèles et la plus grande diversité de modèles pour répondre aux besoins uniques des développeurs d’IA en matière de coût, de latence, de conception et de sécurité. Aujourd’hui, nous proposons la bibliothèque de modèles la plus vaste et la plus complète du marché, comprenant les derniers modèles d’OpenAI, Meta, Mistral et Cohere ainsi que des mises à jour de notre propre famille Phi-3 de petits modèles de langage », note Asha Sharma, vice-présidente de la plateforme d’IA chez Microsoft.</p>'}, {'', ""<p>Avec GitHub Models, Sharma suggère aux développeurs d'explorer les derniers modèles ainsi que d'autres innovations en matière d'IA telles que les « modèles de frontière » de nouvelle génération, c'est-à-dire les modèles à grande échelle avec de nombreux paramètres et une logique algorithmique complexe tels que Google Gemini, Claude d'Anthropic, DeepMind d'AlphaFold et même GPT-4 d'OpenAI.</p>""}, {'', '<p>« Pour la plupart d’entre nous, l’apprentissage du développement ne s’est pas fait de manière linéaire en classe. Il a fallu beaucoup de pratique, de jeu et d’apprentissage par l’expérimentation. Il en va de même aujourd’hui pour les modèles d’IA. Dans le nouveau terrain de jeu interactif, les étudiants, les amateurs et les startups peuvent explorer les modèles privés et ouverts les plus populaires de Meta, Mistral, Azure OpenAI Service Microsoft et d’autres en quelques clics et frappes de touches », a écrit Thomas Dohmke, PDG de GitHub.</p>'}, {'', '<h3>De nombreux modes et modaux de modèles</h3>'}, {'', '<p>Cette opération de rapprochement est probablement le résultat de l’ampleur et de la portée du développement actuel des modèles d’IA. Associée à la nécessité de chevaucher les surfaces d’attaque des déploiements couvrant le cloud, la périphérie, les applications à usage général et peut-être les déploiements d’applications et de services de données plus spécifiques à une tâche ou à un secteur, la complexité actuelle est vertigineuse. GitHub Models vise à ouvrir la porte aux développeurs pour expérimenter plusieurs modèles, en simplifiant l’expérimentation et la sélection de modèles dans le catalogue Azure AI afin que les ingénieurs puissent comparer les modèles, les paramètres et les invites.</p>'}, {'', '<p>En faisant d’Azure AI une plateforme ouverte et modulaire, Microsoft entend aider ses clients à passer rapidement de l’idée au code et au cloud, explique Sharma. Avec Azure AI sur GitHub, les développeurs peuvent utiliser Codespaces pour configurer un prototype ou utiliser l’extension Prompty pour générer du code avec des modèles GitHub directement dans Microsoft Visual Studio Code.</p>'}, {'', '<p>« La sélection accrue de modèles offre aux développeurs la plus large gamme d’options pour les applications individuelles qu’ils créent. Mais chaque modèle apporte naturellement une complexité accrue. Pour contrer ce problème, nous facilitons considérablement l’expérimentation de différents modèles par chaque développeur via l’API d’inférence de modèles Azure AI. Grâce à cette API unique, les développeurs GitHub peuvent désormais accéder à un ensemble commun de fonctionnalités pour comparer les performances d’un ensemble diversifié de modèles fondamentaux de manière uniforme et cohérente, en passant facilement d’un modèle à l’autre pour comparer les performances sans modifier le code sous-jacent », a déclaré Sharma.</p>'}, {'', '<p>Microsoft a annoncé qu’elle allait étendre encore davantage son intégration. Ce que cela signifie, c’est l’engagement de Redmond à apporter le langage, la vision et les services multimodaux d’Azure AI à GitHub, ainsi que d’autres éléments de la chaîne d’outils Azure AI. Les organisations disposant d’un abonnement Azure existant peuvent acheter des produits GitHub en libre-service, directement auprès du service commercial Microsoft ou via des fournisseurs de solutions tiers et peuvent ajuster le nombre de postes GitHub selon les besoins.</p>'}]"
BMC apporte l'IA générative au mainframe,"[{'', ""<p>BMC, dans le cadre d'un engagement plus large visant à créer des assistants d'intelligence artificielle (IA) génératifs pour simplifier la gestion des mainframes, a mis à disposition en version bêta un outil qui explique les fonctionnalités du code.</p>""}, {'', ""<p>John McKenny, vice-président senior et directeur général de l'optimisation et de la transformation intelligentes Z chez BMC, a déclaré que BMC Automated Mainframe Intelligence (AMI) DevX Code Insights fournira, via une interface de chat, des conseils pour aider à déboguer le code écrit dans plusieurs langues, comprendre les processus système et prendre des décisions plus éclairées.</p>""}, {''}, {'', ""<p>Basé sur les grands modèles de langage (LLM) que BMC est en train de former, BMC AMI DevX Code Insights est l'un des nombreux agents d'IA que BMC prévoit de mettre à disposition via une console unique. Ces LLM seront développés par BMC ou basés sur des plateformes tierces. Par ailleurs, BMC prévoit également de permettre aux organisations d'invoquer les LLM qu'elles décident de créer elles-mêmes.</p>""}, {'', '<p>BMC invite également les entreprises à participer à un programme de conception par lequel l’entreprise donnera accès aux fonctionnalités d’IA générative au fur et à mesure de leur développement. En effet, les agents d’IA fourniront l’équivalent d’un expert en la matière (SME) pour chaque tâche assignée, a ajouté McKenny. C’est essentiel car pour que l’IA générative soit efficace, une plateforme doit offrir plus que la possibilité d’utiliser des invites pour poser des questions, a-t-il noté. Au lieu de cela, un agent d’IA doit être capable de faire apparaître des conseils et des informations qui aident à rationaliser les flux de travail, a ajouté McKenny.</p>'}, {'', '<p>Si l’IA générative jouera un rôle majeur dans la simplification de la gestion des mainframes, elle ne constitue qu’un élément des efforts continus de BMC pour simplifier la gestion des plates-formes mainframe au cours des 30 prochaines années ou plus, a déclaré McKenney. Par exemple, BMC permettra aux développeurs de répondre eux-mêmes à leurs besoins à l’aide d’un catalogue de services qui réduit leur dépendance vis-à-vis des équipes d’exploitation informatique, a-t-il noté.</p>'}, {'', '<p>En fin de compte, l’objectif est de faire du mainframe un simple type de plateforme informatique distribuée dont la gestion nécessite beaucoup moins de compétences spécialisées. BMC n’est pas le seul fournisseur de logiciels pour mainframe à aller dans cette direction, mais avec l’essor de l’IA générative, le temps nécessaire pour atteindre cet objectif est sur le point d’être considérablement réduit.</p>'}, {'', '<p>Cela pourrait s’avérer crucial à mesure que le nombre de modèles d’IA déployés sur des plateformes mainframe où se trouvent déjà d’énormes quantités de données va augmenter régulièrement dans les mois et les années à venir. Après tout, il est généralement plus facile d’amener des modèles d’IA là où les données existent déjà que de déplacer ces données vers une autre plateforme.</p>'}, {'', ""<p>Les équipes informatiques peuvent également vouloir revoir quels types de charges de travail s'exécutent sur quelle plateforme dans les environnements informatiques. Bien que toutes les organisations informatiques ne disposent pas d'un mainframe, celles qui en ont un peuvent réduire le coût total de l'informatique en tirant parti des licences mainframe conçues pour encourager les équipes informatiques à consolider davantage de charges de travail sur la plateforme.</p>""}, {'', '<p>Quelle que soit l’approche adoptée, une chose est sûre : avec le temps, les équipes informatiques deviendront plus homogènes, car à l’ère de l’IA, il devient plus simple de gérer les charges de travail, quel que soit leur emplacement physique.</p>'}]"
DevOps pour l'apprentissage automatique et l'intelligence artificielle,"[{'', '<p>Dans le paysage technologique actuel, DevOps est devenu synonyme de processus de développement et d’exploitation rationalisés. Cependant, en ce qui concerne l’apprentissage automatique (ML) et l’intelligence artificielle (IA), les pratiques DevOps traditionnelles sont confrontées à des défis uniques. L’émergence de DevOps pour l’apprentissage automatique, souvent appelé MLOps, fournit le cadre nécessaire pour combler le fossé entre la science des données, les opérations et les applications d’IA innovantes. Il permet aux organisations de développer, de déployer et de gérer efficacement des modèles d’apprentissage automatique et d’IA, favorisant ainsi une intégration transparente de l’intelligence basée sur les données dans leurs flux de travail opérationnels.</p>'}, {'', ""<h3>Défis des opérations de ML et d'IA</h3>""}, {'', '<p>Le développement et le déploiement de modèles ML et IA introduisent des complexités qui remettent en question les méthodologies DevOps traditionnelles\xa0:</p>'}, {'', '<li>Complexité du pipeline de données : le ML et l’IA nécessitent souvent un prétraitement et une gestion complexes des données, ce qui fait de la gestion du pipeline de données une tâche critique et complexe.</li>'}, {'', ""<li>Contrôle de version du modèle : le suivi de plusieurs versions de modèles, de leurs dépendances et de leurs performances au fil du temps est essentiel pour la reproductibilité et la maintenance des projets d'IA.</li>""}, {'', ""<li>Cohérence de l'environnement : il est essentiel de garantir la cohérence des environnements de développement, de test et de production pour éviter les divergences dans le comportement du modèle.</li>""}, {'', '<li>Évolutivité et performances : la mise à l’échelle des modèles ML et IA pour gérer les charges de travail de production tout en maintenant les performances peut être difficile, en particulier pour les modèles IA gourmands en ressources.</li>'}, {'', ""<li>Suivi et gouvernance éthique : le suivi en temps réel des performances du modèle est crucial. Les considérations éthiques liées à la génération de contenu d'IA et à la prévention des abus sont primordiales.</li>""}, {'', ""<h3>Rôle des MLOps pour le ML et l'IA</h3>""}, {'', ""<p>MLOps est une approche qui intègre les systèmes ML dans le flux de travail DevOps plus large. Elle rassemble les équipes de science des données et d'exploitation pour rationaliser le cycle de vie ML de bout en bout :</p>""}, {'', ""<ol>Collaboration interdisciplinaire\xa0: les projets d'IA impliquent souvent des équipes interfonctionnelles, notamment des scientifiques des données, des développeurs et des spécialistes de l'IA. MLOps facilite une collaboration transparente entre ces divers rôles.\xa0 Traitement avancé des données\xa0: l'IA peut fonctionner avec des données structurées, du texte non structuré, des images ou du multimédia. MLOps doit gérer divers types de données et garantir leur qualité et leur disponibilité.\xa0 Contrôle de version\xa0: en appliquant des pratiques de contrôle de version similaires à DevOps traditionnel, MLOps aide à gérer et à suivre les modifications apportées au code, aux données et aux artefacts du modèle.\xa0 Intégration et déploiement continus\xa0: les principes d'intégration continue/déploiement continu (CI/CD) s'étendent à l'IA, permettant des tests, une validation et un déploiement automatisés des modèles.\xa0 Pipelines automatisés\xa0: les pipelines ML automatisés sont au cœur des MLOps, permettant aux organisations d'automatiser le prétraitement des données, la formation, l'évaluation et le déploiement des modèles.\xa0 Conteneurisation et orchestration\xa0: les conteneurs, tels que Docker, et les plates-formes d'orchestration de conteneurs, comme Kubernetes, sont utilisés pour empaqueter et déployer des modèles ML de manière cohérente dans tous les environnements. IA explicable (XAI)\xa0: il est essentiel de garantir la transparence et l'interprétabilité des décisions prises par l'IA. Les MLOps doivent intégrer les techniques XAI pour expliquer les décisions prises par l'IA. Surveillance et observabilité\xa0: la mise en œuvre de solutions de surveillance et d'observabilité robustes garantit que les modèles ML fonctionnent comme prévu en production et facilite le débogage et l'optimisation. Gouvernance et conformité\xa0: les MLOps mettent l'accent sur les pratiques de gouvernance, en veillant à ce que les modèles ML répondent aux exigences réglementaires et adhèrent aux normes éthiques.</ol>""}, {'', ""<li>Collaboration interdisciplinaire : les projets d'IA impliquent souvent des équipes interfonctionnelles, notamment des scientifiques des données, des développeurs et des spécialistes de l'IA. MLOps facilite la collaboration transparente entre ces différents rôles.</li>""}, {'', ""<li>Traitement avancé des données : l'IA peut travailler avec des données structurées, du texte non structuré, des images ou du contenu multimédia. Les MLOps doivent gérer divers types de données et garantir leur qualité et leur disponibilité.</li>""}, {'', '<li>Contrôle de version : en appliquant des pratiques de contrôle de version similaires à celles de DevOps traditionnel, MLOps permet de gérer et de suivre les modifications apportées au code, aux données et aux artefacts du modèle.</li>'}, {'', ""<li>Intégration et déploiement continus : les principes d'intégration/déploiement continu (CI/CD) s'étendent à l'IA, permettant des tests, une validation et un déploiement automatisés des modèles.</li>""}, {'', ""<li>Pipelines automatisés : les pipelines ML automatisés sont au cœur des MLOps, permettant aux organisations d'automatiser le prétraitement des données, la formation, l'évaluation et le déploiement des modèles.</li>""}, {'', '<li>Conteneurisation et orchestration\xa0: les conteneurs, tels que Docker, et les plateformes d’orchestration de conteneurs, comme Kubernetes, sont utilisés pour empaqueter et déployer des modèles ML de manière cohérente dans tous les environnements.</li>'}, {'', ""<li>IA explicable (XAI) : il est essentiel de garantir la transparence et l'interprétabilité des décisions prises par l'IA. Les MLOps doivent intégrer des techniques XAI pour expliquer les décisions prises par l'IA.</li>""}, {'', ""<li>Surveillance et observabilité : la mise en œuvre de solutions de surveillance et d'observabilité robustes garantit que les modèles ML fonctionnent comme prévu en production et facilite le débogage et l'optimisation.</li>""}, {'', '<li>Gouvernance et conformité : MLOps met l’accent sur les pratiques de gouvernance, en garantissant que les modèles ML répondent aux exigences réglementaires et adhèrent aux normes éthiques.</li>'}, {'', ""<h3>Avantages des MLOps pour le ML et l'IA</h3>""}, {'', '<p>L’adoption du MLOps dans le contexte du ML et de l’IA offre plusieurs avantages\xa0:</p>'}, {'', '<li>Projets d’IA accélérés : MLOps rationalise le développement et le déploiement de modèles d’IA, réduisant ainsi le délai de rentabilisation des initiatives d’IA.</li>'}, {'', '<li>Collaboration améliorée : la collaboration entre les scientifiques des données, les développeurs et les spécialistes de l’IA conduit à une exécution plus efficace des projets d’IA.</li>'}, {'', ""<li>Reproductibilité améliorée : MLOps garantit que les expériences d'IA sont bien documentées et reproductibles, prenant en charge l'audit et la conformité des modèles.</li>""}, {'', '<li>Évolutivité : les modèles d’IA peuvent évoluer de manière transparente pour gérer différentes charges de travail tout en maintenant les performances et la fiabilité.</li>'}, {'', '<li>IA éthique : MLOps souligne l’importance d’une utilisation éthique de l’IA, réduisant ainsi le risque de contenu généré par l’IA nuisible ou inapproprié.</li>'}, {'', '<h3>Tendances futures</h3>'}, {'', '<p>L’avenir de DevOps dans l’IA et le ML promet une intégration accrue de l’apprentissage automatique, de l’automatisation et de la transparence. Le MLOps, qui combine DevOps et ML, deviendra la norme, tandis que les outils DevOps pilotés par l’IA optimiseront les flux de travail, amélioreront la sécurité et prédiront le comportement du système. L’informatique sans serveur simplifiera le déploiement de l’IA, l’apprentissage fédéré aidera les équipes distribuées et les pratiques éthiques de l’IA garantiront une utilisation responsable. Ces tendances reflètent l’évolution de DevOps dans son adaptation aux exigences d’un environnement de plus en plus axé sur l’IA.</p>'}, {'', '<h3>Conclusion</h3>'}, {'', ""<p>Le DevOps pour l'apprentissage automatique et l'intelligence artificielle, connu sous le nom de MLOps, évolue pour répondre aux exigences du paysage de l'IA. En intégrant divers rôles, en gérant divers types de données et en prenant en compte les considérations éthiques, MLOps garantit que la synergie entre l'IA, l'intelligence pilotée par les données et les opérations se traduit par des solutions innovantes et responsables basées sur l'IA qui enrichissent notre monde tout en préservant la fiabilité, l'évolutivité et l'intégrité éthique.</p>""}]"
Solutions basées sur l'IA pour les pipelines Azure DevOps,"[{'', '<p>Le paysage technologique est de plus en plus concurrentiel et, pour que les entreprises prospèrent, le dernier recours consiste à exploiter les technologies modernes. L’intelligence artificielle (IA) n’est plus une chose du passé : elle est là et change rapidement le paysage du développement logiciel. Vous serez gagnant si vous trouvez cet article en recherchant l’intégration de l’IA dans vos pipelines Azure DevOps.</p>'}, {'', '<p>Ce guide complet révélera comment l’IA peut augmenter le processus de livraison de logiciels de haute qualité plus rapidement et plus efficacement.</p>'}, {'', '<h3>Comprendre les pipelines Azure DevOps</h3>'}, {'', '<p>Définition</p>'}, {'', ""<p>Les pipelines Azure DevOps sont la pierre angulaire du développement logiciel moderne. Ils servent de workflows automatisés qui coordonnent l'ensemble du processus de livraison de logiciels. Considérez-les comme des chaînes de montage numériques, où votre code est compilé, testé et empaqueté dans des artefacts déployables. Ces pipelines rationalisent et normalisent les étapes impliquées dans la création, le test et le déploiement d'applications, garantissant ainsi la cohérence et la fiabilité dans différents environnements.</p>""}, {'', '<p>Composants clés</p>'}, {'', '<li>Build\xa0: la première étape consiste à créer un code source, à minimiser les dépendances et enfin à le créer. Cette étape consiste à compiler votre code source, à résoudre les dépendances et à former des packages réalisables. Les pipelines Azure DevOps offrent diverses tâches de build pour différents langages et frameworks de programmation.</li>'}, {'', ""<li>Test : les tests sont essentiels pour une automatisation efficace des fonctionnalités et des performances des applications. Les pipelines Azure DevOps ont une capacité unique à s'intégrer à plusieurs frameworks de test. Cela permet d'intégrer différents types de tests, notamment des tests unitaires, d'intégration et d'interface utilisateur, dans le cadre de votre pipeline global.</li>""}, {'', '<li>Déploiement\xa0: une fois votre application créée et testée avec succès, l’étape de déploiement prend le relais. Les pipelines Azure DevOps vous permettent de déployer vos artefacts sur différentes cibles, telles que des machines virtuelles, des clusters Kubernetes ou des services cloud.</li>'}, {'', '<h3>Les défis de l’optimisation traditionnelle des pipelines</h3>'}, {'', ""<p>Bien que les pipelines Azure DevOps offrent un cadre robuste pour l'automatisation, leur optimisation traditionnelle peut s'avérer difficile. Les problèmes courants incluent\xa0:</p>""}, {'', '<li>Configuration manuelle : la configuration et la maintenance de pipelines complexes nécessitent souvent une configuration manuelle approfondie, qui peut prendre du temps et être sujette aux erreurs.</li>'}, {'', ""<li>Boucles de rétroaction longues\xa0: l'identification et la résolution des problèmes dans un pipeline peuvent prendre du temps, en particulier lorsque l'on s'appuie sur des tests et un déploiement manuels. Cela peut entraîner des retards dans les versions et des équipes frustrées.</li>""}, {'', ""<li>Visibilité limitée : obtenir des informations sur les performances du pipeline, les goulots d'étranglement et les défaillances potentielles peut être difficile sans outils de surveillance et d'analyse avancés.</li>""}, {'', ""<li>Défis d'évolutivité : à mesure que les projets gagnent en complexité et en ampleur, la gestion et l'optimisation des pipelines deviennent de plus en plus difficiles, nécessitant des ressources et une expertise supplémentaires.</li>""}, {'', '<p>Notre approche de l’optimisation des pipelines a complètement changé avec l’arrivée de l’IA. En automatisant les tâches répétitives, en analysant de vastes quantités de données et en faisant des prévisions intelligentes, l’IA peut permettre aux équipes de surmonter ces défis et d’atteindre de nouveaux niveaux d’efficacité, de qualité et d’agilité dans leurs processus de livraison de logiciels.</p>'}, {'', ""<h3>Optimiser les pipelines Azure DevOps avec l'IA\xa0: un guide étape par étape</h3>""}, {'', '<p>En suivant les étapes décrites ci-dessous, vous pouvez exploiter la puissance de l’IA pour optimiser les pipelines Azure DevOps. Explorons cela en détail.</p>'}, {'', ""<p>Étape 1 : Surveillance et analyse basées sur l'IA</p>""}, {'', '<p>Visibilité en temps réel : une évaluation appropriée de l’état de santé, des performances et des indicateurs clés de votre pipeline est essentielle pour un flux de travail fluide. Les outils de surveillance basés sur l’IA sont d’une grande aide à cet égard. Vous pouvez mettre en œuvre des solutions telles qu’Azure Application Insights ou des plateformes similaires pour obtenir une visibilité complète et ciblée sur le fonctionnement du pipeline.</p>'}, {'', ""<p>Détection d'anomalies\xa0: les algorithmes d'apprentissage automatique (ML) sont un excellent moyen de détecter les comportements inhabituels dans votre pipeline. Les exemples incluent les contraintes de ressources et les tests en échec. Le principal avantage de cette surveillance assistée par l'IA est l'intervention précoce pour éliminer le problème.</p>""}, {'', ""<p>Analyse des causes profondes : une fois que les moniteurs d'IA ont détecté l'anomalie, ils agissent rapidement pour identifier la cause profonde car ils peuvent facilement inspecter des données et des journaux volumineux. Cela permet également de réduire le temps de réponse et de lancer des processus de dépannage.</p>""}, {'', '<p>Étape 2 : Tests intelligents et automatisation</p>'}, {'', ""<p>Génération de cas de test pilotés par l'IA : avec des outils basés sur l'IA, des cas de test peuvent être générés après toute modification de code ou cas d'utilisation. Cela présente deux avantages majeurs : cela réduit l'effort requis pour tester en minimisant le travail manuel et en permettant une analyse complète des tests.</p>""}, {'', '<p>Exécution de tests automatisée : les cadres de test pilotés par l’IA peuvent effectuer des tests sur plusieurs cas de test et configurations. Ces solutions de tests automatisés augmentent la vitesse et l’efficacité du flux de travail et réduisent les erreurs humaines.</p>'}, {'', ""<p>Analyse intelligente des résultats des tests : après les tests, les algorithmes d'IA peuvent également analyser les résultats des tests et fournir des informations utiles telles que des comportements anormaux et des modèles similaires. L'un des avantages de cette fonctionnalité est la suite de tests améliorée et les déploiements soutenus en toute confiance.</p>""}, {'', '<p>Étape 3\xa0: Déploiement et livraison continus</p>'}, {'', ""<p>Orchestration des versions pilotée par l'IA : exploitez l'IA pour automatiser le processus d'orchestration des versions, y compris les approbations de déploiement, les contrôles d'environnement et les procédures de restauration. Cela simplifie le processus de déploiement, minimise les interventions manuelles et réduit le risque d'erreurs.</p>""}, {'', ""<p>Déploiements Canary\xa0: implémentez des déploiements Canary basés sur l'IA, où de nouvelles fonctionnalités ou modifications de code sont progressivement déployées auprès d'un sous-ensemble d'utilisateurs. L'IA peut analyser l'impact de ces modifications sur les performances, l'expérience utilisateur et d'autres indicateurs clés, ce qui vous permet de prendre des décisions basées sur les données concernant un déploiement plus large.</p>""}, {'', ""<p>Restaurations automatiques : en cas d'échec après le déploiement, l'IA peut déclencher des restaurations automatiques vers une version stable antérieure de votre application. Une récupération rapide et un impact minimal sur les utilisateurs sont les avantages de cette capacité.</p>""}, {'', '<p>Étape 4 : Boucles de rétroaction et amélioration continue</p>'}, {'', ""<p>Surveillance et analyse continues : les informations basées sur l'IA peuvent aider à identifier les domaines à améliorer et à maintenir une boucle de rétroaction constante en surveillant et en analysant en permanence les données de votre pipeline, par exemple en optimisant les temps de création, en affinant les stratégies de test ou en peaufinant les processus de déploiement.</p>""}, {'', '<p>Apprentissage adaptatif : implémentez des algorithmes d’IA capables d’apprendre des exécutions de pipeline passées et d’ajuster automatiquement les configurations ou les paramètres pour améliorer les performances et la fiabilité au fil du temps.</p>'}, {'', '<p>Partage des connaissances : créer un environnement de coordination et de partage d’informations entre les équipes de développement et d’exploitation est essentiel au bon fonctionnement d’un projet. Utilisez des outils basés sur l’IA pour faciliter la communication, documenter les meilleures pratiques et créer une culture d’apprentissage et d’amélioration continue.</p>'}, {'', '<h3>Outils et technologies pour Azure DevOps</h3>'}, {'', ""<ol>Azure Machine Learning : un outil puissant et efficace qui permet de créer, de former et de déployer des frameworks ML facilement intégrés à vos pipelines Azure DevOps. De plus, il est basé sur le cloud, ce qui augmente ses fonctionnalités sur diverses plateformes et réseaux. Vous pouvez l'exploiter pour la détection d'anomalies, l'analyse prédictive, les tests intelligents et d'autres optimisations pilotées par l'IA. Azure DevOps Extensions : un vaste marché d'extensions offre des fonctionnalités basées sur l'IA pour divers aspects de votre pipeline. Ces extensions peuvent aider à l'analyse de code, à l'automatisation des tests, à la gestion des versions et plus encore, facilitant ainsi l'intégration de l'IA dans vos flux de travail existants. Opsera : en exploitant l'IA pour automatiser et optimiser la distribution de logiciels, Opsera donne une longueur d'avance à DevOps. Il fournit des fonctionnalités telles que l'optimisation intelligente des pipelines, l'analyse prédictive et les tests automatisés, permettant aux équipes de rationaliser les flux de travail et d'obtenir des versions plus rapides. Harness : cette plateforme de livraison continue intègre l'IA pour automatiser la vérification du déploiement, optimiser les coûts du cloud et garantir la conformité. Il utilise le ML pour analyser les modèles de déploiement et prédire les problèmes potentiels, permettant aux équipes de livrer des logiciels en toute confiance. GitHub Copilot : cet outil de saisie semi-automatique de code basé sur l'IA peut améliorer considérablement la qualité du code et la productivité des développeurs. Cependant, il est important de savoir que GitHub Copilot n'est actuellement pas intégré à Azure DevOps. Il utilise le ML pour suggérer des extraits de code, des lignes complètes et même générer des fonctions entières, ce qui permet aux développeurs de gagner un temps précieux et de réduire les erreurs.</ol>""}, {'', ""<li>Azure Machine Learning : un outil puissant et efficace qui permet de créer, de former et de déployer des frameworks ML facilement intégrés à vos pipelines Azure DevOps. De plus, il est basé sur le cloud, ce qui augmente ses fonctionnalités sur différentes plateformes et réseaux. Vous pouvez l'exploiter pour la détection d'anomalies, l'analyse prédictive, les tests intelligents et d'autres optimisations pilotées par l'IA.</li>""}, {'', ""<li>Extensions Azure DevOps\xa0: un vaste marché d'extensions offre des fonctionnalités optimisées par l'IA pour divers aspects de votre pipeline. Ces extensions peuvent vous aider dans l'analyse de code, l'automatisation des tests, la gestion des versions et bien plus encore, facilitant ainsi l'intégration de l'IA dans vos flux de travail existants.</li>""}, {'', ""<li>Opsera : En exploitant l'IA pour automatiser et optimiser la distribution de logiciels, Opsera donne une longueur d'avance à DevOps. Il fournit des fonctionnalités telles que l'optimisation intelligente du pipeline, l'analyse prédictive et les tests automatisés, permettant aux équipes de rationaliser les flux de travail et d'obtenir des versions plus rapides.</li>""}, {'', ""<li>Harness : cette plateforme de livraison continue intègre l'IA pour automatiser la vérification du déploiement, optimiser les coûts du cloud et garantir la conformité. Elle utilise le ML pour analyser les modèles de déploiement et prédire les problèmes potentiels, permettant ainsi aux équipes de livrer des logiciels en toute confiance.</li>""}, {'', ""<li>GitHub Copilot : cet outil de saisie semi-automatique de code basé sur l'IA peut améliorer considérablement la qualité du code et la productivité des développeurs. Cependant, il est important de savoir que GitHub Copilot n'est actuellement pas intégré à Azure DevOps. Il utilise le ML pour suggérer des extraits de code, des lignes complètes et même générer des fonctions entières, ce qui permet aux développeurs de gagner un temps précieux et de réduire les erreurs.</li>""}, {'', '<h3>Études de cas</h3>'}, {'', '<p>Microsoft : en tant que créateur d’Azure DevOps, Microsoft est à l’avant-garde de l’intégration de l’IA dans ses pipelines. L’entreprise utilise Azure Machine Learning pour analyser de vastes quantités de données de télémétrie issues de ses processus de développement et de déploiement, ce qui lui permet de signaler les obstacles, d’estimer les échecs et de répartir les ressources. Cela a considérablement amélioré les fréquences de déploiement et la qualité globale des logiciels.</p>'}, {'', '<p>Adobe : Adobe a adopté l’IA pour améliorer ses pipelines DevOps pour les applications cloud créatives. L’entreprise utilise des modèles ML pour analyser les modifications de code et prédire les impacts potentiels sur les performances et la stabilité. Cela lui permet de résoudre les problèmes de manière proactive avant qu’ils n’affectent les utilisateurs, ce qui se traduit par une expérience plus fluide et plus fiable pour des millions de professionnels de la création dans le monde entier.</p>'}, {'', '<h3>Conclusion</h3>'}, {'', '<p>En conclusion, l’intégration de l’IA dans les pipelines Azure DevOps transforme le processus manuel de développement et de livraison de logiciels. En adoptant l’IA, les organisations peuvent atteindre des niveaux d’efficacité, de qualité et d’agilité sans précédent, ce qui leur permet d’innover et d’obtenir un avantage concurrentiel dans le monde en constante évolution du développement de logiciels.</p>'}]"
Valory dévoile un ingénieur logiciel IA pour la création de plusieurs types d'agents,"[{'', ""<p>Valory a lancé un agent d'intelligence artificielle (IA), baptisé Propel Genie, spécialement conçu pour agir comme un ingénieur logiciel permettant aux développeurs d'applications de créer des agents formés pour des descriptions spécifiques définies dans des spécifications en langage naturel.</p>""}, {'', ""<p>Développé à l'origine pour créer des agents pour le framework open source Olas pour les transactions de cryptomonnaie, Propel Genie présente aux développeurs une interface similaire à ChatGPT. Il traduit ensuite les descriptions en langage naturel en une feuille de route technique, connue sous le nom de spécification de machine à états finis (FSM), qui décrit la structure de l'architecture de l'agent. Cette feuille de route est ensuite utilisée pour générer l'agent décrit dans la spécification à l'aide des agents de planification de code, de génération de code et d'évaluation de code Genie.</p>""}, {'', ""<p>David Minarsch, PDG de Valory, a déclaré que Genie est essentiellement un super-agent qui utilise des modèles de langage étendus (LLM) pour convertir une spécification de conception écrite en langage naturel afin de créer le code qui pilote un agent d'IA autonome entièrement fonctionnel. Chaque agent invoque différents LLM qui pourraient mieux se prêter à une tâche qu'à une autre, a déclaré Minarsch.</p>""}, {'', ""<p>Genie permet également un prototypage rapide grâce à des flux de travail automatisés pour faciliter la création rapide d'agents autonomes que les développeurs peuvent créer, tester et affiner de manière itérative selon les besoins sur quelques jours, a-t-il ajouté.</p>""}, {'', ""<p>Disponible via la plateforme Propel que Valory met à disposition des développeurs d'applications cryptographiques, Olas est déjà utilisé pour permettre à plusieurs agents IA créés à l'aide de Propel Genie d'interagir les uns avec les autres. Propel Genie peut cependant également être appliqué plus largement pour permettre aux organisations de créer leurs propres agents personnalisés, a déclaré Minarsch.</p>""}, {'', '<p>À long terme, Valory s’efforce de permettre à chacun de créer son propre agent d’IA en réduisant la complexité. En attendant, cependant, Genie fonctionne essentiellement comme un ingénieur logiciel qui permet aux développeurs de différents niveaux de compétences de créer un agent d’IA basé sur une spécification en langage naturel.</p>'}, {'<h3>Les humains et les agents IA</h3>', ''}, {'', '<p>On ne sait pas encore exactement comment le rôle des ingénieurs logiciels évoluera à l’ère de l’IA, mais de nombreuses tâches manuelles sont de plus en plus automatisées. À l’avenir, la plupart des équipes DevOps seront constituées d’un mélange d’humains et d’agents IA spécialement formés pour gérer des tâches spécifiques. Le défi consistera alors à orchestrer les flux de travail au sein d’une équipe comprenant ces agents IA. À mesure que le temps et les efforts nécessaires à la création de nouveaux agents commencent à diminuer, il devient également possible d’ajouter ou de remplacer des agents IA selon les besoins dans un large éventail de processus métier numériques.</p>'}, {'', '<p>À court terme, la plupart des outils d’IA visent à permettre aux développeurs d’écrire davantage de code. L’accent n’est pas mis sur l’application de l’IA aux workflows que les équipes DevOps utilisent pour intégrer tout ce code dans une application exécutée dans un environnement de production. Cependant, avec l’avènement d’outils d’IA tels que Genie, l’art de ce qui pourrait bientôt être possible prend de l’ampleur.</p>'}, {'', '<p>La seule chose qui reste à voir maintenant est la mesure dans laquelle des agents tels que Genie pourraient un jour changer fondamentalement l’économie du développement de logiciels.</p>'}]"
L'avenir de l'observabilité : comment OpenTelemetry façonne les opérations informatiques à l'ère de l'IA,"[{'', '<p>L’environnement des applications informatiques d’aujourd’hui est complexe et évolue rapidement, notamment avec l’intégration de l’intelligence artificielle (IA). Les processus de développement modernes et les approches collaboratives telles que le code open source ont simplifié la création d’applications sophistiquées, permettant aux équipes de développement de construire rapidement des systèmes complexes. La prolifération des données, alimentée par l’IA, permet à ces applications de générer de vastes quantités d’informations, contribuant ainsi au développement d’applications avancées et impressionnantes. Cependant, à mesure que les applications deviennent plus complexes, leur gestion et leur surveillance deviennent de plus en plus difficiles. L’une des solutions vers lesquelles se tourne le secteur est OpenTelemetry (OTel), un framework open source conçu pour fournir une approche cohérente et complète de la collecte et de la compréhension des données.</p>'}, {'', '<p>Le DevOps et d’autres processus de développement modernes ont considérablement amélioré la façon dont nous créons des applications. Les technologies de conteneurisation telles que Docker et les plateformes d’orchestration telles que Kubernetes prennent en charge ces processus en permettant des environnements d’application cohérents, évolutifs et portables. Ces pratiques modernes améliorent l’efficacité, la fiabilité et l’évolutivité, permettant aux organisations de s’adapter rapidement aux demandes et de fournir des applications de haute qualité.</p>'}, {'', '<p>De plus, le code open source (un code auquel tout le monde peut accéder, utiliser ou modifier) \u200b\u200bjoue un rôle essentiel dans le développement d’applications modernes. En tant qu’approche collaborative, il a joué un rôle déterminant dans la création de solutions complexes, évolutives et innovantes qui auraient été impossibles autrement dans un cadre fermé. Le code open source a permis aux équipes DevOps de réaliser des prouesses étonnantes dans le monde du développement d’applications. La démocratisation d’un code source permet aux utilisateurs de partout dans le monde de partager et d’accéder à des logiciels utiles, mais comme le code est régi par ses utilisateurs, il crée également un environnement dans lequel les utilisateurs peuvent s’appuyer les uns sur les autres, ce qui rend l’avancement des applications hypothétiquement infini.</p>'}, {'', '<p>La possibilité d’utiliser du code open source et, par conséquent, de puiser dans différents référentiels open source a conduit à une explosion des progrès dans le développement d’applications. Cependant, des applications beaucoup plus avancées ont créé des environnements d’application de plus en plus complexes, ce qui rend plus difficile pour les équipes de surveiller leurs systèmes du point de vue des opérations informatiques. En outre, les systèmes pilotés par l’IA ont ajouté de nouveaux niveaux de complexité. Les flux de travail de l’IA impliquent un traitement en temps réel et des quantités de données sans précédent, ce qui nécessite des systèmes de surveillance robustes capables de tout gérer.</p>'}, {'', '<h3>OTel présente une solution prometteuse</h3>'}, {'', ""<p>S'appuyant sur la même croyance en l'open source, il existe une solution à ce défi : OTel. Tout comme le code open source a favorisé la création d'applications avancées et innovantes, OTel, une solution open source, existe pour mieux les gérer et les surveiller. OTel répond à ces problèmes en fournissant une approche standardisée de la collecte et de l'analyse des données de télémétrie, garantissant ainsi aux organisations la possibilité de surveiller et de gérer efficacement les performances de ces systèmes complexes et rapides pilotés par l'IA. Le cadre OTel prend en charge les besoins de surveillance et d'observabilité en temps réel des applications d'IA, aidant les équipes à maintenir la santé, les performances et la fiabilité de leurs systèmes face à des volumes de données massifs et à des tâches de traitement complexes.</p>""}, {'', '<p>OTel permet de gérer de grandes quantités de données générées par les systèmes d’IA et d’améliorer la capacité de l’IA à prédire et à détecter les problèmes. Les systèmes d’IA fonctionnent en traitant des données recueillies à partir de diverses sources, y compris des données de télémétrie gérées par OTel. Lorsque ces riches données de télémétrie sont introduites dans des plateformes d’analyse pilotées par l’IA, elles favorisent la maintenance prédictive, la détection des problèmes et la correction automatisée. En d’autres termes, en accédant facilement à des données de haute qualité et organisées, les systèmes d’IA peuvent apprendre au fil du temps ce qui est normal pour un environnement informatique particulier, puis ajuster et améliorer automatiquement ses réponses pour déboguer les problèmes et maintenir les performances du système. En conséquence, OTel contribue au processus d’aide aux systèmes pour identifier les problèmes de manière précoce et prendre des mesures sans nécessiter une intervention humaine constante, améliorant ainsi l’efficacité et l’efficience globales.</p>'}, {'', '<p>Imaginez un système qui non seulement surveille mais prédit également les anomalies, diagnostique les causes profondes de manière autonome et s’adapte aux conditions changeantes en temps réel – le tout alimenté par des algorithmes d’IA formés à partir de vastes quantités de données de télémétrie. Avec OTel, les systèmes pilotés par l’IA peuvent rapidement révéler et analyser des milliards d’informations, contribuant non seulement à un niveau d’observabilité accru, mais également à un environnement dans lequel les problèmes peuvent être résolus avant même de devenir un problème.</p>'}, {'', ""<p>Nous vivons dans un monde où les processus de développement d'applications modernes ont permis aux équipes de créer des applications incroyablement avancées, utiles et même transformatrices. À mesure que les applications deviennent de plus en plus complexes, la demande de solutions d'observabilité sophistiquées augmente en parallèle. L'avenir de l'observabilité dépend de l'avancement des technologies telles que les logiciels open source et l'IA. Si les logiciels open source ont considérablement amélioré les processus de développement d'applications, ils ont créé un environnement dans lequel les applications sont devenues extrêmement difficiles à surveiller.</p>""}, {'', '<p>La combinaison de solutions d’observabilité modernes basées sur l’IA avec OTel présente une solution prometteuse. En standardisant la collecte et l’analyse des données de télémétrie, OTel garantit aux organisations une observabilité robuste, leur permettant d’exploiter tout le potentiel de l’IA et de continuer à développer des applications avancées tout en étant capables de les gérer. Grâce à cela, grâce à une observabilité améliorée, OTel va faire pour les opérations informatiques ce que les logiciels open source ont fait à l’origine pour le développement d’applications. Il va le transformer.</p>'}]"
Pourquoi les modèles d'IA génériques ne sont pas à la hauteur de l'analyse des causes profondes,"[{'', '<p>Appelez cela la ruée vers l’or de l’IA générative… ou la ruée vers les terres. Des secteurs de la santé à la banque et au-delà se sont précipités pour intégrer l’IA générative (GenAI). Et le domaine de l’observabilité ne fait pas exception. On parle beaucoup de la façon dont les grands modèles de langage (LLM) sont sur le point de transformer le marché de l’observabilité. Le statu quo est loin d’être idéal. Avec des alarmes, des avertissements et un mélange déroutant de signaux provenant des logiciels de surveillance (qu’ils soient des fournisseurs commerciaux ou des piles open source), les ingénieurs en fiabilité des sites sont débordés et souffrent d’une fatigue des alertes.</p>'}, {'', '<p>Si vous gérez un système de production moderne et distribué, vous recherchez peut-être déjà des moyens de simplifier le travail de votre équipe grâce aux LLM. Le diagnostic des problèmes et les tâches qui en découlent vont probablement exiger plus de temps et d’énergie de la part de vos SRE et de vos professionnels DevOps ces dernières années, en raison du réseau de plus en plus complexe d’interdépendances des systèmes logiciels et de l’infrastructure.</p>'}, {'', '<p>Les LLM joueront sans aucun doute un rôle dans ce domaine. Mais en tant qu’industrie, nous sommes encore en train d’apprendre où ils s’intègrent le mieux.</p>'}, {'', '<p>Ce blog explore spécifiquement les raisons pour lesquelles les modèles ne sont pas adaptés à un élément essentiel du dépannage de la production : l’analyse des causes profondes. Il suggère ensuite comment les LLM pourraient être intégrés pour améliorer votre stratégie globale d’observabilité.</p>'}, {'', '<h3>Capture du contexte — Texte ou données structurées</h3>'}, {'', ""<p>Les titulaires d'un LLM excellent dans l'analyse de textes non structurés. Ils apprennent à partir de vastes quantités de données de formation basées sur du texte pour identifier des modèles et faire des prédictions.</p>""}, {'', '<p>Ainsi, en théorie, si vous pouviez fournir à un LLM les bonnes entrées de texte, il serait capable de synthétiser une grande quantité d’informations sur votre environnement pour créer des informations de haute qualité. (Un exemple\xa0: ce microservice est en panne, et c’est probablement à cause du problème X).</p>'}, {'', '<p>Mais réfléchissons maintenant : à quoi ressembleraient les bonnes entrées de texte pour générer ce type d’informations ? En gros, elles relèveraient de la catégorie « contexte » :</p>'}, {'', ""<ol>Heuristique relationnelle\xa0: un modèle des connexions entre les différentes couches de votre environnement (par exemple, un microservice particulier appelle un ensemble spécifique d'API)Connaissances tribales autour des relations de cause à effet antérieures (par exemple, l'activité des utilisateurs augmente toujours à certaines périodes de l'année).</ol>""}, {'', ""<li>Heuristique relationnelle\xa0: un modèle des connexions entre les différentes couches de votre environnement (par exemple, un microservice particulier appelle un ensemble spécifique d'API)</li>""}, {'', ""<li>Connaissances tribales sur les relations de cause à effet antérieures (par exemple, l'activité des utilisateurs connaît toujours des pics à certaines périodes de l'année).</li>""}, {'', ""<p>Ce type de contexte existe naturellement dans une représentation graphique de votre environnement. Un graphique (avec des nœuds et des arêtes) est un moyen naturel et utile de représenter les relations structurées entre différentes couches d'applications, d'API, de réseau et d'infrastructure. Il met en évidence les dépendances et capture les évolutions de la structure de votre environnement au fil du temps.</p>""}, {'', '<p>Cependant, le processus de conversion de données structurées et chronologiques en entrées de texte significatives pour un LLM est loin d’être simple. Il s’agit du principal obstacle à l’utilisation des LLM : transformer une carte de votre environnement en données de formation spécifiques et pertinentes que GenAI peut utiliser pour générer des informations au-delà du générique (c’est-à-dire, qu’obtiendriez-vous si vous recherchez sur Google « pourquoi un microservice pourrait-il échouer ? »).</p>'}, {'', '<p>Par exemple, imaginons que vous avez subi une panne. (Pour simplifier, supposons ce scénario : des pods Web surchargés entraînant des problèmes de service sur la page de paiement.) La cause fondamentale non évidente ici s’est avérée être une dégradation du service de cache. Mais pour qu’un LLM reproduise cette idée, vous devez lui fournir des informations exhaustives, explicites et à jour sur la relation entre vos pods Web et le service de cache, ainsi que sur la configuration réseau sous-jacente qui régit le service de cache.</p>'}, {'', ""<p>Pour les environnements complexes avec un degré élevé d'interdépendance entre les API, les applications, le réseau et les couches d'infrastructure, il n'est pas pratique de convertir en continu toutes ces relations hautement structurées en saisie de texte de manière à ce qu'un LLM puisse analyser les causes profondes. (C'est précisément le domaine dans lequel l'apprentissage automatique des graphes excelle.)</p>""}, {'', '<h3>Analyse des flux de données continus en temps réel</h3>'}, {'', ""<p>Les systèmes de production génèrent des flux de données chronologiques structurés et continus qui nécessitent une visibilité en temps réel lors de l'analyse des causes profondes. De par leur conception, les modèles tels que GPT réagissent aux requêtes et ne peuvent pas traiter les données dynamiques en temps réel du réseau, du système et des applications. De plus, à mesure que vos systèmes évoluent, les LLM seront toujours en retard, ne connaissant que la version de votre environnement sur laquelle ils ont été formés en dernier.</p>""}, {'', ""<p>Pour une analyse efficace des causes profondes, les SRE et les professionnels DevOps ont besoin de visualisations de données claires et intuitives telles que les topologies d'environnement et les cartes d'impact, ainsi qu'un historique des changements et des déploiements récents. Les modèles GenAI s'interfacent principalement via le chat, ce qui rend difficile la visualisation de leurs informations.</p>""}, {'', ""<h3>Lacunes d'explicabilité à grande échelle</h3>""}, {'', '<p>L’explicabilité est importante dans tous les domaines, mais les enjeux sont particulièrement importants dans l’analyse des causes profondes.</p>'}, {'', '<p>Si un modèle ne peut pas expliquer correctement le raisonnement qui le conduit à suggérer une stratégie d’atténuation particulière (comme la mise à niveau d’une base de données spécifique qui provoque un goulot d’étranglement du service de paiement), les SRE ne peuvent pas agir en toute confiance. Et ils peuvent être amenés à effectuer des actions malavisées qui aggravent les problèmes existants ou en créent de nouveaux.</p>'}, {'', '<p>Le fait que les LLM effectuent des opérations de type « boîte noire » n’est pas un problème en soi. (Après tout, tout réseau neuronal implique des défis d’explicabilité en raison du nombre de paramètres et du volume de données d’entraînement.)</p>'}, {'', '<p>Cependant, un défi majeur en matière d’explicabilité découle du fait que les données sous-jacentes (un graphique structuré en séries chronologiques de votre environnement) doivent être transformées en un format textuel non structuré que les LLM peuvent analyser.</p>'}, {'', ""<p>Alors qu'une représentation graphique de votre environnement rend visible la relation entre les couches, la saisie de texte d'un LLM rend presque impossible le raisonnement sur la sortie. Cela peut rendre particulièrement difficile le débogage des défaillances potentielles du modèle\xa0:</p>""}, {'', '<li>Le modèle analyse-t-il la représentation la plus récente de votre environnement ?</li>'}, {'', '<li>Le contexte fourni était-il insuffisant pour générer une recommandation précise ?</li>'}, {'', '<li>Pourquoi une cause fondamentale particulière a-t-elle été identifiée à plusieurs «\xa0sauts de la chaîne\xa0»\xa0?</li>'}, {'', '<p>En bref, la couche supplémentaire requise pour rendre les LLM utiles à l’analyse des causes profondes (transformer et annoter les entrées graphiques en données de formation textuelles) introduit des défis d’explicabilité qui ne font que s’aggraver à grande échelle.</p>'}, {'', '<h3>Comment intégrer les LLM – de la bonne manière</h3>'}, {'', '<p>Nous avons mis en évidence certaines des lacunes des LLM en matière d’investigation et d’analyse des causes profondes des problèmes de production.</p>'}, {'', '<p>Mais ce serait une erreur de négliger complètement GenAI pour l’observabilité. Les LLM ont un rôle clair et puissant à jouer dans le processus de dépannage. Plus précisément, ils peuvent compléter d’autres formes d’IA utilisées dans l’analyse des causes profondes (comme l’apprentissage automatique graphique) en fournissant une interface utilisateur intuitive, flexible et partagée pour l’investigation.</p>'}, {'', '<p>L’utilisation de LLM comme « couche d’interface » lors d’une enquête sur un incident (un chatbot qui permet un dépannage véritablement conversationnel) offre divers avantages :</p>'}, {'', ""<li>Il rend les informations complexes accessibles à différents membres interfonctionnels de l'équipe (SRE, DevOps, développeurs) qui peuvent avoir plusieurs niveaux de familiarité avec l'environnement technique</li>""}, {'', ""<li>Il accélère le processus de dépannage en permettant de poser des questions et d'y répondre de manière itérative, à la manière dont les SRE enquêtent sur les problèmes dans le monde réel.</li>""}, {'', '<li>Il favorise l’alignement en fournissant un langage commun pour les problèmes et les causes profondes au sein de l’équipe.</li>'}, {'', '<p>À quoi cela ressemble-t-il dans la pratique ? Imaginez qu’en cas de dégradation du service (comme une panne sur la page de paiement d’un détaillant en ligne), différentes équipes aient accès à une interface de chat pour poser des questions et explorer des hypothèses – pas de réponses génériques, mais adaptées à leurs environnements spécifiques. Par exemple, « Pourquoi les pods Web peuvent-ils être surchargés ? Quelles sont les causes profondes potentielles de la surcharge de notre service de cache ? »</p>'}, {'', '<p>En bref, une interface conversationnelle pour le dépannage peut rendre les informations sur les causes profondes compréhensibles et exploitables pour les équipes humaines chargées de l’enquête et de la correction.</p>'}, {'', '<h3>Et après ? L’avenir de l’observabilité intelligente</h3>'}, {'', '<p>Il ne fait aucun doute que les modèles d’IA génériques tels que GPT auront un impact révolutionnaire sur l’observabilité. Comme nous l’avons vu, les LLM peuvent considérablement accélérer le processus de dépannage en traduisant les informations sur les causes profondes pour les SRE humains.</p>'}, {'', '<p>Mais une observabilité véritablement intelligente ne se résume pas à l’ajout d’un chatbot sur une plateforme existante. Elle nécessite un système complet conçu par des experts du domaine pour collecter, structurer et analyser les données du point de vue des utilisateurs finaux et de l’impact sur l’entreprise. Les modèles d’IA génériques ne sont pas à la hauteur de la tâche spécialisée et exigeante d’analyse des causes profondes pour les systèmes de production distribués.</p>'}, {'', '<p>Chez Senser, nous avons construit ce système à partir de zéro depuis le premier jour.</p>'}, {'', '<h3>Comment Senser aide</h3>'}, {'', '<p>La plateforme AIOps sans instrument de Senser utilise la collecte de données basée sur eBPF pour fournir une visibilité immédiate et à faible surcharge sur votre environnement de production.</p>'}, {'', '<p>Senser crée automatiquement une topologie de votre environnement, en mappant de manière dynamique les dépendances entre les couches (application, API, réseau, infrastructure) pour fournir un contexte critique pour le dépannage. Notre approche basée sur le ML graphique vous aide à identifier rapidement la cause première des problèmes de service, même dans les environnements les plus complexes.</p>'}, {'', ""<p>En combinant la puissance des LLM (pour le dépannage conversationnel) avec le ML spécialement conçu pour l'analyse des causes profondes, vous offrez à votre équipe le meilleur des deux mondes : les bons outils pour réduire considérablement le temps moyen de détection (MTTD) et le temps moyen de correction (MTTR).</p>""}]"
Naviguer au-delà du sommet : remodeler la transformation informatique et l'évolution organisationnelle pour une croissance durable,"[{'', '<p>Au cours de mes quatre décennies passées à naviguer dans le paysage en constante évolution des technologies de l’information (TI), j’ai été témoin de profonds changements dans les technologies et les structures organisationnelles. Bien que le mouvement DevOps ait sans aucun doute fait des progrès remarquables, il est impératif de reconnaître qu’il a peut-être atteint un palier. Cet article vise à approfondir non seulement les défis et les succès de DevOps, mais également les considérations plus larges essentielles à une croissance durable dans un écosystème informatique en évolution rapide.</p>'}, {""<h3>DevOps et son point d'inflexion</h3>"", ''}, {'', '<p>L’essor de DevOps a inauguré une ère de collaboration, d’accélération des livraisons et d’amélioration de la qualité. Par exemple, la mise en œuvre de pipelines d’intégration et de déploiement continus (CI/CD) a considérablement simplifié les processus de développement et de publication. Cependant, après avoir surmonté les hauts et les bas de la transformation informatique, il est clair que DevOps a atteint un point d’inflexion. Les organisations qui ont initialement connu une augmentation de leur efficacité sont désormais confrontées à la nécessité d’une réévaluation plus profonde de leurs structures.</p>'}, {'<h3>Le rôle crucial de la réorganisation à long terme</h3>', ''}, {'', '<p>Mon expérience dans le domaine informatique souligne l’importance de la dynamique organisationnelle. Prenons l’exemple d’une grande entreprise qui a adopté DevOps. Le passage de structures hiérarchiques traditionnelles à des équipes interfonctionnelles axées sur les produits est illustré par des entreprises telles que Spotify. Ce changement optimise non seulement le flux et la charge cognitive, mais encourage également l’innovation et l’adaptabilité.</p>'}, {'', '<h3>Gérer les défis à court et à long terme</h3>'}, {'', '<p>L’attrait des gains à court terme éclipse souvent la nécessité d’une évolution soutenue. Par exemple, une entreprise qui adopte les pratiques DevOps peut bénéficier d’avantages immédiats tels qu’une mise sur le marché plus rapide. Cependant, sans stratégie plus large, ces gains peuvent s’avérer de courte durée. Prenons le cas d’Amazon, qui, au-delà de son succès en matière de commerce électronique, investit en permanence dans l’adaptabilité à long terme grâce à des innovations comme AWS.</p>'}, {'', '<h3>Dévoiler le problème de l’écart salarial</h3>'}, {'', '<p>Au-delà du secteur informatique, une tendance inquiétante se dessine : l’écart grandissant entre la productivité des entreprises et la rémunération des employés. Les mesures de réduction des coûts à court terme peuvent avoir des répercussions néfastes. Le secteur technologique en est un bon exemple : certaines entreprises, malgré une productivité élevée, ont été critiquées pour leurs structures de rémunération inégales. Pour remédier à cet écart, il faut s’engager en faveur d’une rémunération équitable et de pratiques commerciales responsables, préservant ainsi les fondements d’une économie robuste.</p>'}, {'', ""<h3>Approche centrée sur l'humain</h3>""}, {'', '<p>Dans le monde de l’informatique en constante évolution, une approche centrée sur l’humain est primordiale. Prenons l’exemple de Google, où l’accent mis sur une culture de travail positive, des projets innovants tels que « 20 % du temps » et le bien-être des employés contribuent non seulement au succès technique, mais aussi à la croissance globale.</p>'}, {'', '<h3>Apprentissage continu et développement des compétences</h3>'}, {'', '<p>L’apprentissage continu est la clé de la réussite dans le domaine informatique. L’engagement d’IBM en matière de développement des compétences, illustré par des initiatives telles que IBM Skills Gateway, garantit que les employés restent pertinents dans un paysage technologique en constante évolution. Cela permet non seulement de maintenir l’engagement des employés, mais aussi de favoriser une culture d’innovation et d’adaptabilité, essentielle à une croissance durable.</p>'}, {'', '<h3>Les indicateurs de réussite</h3>'}, {'', '<p>Il est essentiel d’établir des indicateurs pertinents pour mesurer le succès de la transformation informatique. Par exemple, Etsy, au-delà des indicateurs de performance traditionnels, intègre des indicateurs tels que la satisfaction des employés et l’efficacité de la collaboration. Cette approche holistique offre une vue plus complète de l’efficacité de la transformation.</p>'}, {'', '<h3>Intégration de la cybersécurité</h3>'}, {'', '<p>Intégrez de manière transparente les discussions sur la cybersécurité dans les discours de transformation informatique. Les violations de données d’Equifax et plus récentes nous rappellent brutalement les répercussions d’une négligence en matière de sécurité. L’intégration de mesures de sécurité dans les pratiques DevOps est essentielle pour protéger les actifs organisationnels et assurer la pérennité des initiatives informatiques.</p>'}, {'', ""<h3>Collaboration au-delà de l'informatique</h3>""}, {'', '<p>Encouragez la collaboration non seulement au sein des équipes informatiques, mais aussi entre les différents services. Le succès de Salesforce, par exemple, ne repose pas seulement sur son logiciel CRM, mais aussi sur son approche interfonctionnelle, qui garantit l’alignement avec les objectifs organisationnels globaux et favorise l’innovation au-delà des frontières traditionnelles.</p>'}, {'', '<h3>Considérations éthiques</h3>'}, {'', '<p>Mettre en avant les considérations éthiques dans la transformation technologique et informatique. L’engagement de Google en faveur d’une IA éthique, démontré par la création d’un conseil consultatif externe, illustre une approche proactive pour répondre aux implications sociétales et éthiques associées aux avancées technologiques.</p>'}, {'<h3>Engagement communautaire</h3>', ''}, {'', '<p>Encouragez l’engagement de la communauté et la participation aux forums et événements du secteur. L’engagement de Microsoft dans les communautés et événements open source tels que Build contribue non seulement à la base de connaissances collective, mais favorise également un sentiment de camaraderie entre les professionnels confrontés à des défis similaires.</p>'}, {'', '<h3>Leadership adaptatif</h3>'}, {'', '<p>Soulignez l’importance d’un leadership adaptatif dans la gestion des transformations informatiques. Le leadership de Satya Nadella chez Microsoft témoigne de l’importance d’un leadership adaptatif. Son approche de la gestion du changement, de l’inspiration des équipes et de la prise de décisions fondées sur les données a joué un rôle essentiel dans la renaissance de Microsoft en tant que leader technologique.</p>'}, {'', ""<h3>Défis et facteurs d'hésitation à prendre en compte</h3>""}, {'', '<p>Si de nombreuses entreprises reconnaissent l’importance d’adopter une approche globale de la transformation informatique, divers facteurs peuvent freiner les progrès. Voici quelques raisons expliquant l’hésitation ou la résistance à s’engager dans cette direction :</p>'}, {'', '<li>Résistance au changement</li>'}, {'', '<p>Approche : Stratégie de gestion du changement mettant l’accent sur une communication claire, l’engagement des collaborateurs et mettant en avant les avantages de la transformation.</p>'}, {'', '<li>Concentration à court terme et pression pour des résultats immédiats</li>'}, {'', '<p>Approche : Communiquer la valeur à long terme et le retour sur investissement (ROI) de la transformation pour aider les parties prenantes à aligner sur une vision plus durable.</p>'}, {'', '<li>Manque de sensibilisation ou de compréhension</li>'}, {'', '<p>Approche : Les programmes d’éducation et de sensibilisation, ainsi que la présentation d’études de cas réussies, peuvent combler ce manque de connaissances.</p>'}, {'', '<li>Systèmes hérités et dette technique</li>'}, {'', '<p>Approche : Une approche progressive, commençant par les systèmes critiques et tirant parti de technologies telles que la conteneurisation et les microservices, peut aider à gérer la transition sans submerger l’organisation. Ne l’ignorez pas.</p>'}, {'', ""<li>Peur d'une perturbation des opérations</li>""}, {'', '<p>Approche : La mise en œuvre progressive des changements, pendant des périodes moins critiques et la mise en place de plans d’urgence solides peuvent atténuer ces inquiétudes.</p>'}, {'', ""<li>L'inertie dans le leadership</li>""}, {'', '<p>Approche : Promouvoir des ateliers de leadership adaptatif et présenter des exemples de réussite d’organisations qui ont réussi à transformer leur style de leadership peut encourager l’ouverture au changement.</p>'}, {'', '<li>Contraintes budgétaires</li>'}, {'', '<p>Approche : Démontrer la rentabilité de la transformation sur le long terme et explorer les partenariats peuvent aider à surmonter les contraintes budgétaires.</p>'}, {'', '<li>La cupidité et la vision à court terme des cadres supérieurs</li>'}, {'', '<p>Approche : Aligner les incitations des dirigeants sur la santé organisationnelle à long terme, plutôt que sur les gains à court terme, peut contribuer à atténuer ce problème.</p>'}, {'', '<li>Inertie culturelle</li>'}, {'', '<p>Approche : Favoriser une culture qui encourage l’innovation, la collaboration et l’adaptabilité est essentiel pour une transformation réussie.</p>'}, {'', '<li>Incertitude et aversion au risque</li>'}, {'', '<p>Approche : Aborder l’incertitude associée aux initiatives de transformation et aux risques potentiels en soulignant l’importance des mesures audacieuses, en tirant les leçons des échecs et en ne privilégiant pas le statu quo.</p>'}, {'', '<p>En résumé, même si de nombreuses entreprises comprennent les avantages d’une transformation informatique globale, divers facteurs internes et externes peuvent contribuer à l’hésitation ou à la résistance. Surmonter ces défis nécessite souvent une combinaison de leadership stratégique, de communication efficace et d’engagement envers la santé organisationnelle à long terme.</p>'}, {'', '<p>Les dirigeants doivent s’attaquer activement à la résistance au changement, aligner les actions à court terme sur la vision à long terme et investir dans les ressources et les compétences nécessaires pour une transformation réussie. En outre, il est essentiel de favoriser une culture qui favorise l’innovation, la collaboration et l’adaptabilité pour s’orienter dans le paysage dynamique de l’informatique et assurer une croissance durable. En s’attaquant à ces facteurs, les organisations peuvent se positionner pour non seulement survivre, mais aussi prospérer à l’ère de la transformation numérique.</p>'}]"
De l'évolutivité à la vitesse : l'IA générative a donné une impulsion aux tests,"[{'', '<p>En ingénierie de la qualité, l’IA générative (GenAI) est devenue une force transformatrice, bouleversant fondamentalement les paradigmes de test classiques. Traditionnellement, les modèles d’IA utilisaient des données existantes pour la classification et la prédiction et étaient souvent appliqués à la priorisation et à la consolidation des tests. Cependant, l’intégration rapide de l’IA générative au cours des dernières années a déplacé la base vers de nouvelles solutions de test – le prochain niveau dans la course à l’aptitude de l’IA.</p>'}, {'', '<h3>GenAI a modifié les tests pour améliorer activement la qualité</h3>'}, {'', '<p>Par le passé, la qualité des tests était une préoccupation majeure, nécessitant une intégration précoce de l’assurance qualité dans le cycle de vie du développement. Désormais, avec GenAI, l’accent est passé de la simple assurance à l’ingénierie active de la qualité. La principale distinction réside dans l’approche : l’IA classique implique une intervention humaine et des processus manuels, tandis que GenAI automatise et innove en matière de méthodologies de test.</p>'}, {'', '<p>Envisagez de traiter la qualité des exigences dès le début du cycle de vie du développement logiciel. À l’aide de l’IA classique, un analyste métier peut définir des exigences pour couvrir diverses interprétations, ce qui peut entraîner certaines ambiguïtés et des échecs potentiels. Avec GenAI, ce type d’ambiguïté est non seulement rapidement identifié, mais également corrigé.</p>'}, {'', '<p>Prenons l’exemple d’un détaillant qui teste différents types de chaussures. Si vous demandez alors : « Qu’entend le détaillant par « chaussures différentes » ? » Il peut s’agir de différents types de chaussures, comme des baskets, des talons ou des tailles, ou de détails comme le confort, la largeur, etc. En fait, cela peut signifier un certain nombre de choses différentes. Les modèles GenAI peuvent automatiquement comprendre le contexte du client et du secteur, et remédier à l’exigence pour éliminer toute ambiguïté.</p>'}, {'', '<p>Un autre exemple est la consolidation de cas de test similaires. Par le passé, les modèles d’IA traditionnels pouvaient identifier les cas de test en double ou similaires, mais une intervention humaine était nécessaire pour supprimer ou fusionner ces cas de test en double ou similaires. GenAI va encore plus loin en automatisant le processus de consolidation, réduisant ainsi le besoin d’intervention manuelle et garantissant une plus grande efficacité des tests.</p>'}, {'', '<h3>Création automatique de cas de test</h3>'}, {'', ""<p>En repoussant les limites au-delà de la prédiction ou de l'analyse, GenAI s'étend désormais à la création automatique d'artefacts de test tels que des scénarios de test, des cas de test, des fichiers de fonctionnalités et même des scripts d'automatisation. Alors que le monde des développeurs était assez avancé avec des environnements de développement intégrés intelligents (IDE) et des générateurs de code, le monde de l'assurance qualité était encore limité à l'automatisation avec peu d'IA dans les tests.</p>""}, {'', '<p>Avec l’introduction de GenAI en 2023, l’adoption de l’IA dans les tests a connu une croissance exponentielle. GenAI a démocratisé l’IA dans les tests. Si quelqu’un intègre une exigence validée, non ambiguë et de haute qualité dans les modèles GenAI actuels, des scénarios de test ainsi que des cas de test bien définis, des fichiers de fonctionnalités et des scripts Selenium automatisés peuvent être générés d’un simple clic. Cette transformation a considérablement accéléré les processus de test, transformant ce qui prenait auparavant des semaines en une tâche automatisée rapide réalisée en quelques jours.</p>'}, {'<h3>Il ne s’agit pas de remplacer les testeurs humains</h3>', ''}, {'', '<p>Il faut le répéter https://devops.com/smartbear-adds-more-generative-ai-testing-tools-to-platform/ — l’utilisation de GenAI ne vise pas à remplacer la main-d’œuvre humaine, mais à améliorer nos capacités. La pénurie de testeurs d’automatisation expérimentés entraîne une perte de revenus pour l’entreprise. Cependant, avec GenAI, les ingénieurs juniors peuvent désormais exploiter la puissance de l’automatisation activée par l’IA de génération, en effectuant des tâches avec les connaissances intégrées d’un architecte chevronné.</p>'}, {'', '<p>Les prouesses de Gen AI ne sont pas arbitraires ; elle a appris à partir de milliards de points de données. En combinant les connaissances traditionnelles aux capacités de l’IA, de nouvelles solutions apportent évolutivité et rapidité aux tests. Cette transformation n’est pas seulement une amélioration ; c’est une révolution totale qui ouvre de nouvelles possibilités en matière de tests. Au-delà de la génération automatisée de cas de test, GenAI a étendu son impact à la création de données synthétiques, apportant une nouvelle dimension à l’ensemble du processus de test.</p>'}, {'', ""<h3>Il est temps pour les testeurs d'adopter GenAI</h3>""}, {'', '<p>De la résolution des ambiguïtés à l’automatisation de la création de cas de test, GenAI a fait entrer les tests dans une nouvelle ère qui s’accélère. Les économies de temps et d’argent, ainsi que l’efficacité accrue en font une force transformatrice dans le paysage des tests. Bien que les implications futures de GenAI soient vastes et quelque peu inconnues, nous comprenons comment elle a non seulement élevé les tests, mais aussi ouvert des perspectives qui semblaient autrefois impossibles. Le lien entre l’expertise humaine et les capacités de GenAI remodèle le paysage des tests, promettant un avenir dans lequel les tests ne sont pas seulement une question d’assurance, mais aussi de génération de qualité avec rapidité et à grande échelle.</p>'}]"
SRE à l'ère de l'IA,"[{'', ""<p>L'ingénierie de fiabilité des sites (SRE) est un concept introduit par Google en 2004 et adopté depuis par plusieurs grandes sociétés de logiciels. Dans sa forme la plus pure, l'ingénierie de fiabilité des sites (SRE) est ce que vous obtenez lorsque vous traitez les opérations comme s'il s'agissait d'un problème logiciel.</p>""}, {'', '<p>Les principaux rapports du secteur soulignent la valeur stratégique que l’ingénierie de fiabilité des logiciels offre à la communauté axée sur les logiciels. Les principaux points à retenir de la sixième édition du rapport SRE 2024 reflètent l’importance du rôle fondamental de l’ingénierie de fiabilité des logiciels, en particulier l’opérationnalisation à grande échelle des systèmes logiciels distribués natifs du cloud. Une autre évolution importante est l’introduction de la cinquième mesure DORA en 2021 : la fiabilité, qui a clairement souligné l’importance de la fiabilité et des pratiques SRE.</p>'}, {'', ""<p>Avec l'intégration harmonieuse des logiciels dans tous les domaines de la vie, nous devons intégrer étroitement les pratiques de fiabilité de manière pragmatique. La performance opérationnelle de notre écosystème logiciel dynamique et complexe est vitale, et les pratiques d'ingénierie de fiabilité des sites viennent à notre secours.</p>""}, {'', '<p>Avec la prochaine vague d’intégration et de convergence plus complexes des logiciels dans des domaines plus disruptifs, tels que la science des matériaux, les produits pharmaceutiques, les sciences de la santé, les forces de sécurité et les technologies spatiales, il est évident que nous ne pouvons pas négliger les indicateurs de performance opérationnelle et l’assurance logicielle. Faire évoluer les pratiques SRE côte à côte, créer des actifs innovants pour nos SRE et trouver de nouvelles façons d’exploiter un écosystème centré sur les logiciels de manière fiable sont des priorités pour les dirigeants.</p>'}, {'', ""<p>De plus, des transformations importantes se produisent dans le processus de développement logiciel. À commencer par l'essor exponentiel de l'open source, qui oblige les SRE à repenser et à s'ouvrir à de nouvelles formes de collaboration pour répondre aux attentes en matière de fiabilité. La récupération après des incidents opérationnels ou même des mises à jour ou des corrections nécessitent de nouvelles connaissances, de nouveaux partenaires et de nouvelles façons d'opérationnaliser la fiabilité.</p>""}, {'', ""<p>Le prochain défi immédiat est le déploiement d'applications logicielles dans un environnement hybride, cloud et sur site. Les nouveaux modèles de déploiement posent un nouvel ensemble de défis en matière de fiabilité et introduisent de nouvelles formes de risques à gérer pendant l'exécution. Certains domaines de référence qui peuvent être affectés sont la stratégie de basculement, le réglage des performances et la reprise après sinistre et la restauration. Les pratiques SRE telles que l'observabilité jouent un rôle crucial dans cette évolution.</p>""}, {'', '<p>Une autre rupture majeure est celle de l’IA générative, des assistants de codage et de l’adoption générale de l’IA à grande échelle. Enfin, l’introduction de nouvelles réglementations sur les données, une chaîne d’approvisionnement sécurisée et la demande croissante de logiciels durables et écologiques contribuent à l’évolution de l’ingénierie de la fiabilité des données.</p>'}, {'', '<p>Nous avons discuté des principaux facteurs qui expliquent pourquoi les pratiques SRE évolueront au-delà du statu quo. Explorons plus en détail la manière dont les SRE se préparent à répondre à l’expérience utilisateur de nouvelle génération et à la demande des clients avec une suite de nouvelles fonctionnalités. Dans la section suivante, nous explorerons plus en détail les opérations logicielles pilotées par SRE à grande échelle et associerons les principales tendances à l’évolution des pratiques.</p>'}, {'', ""<h3>Pratique SRE de premier plan à l'ère de l'IA</h3>""}, {'', '<p>Les pratiques SRE sont essentielles pour créer un écosystème logiciel résilient. Les organisations commenceront à voir des résultats tangibles une fois que le SRE se concentrera sur l’intégration de nouvelles technologies et méthodes et sur la collaboration avec de nouveaux partenaires de l’écosystème. Plus loin dans cet article, nous explorerons l’approche de la réalisation de valeur de l’injection de l’IA dans l’évolution du SRE.</p>'}, {'', ""<h3>S'attaquer à l'Open Source — SRE en action avec des ressources alimentées par l'IA</h3>""}, {'', '<p>Examinons en détail les domaines clés décrits ci-dessus, en commençant par l’open source. L’un des principaux défis pour les SRE est la maintenance d’exécution de l’open source. On estime que 96 % des bases de code contiennent des éléments open source qui peuvent entraîner des frais opérationnels si les organisations ne sont pas suffisamment préparées. L’un des exemples est Log4j, où plusieurs organisations ont été affectées par la dépendance transitive, et il a été signalé que des centaines d’heures ont été gaspillées dans le processus de gestion de cette dépendance.</p>'}, {'', ""<p>Avec l'essor de l'intégration open source dans les applications grand public, les SRE sont chargés de surveiller et de gérer les vulnérabilités open source en temps réel, consciemment ou non. À mesure que la complexité de l'écosystème augmente, il devient difficile d'y parvenir sans une approche pragmatique. Il faut commencer par s'assurer que la nomenclature logicielle (SBOM) est en place et intégrer intelligemment la SBOM dans l'environnement d'exécution pour comprendre et réagir rapidement aux incidents, utiliser des outils pour détecter rapidement ces vulnérabilités et y réagir sans créer de travail supplémentaire.</p>""}, {'', ""<p>Une autre approche possible pour les SRE consiste à développer de manière proactive une collaboration accrue avec le bureau du programme open source (OSPO) pour créer des ressources SRE telles que des manuels, des listes de contrôle, des outils et leur intégration avec des opérations telles que la gestion de l'inventaire SBOM et SW, y compris des bibliothèques open source pour une meilleure performance opérationnelle. L'OSPO, ainsi que le SRE, peuvent déclencher des audits assistés par l'IA de temps à autre pour suivre les risques opérationnels et les stratégies d'atténuation.</p>""}, {'', '<h3>SRE dévoile le potentiel de l’IA pour la fiabilité de nouvelle génération</h3>'}, {'', '<p>Bientôt, les SRE devront gérer du code écrit par machine et généré par l’IA. Cela pose des problèmes de sécurité et de conformité à une échelle différente. Les SRE doivent être prêts à faire face à de nouveaux types de vulnérabilités introduites par ces assistants de codage. Un autre domaine à prendre en compte est que les acteurs malveillants peuvent améliorer leurs capacités et injecter du code malveillant en exploitant les assistants de codage. Les SRE continuent de faire évoluer leurs connaissances et leurs ressources avec de nouveaux outils qui peuvent se défendre contre ces menaces introduites par les assistants d’IA.</p>'}, {'', ""<p>Pour aller plus loin dans la technologie de l'IA générative, la plupart des entreprises utilisent des modèles LLM open source en raison de contraintes de coût. L'utilisation de LLM open source nécessite une personnalisation pour s'adapter aux flux de travail et l'intégrer à des données propriétaires pour en améliorer la valeur. Les SRE peuvent s'intégrer à la communauté des data scientists pour fournir un retour d'information précoce. Ce retour d'information contribuerait à réduire l'hallucination, qui se produit lorsque les modèles d'IA inventent des choses. Les SRE peuvent également mesurer le quotient d'hallucination, grâce à des mesures permettant de surveiller les performances des modèles.</p>""}, {'', ""<p>Une autre possibilité est d'utiliser des agents d'IA. L'idée d'associer des agents d'IA à des SRE suscite un vif intérêt. Les SRE joueront un rôle essentiel dans l'amélioration des résultats des agents d'IA en fonction des préférences humaines et dans la mise en place d'une utilité accrue au fil du temps.</p>""}, {'', '<p>Il existe de nombreuses autres possibilités d’intégrer l’IA générative dans les flux de travail des SRE. L’IA générative est nouvelle, mais son adoption se développe à grande échelle. En mettant en œuvre une approche proactive, les SRE peuvent garder une longueur d’avance et minimiser les risques associés.</p>'}, {'', ""<h3>L'importance stratégique de l'ingénierie de référencement optimisée par l'IA pour une posture agnostique du cloud</h3>""}, {'', ""<p>La transition vers une posture agnostique du cloud des grandes entreprises est une opportunité pour les SRE de passer à l'étape supérieure. Certains des principaux défis opérationnels liés à l'intégration dans le cloud incluent la prolifération des coûts, l'encombrement accru et les risques de sécurité et de conformité, comme l'a souligné mon article précédent.</p>""}, {'', ""<p>L'ingénierie de sécurité peut jouer un rôle essentiel dans le réglage des performances du déploiement cloud en cours d'exécution, améliorant la posture de restauration et de récupération dans le cloud. La sauvegarde basée sur l'IA peut aider les ingénieurs de sécurité à ajuster de manière dynamique les calendriers de sauvegarde réguliers et à recommander des stratégies de sauvegarde pour les applications en fonction de l'utilisation et de la criticité. Enfin, les planificateurs de restauration modélisent intelligemment les restaurations en définissant des points de contrôle, en analysant les journaux, etc. L'ingénierie de sécurité peut exploiter les capacités de l'IA pour l'optimisation des ressources et la maintenance prédictive basée sur des données historiques.</p>""}, {'', '<p>Les pratiques SRE telles que l’observabilité, lorsqu’elles sont alimentées par des capacités d’IA, peuvent aider les SRE dans ce parcours de transformation. De la surveillance des KPI opérationnels et de l’ajustement dynamique des seuils aux idées exponentielles de capacités d’auto-réparation, les outils d’observabilité de nouvelle génération évoluent dans de multiples dimensions.</p>'}, {'', '<p>Au lieu de l’écosystème traditionnel fragmenté de l’observabilité, si nous pouvons standardiser les interfaces pour démocratiser le développement du dépannage, de la maintenance prédictive et des informations sur les applications en plus des outils d’observabilité, alors SRE a le potentiel de créer davantage d’actifs pour la communauté afin de simplifier les opérations pour les applications cloud natives complexes. Les applications d’automatisation basées sur l’IA peuvent produire des informations commerciales clés, des tableaux de bord de commentaires des développeurs, des applications de dépannage et bien plus encore. Les possibilités de l’IA sont illimitées si nous standardisons l’écosystème d’une manière ou d’une autre.</p>'}, {'', '<p>Un autre domaine est l’atténuation des incidents de sécurité. Grâce aux outils d’IA génératifs, les SRE peuvent analyser intelligemment les applications pour détecter les vulnérabilités d’exécution et recommander des mesures correctives, et les agents d’IA peuvent accompagner les SRE pour les aider à relever le défi de la sécurité. Lorsqu’un incident de sécurité se produit, l’évaluation et les audits de sécurité alimentés par l’IA et la défense proactive via la reconnaissance de modèles et le comportement prédictif du système fournissent aux SRE des informations pratiques pour une meilleure résilience.</p>'}, {'', '<h3>Tirer parti de nouvelles compétences</h3>'}, {'', '<p>La sixième édition du rapport SRE 2024 indique que 53 % des professionnels estiment que l’IA sera valorisée. La technologie peut améliorer l’efficacité et l’efficience des SRE et leur permettre à terme de se préparer et de réagir rapidement aux incidents de fiabilité complexes liés à la technologie. Dans le développement de stratégies de talents au niveau organisationnel pour les opérations, le côté « Ops » de DevOps est essentiel. Souvent, la requalification et la mise à niveau du personnel « Ops » ne figurent pas sur la liste des priorités des grandes organisations, ce qui crée un écart stratégique pour la fiabilité. La valeur des SRE tirant parti de ces nouvelles compétences se verra probablement au fil du temps, principalement dans la dernière étape de l’introduction des technologies dans le flux de développement. Le principal défi pour les SRE est de déterminer par où commencer à exploiter la puissance des nouvelles technologies. Les communautés SRE seront particulièrement précieuses pour adopter la feuille de route évolutive des SRE.</p>'}, {'', '<h3>Conclusion</h3>'}, {'', '<p>À l’heure actuelle, les SRE sont susceptibles d’évoluer et d’améliorer régulièrement leurs capacités. Pour instaurer la confiance dans l’écosystème numérique, il est important que les organisations se concentrent sur le côté « Ops » de DevOps. À long terme, les SRE peuvent concevoir un avenir résilient en assurant une opérationnalisation fluide des logiciels combinée à de nouvelles technologies. Le scénario le plus optimiste consiste à trouver le bon équilibre entre l’investissement dans la productivité des développeurs et l’investissement dans la résilience et l’efficacité opérationnelle avec de nouveaux actifs, outils et compétences pour le SRE.</p>'}]"
Datadog étend la portée et la portée de sa plateforme d'observabilité,"[{'', ""<p>Lors de sa conférence DASH 2024, Datadog a ajouté une multitude d'outils et de fonctionnalités pour rationaliser les flux de travail DevSecOps, notamment l'intégration avec le logiciel d'agent open source OpenTelemetry développé sous les auspices de la Cloud Native Computing Foundation (CNCF) et l'outil Datadog On-Call pour optimiser les flux de travail de gestion des incidents de manière à maintenir le contexte avec les données d'observabilité déjà collectées.</p>""}, {'', ""<p>De plus, la prise en charge de l'analyse sans agent ainsi que des outils de découverte des vulnérabilités et des données sensibles ont été ajoutés, ainsi qu'un outil de collaboration Log Workspaces qui permet aux analystes et aux ingénieurs d'utiliser des interfaces en langage naturel pour invoquer un agent d'intelligence artificielle (IA) générative afin d'associer plus facilement les journaux et d'autres ensembles de données à des applications spécifiques, et un outil Live Debugger qui permet aux développeurs d'utiliser des données de production en direct pour mieux dépanner les applications.</p>""}, {'', ""<p>Enfin, la société a ajouté une fonctionnalité de mise à l'échelle automatique Kubernetes pour fournir aux équipes informatiques davantage de contrôle sur la manière dont les applications cloud natives évoluent dynamiquement.</p>""}, {'', ""<p>Hugo Kaczmarek, directeur produit chez Datadog, a déclaré qu'à mesure que l'observabilité continue d'évoluer, il deviendra plus facile pour les équipes informatiques de trianguler la cause profonde des problèmes jusqu'à des lignes de code spécifiques. Il ne sera peut-être jamais possible d'éviter toutes les erreurs, mais le temps nécessaire à leur correction continuera de diminuer rapidement à mesure que de nouvelles avancées en matière d'intelligence artificielle (IA) seront réalisées, par exemple en utilisant des outils de visualisation pour rejouer le flux d'exécution du code. Les équipes informatiques pourront également exploiter l'IA générative pour reproduire les problèmes à l'aide des données de production.</p>""}, {'', '<p>Dans l’ensemble, Datadog continue d’étendre la portée de sa plateforme d’observabilité d’une manière qui promet de permettre aux organisations de réduire le nombre d’outils qu’elles devraient autrement acquérir, gérer et intégrer. Dans le même temps, les intégrations avec des outils open source tels qu’OpenTelemetry rendront simultanément moins coûteuse l’instrumentation des environnements informatiques. Datadog simplifie également la gestion centralisée de toutes les instances d’OpenTelemetry qui pourraient être déployées à terme.</p>'}, {'', '<p>Les extensions de la plateforme Datadog, principalement disponibles en version bêta, sont réalisées à un moment où de nombreuses organisations commencent à adopter l’ingénierie de plateforme comme méthodologie de gestion des flux de travail DevOps à grande échelle. Le défi auquel nombre d’entre elles seront confrontées dans un premier temps consistera à déterminer exactement combien de plateformes seront nécessaires pour unifier la gestion de ces flux de travail.</p>'}, {'', '<p>Il est trop tôt pour dire comment l’ingénierie de la plateforme va évoluer, mais avec l’essor des outils d’IA pour générer du code, l’observabilité va devenir de plus en plus une exigence. Le problème est que de nombreuses équipes DevOps ne savent pas toujours quelles requêtes lancer pour faire apparaître les problèmes. Cependant, avec l’aide des agents d’IA, la plateforme elle-même suggérera non seulement des requêtes, mais les exécutera également automatiquement selon les besoins. L’objectif est d’augmenter les équipes d’ingénierie DevOps existantes pour leur permettre de gérer des applications à des niveaux d’échelle beaucoup plus élevés sans nécessairement augmenter les effectifs.</p>'}, {'', '<p>En attendant, il est évident que de nombreuses équipes DevOps ont déjà du mal à gérer les charges de travail déjà déployées. À cet égard, les outils d’IA qui promettent de réduire les efforts existants ne peuvent pas arriver trop tôt.</p>'}]"
Favoriser une équipe d'ingénierie axée sur l'IA qui accélère la maturité DevOps,"[{'', '<p>Les entreprises peuvent accélérer la maturité de DevOps en exploitant la puissance de l’intelligence artificielle (IA), et plus particulièrement de l’automatisation. Cependant, pour qu’une entreprise puisse réellement tirer parti de l’IA, elle doit aller au-delà des outils tels que Copilot et utiliser des solutions plus avancées, notamment TuringBots ou des agents IA. Ces solutions permettront une meilleure adhésion aux normes de pipeline et une plus grande productivité pour les équipes d’ingénierie.</p>'}, {'', '<h3>Agents IA : l’outil qui renforce les équipes d’ingénierie de demain</h3>'}, {'', ""<p>Un agent IA est une série d'invites qui exploitent l'IA pour remplir un rôle au sein de l'équipe d'ingénierie, comme le propriétaire de l'entreprise ou le testeur. Les agents IA agissent comme des assistants pour garantir que le code adhère à toutes les normes de sécurité, de qualité et de codage requises avant de fusionner avec la ligne principale et les différents environnements, ce qui permet au développeur de gagner un temps précieux.</p>""}, {'', '<p>Par exemple, si un ingénieur doit vérifier son code, il peut demander à un agent d’IA de l’interpréter, de le valider et de lui faire des suggestions. Sur la base de ces suggestions et avec l’approbation humaine, l’agent d’IA peut alors créer, réécrire ou refactoriser tout ce qui manque, qu’il s’agisse de tests ou de contrôles de qualité. Les retours d’information plus rapides et les charges de travail réduites possibles avec les agents d’IA permettent aux équipes d’ingénierie de devenir compétentes dans la production de code, sa mise à l’essai et son transfert dans différents environnements.</p>'}, {'', '<p>De même, comme ces agents d’IA garantissent la qualité du code en amont, il y aura moins de défauts plus tard dans le cycle de vie du développement logiciel (SDLC), ce qui signifie moins de retouches. Cela améliorera donc considérablement la productivité des équipes d’ingénierie, ce qui se traduira par une mise sur le marché plus rapide. En fin de compte, cet environnement DevOps assisté par l’IA permettra aux équipes d’atteindre un niveau de maturité DevOps plus élevé à un rythme sans précédent.</p>'}, {'', ""<h3>Comment les entreprises peuvent réorganiser leurs équipes à la lumière de l'IA</h3>""}, {'', ""<p>La création d'une équipe d'ingénierie axée sur l'IA nécessitera que les équipes existantes subissent des changements de fond en comble. Il y a cinq ans, un responsable DevOps gérait tous les outils nécessaires à la gestion du code, des tests, etc. Il existe désormais une tendance continue dans l'espace DevOps pour que ce poste/titre se transforme en un poste/titre axé sur l'ingénierie de plateforme.</p>""}, {'', '<p>Ces nouveaux responsables de l’ingénierie des plateformes sont chargés de mettre en œuvre une variété d’objectifs liés à l’IA dans leurs organisations. Ils possèdent toutes les plateformes que les ingénieurs utilisent quotidiennement. Ces dirigeants doivent également tenir compte de l’activation de l’IA, qu’il s’agisse de superviser tous les modèles d’IA auxquels l’entreprise s’est intégrée ou de gérer les coûts de ces modèles grâce à des informations sur l’utilisation des jetons (les jetons sont la façon dont les différents LLM facturent leurs clients).</p>'}, {'', '<p>À l’instar des responsabilités du responsable de l’ingénierie de la plateforme, une équipe dédiée doit être chargée de prendre soin et d’alimenter les plateformes et les outils utilisés pour DevOps. Cette équipe doit posséder et prendre en charge les agents d’IA, car à mesure que les applications mûrissent, les agents d’IA doivent évoluer. De même, elle sera chargée d’encourager l’adoption de l’IA au sein des équipes d’ingénierie.</p>'}, {'', '<p>Même les outils les plus simples comme Copilot, dont le seuil d’entrée est bas, nécessitent une activité de gestion du changement importante. Néanmoins, il peut être difficile d’enseigner aux ingénieurs seniors de nouvelles méthodes de travail. Les entreprises doivent s’appuyer sur des mécanismes d’éducation et de récompense, tels que des initiatives de gestion de la chaîne organisationnelle, pour aider à requalifier les équipes existantes. Malgré tous ces efforts, il faudra peut-être trois mois ou plus pour que les ingénieurs adoptent de nouveaux outils d’IA.</p>'}, {'', '<p>En outre, les entreprises doivent se préparer à l’inévitable restructuration de leurs équipes d’ingénierie. Les entreprises devront probablement réduire leurs effectifs une fois que les agents d’IA commenceront à automatiser et à rationaliser davantage de processus dans le SDLC. Actuellement, la plupart des équipes d’ingénierie se composent d’un responsable produit, d’un scrum master, d’un commercial, de plusieurs développeurs et de quelques testeurs. Bientôt, ces équipes n’auront besoin que d’un responsable produit, d’un ingénieur (comme un chef d’ingénierie ou un architecte), d’un testeur et d’un commercial. Dans le même temps, les entreprises n’ont pas besoin d’ingénieurs hautement qualifiés et capables de construire quoi que ce soit, leur priorité est plutôt de trouver quelqu’un avec une plus grande expertise du domaine et une meilleure compréhension de l’entreprise.</p>'}, {'', ""<h3>Mise en œuvre intentionnelle de l'IA</h3>""}, {'', '<p>Malgré les avantages indéniables de l’IA en termes de productivité et de rentabilité, de nombreuses entreprises hésitent encore, voire craignent, cette technologie et les conséquences qu’elle pourrait avoir sur leurs équipes d’ingénierie. Si l’IA est vouée à provoquer des bouleversements, ce n’est pas inhabituel pour toutes les innovations technologiques à un moment ou à un autre. Néanmoins, les entreprises ne doivent pas se lancer immédiatement dans l’aventure. Elles doivent trouver des moyens d’intégrer l’IA en expérimentant et en testant. De plus, les entreprises peuvent faire appel à des partenaires expérimentés pour élaborer des stratégies technologiques et commerciales progressives.</p>'}]"
Agents de test : plus qu'un copilote,"[{'', ""<p>On a beaucoup parlé de l'adoption de copilotes de codage pour les développeurs. Ce que l'on sait moins, c'est que les copilotes peuvent également écrire des scripts de test et traduire des tests d'un langage de script à un autre. Les copilotes augmentent la productivité pour les deux cas d'utilisation, mais ce sont généralement des outils passifs qui doivent être alimentés avec des exigences précises pour obtenir les résultats souhaités.</p>""}, {'', '<p>Un nouveau niveau d’assistant émerge, qui amène cette capacité à un état plus actif, car la technologie de l’IA est intégrée directement dans les outils de développement et de test.</p>'}, {'', '<p>Un responsable de l’assurance qualité d’une entreprise technologique de premier plan a parfaitement résumé la situation : « La capacité de l’IA à non seulement exécuter des tests, mais aussi à en tirer des enseignements et à s’adapter, transforme notre approche de l’assurance qualité. C’est comme avoir un membre de l’équipe en constante évolution qui garantit la robustesse et la fiabilité de nos versions. »</p>'}, {'', ""<p>Ces agents de test commencent à apparaître dans la nature et sont voués à évoluer rapidement au cours des prochains mois et des prochaines années. Ils répondront à certains des principaux défis auxquels les entreprises sont confrontées dans le développement d'applications métier.</p>""}, {'', ""<h3>Élaboration d'un plan de test</h3>""}, {'', '<p>Le premier défi est que les personnes qui comprennent ce que l’application est censée faire ne sont généralement pas compétentes en matière de développement de tests. Les nouveaux agents de test relèveront ce défi en travaillant avec des experts en la matière (SME) pour élaborer un plan de test basé sur l’exploration des nouvelles fonctionnalités par ces derniers.</p>'}, {'', '<p>Le deuxième défi est le fait que les tests ne sont pas réalisés une seule fois. La première version d’un test ne durera pas longtemps avant de devoir être mise à jour. Pour paraphraser von Moltke l’Ancien, « les plans de tests fonctionnels ne survivront pas au premier contact avec une mise à niveau de la plateforme », en particulier pour les SaaS d’entreprise. Pour les équipes de développement de tests traditionnelles, cela signifie qu’il y a un retard constant dans les retouches. Mais les agents de test peuvent régénérer une nouvelle version du test aussi facilement qu’ils ont écrit la première. Le troisième défi consiste à exécuter les bons tests au bon moment. Les meilleures entreprises créent des suites de régression qui peuvent tester la plupart des fonctionnalités importantes de leurs applications métier avant de les publier.</p>'}, {'', '<p>Tout va bien jusqu’à ce que vous réalisiez que 90 % de ces tests ne peuvent pas échouer compte tenu de l’ampleur des modifications apportées. Cette surdose coûte du temps et de l’argent. Ne serait-il pas formidable que votre système de test soit suffisamment intelligent pour savoir quels tests doivent être exécutés et lesquels peuvent être ignorés ? Les agents de test correctement intégrés au système CI/CD seraient en mesure de faire ce choix.</p>'}, {'', '<h3>Minimiser les risques</h3>'}, {'', ""<p>En relevant ces défis, les agents de test modifient efficacement le calcul du retour sur investissement pour l'automatisation des tests. Les tests créés par les agents sont moins coûteux à maintenir, ce qui réduit l'investissement. Ils ne s'exécutent qu'en cas de besoin, ce qui réduit encore les coûts. Étant donné que la PME est impliquée dans le processus de création, les tests sont plus susceptibles de détecter les problèmes les plus critiques, minimisant ainsi le risque d'indisponibilité des applications. Gagnez, gagnez et gagnez.</p>""}, {'', '<p>Tout cela semble formidable, mais il doit y avoir un inconvénient. Quels sont les risques ?</p>'}, {'', '<p>Le risque à court terme est de penser qu’un agent de test est entièrement automatique. Nous y parviendrons peut-être bientôt, mais à court terme, il est préférable de considérer ces agents comme un « membre de l’équipe qui garantit que nos versions sont robustes et fiables ». Les humains impliqués dans la boucle ont la responsabilité de gérer le processus et de garantir le bon résultat. Même les meilleurs LLM ont encore des hallucinations de temps en temps, donc jusqu’à ce que ces agents établissent un historique, faites-leur confiance mais vérifiez.</p>'}]"
L’IA peut-elle remplacer les ingénieurs DevOps ?,"[{'', ""<p>Il y avait des administrateurs système. Ils effectuaient des sauvegardes, concevaient des VPN et créaient des plans de reprise après sinistre (PRA). Les responsables ne les comprenaient pas et pensaient qu'ils ne fonctionnaient pas vraiment. Pendant un certain temps, tout était calme et bien.</p>""}, {'', ""<p>Puis est arrivé DevOps. Le développement s'est immédiatement accéléré, les pipelines ont commencé à mieux fonctionner, les produits avaient moins de bugs et les développeurs étaient enfin satisfaits. Pendant un certain temps, tout allait à nouveau bien.</p>""}, {'', ""<p>Puis les conteneurs sont arrivés. Tout le monde s'est précipité sur Kubernetes comme si le train avait déjà quitté la gare. Ils ont pratiqué Kubernetes pour le plaisir de Kubernetes, pour le buzz et pour avoir l'occasion de montrer qu'ils étaient à la pointe. Pendant un certain temps, tout allait à nouveau bien.</p>""}, {'', '<p>Et puis l’intelligence artificielle (IA) est arrivée, une véritable révolution. L’hystérie s’est emparée du secteur informatique, qui pensait que personne ne serait plus nécessaire dans un avenir proche, puisque tout serait fait par l’IA. Mais avant de paniquer, essayons de comprendre ce que l’IA fait bien, où elle échoue dans le DevOps, et si les professionnels du DevOps devraient la craindre et commencer à se perfectionner dès aujourd’hui.</p>'}, {'', ""<h3>Là où l'IA excelle et là où elle trébuche</h3>""}, {'', ""<p>L'IA est une aide précieuse pour automatiser les tâches répétitives et un excellent assistant en développement. Elle vérifie la syntaxe, embellit le code, insère des fonctions, écrit des commentaires de code, trouve des bugs que vous auriez pu manquer, réussit à tester et à générer des tests et fonctionne bien dans l'analyse des journaux. Elle permet également de prédire le comportement, les charges et les problèmes du système et fonctionne bien pour détecter les menaces et répondre aux événements.</p>""}, {'', '<p>Cependant, l’IA manque d’une compréhension approfondie du contexte, de l’analyse critique, de la créativité et de la motivation – des aspects cruciaux du DevOps, qui repose fondamentalement sur la communication. L’IA a également du mal à créer des processus et des interactions complexes entre des parties diverses et complexes d’un système.</p>'}, {""<h3>Là où DevOps surpasse l'IA</h3>"", ''}, {'', '<p>DevOps est une pratique qui favorise la collaboration entre les développeurs de logiciels et les professionnels de l’informatique tout en automatisant la livraison de logiciels et les changements d’infrastructure. Son objectif est de créer une culture et un environnement où le développement, les tests et la publication de logiciels peuvent se dérouler rapidement, fréquemment et de manière fiable. L’IA n’est pas encore la baguette magique qui peut résoudre les tâches centrées sur l’humain.</p>'}, {'', ""<p>DevOps s'étend au-delà des pipelines, des serveurs, de Kubernetes et de Docker, en mettant l'accent sur le travail d'équipe et les relations solides au sein d'une équipe. Avec DevOps, aucune équipe distincte ne s'en tient à ses propres règles ; à la place, il existe une équipe de livraison unifiée où chacun est responsable des problèmes et désireux de les résoudre. DevOps implique également une communication interdépartementale, un transfert de connaissances et une interaction lors des déploiements.</p>""}, {'', ""<h3>Qu'en est-il du côté technique ?</h3>""}, {'', '<p>Bien entendu, l’IA peut vous aider à écrire des scripts en Bash, Python ou PowerShell. Demandez à ChatGPT et il vous sera fourni avec des commentaires. Cependant, réfléchir à un plan de reprise après sinistre complet avec tests et vérification des sauvegardes dépasse les capacités de l’IA. Comment lui expliquer que les bandes que nous utilisons pour les sauvegardes doivent être stockées dans une banque et que le processus de récupération des données doit être testé tous les trimestres, avec un risque d’échec en raison d’un numéro de téléphone mal écrit ?</p>'}, {'', '<p>Prenons par exemple les conteneurs et Kubernetes. Les configurations précises de vos déploiements nécessitent beaucoup de travail : PSP, PodDisruptionBudget, bonnes limites et requêtes, sondes solides (configurées de manière à ne pas faire planter le cluster), configuration de sécurité et surveillance appropriée. S’appuyer sur l’IA pour ces tâches n’est pas pratique : vous avez besoin d’un ingénieur DevOps compétent pour les mettre en œuvre.</p>'}, {'', '<h3>IA vs DevOps : qui est le gagnant ?</h3>'}, {'', ""<p>Combien de fois avez-vous écrit un script et vous êtes senti trop paresseux pour ajouter des commentaires ou rédiger un fichier README ? Combien de fois avez-vous recherché manuellement un bug dans le code et l'avez-vous corrigé (ligne par ligne, au cours de longues nuits) ? Combien de fois avez-vous dû vous réveiller la nuit à cause d'une notification de production ? C'est là qu'intervient l'IA, en automatisant ces tâches et d'autres similaires.</p>""}, {'', '<p>Il n’y aura plus de lutte pour la survie à l’avenir. La puissance de l’IA est révélée lorsqu’elle est associée à une approche centrée sur l’humain. Cela signifie que DevOps planifiera, exécutera, contrôlera et améliorera, tandis que l’IA aidera à gérer les tâches plus rapidement et avec plus de précision.</p>'}, {'', '<h3>Les bons spécialistes apprennent en permanence</h3>'}, {'', '<p>Les DevOps doivent améliorer leurs connaissances, perfectionner leurs compétences et apprendre à gérer les situations conflictuelles, non seulement en raison des menaces perçues par l’IA, mais aussi pour devenir et rester des spécialistes recherchés. Si vous êtes flexible, que vous vous adaptez facilement aux changements et que vous faites face aux défis sans vous lamenter sur le passé, aucune IA ne peut vous remplacer. L’IA vous aidera en vous demandant : « Comment ça se passe aujourd’hui ? Que pouvons-nous améliorer et accomplir aujourd’hui ? »</p>'}]"
Mend.io ajoute un outil pour détecter la présence de code d'IA générative,"[{'', ""<p>Mend.io a ajouté cette semaine un outil MendAI à son portefeuille de sécurité des applications qui identifie le code généré par un modèle d'intelligence artificielle (IA).</p>""}, {'', ""<p>De plus, l'outil d'analyse de la composition du logiciel Mend.io (SCA) a été étendu pour faire apparaître des informations détaillées sur les versions des modèles d'IA et les mises à jour pour chaque modèle d'IA utilisé, y compris les dépendances obsolètes.</p>""}, {'', ""<p>Mend.io a indexé plus de 35 000 modèles de langage à grande échelle (LLM) pré-entraînés et accessibles au public pour identifier les modèles d'IA utilisés.</p>""}, {'', '<p>Jeffery Martin, vice-président des produits chez Mend.io, a déclaré que cette fonctionnalité permet aux entreprises de gérer plus facilement les problèmes de licence, de compatibilité et de conformité dans le cadre d’une nomenclature logicielle (SBOM). Il s’agit d’une capacité essentielle lorsque les entreprises cherchent à appliquer des politiques de gouvernance au code généré par des plateformes d’IA, a-t-il ajouté.</p>'}, {''}, {'', '<p>Il est encore trop tôt pour dire que l’utilisation de l’IA pour générer du code est une pratique courante. Cependant, il est clair que les équipes de science des données qui s’appuient sur des workflows d’opérations d’apprentissage automatique (MLOps) pour créer des modèles devront avoir accès aux mêmes types d’outils SCA et SBOM que ceux que de nombreux développeurs utilisent couramment aujourd’hui. Il est moins évident de savoir dans quelle mesure les équipes DevSecOps fusionneront les workflows pour unifier la gestion de la sécurité des applications et des modèles d’IA, a-t-il ajouté.</p>'}, {'', '<p>Malheureusement, les équipes de data science n’ont généralement pas beaucoup d’expertise en cybersécurité. C’est problématique car les applications d’IA étant développées à l’aide d’outils et de plateformes présentant des vulnérabilités connues, il devient relativement facile pour les cybercriminels de les exploiter. Par conséquent, les équipes DevSecOps qui ont déployé des applications d’IA auront besoin d’outils qui leur permettent d’identifier quel code potentiellement vulnérable généré par un modèle d’IA s’exécute à quel endroit d’un environnement d’application.</p>'}, {'', ""<h3>Les cybercriminels ciblent généralement et spécifiquement les modèles d'IA</h3>""}, {'', '<p>Malheureusement, les cybercriminels sont désormais conscients de ces vulnérabilités. Il est donc de plus en plus courant de voir des campagnes de malwares ciblant spécifiquement les modèles d’IA. Le but de ces attaques peut aller de la simple exfiltration de données à l’empoisonnement du pool de données utilisé pour former les modèles d’IA, qui deviennent rapidement l’actif logiciel le plus précieux qu’une organisation puisse posséder. Le défi est que les équipes de science des données peuvent avoir besoin de remplacer un modèle d’IA entier pour remédier à ces vulnérabilités, car de nombreux modèles d’IA ne sont pas aussi faciles à corriger que d’autres artefacts logiciels.</p>'}, {'', '<p>Il n’est pas encore clair qui, au sein des organisations, assumera en fin de compte la responsabilité de la sécurité des applications d’IA. Le problème est qu’il existe déjà une pénurie générale d’experts en cybersécurité et que le nombre de professionnels de la cybersécurité possédant une expertise en IA est extrêmement limité. Il ne fait aucun doute que les organisations devront à terme fusionner les flux de travail MLOps et de cybersécurité pour définir un ensemble de meilleures pratiques MLSecOps.</p>'}, {'', '<p>En attendant, les équipes DevOps doivent s’attendre à ce que la quantité de code exécuté dans leurs environnements applicatifs, qui aurait pu être généré par un modèle d’IA, augmente considérablement. Avec ou sans autorisation, de nombreux développeurs réutilisent du code généré par des plateformes d’IA, dont la plupart ont été entraînés à l’aide de code de qualité inégale collecté sur Internet. Par conséquent, le code généré par le modèle d’IA peut être tout aussi défectueux que le code utilisé pour entraîner le modèle d’IA.</p>'}, {'', ""<p>Quelle que soit la manière dont ce code a été créé, une chose est sûre : une équipe DevSecOps devra s'attaquer à tous les problèmes de sécurité des applications qui surviendront presque inévitablement.</p>""}]"
Shreds.AI lance une plateforme LLM destinée à l'ingénierie logicielle,"[{'', ""<p>Shreds.AI a dévoilé aujourd'hui une plateforme d'intelligence artificielle (IA) générative du même nom basée sur un grand modèle de langage (LLM) qu'elle a formé pour automatiser spécifiquement les tâches d'ingénierie logicielle. Disponible en version bêta, la plateforme Shreds.AI peut attribuer des tâches à huit autres LLM en invoquant les interfaces de programmation d'applications (API) qu'elles exposent.</p>""}, {'', ""<p>Selon Soufiane Amar, PDG de Shreds.AI, plutôt que de produire de petites quantités de code, la plateforme a été formée pour créer les dizaines de milliers de lignes et de fichiers de code nécessaires pour piloter des flux de travail d'ingénierie logicielle complexes. La plateforme Shreds.AI orchestrera également avec précision l'intégration de divers composants logiciels pour créer une application, car elle a été formée à l'aide d'outils que les développeurs utilisent régulièrement, a-t-il ajouté.</p>""}, {'', ""<p>Un développeur saisit une description simple en langage naturel du logiciel qu'il souhaite que Shreds.AI crée, et la plateforme génère ensuite des diagrammes d'architecture et le code des fonctionnalités indépendantes et isolées appelées shreds. Les équipes DevOps n'ont plus qu'à valider le code avant de l'utiliser, un processus que Shreds.AI simplifie grâce à un réseau de développeurs indépendants que les organisations peuvent engager pour examiner le code, a déclaré Amar.</p>""}, {'', '<p>Shreds.AI est une méta-IA, dans la mesure où, en plus de générer du code et de raisonner sur plusieurs processus, elle est capable de classer les LLM tiers en fonction de leur capacité à effectuer des tâches spécifiques, a-t-il noté.</p>'}, {''}, {'', '<p>La plateforme Shreds.AI est déjà testée par le conglomérat automobile Stellantis et le Réseau de Transport d’Électricité (RTE), le gestionnaire du réseau de transport d’électricité français. Shreds.AI estime qu’une application qui aurait pu coûter auparavant 1 million de dollars peut désormais être créée pour moins de 30 000 dollars. L’entreprise affirme que Shreds.AI réduit le délai de mise sur le marché des logiciels, ainsi que la taille des équipes et les coûts, de plus de 80 % par rapport aux méthodes de développement de logiciels traditionnelles. En permettant la maintenance automatique, elle résout également le problème de l’obsolescence des logiciels. Elle augmente la durée de vie des logiciels de plus de 60 %, par exemple en facilitant la conversion du langage de programmation utilisé pour créer cette application en un autre langage que davantage de développeurs connaissent.</p>'}, {'', '<p>L’intégration de l’IA dans les workflows DevOps est encore trop récente et le code généré par ces plateformes doit encore être géré. Le défi est que, grâce à l’essor de l’IA, on s’attend à ce que la quantité de logiciels créés et déployés au cours des deux prochaines années dépasse la quantité de logiciels déployés au cours des deux dernières décennies. La seule façon de suivre ce rythme de développement sera d’appliquer également l’IA à la gestion des workflows DevOps.</p>'}, {'', '<p>Dans l’espoir d’éliminer autant de tâches que possible, les équipes DevOps doivent identifier les processus manuels qu’elles effectuent régulièrement aujourd’hui, en vue d’y appliquer l’IA demain. Après tout, l’objectif initial de l’adoption de DevOps était d’automatiser impitoyablement autant de processus d’ingénierie logicielle que possible, afin de permettre la création et le déploiement d’un plus grand nombre d’applications le plus rapidement possible.</p>'}]"
Cisco ajoute l'IA générative et des intégrations Splunk plus poussées à AppDynamics,"[{'', ""<p>Cisco a présenté cette semaine un assistant d'intelligence artificielle (IA) générative pour la plateforme d'observabilité Cisco AppDynamics ainsi que des intégrations à la plateforme IT Service Intelligence (ITSI) de Splunk et un Splunk Log Observer Connect pour Cisco AppDynamics à la plateforme qu'il a acquise plus tôt cette année en acquérant Splunk.</p>""}, {'', ""<p>Annoncé lors de l'événement Cisco Live! 2024, Cisco permettra également au troisième trimestre d'héberger Cisco AppDynamics sur la plateforme Microsoft Azure.</p>""}, {'', ""<p>Cisco a également annoncé qu'elle ne commercialiserait plus la plateforme Cisco AppDynamics pour la gestion des performances des applications natives du cloud (APM). Cette plateforme sera remplacée par Splunk Observability Cloud.</p>""}, {'', '<p>Cisco a également ajouté des fonctionnalités supplémentaires de science des données et d’algorithmes d’apprentissage automatique à ITSI pour rationaliser le nombre d’alertes générées par la plateforme. Désormais disponible, l’assistant de configuration de Splunk ITSI fournit une console centralisée en fournissant des informations sur les modèles de seuil obsolètes qui peuvent être optimisés à l’aide de corrections guidées mises en évidence par les modèles d’IA.</p>'}, {'', '<p>Enfin, Cisco a mis en place des fonctionnalités d’authentification unique pour rationaliser les flux de travail partagés entre Cisco AppDynamics et Splunk via une console unique. Ces fonctionnalités d’authentification unique seront étendues à l’ensemble du portefeuille Cisco pour permettre une collaboration accrue.</p>'}, {'', ""<p>Tom Casey, vice-président senior et directeur général des produits et technologies pour le portefeuille de produits Splunk, a déclaré aux participants à la conférence que les plateformes seront progressivement intégrées dans une plateforme de données unique, comprenant Cisco AppDynamics, le service de surveillance réseau Cisco ThousandEyes et la plateforme Splunk. Cette approche permettra également à Cisco de fournir les meilleures capacités de sa catégorie pour tout, de la surveillance des performances des applications à la surveillance de l'expérience numérique dans un environnement de cloud computing hybride, a-t-il ajouté.</p>""}, {'', ""<p>Paul Nashawaty, responsable du développement d'applications chez The Futurum Group, a déclaré que ces capacités combinées soulignent l'engagement de Cisco à fournir une visibilité complète à un moment où les environnements informatiques distribués deviennent de plus en plus difficiles à gérer.</p>""}, {'', '<p>On ne sait pas encore dans quelle mesure les entreprises unifient l’observabilité, la surveillance du réseau et la gestion traditionnelle des services informatiques (ITSM), mais Cisco parie clairement que les responsables informatiques préféreront s’appuyer sur un seul fournisseur pour réduire les coûts d’intégration qui seraient autrement encourus lors de l’utilisation de plateformes fournies par des fournisseurs disparates. Cette approche centralisée simplifiera également la formation de plusieurs modèles d’IA qui seront utilisés pour automatiser davantage une gamme de flux de travail qui s’étendent aujourd’hui sur plusieurs silos informatiques.</p>'}, {'', '<p>Il est toutefois moins clair dans quelle mesure l’acquisition de Splunk pourrait contribuer à une consolidation des plateformes informatiques, alors que les concurrents réalisent des acquisitions similaires pour favoriser une approche plus centrée sur la plateforme afin d’unifier la gestion de l’informatique.</p>'}, {'', '<p>Chaque organisation informatique doit décider de la meilleure façon de se structurer à mesure que les flux de travail de ces équipes deviennent plus intégrés. Il est certain que la taille globale du portefeuille d’applications à gérer ne fera que continuer à augmenter à mesure que l’IA simplifie la création et le déploiement de logiciels. Par conséquent, les équipes informatiques doivent très certainement appliquer les meilleures pratiques DevSecOps à des niveaux d’échelle plus élevés pour garantir la disponibilité des applications dans des environnements informatiques hautement distribués.</p>'}]"
Créer une nouvelle boîte à outils pour garantir des prévisions d'IA précises,"[{'', '<p>Tout système informatique n’est utile que dans la mesure où les données qu’il génère sont précises et exploitables, qu’il s’agisse d’une modeste application départementale ou d’un outil de modélisation prédictive utilisé par un énorme fournisseur de services cloud. Lorsqu’un hyperscaler (qui dispose d’un vaste réseau de centres de données et d’une large gamme de services) développe des modèles d’IA, il doit simplement produire des prédictions robustes, sinon ses efforts seront vains.</p>'}, {'', '<p>De nombreuses variables peuvent affecter la précision des prédictions de l’apprentissage automatique (ML), de la distribution des données d’entraînement aux (hyper)paramètres du modèle en passant par la configuration des systèmes. Cela rend l’identification de la cause profonde d’un problème complexe, en particulier à l’échelle de Meta. Notre infrastructure englobe des centres de données dans le monde entier et des dizaines de modèles ML, ce qui exacerbe les défis liés aux flux de travail de débogage efficaces.</p>'}, {'', ""<p>Au cours des deux dernières années, une nouvelle boîte à outils interne appelée HawkEye a permis des améliorations exponentielles. HawkEye est devenu une ressource puissante pour la surveillance, l'observabilité et la débogage du flux de travail ML de bout en bout qui alimente les produits basés sur le ML de Meta.</p>""}, {'', '<p>Nous pensons que le développement responsable de l’IA est un engagement commun. C’est pourquoi, dans cet article, nous décrivons la conception de HawkEye pour la communauté. Au fur et à mesure que nous poursuivons le projet, nous espérons partager ce que nous avons appris en résolvant les défis du débogage ML à grande échelle.</p>'}, {'', '<h3>Rationalisation du flux de travail, optimisation de la productivité</h3>'}, {'', ""<p>HawkEye fait partie du programme Prediction Robustness que Meta a créé pour innover en matière d'outils et de services afin de garantir la qualité de nos produits qui s'appuient sur les prédictions des modèles ML. HawkEye comprend une infrastructure permettant de collecter en continu des données sur les modèles de service et de formation, la génération de données et les composants d'analyse pour l'exploration des causes profondes. Il prend en charge les flux de travail d'expérience utilisateur (UX) pour l'exploration guidée, l'investigation et le lancement d'actions d'atténuation.</p>""}, {'', ""<p>Grâce à HawkEye, nous avons amélioré la résolution des problèmes en réduisant considérablement le temps consacré au débogage des problèmes de production complexes et en simplifiant les flux de travail opérationnels. Il a permis aux non-experts de trier les problèmes complexes avec un minimum de coordination et d'assistance.</p>""}, {''}, {'', ""<p>Heureusement, l'identification et la résolution des problèmes dans les flux de production des fonctionnalités et des modèles sont devenues plus simples. Cela ne nécessite plus de connaissances spécialisées ni de familiarité avec les processus et la télémétrie impliqués, et cela minimise la coordination substantielle entre les différentes organisations. HawkEye a réduit la pression exercée sur les ingénieurs de garde pour déboguer les modèles, et nous a aidés à partager des blocs-notes et du code pour les analyses des causes profondes sur de petites parties du processus de débogage.</p>""}, {'', '<p>L’arbre de décision de HawkEye permet la collecte continue de données sur les modèles de service et de formation, permettant aux utilisateurs de naviguer et d’identifier la cause profonde de problèmes complexes, simplifiant ainsi le processus.</p>'}, {'', '<p>Prenons un exemple hypothétique : autrefois, le scénario de débogage d’une équipe produit qui devait résoudre une anomalie sur un flux Instagram recommandé par l’IA pouvait nécessiter de réunir un éventail d’expertises au sein de Meta. Il s’agissait notamment des membres de l’équipe qui géraient l’aspect diffusion du modèle, de ceux qui avaient créé le modèle de diffusion lui-même et d’autres personnes chargées de travailler avec les données derrière le processus de formation. Toutes ces personnes se réunissaient pour découvrir les causes profondes du problème afin de le résoudre. HawkEye rationalise essentiellement ce processus très complexe dans un seul produit.</p>'}, {'', '<h3>Composants essentiels de HawkEye</h3>'}, {'', ""<p>L'apprentissage automatique joue un rôle important dans de nombreux produits et services Meta. Nous l'utilisons pour les recommandations, la compréhension et la génération de contenu, et bien plus encore. La mise en production de modèles ML implique la création de flux de travail qui incluent les pipelines de données d'informations nécessaires à la formation des modèles, d'autres pour créer et améliorer les modèles au fil du temps, des systèmes d'évaluation pour les tests et des flux de travail d'inférence pour utiliser les modèles dans les produits.</p>""}, {''}, {'', '<p>Trois composants principaux sont nécessaires pour créer une solution comme HawkEye.</p>'}, {'', ""<li>La première étape consiste à collecter des données d'instrumentation, qui comprennent des informations sur le modèle, les données et le processus de formation. Cela comprend également la lignée des ensembles de données utilisés dans le processus de formation.</li>""}, {'', ""<li>La deuxième étape consiste à analyser ces données. Nous analysons le grand volume de données à l'aide d'algorithmes permettant de détecter les anomalies, les corrélations et les causes profondes. Cette analyse est effectuée à la fois pour les données et pour l'état interne du modèle formé.</li>""}, {'', ""<li>Le troisième composant est la couche produit, qui présente toutes les informations dans une expérience guidée. Les développeurs de modèles ML peuvent naviguer à travers différents artefacts liés au temps de prédiction, au temps de formation et aux fonctionnalités utilisées. HawkEye suggère la cause profonde possible d'un problème en fonction des anomalies et des corrélations détectées.</li>""}, {'', ""<p>En plus de son arbre de décision, HawkEye inclut l'isolation des fonctionnalités en temps réel, la mise en œuvre de l'explicabilité du modèle et l'identification d'une liste de fonctionnalités classées responsables des anomalies de prédiction. HawkEye isole les causes en amont des problèmes de fonctionnalités à l'aide d'un flux de travail visuel pour faciliter l'analyse des causes profondes, en suivant la lignée des données et des pipelines en amont. En diagnostiquant les instantanés du modèle, HawkEye compare les instantanés actuels avec ceux stables sur le plan opérationnel et identifie les problèmes de pondération, de biais et d'améliorations potentielles.</p>""}, {''}, {'', ""<p>Lors de la conception de HawkEye, notre priorité a été déterminée par la fiabilité du système et les points chauds au sein du flux de travail ML. En tant que solution de débogage de bout en bout, les défis les plus importants auxquels nous avons été confrontés ont été l'intégration de systèmes de lignage de données disparates et incomplets, d'ensembles de données et de plateformes d'analyse des causes profondes (RCA), leur mise à l'échelle en temps quasi réel, ainsi que l'alignement et la collaboration entre plusieurs équipes et organisations.</p>""}, {'', ""<p>Réduire la complexité traditionnelle du débogage à grande échelle est un travail important pour les entreprises qui s'appuient sur l'IA. Nous espérons que la communauté bénéficiera des informations que nous partageons ici.</p>""}, {'', ""<p>Srikanth Kamath est un ingénieur logiciel qui dirige plusieurs efforts visant à améliorer la surveillance et l'observabilité des systèmes d'IA/ML chez Meta.</p>""}, {'', ""<p>Animesh Dalakoti est chef de produit chez Meta, spécialisé dans la surveillance et l'observabilité de l'IA. Il soutient le développement d'outils de débogage des problèmes de modèles et de systèmes dans la formation et l'inférence.</p>""}, {'', ""<p>Partha Kanuparthy est ingénieur logiciel chez Meta. Il travaille sur la surveillance, l'observabilité et le débogage des systèmes et outils pour les données, la formation et l'inférence de l'IA.</p>""}]"
Tricentis ajoute des copilotes IA supplémentaires à sa plateforme d'automatisation des tests,"[{'', ""<p>Cette semaine, Tricentis a mis à disposition du public un module complémentaire copilote pour sa plateforme d'automatisation des tests Tricentis Tosca. Ce module complémentaire simplifie la recherche, la compréhension et l'optimisation des tests via une interface de chat.</p>""}, {'', ""<p>Mav Turner, directeur des produits et de la stratégie chez Tricentis, a déclaré que la société ajoutait des copilotes à chacune de ses offres de produits en utilisant de grands modèles de langage (LLM) développés par Open AI. Auparavant, Tricentis avait lancé Tricentis Testim Copilot ; la société prévoit d'ajouter prochainement un Tricentis Copilot pour Tricentis qTest.</p>""}, {'', ""<p>Au fil du temps, ces assistants IA pourront collaborer sur des tâches entre eux et éventuellement avec des assistants IA fournis par d'autres fournisseurs d'autres plateformes, notamment SAP, a ajouté Turner.</p>""}, {'', ""<p>En attendant, les équipes DevOps peuvent utiliser Tosca Copilot pour trouver des cas de test inutilisés, des tests en double, des ressources non liées, des exécutions spécifiques et des tests liés à des éléments d'application. Turner a noté que les résumés rendus possibles par Tosca Copilot devraient simplifier l'intégration de testeurs supplémentaires par les équipes DevOps.</p>""}, {'', '<p>Les équipes DevOps peuvent également modifier n’importe quelle combinaison de ces tests via une invite d’IA générative plutôt que de devoir maîtriser directement le langage de requête Tosca existant. Cette capacité devrait également réduire considérablement le temps et les efforts actuellement nécessaires pour résoudre les défauts des tests, a déclaré Turner.</p>'}, {'', '<p>Globalement, Tricentis indique que le temps consacré aux activités de test complexes peut être réduit de moitié, par exemple en réduisant le nombre de tâches de test répétitives auparavant requises. En fait, Tricentis constate déjà une réduction de 16 à 43 % des taux d’échec des tests avec les outils d’IA de Tricentis et une augmentation allant jusqu’à 50 % de la génération de cas de test.</p>'}, {''}, {'', '<p>L’objectif n’est pas nécessairement d’augmenter le nombre de tests exécutés, mais plutôt de s’assurer que les bons tests sont effectués au bon moment pour améliorer la qualité des applications, a déclaré Turner.</p>'}, {'', '<p>Il sera toutefois plus facile pour n’importe quel membre d’une équipe DevOps, y compris les développeurs d’applications, de générer un test de manière itérative. C’est mieux que de devoir attendre qu’une équipe de test dédiée crée un test. L’IA générative devrait également permettre aux équipes DevOps de réutiliser plus facilement les tests, de les exécuter plus rapidement, de générer moins d’erreurs, de réduire les coûts et d’augmenter la productivité globale.</p>'}, {'', '<p>On ne sait pas encore dans quelle mesure l’IA générative démocratisera les tests d’applications, mais à mesure que les tests deviennent plus faciles à créer, il devrait y avoir plus de temps pour exécuter une plus large gamme de tests qui pourraient, par exemple, résoudre des problèmes de cybersécurité. La plupart des tests exécutés aujourd’hui font généralement apparaître des erreurs de programmation courantes et il existe toujours une tendance à sauter des tests chaque fois qu’un projet de développement d’application commence à prendre du retard.</p>'}, {'', '<p>Cependant, à mesure que les tests deviennent plus rapides à l’ère de l’IA, il devrait y avoir plus de temps pour exécuter une gamme plus large de tests. Le défi consiste à trouver la meilleure façon d’orchestrer ces tests sur un portefeuille d’applications qui ne fera que s’élargir à mesure que les outils d’IA permettront aux développeurs d’écrire du code plus rapidement et plus facilement.</p>'}]"
SmartBear applique l'IA générative à l'ensemble de son portefeuille d'outils API,"[{'', ""<p>SmartBear a ajouté des capacités d'intelligence artificielle (IA) générative à son portefeuille d'outils pour la création, le test et la surveillance d'interfaces de programmation d'applications (API).</p>""}, {'', ""<p>Dan Faulkner, directeur des produits et de la technologie chez SmartBear, a déclaré que SmartBear HaloAI s'appuie initialement sur de grands modèles de langage (LLM) d'Open AI. Cependant, l'entreprise prévoit d'utiliser plusieurs LLM de différents fournisseurs à mesure que les cas d'utilisation continuent d'évoluer.</p>""}, {'', ""<p>SmartBear, plutôt que de créer des LLM, concentre ses efforts sur l'utilisation des données qu'il collecte pour exploiter les capacités de plusieurs plateformes d'IA génératives qui coûteraient des millions de dollars à reproduire, a-t-il ajouté.</p>""}, {'', ""<h3>Les capacités de GenAI s'appuient sur les investissements antérieurs dans l'IA</h3>""}, {'', '<p>Disponible en version bêta, SmartBear HaloAI s’appuie sur les investissements antérieurs de l’entreprise en matière d’IA, notamment l’acquisition récente de Reflect, un fournisseur d’un outil d’IA générative pour automatiser la gestion des tests. L’IA générative a déjà été utilisée pour automatiser des cas de test sans avoir à écrire et déboguer les scripts auparavant requis. Une équipe d’assurance qualité (QA) qui devait créer 500 tests manuels d’une durée moyenne de cinq minutes par test a pu exécuter automatiquement ces tests en cinq secondes, économisant ainsi 20 heures de test par cycle de régression.</p>'}, {'', '<p>L’objectif global est de permettre aux équipes DevOps existantes de créer et de tester plus facilement des API à un moment où il existe encore une pénurie de professionnels de l’informatique possédant ces compétences spécifiques, a noté Faulkner.</p>'}, {'', '<p>En général, SmartBear s’oriente vers l’agrégation des outils et des plateformes qu’elle fournit dans une série de centres de solutions qui faciliteront la création, le test, la publication et la surveillance des logiciels, a noté Faulkner. SmartBear HaloAI simplifiera ensuite l’automatisation des processus sur tous les éléments qui composent une solution, a-t-il ajouté.</p>'}, {'', '<p>Il sera particulièrement important de trouver un moyen d’automatiser les tests de code, car à mesure que de plus en plus de développeurs s’appuient sur des outils d’IA à usage général tels que ChatGPT pour écrire du code, la qualité du code écrit est susceptible de diminuer, a noté Faulkner. Les plateformes d’IA générative ont été formées à l’aide de codes de qualité variable provenant d’Internet, de sorte que la qualité du résultat fourni peut varier considérablement, a-t-il noté.</p>'}, {'', '<p>Selon Faulkner, la seule façon de remédier à ce problème sera de tester en continu le code à l’aide de plateformes qui génèrent des tests à l’aide d’exemples validés par le fournisseur d’une plateforme d’automatisation des tests. En effet, SmartBear fournit la « sauce spéciale » nécessaire pour atteindre cet objectif à grande échelle, a-t-il ajouté.</p>'}, {'', '<p>L’adoption de l’IA générative n’en est qu’à ses débuts. Cependant, il ne faudra pas longtemps avant que ces outils soient largement utilisés non seulement pour générer du code, mais aussi pour automatiser les flux de travail DevOps. Le prochain défi majeur consistera à orchestrer tous les assistants IA optimisés pour effectuer des tâches spécifiques, telles que le test des API, qui seront bientôt intégrés à ces flux de travail.</p>'}, {'', '<p>En attendant, les équipes DevOps devront déterminer dans quelle mesure elles peuvent étendre leurs flux de travail existants de manière à intégrer des assistants IA plutôt que de devoir remplacer leurs plateformes existantes. Après tout, le débat ne se limite plus à déterminer les avantages de l’IA, mais s’intéresse également au coût réel de cette transition.</p>'}]"
Intégration de Catchpoint dans les pipelines CI/CD,"[{'', '<p>La proposition de valeur de Catchpoint est simple à comprendre. Elle surveille les performances des sites Web et des applications au-delà de votre propre infrastructure pour fournir une « vue d’ensemble de l’utilisateur ».</p>'}, {'', ""<p>Je connais Catchpoint depuis plusieurs années, car nous en avons déjà parlé ici sur DevOps.com et sur d'autres sites Techstrong. Le deuxième jour du #AppDev Tech Field Day à Santa Clara, le PDG de Catchpoint, Mehdi Daoudi, a présenté l'histoire de Catchpoint, expliquant la différence entre la gestion des performances des applications (APM) classique et ce que fait Catchpoint, qu'il appelle Internet Performance Monitoring (IPM).</p>""}, {'', '<p>Je n’ai pas pu m’empêcher de repenser à l’époque des dot-com. Plusieurs de mes amis – Rajat Bhargava, Tom Higley et Neil Robertson – ont lancé une entreprise appelée Service Metrics. Là encore, le principe était simple : comprendre les performances d’un site Web à partir de différents endroits sur Internet.</p>'}, {'', '<p>Un événement étrange s’est produit avec Service Metrics. Moins de 18 mois après la création de l’entreprise – avant même que le produit ne soit complètement prêt – elle a été rachetée par Exodus Communications, l’un des plus grands fournisseurs de centres de données de l’époque. Dans la folie typique des dot-com, les actions d’Exodus ont été divisées plusieurs fois et ont explosé entre le moment où l’accord a été annoncé et celui où il a été conclu. Ce qui était une acquisition de 280 millions de dollars est devenue une transaction de plus d’un milliard de dollars au moment de la clôture. Telle était la vie à l’époque des dot-com.</p>'}, {'', '<p>C’était il y a plus de 25 ans, mais le principe de base – surveiller les performances d’Internet – est similaire aux objectifs actuels de Catchpoint. Bien sûr, le monde et le réseau d’aujourd’hui sont beaucoup plus complexes. Mais au final, nous voulons tous savoir comment nos sites Web et nos applications fonctionnent pour les utilisateurs finaux.</p>'}, {'', '<p>L’intérêt de Catchpoint réside dans le fait qu’il a atteint un niveau de maturité tel que les services de surveillance peuvent être transmis directement au pipeline CI/CD via des boucles de rétroaction. En utilisant des outils standard du secteur tels qu’Open Telemetry, Catchpoint fonctionne également avec des outils. Ces intégrations et cette ingénierie basée sur des normes permettent à Catchpoint de fonctionner parfaitement avec le reste des outils d’une organisation, de contribuer aux objectifs de niveau de service (SLO) et de tirer parti des investissements informatiques existants.</p>'}, {'', '<p>Ce qui m’a vraiment enthousiasmé, c’est la réponse de Mehdi à ma question sur l’ajout de réponses automatisées aux données surveillées par Catchpoint. L’IA peut vraiment changer la donne. L’entreprise dispose d’environ 15 ans de données de surveillance, ce qui constituerait un filon-mère pour former une IA afin qu’elle puisse ajouter des réponses automatisées et exploitables à ce que Catchpoint considère comme des problèmes.</p>'}, {'', '<p>Daoudi envisage un avenir dans lequel un SRE peut se réveiller, consulter un tableau de bord « sonar » de Catchpoint et recevoir un rapport complet sur les performances sous-optimales observées par Catchpoint et les mesures prises pour y remédier. Et bien sûr, dans ce scénario, les correctifs et les résultats ont été intégrés à la version suivante du code et des tests pour le vérifier avant le déploiement étaient déjà préparés.</p>'}, {'', ""<p>Utopique ? Peut-être. Mais c'est le genre de chose dont tous les professionnels du réseau que je connais ne pouvaient que rêver à l'époque des métriques de service.</p>""}, {'', '<p>Au fait, j’ai adoré ma première expérience en tant que délégué du Tech Field Day et j’attends avec impatience la prochaine. Ce sera peut-être en novembre, à Salt Lake City, autour de la conférence Kubecon North America.</p>'}]"
