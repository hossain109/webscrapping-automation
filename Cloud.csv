Title,Content
Pourquoi GitOps pourrait être l'avenir de DevOps : tendances et prévisions pour 2025 et au-delà,"[{'', '<p>Le DevOps évolue rapidement, avec l’émergence de nouvelles méthodologies et pratiques pour répondre à l’évolution constante du paysage de la distribution de logiciels. Parmi ces innovations, GitOps a suscité une attention considérable en tant que future norme potentielle dans les flux de travail DevOps. Alors que les organisations s’efforcent d’améliorer l’efficacité, la sécurité et l’évolutivité, GitOps offre une approche prometteuse qui s’intègre parfaitement aux principes fondamentaux de DevOps. Cet article explique pourquoi GitOps pourrait être l’avenir de DevOps, en s’appuyant sur les tendances, les prévisions et les informations du secteur pour 2025 et au-delà. En outre, nous explorerons comment les meilleures pratiques de sécurité DevOps sont essentielles à l’adoption et au succès de GitOps.</p>'}, {'', '<h3>Comprendre GitOps : la prochaine évolution de DevOps</h3>'}, {'', ""<p>GitOps est, à la base, un cadre opérationnel qui applique les principes de Git, un système de contrôle de version, pour gérer les déploiements d'infrastructure et d'application. Contrairement au DevOps traditionnel, qui implique souvent la gestion de la configuration et des déploiements via une variété d'outils et d'interfaces, GitOps centralise ce processus dans les référentiels Git. Cette approche permet un modèle déclaratif dans lequel l'état souhaité du système est défini dans Git, et des processus automatisés garantissent en permanence que l'état réel correspond à cet état souhaité.</p>""}, {'', '<p>L’intérêt croissant pour GitOps découle de son potentiel à rationaliser les pipelines de distribution de logiciels, à réduire les erreurs humaines et à améliorer la sécurité grâce au contrôle des versions et aux pistes d’audit. Une enquête récente de la Cloud Native Computing Foundation (CNCF) a révélé que 78 % des personnes interrogées utilisaient ou prévoyaient d’adopter GitOps dans les deux prochaines années. Cette tendance souligne la conviction croissante selon laquelle GitOps pourrait bientôt devenir la norme de facto dans DevOps, en particulier à l’heure où les organisations cherchent à adhérer aux meilleures pratiques de sécurité DevOps en exploitant les capacités inhérentes à Git pour le suivi et la gestion des modifications.</p>'}, {'', '<h3>La convergence de GitOps et DevOps : une intégration transparente</h3>'}, {'', '<p>À mesure que l’écosystème DevOps continue de mûrir, la convergence de GitOps et DevOps devient de plus en plus évidente. Cette intégration est motivée par le besoin de méthodes plus efficaces et plus sûres pour gérer des environnements cloud natifs complexes. Les pratiques DevOps traditionnelles ont longtemps mis l’accent sur l’importance de l’automatisation, de l’intégration continue/du déploiement continu (CI/CD) et de l’infrastructure en tant que code (IaC). GitOps s’appuie sur ces principes en fournissant une approche plus structurée et contrôlée par version pour la gestion des déploiements d’infrastructures et d’applications.</p>'}, {'', '<p>L’un des principaux avantages de GitOps est sa capacité à simplifier la gestion des clusters Kubernetes, qui sont devenus la pierre angulaire des applications cloud natives modernes. Une étude de Red Hat a révélé que 85 % des entreprises utilisent désormais Kubernetes en production et que bon nombre d’entre elles se tournent vers GitOps pour gérer leurs clusters plus efficacement. En utilisant Git comme source unique de vérité, les équipes peuvent automatiser le processus de déploiement, réduire le risque de dérive de configuration et garantir des déploiements cohérents dans tous les environnements.</p>'}, {'', '<p>De plus, l’intégration de GitOps aux workflows DevOps existants est relativement transparente. Des outils comme Flux, Argo CD et Jenkins X permettent aux entreprises d’adopter GitOps sans avoir à remanier l’ensemble de leurs pipelines CI/CD. Cette facilité d’adoption a contribué à la popularité croissante de GitOps, les experts du secteur prédisant qu’il deviendra une pratique standard pour la gestion des applications cloud natives d’ici 2025.</p>'}, {'<h3>Les avantages de GitOps en matière de sécurité : une révolution pour DevOps</h3>', ''}, {'', ""<p>La sécurité est une préoccupation majeure dans tout environnement DevOps, et GitOps offre plusieurs avantages qui s'alignent sur les meilleures pratiques de sécurité DevOps. L'un des principaux avantages de GitOps en matière de sécurité est l'utilisation de Git comme référentiel central pour tous les fichiers de configuration et de déploiement. Cette approche garantit que toutes les modifications sont suivies, auditées et contrôlées par version, fournissant un historique clair de qui a effectué les modifications, quand elles ont été effectuées et pourquoi.</p>""}, {'', ""<p>Dans un environnement DevOps traditionnel, les fichiers de configuration et les scripts peuvent être dispersés sur plusieurs systèmes et gérés par différentes équipes, ce qui rend difficile l'application de politiques de sécurité cohérentes. GitOps relève ce défi en consolidant toute la gestion de la configuration dans un référentiel Git unique. Cela simplifie non seulement la gestion des politiques de sécurité, mais permet également aux organisations de mettre en œuvre des contrôles de sécurité automatisés dans le cadre de leurs pipelines CI/CD.</p>""}, {'', '<p>Selon un rapport de Gartner, 99 % des défaillances de sécurité du cloud d’ici 2025 seront imputables au client en raison de mauvaises configurations et de contrôles inadéquats. En adoptant GitOps, les entreprises peuvent atténuer ce risque en s’assurant que toutes les configurations d’infrastructure sont stockées dans Git et soumises à des processus de vérification rigoureux. De plus, GitOps permet aux entreprises d’automatiser l’application des politiques de sécurité, réduisant ainsi le risque d’erreur humaine et améliorant la posture de sécurité globale.</p>'}, {'', '<p>Un autre avantage de GitOps en matière de sécurité est sa prise en charge des infrastructures immuables, où les systèmes sont remplacés plutôt que modifiés sur place. Cette approche réduit la surface d’attaque et contribue à maintenir un environnement cohérent et sécurisé. Alors que les organisations continuent de donner la priorité à la sécurité dans leurs pratiques DevOps, l’adoption de GitOps est susceptible de s’accélérer, les experts prédisant qu’il deviendra une pierre angulaire des meilleures pratiques de sécurité DevOps d’ici 2025.</p>'}, {'', ""<h3>Tendances favorisant l'adoption de GitOps</h3>""}, {'', '<p>Plusieurs tendances clés favorisent l’adoption de GitOps, le positionnant comme un futur standard potentiel dans DevOps. L’une des tendances les plus significatives est la croissance continue des applications cloud natives et des architectures de microservices. À mesure que les organisations évoluent vers des systèmes plus complexes et distribués, le besoin de stratégies de déploiement robustes et évolutives devient primordial. GitOps, avec son accent sur l’automatisation et le contrôle des versions, est bien adapté pour répondre aux exigences de ces architectures modernes.</p>'}, {'', '<p>Une autre tendance qui contribue à l’essor de GitOps est l’importance croissante accordée à la productivité et à la collaboration des développeurs. Dans une enquête menée par Puppet, 62 % des personnes interrogées ont indiqué que l’amélioration de la productivité des développeurs était une priorité absolue pour leur organisation. GitOps soutient cet objectif en permettant aux développeurs de gérer les déploiements d’infrastructure et d’applications à l’aide d’outils et de flux de travail familiers, tels que Git. Cela réduit la courbe d’apprentissage et permet aux équipes de collaborer plus efficacement, ce qui conduit à des déploiements plus rapides et plus fiables.</p>'}, {'', '<p>L’essor des environnements hybrides et multicloud favorise également l’adoption de GitOps. À mesure que les entreprises répartissent leurs charges de travail entre plusieurs fournisseurs de cloud, la gestion des configurations et des déploiements devient de plus en plus complexe. GitOps offre une approche unifiée de la gestion de ces environnements, garantissant la cohérence et réduisant le risque de mauvaises configurations. Un rapport de Flexera a révélé que 92 % des entreprises ont une stratégie multicloud, ce qui souligne encore davantage la nécessité de solutions comme GitOps pour gérer efficacement ces environnements.</p>'}, {'', '<p>Enfin, l’importance croissante de la conformité et de l’auditabilité dans les pratiques DevOps suscite l’intérêt pour GitOps. Avec des réglementations telles que le RGPD et la loi HIPAA imposant des exigences strictes en matière de traitement et de sécurité des données, les organisations doivent s’assurer que leurs pratiques DevOps sont conformes. GitOps, qui met l’accent sur le contrôle des versions et les pistes d’audit, fournit un cadre solide pour répondre à ces exigences, ce qui en fait une option attrayante pour les organisations des secteurs réglementés.</p>'}, {'', '<h3>Prévisions pour 2025 et au-delà : GitOps comme nouvelle norme</h3>'}, {'', '<p>À l’horizon 2025 et au-delà, plusieurs prévisions suggèrent que GitOps pourrait devenir la nouvelle norme en matière de DevOps. L’une des prévisions les plus convaincantes est que GitOps sera le moteur de la prochaine vague d’automatisation dans DevOps. Avec la complexité croissante des applications cloud natives, les processus manuels et les configurations ad hoc deviendront intenables. GitOps, avec son accent sur l’automatisation et les configurations déclaratives, est bien placé pour mener cette transition vers des pipelines de déploiement entièrement automatisés.</p>'}, {'', '<p>Les analystes du secteur prévoient également que GitOps jouera un rôle essentiel dans l’évolution continue de DevSecOps, où la sécurité est intégrée à chaque étape du processus de développement. À mesure que les organisations accordent la priorité à la sécurité et à la conformité, la nature contrôlée par version et vérifiable de GitOps deviendra indispensable. Cela conduira probablement à une adoption plus large de GitOps à mesure que les organisations chercheront à mettre en œuvre les meilleures pratiques de sécurité DevOps plus efficacement.</p>'}, {'', '<p>Une autre prédiction pour 2025 est que GitOps deviendra l’approche par défaut pour la gestion des déploiements Kubernetes. Alors que Kubernetes continue de dominer le paysage cloud natif, le besoin de stratégies de déploiement évolutives et fiables ne fera que croître. GitOps, avec sa forte adéquation avec la nature déclarative de Kubernetes, devrait devenir la méthode préférée pour la gestion des clusters Kubernetes. Un rapport de la CNCF prédit que d’ici 2025, plus de 90 % des déploiements Kubernetes seront gérés à l’aide de GitOps.</p>'}, {'', '<p>En outre, l’essor de l’informatique de pointe et de l’IoT favorisera l’adoption de GitOps dans les cas d’utilisation nouveaux et émergents. À mesure que les entreprises déploient des applications et des services en périphérie, la gestion de ces environnements distribués nécessitera une nouvelle approche de DevOps. GitOps, qui met l’accent sur l’automatisation et la cohérence, est bien adapté pour relever les défis de l’informatique de pointe. D’ici 2025, nous pouvons nous attendre à ce que GitOps joue un rôle clé dans la gestion des déploiements en périphérie et dans l’IoT.</p>'}, {'', '<p>Enfin, la disponibilité croissante des outils et plateformes GitOps favorisera une adoption plus large dans tous les secteurs. À mesure que l’écosystème GitOps continue de mûrir, nous pouvons nous attendre à davantage d’outils et d’intégrations qui faciliteront l’adoption de GitOps par les organisations. Ceci, combiné à la prise de conscience croissante des avantages de GitOps, conduira probablement à une adoption généralisée dans tous les secteurs d’ici 2025 et au-delà.</p>'}, {'', '<h3>Défis et considérations : le chemin vers une adoption généralisée de GitOps</h3>'}, {'', '<p>Si l’avenir de GitOps semble prometteur, les entreprises doivent relever plusieurs défis et prendre en compte plusieurs considérations pour garantir une adoption réussie. L’un des principaux défis est le changement culturel nécessaire à l’adoption de GitOps. Le passage à un modèle GitOps implique l’adoption d’une approche déclarative de la gestion de l’infrastructure et des applications, qui peut être inconnue de ceux qui sont habitués aux pratiques DevOps plus traditionnelles. Les entreprises doivent investir dans la formation et l’éducation pour aider les équipes à effectuer cette transition de manière efficace.</p>'}, {'', '<p>Un autre défi consiste à gérer GitOps à grande échelle. Bien qu’il offre de nombreux avantages, il peut également introduire de la complexité, en particulier dans les environnements de grande taille et multi-équipes. Les organisations doivent développer des stratégies pour gérer cette complexité, comme la mise en œuvre de flux de travail standardisés et de meilleures pratiques pour GitOps. De plus, l’intégration de GitOps aux pipelines et outils CI/CD existants peut s’avérer complexe et nécessiter une planification et une exécution minutieuses.</p>'}, {'', '<p>La sécurité est également un élément essentiel à prendre en compte lors de l’adoption de GitOps. Bien que GitOps offre plusieurs avantages en matière de sécurité, il introduit également de nouveaux risques, tels que le risque de modifications non autorisées des référentiels Git. Les organisations doivent mettre en œuvre des contrôles de sécurité robustes, tels que le contrôle d’accès et le chiffrement, pour atténuer ces risques. En outre, l’utilisation de contrôles et de politiques de sécurité automatisés serait essentielle pour garantir que les déploiements GitOps respectent les meilleures pratiques de sécurité DevOps.</p>'}, {'', '<p>Enfin, les organisations doivent tenir compte de la durabilité à long terme de l’adoption de GitOps. Comme pour toute nouvelle méthodologie, il existe un risque de lassitude face à l’adoption, où les équipes peuvent avoir du mal à maintenir la discipline et la cohérence nécessaires au fil du temps. Pour y remédier, les organisations doivent établir des objectifs et des mesures clairs pour l’adoption de GitOps, ainsi que fournir un soutien et des ressources continus pour aider les équipes à réussir.</p>'}, {'', ""<h3>Conclusion : embrasser l'avenir avec GitOps</h3>""}, {'', ""<p>GitOps représente une évolution significative dans le monde de DevOps, offrant une approche plus structurée, sécurisée et automatisée de la gestion des déploiements d'infrastructures et d'applications. Alors que les entreprises continuent de naviguer dans les complexités des environnements cloud natifs, le potentiel de GitOps pour devenir la méthodologie dominante dans DevOps est évident. Soutenu par les tendances du secteur et les prévisions des experts, il est sur le point de jouer un rôle essentiel dans l'avenir de la distribution de logiciels.</p>""}, {'', '<p>Cependant, l’adoption généralisée de GitOps n’est pas sans difficultés. Les entreprises doivent être prêtes à investir dans la formation, à développer des stratégies de gestion de la complexité et à mettre en œuvre des contrôles de sécurité robustes pour garantir une adoption réussie. En adoptant GitOps et en l’intégrant aux pratiques DevOps existantes, les entreprises peuvent se positionner pour réussir dans les années à venir, en tirant parti de sa puissance pour stimuler l’innovation, améliorer la sécurité et accroître l’efficacité de leurs pipelines de livraison de logiciels.</p>'}, {'', '<p>À l’horizon 2025 et au-delà, l’avenir de GitOps s’annonce prometteur. Grâce à son alignement solide avec les principes fondamentaux de DevOps, à sa capacité à améliorer la sécurité et la conformité et à son potentiel à conduire la prochaine vague d’automatisation, GitOps est en passe de devenir la nouvelle norme de DevOps. Les organisations qui adoptent GitOps aujourd’hui seront bien placées pour ouvrir la voie à l’avenir de la distribution de logiciels, en capitalisant sur les tendances et les opportunités qui se présentent. Alors que nous entrons dans cette nouvelle ère de distribution de logiciels, l’adoption de GitOps aidera non seulement les organisations à rester compétitives, mais favorisera également une culture d’amélioration continue et d’innovation. Le passage à GitOps est plus qu’une simple tendance ; il s’agit d’une évolution stratégique vers un avenir DevOps plus résilient, plus sûr et plus efficace.</p>'}]"
OpenTelemetry n’est pas le héros dont nous avons besoin : voici pourquoi il fait défaut à notre pile,"[{'', '<p>OpenTelemetry promettait d’être une norme unifiée qui permettrait à chacun de collecter et de corréler plus facilement les traces, les journaux et les mesures des systèmes distribués. Cela ressemble à un rêve, n’est-ce pas ? Eh bien, voici la dure vérité : OpenTelemetry n’est que satisfaisant. Lorsque je le compare à des technologies de traçage plus approfondies telles que eBPF, OpenTelemetry semble gonflé, inefficace, incomplet et dépourvu d’un élément clé du traçage. Et cela est en grande partie dû au détournement par les entreprises de ce qui aurait dû être un projet léger et axé sur la communauté.</p>'}, {'', '<p>Plongeons dans les raisons pour lesquelles eBPF et OpenTelemetry ont leur place dans le monde, mais aussi pourquoi je pense qu’OpenTelemetry n’est qu’un acteur de l’équipe de support et qu’eBPF est le véritable MVP.</p>'}, {'', '<h3>OpenTelemetry : le couteau suisse qui en fait trop</h3>'}, {'', ""<p>OpenTelemetry est comme un couteau suisse que j'ai reçu pour mon anniversaire : il propose des tonnes d'outils, mais tous sont médiocres. Il essaie de tout faire : journaux, métriques et traces, avec des dizaines d'intégrations. Il fournit un moyen standardisé et indépendant des fournisseurs de collecter des données d'observabilité. Dans un monde rempli d'outils fragmentés et de solutions de surveillance, c'est précieux. Mais sa vaste portée a un prix : il est gonflé, lent et inefficace.</p>""}, {'', '<p>Alors, pourquoi OpenTelemetry est-il gonflé ?</p>'}, {'', '<h4>1. Co-optation des entreprises et prolifération des fonctionnalités</h4>'}, {'', '<p>OpenTelemetry a débuté comme un projet open source avec une vision claire et ciblée. Mais, comme de nombreux projets open source qui attirent l’attention, les grandes entreprises ont vu une opportunité. Elles se sont lancées dans l’aventure, en apportant des ressources et des fonctionnalités, mais pas nécessairement par altruisme. Ces entreprises ont leurs propres objectifs, ajoutant des fonctionnalités pour s’assurer que leurs plateformes, leurs services cloud et leurs outils propriétaires soient couverts. Alors que de plus en plus d’entreprises se lancent, OpenTelemetry doit s’étendre davantage pour répondre aux besoins de tous.</p>'}, {'', '<p>Ce qui a commencé comme une solution simple et élégante est maintenant devenu une bête gonflée. Nous nous sommes retrouvés avec un outil qui comporte trop de boutons, d’options et de complexité. Chaque entreprise veut que ses exigences spécifiques soient intégrées au cœur du projet, ce qui conduit à une prolifération de fonctionnalités qui ralentit les performances d’OpenTelemetry. Plus nous ajoutons de fonctionnalités, plus nous diluons la concentration et l’efficacité de l’outil.</p>'}, {'', '<p>J’ai suffisamment d’expérience pour le constater dans la nature : l’inefficacité totale du collecteur OTel entraîne une quantité déraisonnable et absurde de mise à l’échelle, de ressources et de capacités juste pour suivre un environnement marginalement chargé. Cela devient impraticable et ingérable à toute échelle raisonnable !</p>'}, {'', ""<h4>2. Inefficacité : essayer d'être tout pour tout le monde</h4>""}, {'', ""<p>OpenTelemetry s'efforce de tout couvrir, mais n'excelle dans aucun domaine. Lorsque je trace une requête à l'aide d'OpenTelemetry, je constate une baisse des performances qui me fait me demander si cela en vaut la peine. Bien qu'il soit excellent à un niveau élevé (tracer le flux d'une application ou montrer où se trouvent les goulots d'étranglement), il manque de précision et de profondeur dont vous pourriez avoir besoin pour creuser dans les moindres détails.</p>""}, {'', '<p>Cette inefficacité découle directement du mandat étendu d’OpenTelemetry. Il ne s’agit pas seulement de capturer des traces, mais aussi de jongler avec les métriques et les journaux, tout en s’intégrant à des centaines d’autres outils et systèmes. La surcharge devient plus perceptible à grande échelle, ce qui rend difficile sa recommandation pour les environnements critiques en termes de performances. Lorsque vous avez besoin d’informations précises en temps réel, OpenTelemetry peut donner l’impression d’être une ancre traînant derrière l’application.</p>'}, {'', '<h4>3. Incomplet : juste « assez bon » pour que les entreprises puissent en tirer profit</h4>'}, {'', '<p>Le hic, c’est que malgré tout ce surcroît de travail et cette inefficacité, OpenTelemetry semble encore incomplet. En effet, les grandes entreprises sont davantage intéressées par la création d’un produit open source « suffisamment bon » pour nous attirer, mais pas suffisamment bon pour répondre à tous nos besoins. Si vous souhaitez bénéficier d’un package d’observabilité complet (performances optimisées, analyses avancées, mise à l’échelle fluide), vous serez poussé vers leurs outils et services propriétaires. C’est un classique du leurre.</p>'}, {'', '<p>En d’autres termes, OpenTelemetry est suffisamment fonctionnel pour une observabilité de base, mais lorsque les choses deviennent complexes, vous êtes souvent poussé vers des modules complémentaires d’entreprise premium. Ce n’est pas une coïncidence – cela fait partie de la stratégie de l’entreprise. Ils contribuent juste assez au projet open source pour le rendre largement adopté, mais réservent les meilleures fonctionnalités à leurs offres payantes.</p>'}, {'', ""<h3>eBPF : l'outil de traçage de bas niveau qui fait encore mieux</h3>""}, {'', '<p>Quelle est l’alternative à la médiocrité pléthorique d’OpenTelemetry ? eBPF, le héros dont nous ignorions avoir besoin. Alors qu’OpenTelemetry opère à un niveau élevé (instrumentation des applications et des services), eBPF (extended Berkeley packet filter) fonctionne au niveau du noyau. C’est la recette secrète d’une observabilité en temps réel et à faible surcharge, générant des informations directement à partir du système d’exploitation. Par exemple, si vous souhaitez savoir exactement pourquoi la latence de votre réseau augmente ou quel processus est à l’origine d’un goulot d’étranglement des performances, eBPF est là pour vous.</p>'}, {'', '<p>Voici pourquoi eBPF est le véritable MVP.</p>'}, {'', '<h4>1. Léger et rapide</h4>'}, {'', ""<p>Contrairement à OpenTelemetry, qui peut paraître lourd et lent, eBPF est léger et incroyablement efficace. Il ne tente pas de tout faire : il génère uniquement des données brutes en temps réel à partir du noyau. eBPF vous permet d'observer et de manipuler le comportement du système de bas niveau, des opérations d'E/S au trafic réseau, sans surcharge significative.</p>""}, {'', '<p>Dans les environnements où les performances sont importantes, eBPF est un outil incontournable. Il offre une visibilité approfondie sur le comportement du système sans le fardeau des couches supplémentaires et de la complexité qui accompagnent OpenTelemetry.</p>'}, {'', '<h4>2. Informations détaillées et approfondies</h4>'}, {'', '<p>eBPF offre une visibilité qu’OpenTelemetry ne peut égaler. Alors qu’OpenTelemetry est idéal pour tracer les requêtes entre les services, eBPF fournit des informations au niveau du noyau. Si vous avez besoin de savoir pourquoi votre application consomme trop de CPU ou pourquoi les opérations d’E/S ralentissent, eBPF vous permet de voir les événements système exacts qui sont à l’origine du problème. C’est la différence entre écouter les prévisions météorologiques et pouvoir mesurer la pression atmosphérique, la vitesse du vent et l’humidité dans le jardin.</p>'}, {'', ""<p>Alors qu'OpenTelemetry nous offre une vue à 10 000 pieds, eBPF nous permet de zoomer au niveau moléculaire.</p>""}, {'', '<h4>3. Pas encore coopté par les entreprises</h4>'}, {'', '<p>eBPF reste un outil relativement spécialisé, ce qui signifie qu’il n’a pas encore été adopté par les entreprises comme l’a été OpenTelemetry. Il s’agit d’une technologie puissante, au niveau du noyau, qui n’a pas encore été entachée par les agendas des entreprises. Il fait toujours ce pour quoi il a été conçu : fournir un aperçu approfondi et granulaire du système avec un minimum de frais généraux. Il n’a pas encore été gonflé de fonctionnalités inutiles et n’essaie pas de nous orienter vers une solution payante.</p>'}, {'', ""<h3>OpenTelemetry est excellent pour une vue d'ensemble, mais eBPF ouvre les profondeurs</h3>""}, {'', ""<p>En fin de compte, OpenTelemetry et eBPF ont leur place dans la pile d'observabilité. OpenTelemetry, avec son adoption généralisée et son approche standardisée, excelle dans la fourniture d'une visibilité de haut niveau sur les systèmes distribués. C'est la solution de référence pour le traçage distribué, offrant une vue cohérente de la façon dont les services interagissent dans des environnements complexes. Pour quiconque essaie de donner un sens à une architecture multiservices, OpenTelemetry fournit une carte indispensable.</p>""}, {'', ""<p>Cependant, lorsqu'il s'agit d'obtenir des informations plus approfondies et plus granulaires, c'est-à-dire de comprendre ce qui se passe dans le système au niveau du noyau, eBPF est un outil indispensable. Sa force réside dans sa capacité à capturer des données de bas niveau avec une surcharge minimale, offrant une visibilité brute et détaillée sur les performances et le comportement du système. Il est parfait pour analyser les goulots d'étranglement des performances, les conflits de ressources ou les problèmes de réseau qui pourraient ne pas être visibles via la télémétrie de haut niveau.</p>""}, {'', '<p>Plutôt que de considérer ces outils comme des solutions concurrentes, considérez-les comme complémentaires. OpenTelemetry vous offre une vue d’ensemble, tandis qu’eBPF vous permet de zoomer lorsque vous avez besoin de résoudre un problème avec précision. Si vous êtes sérieux au sujet de l’observabilité, la combinaison de la facilité d’utilisation d’OpenTelemetry et du traçage distribué standardisé avec la puissance et la profondeur d’eBPF sera la clé d’une compréhension complète de votre système.</p>'}, {'', ""<p>On se retrouve dans l'espace noyau !</p>""}]"
SBOM comme pierre angulaire du développement de logiciels sécurisés,"[{'', ""<p>Dans le monde en constante évolution du développement logiciel, la sécurité est devenue plus critique que jamais. Les applications sont désormais plus complexes et interconnectées, ce qui signifie qu'il existe de nombreux points d'entrée potentiels pour les vulnérabilités. Ces faiblesses peuvent être cachées au plus profond du logiciel, passant souvent inaperçues jusqu'à ce qu'elles causent un problème important. C'est là qu'entre en jeu la nomenclature logicielle (SBOM). Une SBOM agit comme un inventaire détaillé de tous les composants, bibliothèques et modules utilisés pour créer une application logicielle, offrant une transparence inégalée dans la chaîne d'approvisionnement logicielle.</p>""}, {'', '<p>Imaginez une SBOM comme une liste complète des ingrédients d’une application logicielle. Tout comme une liste qui répertorie tous les ingrédients d’une recette, une SBOM inclut tous les composants impliqués dans le logiciel, allant des bibliothèques tierces et des modules open source au code propriétaire. Par exemple, si une application logicielle utilise une bibliothèque open source populaire comme OpenSSL, la SBOM la répertorie avec sa version spécifique. Ce niveau de détail est similaire à une nomenclature dans la fabrication, qui détaille toutes les pièces nécessaires à la fabrication d’un produit physique, comme une automobile ou un smartphone. De la même manière, une SBOM fournit une vue claire de ce qui constitue le logiciel, garantissant que chaque composant est pris en compte et correctement géré.</p>'}, {'', '<h3>Pourquoi les SBOM sont essentiels pour la sécurité des logiciels</h3>'}, {'', '<p>L’un des principaux avantages d’un SBOM est la transparence qu’il offre. En répertoriant tous les composants, les entreprises peuvent retracer chaque partie de leur logiciel jusqu’à son origine. Cela garantit une connaissance précise des composants du logiciel et de son origine, qu’il s’agisse d’un module propriétaire développé en interne ou d’une bibliothèque externe provenant d’un référentiel public. Par exemple, imaginez découvrir qu’une version particulière d’une bibliothèque utilisée dans votre logiciel présente une vulnérabilité de sécurité. Avec un SBOM, il est facile d’identifier les applications concernées, ce qui permet une réponse rapide. Cette traçabilité est essentielle dans la gestion de la sécurité des logiciels, en particulier lorsque des vulnérabilités sont découvertes après le déploiement.</p>'}, {'', ""<p>La gestion des vulnérabilités logicielles est un défi permanent qui nécessite une surveillance et des mises à jour continues. Un SBOM aide les entreprises à suivre tous les composants utilisés dans leurs logiciels, ce qui facilite la détection des problèmes potentiels avant qu'ils ne deviennent graves. Par exemple, si une vulnérabilité critique est découverte dans une bibliothèque open source largement utilisée, le fait de disposer d'un SBOM permet aux équipes de sécurité d'identifier rapidement les composants logiciels concernés et de prendre des mesures correctives, telles que l'application de correctifs ou de mises à jour. Cette approche proactive de la gestion des vulnérabilités permet de prévenir les failles de sécurité et de réduire le risque d'exploitation.</p>""}, {'', '<p>Il ne s’agit peut-être pas seulement de trouver et de corriger les vulnérabilités. Un SBOM peut changer la donne en cas de problème. Il fournit une carte détaillée des composants du logiciel et de leurs versions, ce qui permet aux équipes de sécurité d’identifier, d’isoler et de réparer rapidement les parties affectées. Cela accélère non seulement la réponse aux incidents, mais minimise également les dommages potentiels. De plus, l’intégration des pratiques SBOM dans le cycle de vie du développement logiciel encourage une culture de sécurité et de responsabilité. Les développeurs deviennent plus conscients des composants qu’ils utilisent et des risques associés, ce qui se traduit par des logiciels plus sûrs et plus robustes. Cette prise de conscience accrue est inestimable pour créer un état d’esprit axé sur la sécurité au sein des équipes de développement.</p>'}, {'', '<h3>Comment mettre en œuvre les pratiques SBOM dans votre organisation</h3>'}, {'', '<p>La mise en œuvre d’un SBOM dans votre organisation peut sembler intimidante, mais ce n’est pas forcément le cas. Plusieurs outils et stratégies peuvent contribuer à rationaliser ce processus. Les générateurs SBOM automatisés, par exemple, analysent la base de code et les dépendances du logiciel pour générer automatiquement un SBOM. Des outils comme CycloneDX ou SPDX peuvent analyser les fichiers d’un projet et compiler une liste complète de toutes les dépendances et de leurs versions, ce qui facilite la mise à jour d’un inventaire.</p>'}, {'', ""<p>En plus des générateurs automatisés, les scanners de vulnérabilité jouent un rôle crucial dans le maintien d'un SBOM efficace. Ces outils analysent les composants à la recherche de vulnérabilités connues et mettent à jour le SBOM si nécessaire. Un outil comme OWASP Dependency-Check peut vérifier régulièrement les vulnérabilités et garantir que le SBOM est mis à jour pour refléter les changements. Cette surveillance continue est essentielle pour maintenir la sécurité des logiciels et traiter rapidement toute vulnérabilité nouvellement détectée.</p>""}, {'', ""<p>Les gestionnaires de dépendances sont un autre outil essentiel pour gérer un SBOM. Ils aident à gérer les dépendances logicielles et à maintenir le SBOM à jour. Des outils comme Maven ou npm gèrent non seulement les dépendances, mais peuvent également aider à automatiser la mise à jour d'un SBOM chaque fois qu'un nouveau composant est ajouté ou mis à jour. Cette automatisation réduit la charge de travail manuelle des équipes de développement et garantit que le SBOM reste précis et à jour.</p>""}, {'', ""<p>Pour mettre en œuvre efficacement un SBOM dans votre organisation, il est important de l'intégrer au cycle de vie du développement logiciel (SDLC). Cela implique de mettre à jour régulièrement le SBOM pour refléter les changements apportés au logiciel, comme l'ajout de nouvelles bibliothèques ou la mise à jour des bibliothèques existantes. Il est également essentiel de garantir l'exactitude des informations fournies par les fournisseurs. Tous les fournisseurs tiers doivent fournir des informations précises et à jour sur leurs composants, y compris sur les vulnérabilités connues. Il est également essentiel d'éduquer et de former les développeurs et les principales parties prenantes sur l'importance de maintenir un SBOM précis et son rôle dans la sécurisation de la chaîne d'approvisionnement logicielle pour une mise en œuvre réussie.</p>""}, {'', '<p>L’un des plus grands défis des SBOM est de les maintenir à jour. Les composants logiciels sont fréquemment mis à jour et de nouvelles vulnérabilités sont constamment découvertes. Pour résoudre ce problème, les organisations doivent automatiser la génération des SBOM dans la mesure du possible. L’utilisation d’outils qui génèrent et mettent à jour automatiquement les SBOM dans le cadre du processus de développement peut faire gagner du temps et réduire le risque d’erreur humaine. La mise en œuvre d’une surveillance continue des nouvelles vulnérabilités et la mise à jour régulière du SBOM en conséquence constituent une autre bonne pratique qui peut contribuer à sécuriser votre logiciel.</p>'}, {'', ""<p>Des audits réguliers du SBOM sont également importants pour garantir qu'il reste précis et à jour. Cela implique de réviser périodiquement le SBOM pour s'assurer que tous les composants sont pris en compte et que toutes les modifications ou mises à jour ont été correctement documentées. En restant vigilantes et proactives, les organisations peuvent maintenir un SBOM efficace et mieux protéger leurs logiciels contre les menaces potentielles.</p>""}, {'', ""<h3>L'avenir des SBOM dans le développement et la sécurité des logiciels</h3>""}, {'', '<p>La sécurité de la chaîne d’approvisionnement des logiciels devenant de plus en plus importante, l’adoption des SBOM devrait augmenter. Plusieurs tendances clés sont à l’origine de cette croissance. Les exigences réglementaires en sont une, les gouvernements et les organismes de réglementation commençant à imposer l’utilisation des SBOM dans des secteurs critiques comme la santé et la finance pour garantir une sécurité et une responsabilité accrues. Les efforts de normalisation facilitent également l’adoption des pratiques SBOM par les organisations. Des organisations comme le National Institute of Standards and Technology (NIST) travaillent à la normalisation des formats et du contenu des SBOM, simplifiant ainsi le processus d’adoption pour les entreprises.</p>'}, {'', '<p>L’intégration avec les workflows DevOps est une autre tendance qui facilite l’utilisation des SBOM. En intégrant les pratiques SBOM aux workflows DevOps, les organisations peuvent faciliter la gestion continue et automatisée des SBOM, améliorant ainsi la sécurité globale. Cette intégration garantit que la sécurité est prise en compte à chaque étape du processus de développement, réduisant ainsi le risque de vulnérabilités et facilitant la gestion et la maintenance d’un SBOM.</p>'}, {'', '<h3>Conclusion</h3>'}, {'', ""<p>Un SBOM est un outil puissant pour améliorer la sécurité des applications. En offrant une transparence sur la chaîne d'approvisionnement des logiciels, un SBOM aide les organisations à identifier et à gérer les vulnérabilités des composants tiers et open source. La mise en œuvre des pratiques SBOM peut favoriser une culture de sécurité et de responsabilité, améliorer la réponse aux incidents et garantir une intégration sécurisée. Bien que des défis existent, les meilleures pratiques et les outils automatisés peuvent aider les organisations à gérer et à maintenir efficacement les SBOM. À mesure que les exigences réglementaires et les efforts de normalisation progressent, les SBOM deviendront un élément essentiel des stratégies de développement et de sécurité des logiciels, garantissant la résilience et la sécurité des applications dans un paysage numérique de plus en plus complexe.</p>""}]"
CortexClick exploite l'IA générative pour automatiser la documentation logicielle,"[{'', ""<p>CortexClick a lancé aujourd'hui une plateforme de génération de contenu basée sur de grands modèles de langage (LLM) qui ont été spécifiquement formés pour créer de la documentation, des tutoriels et des articles de blog techniques, y compris des captures d'écran, pour les organisations créant des logiciels.</p>""}, {'', ""<p>Le PDG de la société, Evan Boyle, a déclaré que l'objectif général est de permettre aux organisations de créer plus facilement de la documentation à l'aide d'agents d'IA formés à l'aide de plusieurs modèles capables de comprendre les fonctions du code sous-jacent qui a été créé.</p>""}, {'', '<p>Bien que les outils d’écriture d’IA ne manquent pas, la plateforme SaaS (Software-as-a-Service) créée par CortexClick va encore plus loin en donnant accès à plusieurs agents qui remettent en question le résultat créé, a déclaré Boyle. En effet, un agent agit comme un éditeur qui remet en question la véracité du contenu créé par un autre agent, a-t-il noté.</p>'}, {'', ""<p>De plus, CortexClick a fourni une intégration avec le référentiel de logiciels GitHub largement utilisé en plus de mettre à disposition une interface de programmation d'applications de développeur (API) et un kit de développement logiciel (SDK) pour simplifier l'intégration d'autres plates-formes.</p>""}, {'', ""<p>En plus de fournir un accès à des outils de gouvernance, l'entreprise permet d'exiger que les résultats générés respectent des directives spécifiques, telles que la voix de marque de l'organisation ou d'autres règles définies par les organisations, a déclaré Boyle.</p>""}, {'', '<p>Peu de développeurs et les équipes DevOps qui les soutiennent aiment créer de la documentation. L’un des problèmes qui complique souvent le support technique est le manque de documentation disponible expliquant le fonctionnement d’une application. En l’absence de cette documentation, les équipes informatiques sont obligées de créer des tickets de support auxquels les développeurs finissent inévitablement par devoir répondre. Chaque minute passée à répondre à ces demandes représente, bien sûr, un temps en moins qui aurait pu être consacré à l’écriture de code.</p>'}, {'', '<p>La plateforme CortexClick permet de démarrer le processus de documentation en générant du contenu qu’ils peuvent modifier à leur guise plutôt que de demander aux développeurs de partir d’une page blanche, a déclaré Boyle. C’est essentiel car de nombreux développeurs ne sont pas des rédacteurs naturellement doués qui aiment particulièrement créer de grandes quantités de contenu, a-t-il ajouté. Dans de nombreux cas, le niveau actuel de documentation fourni par les équipes de développement de logiciels est si limité qu’il est presque inutile.</p>'}, {'', '<p>Parallèlement, les équipes marketing qui n’ont pas forcément beaucoup d’expertise technique peuvent utiliser la plateforme pour créer également du contenu technique que les équipes de développement pourront examiner plus facilement, a noté Boyle. Ce contenu est essentiel dans un monde où la plupart des ventes de logiciels sont générées par l’engagement en ligne, a-t-il ajouté.</p>'}, {''}, {'', '<p>On ne sait pas exactement combien d’équipes de développement d’applications utilisent déjà des outils d’IA pour générer de la documentation. Le problème est que la plupart de ces plateformes à usage général n’ont qu’une compréhension limitée, voire inexistante, du jargon technique.</p>'}, {'', '<p>Bien entendu, la possibilité de générer davantage de documentation ne la rend pas automatiquement utile. Les développeurs devront toujours examiner attentivement la documentation avant de la partager avec les organisations qui utilisent leurs logiciels. Cependant, avec un peu de chance, le nombre d’appels au support technique que les équipes DevOps finissent par recevoir devrait diminuer à mesure que la documentation devient plus accessible.</p>'}, {'', '<p>Entre-temps, il est devenu évident que la documentation est devenue l’un des meilleurs cas d’utilisation de l’IA générative. Le défi consiste désormais à inciter les développeurs à se concentrer davantage sur la fourniture d’une meilleure documentation à l’aide d’outils d’IA qui devraient réduire le niveau de pénibilité bien trop élevé associé à l’exécution de cette tâche aujourd’hui.</p>'}]"
Gearset acquiert Clayton pour ajouter Salesforce Code Analytics aux workflows DevOps,"[{'', ""<p>Gearset a révélé cette semaine avoir acquis Clayton, un fournisseur d'une plate-forme d'analyse de code pour les applications logicielles en tant que service (SaaS) exécutées sur le service cloud Salesforce.</p>""}, {'', '<p>La plateforme Clayton identifie les anti-modèles et les vulnérabilités dans le cycle de vie de développement logiciel des applications personnalisées qui étendent les fonctionnalités de base fournies par les applications Salesforce.</p>'}, {'', ""<p>Le PDG de Gearset, Kevin Boyle, a déclaré que l'ajout de Clayton à la plateforme DevOps fournie par Gearset pour créer et déployer ces applications permettra aux organisations de déployer plus facilement des applications sécurisées sans impacter négativement la vitesse à laquelle elles sont créées et déployées.</p>""}, {'', ""<p>Clayton a déjà conclu une alliance avec Gearset et continuera à fonctionner comme une unité commerciale. La société fusionnée travaillera en même temps à renforcer l'intégration entre les deux plateformes, a déclaré Boyle. Les conditions financières de l'accord n'ont pas été divulguées.</p>""}, {'', ""<p>En général, une récente enquête de Gearset révèle que plus de la moitié des entreprises qui créent des applications personnalisées sur la plateforme Salesforce ont adopté des plateformes d'intégration continue/déploiement continu (CI/CD), et que 28 % d'entre elles prévoient de les adopter. L'enquête révèle également que 59 % des répondants travaillent pour des entreprises qui ont déjà adopté ou prévoient d'adopter le contrôle de version, tandis que 54 % ont mis en œuvre des tests automatisés.</p>""}, {'', ""<p>Globalement, plus de la moitié des répondants (54 %) ont déclaré que les équipes DevOps avaient amélioré la qualité des versions, tandis que 46 % ont signalé une amélioration de la collaboration. Cependant, seuls 38 % et 33 % ont déclaré que des versions plus fréquentes ou des délais de livraison réduits, respectivement, étaient un facteur justifiant leur retour sur investissement (ROI) dans les workflows DevOps. En effet, les organisations signalent qu'elles sont beaucoup plus préoccupées par la qualité des applications que par la vitesse à laquelle cet objectif est atteint.</p>""}, {'', '<p>Il est plus simple d’atteindre cet objectif en utilisant une plateforme CI/CD dédiée, car la création d’applications Salesforce nécessite la maîtrise d’un mélange exclusif de langages de programmation, de formats et de métadonnées. Le défi auquel sont confrontées les organisations qui créent ces applications est que les développeurs qui les créent ont des niveaux d’expertise variables, y compris les développeurs dits citoyens qui ont peu d’expertise en programmation. Par conséquent, les risques que les applications soient mal configurées ou contiennent des vulnérabilités susceptibles d’être exploitées par des cybercriminels sont assez élevés. Gearset fournit une plateforme qui simplifie l’application des meilleures pratiques DevSecOps lors de la création et du déploiement de ces applications.</p>'}, {'', '<p>À long terme, il est déjà évident que l’intelligence artificielle générative (IA) accélérera également le rythme de développement de bon nombre de ces applications. À mesure que ce rythme s’accélère, le besoin de meilleurs workflows DevOps pour gérer efficacement ce rythme de développement accru ne fera qu’augmenter. Le défi et l’opportunité, comme toujours, consistent à déterminer comment garantir que ces applications soient de la plus haute qualité possible, car la fréquence à laquelle elles sont créées et déployées ne cesse d’augmenter.</p>'}, {'', '<p>En fin de compte, les entreprises qui adoptent DevSecOps vont systématiquement créer et déployer des applications de meilleure qualité, a déclaré Boyle. À l’heure actuelle, cependant, on ne sait pas vraiment combien d’entreprises qui créent des applications SaaS sur la plateforme Salesforce savent qu’il existe un ensemble de bonnes pratiques pour les créer.</p>'}]"
Cribl étend la portée de sa plateforme de routage des données de télémétrie DevOps,"[{'', '<p>Cribl a ajouté cette semaine la prise en charge de plusieurs plates-formes supplémentaires à son service cloud pour la collecte et le routage des données de télémétrie collectées à partir de plusieurs outils et plates-formes DevOps.</p>'}, {'', '<p>Le dernier ajout de Cribl Stream ajoute la prise en charge de Microsoft Azure et améliore sa capacité à rechercher des données stockées dans le lac de données à partir de Snowflake.</p>'}, {'', ""<p>Parallèlement, Cribl peut désormais également collecter des données via l'interface de programmation d'application (API) de distribution mise à disposition par Datadog et depuis la plateforme ServiceNow Observability que l'entreprise a acquise lors de l'acquisition de LightStep.</p>""}, {'', ""<p>Enfin, Cribl Edge a été mis à jour pour offrir la possibilité de surveiller l'état de santé des nœuds et des flottes de sources de données, tandis que Cribl Lake a ajouté une capacité de groupe de travailleurs hybrides pour mieux permettre aux équipes d'écrire, de relire et de mélanger et de faire correspondre les sources de données.</p>""}, {'', '<p>Vlad Melnik, vice-président du développement commercial et des alliances chez Cribl, a déclaré que la dernière mise à jour de Cribl Streams simplifie la configuration de la plateforme pour acheminer les données vers le cloud Microsoft Azure. Auparavant, la plateforme ne fournissait que des intégrations avec le cloud Amazon Web Services (AWS).</p>'}, {'', '<p>Bien sûr, certaines équipes DevOps ont déjà intégré Cribl Streams au cloud Microsoft Azure, mais cette fonctionnalité est désormais intégrée à la plateforme d’une manière gérée par Cribl, a déclaré Melnik. Cependant, comme Azure Event Hubs est déjà largement utilisé pour collecter les données de journal collectées à partir de l’ensemble du service Azure, Cribl facilite désormais la collecte et la normalisation des données de télémétrie collectées depuis n’importe où, a noté Melnik.</p>'}, {'', '<p>Les équipes DevOps collectent plus de données de télémétrie que jamais, en partie grâce à l’essor d’Open Telemetry, un logiciel agent open source qui réduit le coût total de la collecte des données de télémétrie générées par plusieurs plates-formes et applications. Le défi consiste à trouver un moyen de rationaliser la collecte des données de télémétrie de manière à ce qu’elles soient plus simples à normaliser, quelle que soit la manière dont elles ont été générées. Cela est particulièrement important lorsque, par exemple, les équipes DevSecOps tentent d’identifier la cause profonde d’un éventuel incident de cybersécurité, a noté Melnik.</p>'}, {'', '<p>L’agrégation de toutes ces données revêt également une urgence accrue à l’ère de l’intelligence artificielle générative (IA). Alors que les équipes DevOps cherchent à rendre ces plateformes opérationnelles, elles doivent être en mesure d’exposer les modèles d’IA aux données de télémétrie collectées dans des environnements informatiques hautement distribués.</p>'}, {'', '<p>On ne sait pas exactement comment les équipes DevOps se développent pour ajouter une expertise en gestion des données. Dans certains cas, elles peuvent ajouter une expertise en ingénierie des données, mais dans la plupart des cas, les ingénieurs DevOps existants étendent probablement leurs compétences au domaine de la gestion des données.</p>'}, {'', '<p>D’une manière ou d’une autre, les équipes DevOps devront trouver des moyens de rationaliser la gestion des données de télémétrie dont le volume ne cesse de croître. Chaque équipe DevOps devra finalement déterminer la durée pendant laquelle elle doit stocker toutes ces données mais, comme toujours, plus un secteur est réglementé, plus les exigences en matière de stockage des données de télémétrie deviennent onéreuses.</p>'}, {'', ""<p>En fin de compte, tout effort visant à optimiser véritablement les performances d'une application commence par les données de télémétrie. Le problème n'est pas tant de trouver ces données que de les acheminer vers un endroit où des outils sont capables de les comprendre.</p>""}]"
Optimisation des coûts du cloud : les principes comptables au service de l'efficacité technique,"[{'', '<p>La gestion des plateformes cloud est devenue essentielle pour de nombreuses entreprises aujourd’hui. Elles offrent une grande flexibilité et une grande puissance, mais elles comportent des défis, notamment en matière de coûts. Il peut être complexe de suivre les dépenses d’une entreprise en matière de services cloud. Chaque service a sa propre structure tarifaire et les coûts peuvent rapidement augmenter lorsqu’on en utilise davantage. De plus, le suivi manuel de ces dépenses peut prendre du temps et être presque impossible sans les bons outils et stratégies.</p>'}, {'', ""<p>Pour les professionnels DevOps, il est essentiel de trouver des moyens de contrôler ces coûts sans sacrifier les performances ou la disponibilité. La bonne nouvelle est que plusieurs techniques combinent les principes comptables et les pratiques d'ingénierie pour rendre l'optimisation des coûts du cloud pratique et efficace.</p>""}, {""<h3>L'importance de la collaboration interdépartementale pour l'optimisation des coûts du cloud</h3>"", ''}, {'', '<p>La collaboration entre les équipes financières et techniques est essentielle pour parvenir à une gestion équilibrée des coûts du cloud. Ce partenariat garantit que les objectifs financiers d’une organisation sont alignés sur ses opérations techniques, ce qui conduit à une prise de décision plus éclairée concernant les dépenses liées au cloud.</p>'}, {'', '<p>Les équipes financières et d’ingénierie qui travaillent ensemble peuvent développer une compréhension commune des implications financières des choix techniques et identifier les opportunités d’économies sans compromettre les performances ou la disponibilité.</p>'}, {'', ""<h3>Stratégies d'optimisation des coûts du cloud</h3>""}, {'', '<p>Les professionnels DevOps peuvent s’assurer d’optimiser les coûts cloud de leur organisation en mettant en œuvre plusieurs stratégies qui suivent les principes comptables et les techniques d’ingénierie.</p>'}, {'', '<h3>Répartition des coûts et étiquetage des ressources</h3>'}, {'', ""<p>L'allocation des coûts et l'étiquetage des ressources impliquent l'attribution d'étiquettes détaillées à chaque ressource cloud, ce qui permet aux organisations de suivre les dépenses avec précision sur différents projets, départements et services. Ces stratégies clés permettent de prendre des décisions budgétaires éclairées et d'identifier les opportunités d'économies au sein de l'environnement cloud.</p>""}, {'', '<p>Envisagez de créer dès le départ une stratégie de balisage cohérente pour toutes les ressources. Cela implique de définir un ensemble standard de balises (centre de coûts, environnement et projet, par exemple) et de les appliquer de manière uniforme à chaque ressource.</p>'}, {'', ""<h3>Mise à l'échelle automatisée et approvisionnement efficace des ressources</h3>""}, {'', ""<p>La mise à l'échelle et l'approvisionnement en ressources automatisés garantissent que les ressources s'adaptent aux exigences de la charge de travail. Cela leur permet d'augmenter leur capacité lorsque cela est nécessaire pour maintenir les performances et de la diminuer en cas de faible utilisation pour réduire les coûts.</p>""}, {'', '<p>Prenons par exemple la dynamique opérationnelle d’un grand centre de données, où la consommation énergétique peut représenter 20 % des coûts opérationnels. Les centres de données peuvent réduire considérablement leur consommation énergétique pendant les périodes de faible demande et réaliser des économies considérables en matière d’énergie en mettant en œuvre ces stratégies.</p>'}, {'', ""<p>Utilisez les outils et services des fournisseurs de cloud pour configurer des politiques de mise à l'échelle automatisées. Définissez des règles claires basées sur des mesures de performances telles que l'utilisation du processeur et le nombre de requêtes pour déclencher des actions de mise à l'échelle. Cela garantit que les systèmes évoluent efficacement, en fournissant les ressources nécessaires pendant les heures de pointe.</p>""}, {'', ""<h3>Instances réservées et plans d'épargne</h3>""}, {'', ""<p>Les instances réservées et les plans d'économies impliquent un engagement sur un certain niveau d'utilisation des ressources cloud sur une période donnée en échange de tarifs réduits. Analysez soigneusement l'utilisation historique du cloud pour identifier des modèles et des charges de travail cohérents, stables et prévisibles. Sur la base de cette analyse, choisissez les instances réservées ou les plans d'économies appropriés qui correspondent aux besoins d'utilisation. Cette approche optimisera les économies tout en répondant aux besoins de l'infrastructure cloud.</p>""}, {'', '<h3>Dimensionnement des ressources et consolidation</h3>'}, {'', ""<p>Le dimensionnement et la consolidation garantissent que les ajustements des ressources cloud correspondent étroitement à l'utilisation réelle et combinent les charges de travail lorsque cela est possible pour optimiser l'efficacité. Examinez régulièrement les données d'utilisation du cloud pour identifier et éliminer les ressources sous-utilisées et les instances surprovisionnées. Ajustez-les à leur taille optimale en fonction des besoins et consolidez les charges de travail similaires pour réduire les frais généraux.</p>""}, {'', '<h3>Instances ponctuelles et calcul sans serveur</h3>'}, {'', ""<p>L'utilisation de ces deux approches peut avoir un impact significatif sur l'optimisation des coûts du cloud. Elles impliquent l'exploitation de modèles de tarification du cloud pour les activités transitoires, l'offre de capacité de réserve à moindre coût et l'exécution de code en réponse à des événements sans provisionnement de serveurs.</p>""}, {'', ""<p>Implémentez des charges de travail flexibles et tolérantes aux pannes capables de gérer les interruptions potentielles des instances ponctuelles. L'informatique sans serveur se concentre sur les applications sans état qui définissent des déclencheurs et entraînent des coûts uniquement pour le temps de calcul consommé.</p>""}, {'', '<h3>Gestion plus intelligente des ressources et économies de coûts</h3>'}, {'', ""<p>L'entreprise moyenne exploite 254 applications SaaS mais n'en utilise que 45 % régulièrement. Il s'agit d'un problème courant auquel de nombreuses entreprises sont confrontées, ce qui accroît la nécessité d'une gestion plus intelligente des ressources et de réductions des coûts. Cela peut être facile à réaliser lorsque les équipes DevOps mettent en œuvre certaines techniques et technologies.</p>""}, {'', ""<p>Par exemple, la combinaison d'outils de surveillance, d'identification des déchets et d'analyse de l'utilisation crée une stratégie cohérente pour l'optimisation des coûts du cloud. Les organisations qui déploient des outils de surveillance peuvent obtenir un aperçu immédiat de l'utilisation des ressources, en identifiant rapidement les actifs surutilisés ou sous-utilisés. Cette visibilité en temps réel est essentielle pour prendre des décisions plus judicieuses en déplacement.</p>""}, {'', '<p>Il s’agit alors d’identifier les gaspillages, en identifiant les ressources qui ne sont pas pleinement utilisées et qui peuvent être ajustées ou éliminées pour réduire les dépenses inutiles. L’analyse de l’utilisation complète ces efforts en fournissant une compréhension plus approfondie des habitudes de consommation au fil du temps.</p>'}, {'', ""<p>Elle permet de mettre en évidence des opportunités d'optimisation, comme la réduction des effectifs pendant les périodes de faible demande. L'application conjointe de ces techniques constitue une approche efficace de gestion des ressources cloud et garantit que les entreprises ne paient que ce qu'elles utilisent.</p>""}, {""<h3>Soyez proactif dans l'optimisation des coûts du Cloud</h3>"", ''}, {'', '<p>L’optimisation des coûts du cloud peut être complexe, mais les équipes DevOps peuvent y parvenir en adoptant une attitude proactive. Elles doivent évaluer régulièrement leur infrastructure cloud à l’aide des meilleures stratégies disponibles. En procédant ainsi et en s’adaptant aux besoins évolutifs de leur organisation, elles peuvent garantir une utilisation efficace des ressources et réaliser des économies de coûts significatives.</p>'}]"
Naviguer dans les eaux agiles : pourquoi l'intégration de Copilot exige des ajustements méthodologiques,"[{'', ""<p>Le Congrès américain a récemment interdit à son personnel d’utiliser l’IA Copilot de Microsoft, un chatbot intégré à grand modèle de langage qui permet l’automatisation des produits Microsoft tels que Word, Excel, PowerPoint, Outlook et Teams, en invoquant des problèmes de sécurité. Et ils ne sont pas les seuls à penser ainsi, car de nombreux professionnels oscillent entre enthousiasme et crainte lorsqu’il est question de l’IA. En attendant, Microsoft a vanté avec assurance Copilot, promettant que la puissance de son IA réduira le travail quotidien de gestion d’une entreprise. L’entreprise est passée du simple discours sur l’IA à l’intégration de celle-ci dans toutes les couches de sa pile technologique. Son introduction récente de Copilot Runtime permet même aux développeurs d’utiliser l’IA dans leurs propres programmes, il n’est donc pas surprenant que la plupart des directeurs des systèmes d’information expérimentent Copilot. Ses promesses de productivité et l’élargissement du champ d’application et les économies de coûts qui en résultent pour un réinvestissement potentiel sont trop alléchantes pour être ignorées. Microsoft montre certainement l’exemple, et la seule question est de savoir comment les autres devraient suivre. Au cours de la dernière décennie, les organisations agiles ont surpassé les autres en prenant et en gérant les décisions plus rapidement. L'adoption de l'agilité dans le domaine informatique a entraîné des changements dans la manière dont les infrastructures, les applications, les données et les compétences sont produites, consommées ou les deux. Les éléments fondamentaux de l'agilité, tels que la collaboration, l'automatisation et les améliorations continues, sont les principales sources d'innovation pertinentes pour les applications, le développement et le déploiement.</p>""}, {'', '<p>Les événements récents ont perturbé la progression agile dans le domaine informatique. L’ère de flexibilité induite par la COVID-19 dans la manière, le moment et le lieu de travail remet en question le statu quo dans nos modes de collaboration. En outre, l’IA générative et les complexités associées à la gouvernance informatique ont également bouleversé la progression agile. Par conséquent, une économie incrémentale a été créée qui oblige chaque entreprise à prendre les opportunités et les défis plus au sérieux.</p>'}, {'', ""<p>L'agilité distribuée est déjà en pratique depuis un certain temps. Les réalités du travail à distance et hybride ne sont que des extensions de ce que nous avons déjà vu dans les équipes distribuées. Cependant, les promesses de productivité d'outils tels que Copilot sont nouvelles, donc supposer que les pratiques Agile actuelles fonctionneront avec les pratiques GenAI est une erreur. Alors que pouvons-nous faire ?</p>""}, {'', '<p>Voici quelques réflexions sur la manière d’intégrer la méthodologie agile dans le cadre de l’adoption de Copilot.</p>'}, {'', '<h3>Étendre DevOps pour inclure la représentation de DataOps et MLOps</h3>'}, {'', '<p>Étant donné l’importance des données ainsi que des modèles d’IA et d’apprentissage automatique, les équipes DevOps doivent inclure des représentants des équipes DataOps et MLOps (ModelOps est un sous-ensemble de MLOps). Ce n’est qu’à ce moment-là que l’objectif de rapprocher la « production » et les « opérations » peut être atteint. À première vue, le remplacement peut sembler être la plus grande menace de l’IA, mais son premier acte sera plutôt de révéler et d’approfondir les fissures dans la collaboration.</p>'}, {'<h3>L’intelligence logicielle est plus importante que jamais</h3>', ''}, {'', '<p>Ne pas comprendre les systèmes d’application de manière globale avant de produire automatiquement le code en production sera désastreux. L’informatique d’entreprise est un mélange d’applications et d’actifs informatiques IA et non IA. De plus, l’« explicabilité » du code ne peut être obtenue que lorsque l’intelligence logicielle sur le code produit par GenAI est atteinte. En fin de compte, ce n’est pas l’exactitude fonctionnelle, mais l’adéquation architecturale qui compte le plus pour débloquer des améliorations de productivité. L’IA évolue rapidement, mais laisser la vitesse prendre trop de priorité ouvre la voie à l’échec.</p>'}, {'', '<h3>La conformité continue et la sécurité continue sont tout aussi importantes</h3>'}, {'', '<p>L’une des principales préoccupations des outils GenAI concerne les vulnérabilités que le code généré automatiquement peut introduire dans l’informatique des entreprises, ce qui est la principale raison pour laquelle le Congrès américain a interdit l’utilisation de Copilot. Il est important de procéder aux ajustements appropriés au niveau des modules pour la conformité et la sécurité, afin qu’ils soient conçus et livrés en continu, plutôt que d’être vérifiés et assurés périodiquement. La réglementation est connue pour être à l’origine de l’innovation, et les entreprises doivent délibérément anticiper les difficultés futures.</p>'}, {'<h3>Augmenter les portes de qualité dans votre pipeline CI/CD pour les assistants IA</h3>', ''}, {'', ""<p>Les principes fondateurs de l'open source (transparence, inspection et adaptation) peuvent être étendus aux produits GenAI. L'« inspection » ne doit pas seulement couvrir les aspects qualité, performance, sécurité et UX du code fourni par les outils, mais également l'adéquation architecturale au sein de l'informatique de l'entreprise.</p>""}, {'', '<h3>Mesurez le succès et soyez transparent sur vos lacunes</h3>'}, {'', '<p>L’impact de l’IA peut être flou, mais certains résultats doivent être mesurables pour justifier son adoption. L’élaboration d’indicateurs de performance clés spécifiques à l’IA peut contribuer à consolider le rôle du copilote au sein de l’équipe. Trouver les bons indicateurs à mesurer est de la plus haute importance et constitue en soi un défi de taille.</p>'}, {'', '<p>Dans l’analyse détaillée de l’impact de l’IA, il est également important d’accepter ouvertement les défauts. Le système est, bien sûr, imparfait par nature, et ces imperfections doivent être suivies et corrigées. L’IA évoluant si rapidement, de nombreux problèmes seront probablement résolus à court terme. Notez les défauts et faites-en un examen régulier.</p>'}, {'', '<h3>L’inadéquation des compétences aura un impact sur les promesses de productivité</h3>'}, {'', ""<p>Les outils ne sont efficaces que si ceux qui les utilisent le savent. Un développeur expérimenté peut démontrer un niveau de productivité supérieur à la moyenne avec un assistant IA, mais un développeur inexpérimenté peut rapidement créer plus de problèmes que de solutions. Former les communautés de développement et d'assurance qualité à maîtriser les outils et les directives de gouvernance du code et des tests nécessite d'ajuster le modèle opérationnel Agile.</p>""}, {'', '<p>N’oubliez pas que les équipes DevOps sont bien plus que des développeurs, les équipes DataOps bien plus que des ingénieurs de données et les équipes ModelOps bien plus que des data scientists. Les compétences interdisciplinaires des équipes DevOps vont considérablement évoluer lorsque l’IA fera partie de la conversation. À mesure que les frontières entre ces disciplines s’estomperont, ceux qui seront prêts à s’adapter se hisseront au sommet.</p>'}, {'', '<p>L’adaptation de la méthodologie aux défis perçus ne doit pas limiter les avantages potentiels que GenAI peut produire. S’il est utilisé correctement, GenAI peut contribuer à l’hyper-automatisation des tâches de développement et d’assurance qualité, à l’évaluation des options de conception grâce au prototypage rapide, à la simplification du processus de documentation, à la surveillance de l’environnement de production pour prévoir les goulots d’étranglement des performances, etc.</p>'}, {'', '<p>Si nous n’adaptons pas nos méthodes agiles pour répondre à ces nouvelles réalités et dégager de la valeur plus rapidement, le « time to market » et les avantages en termes de coûts associés seront mal perçus. Les changements dans la méthodologie agile sont inévitables, car GenAI et Agile offrent de réels avantages concurrentiels. Ne vous laissez pas aller.</p>'}]"
Lancez-vous : passez au mode multijoueur pour dynamiser votre plateforme interne,"[{'', '<p>Comment les organisations peuvent-elles accroître la valeur de leurs plateformes internes sans élargir massivement leurs équipes de plateformes ?</p>'}, {'', '<p>Nous célébrons souvent le héros ou l’héroïne qui crée seul quelque chose d’ingénieux – l’innovateur non-conformiste. Cependant, la réalité nous montre constamment que la plupart des innovations sont le résultat d’un travail d’équipe. Cela est également vrai pour la conception d’une plateforme de développement interne. La plateforme interne peut offrir une valeur exceptionnelle. Cependant, pour atteindre son plein potentiel, il est logique de s’éloigner du développement de plateformes en mode solo et de passer en mode multijoueur pour dynamiser votre plateforme interne. Mais qu’est-ce que cela signifie exactement ?</p>'}, {'', '<p>Si de nombreuses organisations se lancent à fond dans la création d’une plateforme interne, elles n’exploitent pas toujours toutes les ressources et l’expertise dont elles disposent en interne. Au lieu de cela, il y a souvent plusieurs équipes cloisonnées qui construisent des plateformes qui résolvent leurs problèmes, mais elles ne communiquent pas avec d’autres équipes qui pourraient utiliser et améliorer la plateforme qu’elles construisent. S’il ne s’agit pas de plusieurs équipes avec des plateformes concurrentes ou redondantes, les organisations font parfois reposer la responsabilité du développement d’une plateforme sur les équipes de plateforme. Mais ces approches répondent-elles aux difficultés auxquelles les organisations sont confrontées et aux défis qu’elles cherchent à résoudre en adoptant une plateforme interne ?</p>'}, {'', ""<h3>Un bref historique : de DevOps à l'ingénierie de plateforme</h3>""}, {'', '<p>Par le passé, les développeurs passaient beaucoup de temps à attendre que les opérateurs leur fournissent des services ou des outils. Cela entraînait des frictions, des retards et de la frustration. Les développeurs et les opérateurs étaient isolés les uns des autres et n’avaient aucun moyen de briser les silos qui les empêchaient d’être productifs. Cela a donné naissance à DevOps, qui vise à donner aux équipes les moyens de tout créer et de tout gérer elles-mêmes.</p>'}, {'', '<p>Le DevOps fonctionne – et fonctionne bien – jusqu’à ce que les organisations évoluent et que des problèmes doubles apparaissent. Tout d’abord, les équipes DevOps finissent par se débattre avec des problèmes répétitifs et redondants, et ensuite, la charge de travail DevOps augmente et s’accélère à mesure que l’entreprise évolue. Cela signifie que les équipes DevOps ont non seulement beaucoup plus de travail à faire au quotidien, mais doivent également gérer la création et l’exécution de tout, de l’infrastructure au réseau en passant par le CI/CD et l’observabilité. Dans le même temps, les équipes DevOps doivent gérer une multitude de processus et de préoccupations internes, tels que la sécurité et la conformité. Naturellement, la surcharge cognitive est inévitable.</p>'}, {'', ""<h3>Tuer le dragon DevOps : découvrez l'ingénierie de plateforme</h3>""}, {'', '<p>Pour résoudre les défis croissants de DevOps, l’ingénierie de plateforme a émergé avec pour objectif principal d’offrir une meilleure expérience aux développeurs. Que signifie l’expérience des développeurs dans ce contexte ? Cela signifie réduire la charge cognitive des développeurs et leur permettre de se concentrer sur leur travail principal : apporter de la valeur aux clients. Mais pour y parvenir, il serait essentiel de réduire les charges de travail cognitives et littérales, et de rationaliser le travail des équipes DevOps. C’est là que l’ingénierie de plateforme et la formation d’équipes de plateforme jouent un rôle crucial.</p>'}, {'', '<p>Cette évolution n’a cependant pas complètement résolu le problème. La charge cognitive a simplement été transférée aux équipes de plateformes, chargées de fournir une plateforme de développement interne personnalisée qui réponde aux besoins des développeurs et offre des outils et des services à la demande. Pourtant, les équipes de plateformes finissent par prendre en charge un ensemble d’outils et de services déjà vaste et en plein essor et ressentent la pression de devoir fournir une expérience utilisateur agréable aux développeurs. L’ensemble du secteur fait écho à l’importance de l’expérience et des flux des développeurs lorsqu’ils créent leurs logiciels. Et si c’est vrai, comment pouvons-nous tirer le meilleur parti de ces plateformes internes sans épuiser toute l’équipe ou augmenter massivement ses effectifs ?</p>'}, {'', '<h3>Jouer en équipe</h3>'}, {'', '<p>Pour obtenir une valeur plus importante et potentiellement plus rapide avec la plateforme interne, commencez par réfléchir à la manière dont les équipes sont organisées, puis déterminez leurs rôles et responsabilités. D’après l’expérience des développeurs, des équipes DevOps et des équipes de plateforme, il est clair qu’aucun individu ou équipe ne doit (ou ne peut) tout gérer. Dans l’ouvrage phare Team Topologies, quatre types d’équipes sont décrits : équipe alignée sur le flux (équipe d’application), équipe d’activation, équipe de sous-systèmes complexes et équipe de plateforme.</p>'}, {''}, {'', ""<p>Lorsque l'idée de plateforme interne commence à s'effondrer, les équipes alignées sur les flux ont trop de charge cognitive et de bricolage dans le cadre de leur travail. Dans le même temps, avec l'ingénierie de plateforme, une grande partie du travail préparatoire et de la maintenance est transférée aux équipes de plateforme, ce qui alourdit leur charge de travail.</p>""}, {'', '<p>En comprenant ces types d’équipes et leur dynamique, nous pouvons commencer à nous demander : comment pouvons-nous partager la charge et même impliquer d’autres équipes dans le travail de construction de la plateforme interne ? Comment pouvons-nous répartir la responsabilité de manière plus équitable et de manière à offrir une valeur maximale ?</p>'}, {'', '<h3>Jouer sur la plateforme avec une approche multijoueur</h3>'}, {'', '<p>C’est là que l’intérêt d’une approche multijoueur pour la création de plateformes se fait sentir. Si les développeurs et les équipes de plateformes détiennent une grande partie de l’expertise nécessaire pour créer une plateforme interne efficace, la responsabilité de créer une plateforme utile capable de susciter une adhésion et une adoption quasi universelles n’incombe pas entièrement à l’un ou l’autre. Les autres équipes de l’organisation peuvent avoir une grande influence sur le développement de la plateforme en y apportant leurs propres compétences, expertise et connaissances, ce qui profite en fin de compte à l’organisation.</p>'}, {'', '<h3>Prêt, joueur 1</h3>'}, {'', '<p>Un exemple d’injection de compétences et d’expertise spécifiques dans une plateforme est celui où plusieurs équipes d’application ont besoin d’accéder à une base de données. Dans un monde idéal, les développeurs auraient un accès à la demande, par exemple à une base de données Postgres en tant que service. Ils pourraient en fournir une à chaque fois qu’ils en ont besoin. De nombreuses organisations disposent en interne d’une équipe spécialisée dans les bases de données qui sait exactement comment fonctionne Postgres, comment il doit être configuré et comment les équipes alignées sur les flux doivent l’utiliser.</p>'}, {'', '<p>Il n’y a aucune raison pour que les développeurs ou les équipes de plateforme d’une entreprise soient obligés de posséder des connaissances approfondies en matière de bases de données lorsqu’une équipe de sous-systèmes composée d’experts en la matière peut facilement proposer cette option de base de données en tant que service au sein de la plateforme. En déchargeant cette charge, les équipes d’application peuvent obtenir ce dont elles ont besoin de la plateforme, et les équipes de plateforme peuvent facilement utiliser l’option de base de données en tant que service comme élément de base au sein de la plateforme, qu’elles peuvent regrouper avec d’autres services, le tout en s’appuyant sur l’expertise de l’équipe de base de données.</p>'}, {'', '<h3>Prêt Joueur Deux</h3>'}, {'', '<p>Un deuxième exemple concerne une exigence spécifique plutôt qu’un outil spécifique. Les processus de sécurité, par exemple, sont des exigences qui doivent être intégrées à de nombreuses charges de travail de livraison de logiciels. Mais il est probable que les ressources de sécurité ne soient pas suffisantes pour les intégrer aux équipes chargées des applications et des plateformes. Au lieu de cela, l’approche multi-acteurs permet aux équipes de sécurité de sauver la mise en définissant des processus et en les intégrant à la plateforme interne.</p>'}, {'', '<p>Encore une fois, les équipes de développement et de plateforme ne sont pas obligées de comprendre tous les tenants et aboutissants de la sécurité, mais peuvent suivre les étapes et les garde-fous définis par l’équipe de sécurité au fur et à mesure de leur intégration dans les charges de travail de la plateforme.</p>'}, {'', '<h3>Tout le monde peut contribuer à rendre la plateforme interne plus puissante</h3>'}, {'', '<p>L’utilisation de compétences et de connaissances spécialisées provenant de toute l’organisation et leur intégration dans la plateforme interne de manière simple à utiliser constituent une stratégie efficace pour réduire la charge cognitive au sein des équipes.</p>'}, {'', '<p>Cette approche, parfois appelée « sourcing interne » ou « démocratisation de la plateforme », s’est révélée très fructueuse. L’essentiel est de tirer le meilleur parti des talents, des compétences et des connaissances dont dispose votre organisation. Cela permet non seulement de soulager la charge cognitive et d’éviter de devoir constituer une équipe massive ou de submerger une petite équipe avec trop d’informations, mais cela ouvre également la voie à l’adhésion et à l’appropriation de la plateforme à l’échelle de l’organisation.</p>'}, {'', ""<h3>Comment démarrer avec l'approche multijoueur</h3>""}, {'', ""<p>L'ingénierie des plateformes étant considérée comme une pratique sociotechnique, il est important d'impliquer à la fois les personnes et la technologie. Considérez à nouveau les topologies d'équipe et les trois modes d'interaction décrits dans le livre : la collaboration, le x-as-a-service et la facilitation.</p>""}, {''}, {'<h3>Collaboration : Jouons ensemble</h3>', ''}, {'', '<p>Pour comprendre à quoi pourrait ressembler une plateforme interne multijoueur, les équipes doivent se concentrer sur la collaboration. Elles doivent se comprendre, développer l’empathie et s’aligner. La collaboration est le meilleur type d’interaction pour comprendre les exigences, telles que les besoins des équipes d’application, la façon dont elles aimeraient utiliser la plateforme et si et comment les équipes d’application souhaiteraient contribuer aux outils et services qu’elles ont elles-mêmes développés dans la plateforme.</p>'}, {'', ""<p>L'équipe de la plateforme doit également collaborer avec les autres équipes, en particulier les spécialistes qui fournissent des outils et des services spécifiques. Cela permet de mieux comprendre ce que sont ces services, comment ils peuvent être produits et intégrés à la plateforme, ainsi que ce que ces équipes doivent consommer des autres équipes. Pour faciliter ce travail, une équipe de facilitation de courte durée axée sur la facilitation dans toute l'organisation peut être utile pour cartographier les connexions et les collaborations.</p>""}, {'', '<h3>X-as-a-Service : là où le mode multijoueur prend vie</h3>'}, {'', ""<p>Le mode multijoueur prend vie lorsque les équipes de l'organisation sont habilitées à apporter leur expertise à la plateforme. L'équipe de la plateforme peut le permettre. L'objectif final est de faire en sorte que la collaboration mène à une expertise qui se transforme en outils et services à la demande X-as-a-service.</p>""}, {'', ""<p>En utilisant l'exemple précédent, nous pouvons voir que la base de données Postgres est proposée en tant que service au sein de la plateforme et qu'elle bénéficie de l'expertise en sécurité apportée par l'équipe de sécurité intégrée à la plateforme. Les consommateurs de la plateforme interagissent désormais avec la plateforme en mode x-as-a-service, ce qui signifie qu'ils obtiennent l'outil dont ils ont besoin à la demande avec une sécurité renforcée déjà intégrée.</p>""}, {''}, {'', '<p>D’un point de vue technique également, l’équipe de la plateforme doit tenir compte de la facilité avec laquelle les autres équipes peuvent contribuer à la plateforme. S’agit-il d’une boîte noire que personne ne comprend ou est-il facile pour les autres équipes de contribuer ? Quel est ce processus de contribution, non seulement sous forme d’interaction ponctuelle en amont, mais également de manière continue pour garantir que la plateforme reste à jour et adaptée à ses objectifs ? Ce processus est-il compris et documenté ?</p>'}, {'', '<p>C’est l’un des domaines dans lesquels une infrastructure de plateforme peut prendre en charge ce modèle multijoueur et même permettre le mode multijoueur prêt à l’emploi. Si votre plateforme est constituée de blocs de construction composables fournis et maintenus par des équipes d’experts, elle permettra à l’équipe de plateforme d’utiliser ces blocs de construction pour composer des routes pavées ou des chemins d’or. Ceux-ci peuvent ensuite être utilisés par les équipes d’application pour une expérience de développement encore plus simple et plus fluide. En fin de compte, la plateforme interne finit par être supérieure à la somme de ses parties et devient un moteur d’innovation.</p>'}, {""<h3>Le jeu n'est pas terminé : passez en mode multijoueur pour obtenir plus de jetons</h3>"", ''}, {'', ""<p>Le passage au mode multijoueur est au cœur de la volonté de permettre une meilleure expérience aux développeurs et de réduire la charge cognitive des équipes de développement et de plateforme. Et même si ces équipes sont celles qui en bénéficient le plus en apparence, une plateforme interne inspirée du multijoueur peut bénéficier de la participation de l'ensemble de l'organisation.</p>""}, {'', '<p>En passant du mode solo au mode multijoueur, chacun peut devenir un héros de plateforme interne.</p>'}, {''}]"
Les opérations en tant que code : transformer l'excellence opérationnelle,"[{'', '<p>La transformation numérique et l’infrastructure cloud native sont inévitables. Cette évolution implique la nécessité de gérer les opérations avec la même rigueur et la même automatisation que celles appliquées à l’infrastructure ou à la sécurité. De nombreuses organisations ont adopté l’idée que tout se trouve dans un pipeline et que tout est sous forme de code. Alors que les ingénieurs de plateforme et d’autres équipes ont créé des solutions pour créer et fournir des applications et les cadres nécessaires pour les exécuter, les opérations réelles de fourniture de services sont souvent décousues et purement réactives.</p>'}, {'', '<p>Saisissez les opérations sous forme de code.</p>'}, {'', ""<p>En exploitant des outils tels que Terraform avec des pipelines d'automatisation et CI/CD, les équipes d'ingénierie de fiabilité du site (SRE), DevOps et DevSecOps peuvent standardiser et automatiser les tâches opérationnelles, garantissant ainsi la cohérence, l'efficacité et la fiabilité.</p>""}, {'', ""<p>Les opérations en tant que code étendent les principes de l'infrastructure en tant que code (IaC) aux procédures opérationnelles. Elles impliquent la définition, la gestion et l'exécution de tâches opérationnelles (telles que la définition de politiques d'escalade, la définition de manuels d'exécution et l'exécution de manuels de jeu) à l'aide de codes et d'outils d'automatisation. Cette approche garantit que les pratiques opérationnelles sont reproductibles, contrôlées par version et peuvent être exécutées avec une intervention humaine minimale.</p>""}, {'<h3>Éviter les goulots d’étranglement</h3>', ''}, {'', '<p>L’un des principaux avantages de l’approche des opérations en tant que code est la possibilité de supprimer la dépendance vis-à-vis des équipes centralisées. À mesure que le besoin de rapidité dans les pratiques DevOps augmente, ces équipes ne peuvent plus dépendre de l’ITSM centralisé ou d’autres équipes pour leurs besoins d’intégration de nouvelles fonctions de surveillance, d’enrichissement d’événements ou de création de nouveaux runbooks.</p>'}, {'', '<p>De même, les équipes centralisées qui consacrent des compétences coûteuses et spécialisées à la surveillance des intégrations, à la gestion des événements, à l’enrichissement et à l’automatisation, qui peuvent être gérées via des opérations en tant que code, n’ont guère de sens sur le plan économique. Ces équipes, en particulier dans les grandes organisations, sont déjà surchargées et le retard de travail augmente de jour en jour. Tirer parti de Terraform ou d’autres mécanismes pour atteindre les mêmes objectifs tout en offrant de meilleurs résultats est plus logique pour toutes les équipes travaillant ensemble.</p>'}, {'<h3>Tirer parti des pipelines et de Terraform pour les opérations</h3>', ''}, {'', ""<p>Terraform, traditionnellement utilisé pour l'IaC, est devenu la lingua franca de DevOps. En écrivant des configurations Terraform, les équipes peuvent automatiser le provisionnement et la gestion non seulement de l'infrastructure, mais aussi des flux de travail opérationnels qui garantissent l'excellence opérationnelle. Par exemple, les configurations Terraform peuvent définir des tâches telles que les définitions de services, la configuration des utilisateurs, des équipes et des rôles, la définition des politiques et des calendriers d'escalade, la définition de la corrélation et de l'orchestration des événements et la définition de l'automatisation telle que les runbooks et les diagnostics automatisés.</p>""}, {'', '<p>Les pipelines CI/CD jouent un rôle crucial dans les opérations en tant que code. En intégrant des tâches opérationnelles dans les pipelines CI/CD, vous pouvez garantir que les modifications sont testées, examinées et déployées de manière contrôlée et automatisée.</p>'}, {'', ""<p>Les portes de qualité sont traditionnellement utilisées pour les révisions de code, les tests automatisés, les contrôles de sécurité, etc. Pour les opérations en tant que code, elles peuvent être exploitées à des fins de normalisation en garantissant la cohérence des fonctionnalités de base telles que les normes de service, les niveaux de politiques d'escalade, les exigences minimales pour les runbooks, etc. En effectuant des contrôles de conformité pour garantir que les changements opérationnels sont conformes aux politiques internes et aux réglementations externes. Et à terme, elles peuvent être utilisées pour évaluer les applications en termes de préparation opérationnelle.</p>""}, {'', '<h3>Avantages des opérations en tant que code</h3>'}, {'', '<p>Les organisations qui déploient des opérations sous forme de code verront plusieurs avantages, dont beaucoup avec un retour sur investissement (ROI) immédiat.</p>'}, {'', '<p>La réduction des efforts est essentielle. Trop de temps est consacré à « ClickOps », et en abandonnant les configurations manuelles, vous pouvez consacrer plus de temps à l’automatisation et libérer des ressources pour des tâches plus utiles. Vous pouvez également réduire le risque opérationnel en garantissant la traçabilité des modifications apportées aux configurations, au contrôle des versions et aux modèles qui réduisent le risque d’erreur. De même, vous pouvez opérationnaliser la gouvernance et la conformité en exploitant les analyseurs, les contrôles de qualité et les modèles approuvés, tandis que la direction peut définir des normes minimales acceptables et les résultats attendus.</p>'}, {'', ""<p>L'excellence opérationnelle est améliorée en réduisant la fréquence, la gravité et la durée des pannes, en garantissant des résultats reproductibles et une réduction des erreurs. Vous pouvez vous éloigner des connaissances tribales en donnant aux cadres supérieurs une méthode simplifiée et reproductible pour enregistrer leurs connaissances innées afin de les réutiliser et en créant un contexte pour le personnel subalterne.</p>""}, {'', '<p>L’expérience du développeur est améliorée en réduisant le temps de montée en puissance des nouveaux membres de l’équipe, ce qui leur permet de se concentrer davantage sur le travail à forte valeur ajoutée et le renforcement des capacités, tout en consacrant moins de temps à la recherche de «\xa0comment faire\xa0» ou à la remontée des connaissances vers des experts. Plus important encore, vous pouvez amorcer une transition de l’exécution à la construction en réduisant le temps consacré à maintenir les lumières allumées et à poursuivre les travaux de réparation, tandis que le personnel senior peut se concentrer sur la réduction de la dette technologique (ou l’exploitation de la richesse technologique si vous êtes optimiste) pour offrir une expérience client exceptionnelle.</p>'}, {'', '<h3>Commencer</h3>'}, {'', '<p>Le déploiement réussi des opérations en tant que code implique plusieurs étapes clés\xa0:</p>'}, {'', '<li>Tout d’abord, vous devez définir le succès. Comment allez-vous mesurer l’efficacité de vos opérations ? Pensez au-delà du temps moyen de réparation (MTTR). Qu’en est-il du coût de maintien des lumières ou de réduction du temps et du coût des travaux de réparation ? Comment pouvez-vous mieux vous attaquer à la dette technologique ?</li>'}, {'', '<li>Ensuite, évaluez les opérations actuelles et identifiez les domaines initiaux qui peuvent bénéficier de l’automatisation et des modèles. Quels sont les risques que vous pouvez immédiatement éliminer ou les résultats que vous pourriez influencer en standardisant les opérations ?</li>'}, {'', ""<li>Ensuite, assurez-vous de disposer des outils appropriés qui correspondent à votre environnement et à vos objectifs architecturaux et que vos équipes sont formées à ces outils et aux meilleures pratiques associées. Vous souhaiterez établir un centre d'excellence en constituant des équipes de passionnés et d'experts qui peuvent vous aider avec les questions-réponses, devenir les gardiens des modèles et contribuer à la création d'améliorations continues en matière d'automatisation et d'orchestration.</li>""}, {'', '<li>Enfin, concentrez-vous sur une mise en œuvre progressive en commençant par des domaines simples mais impactants, puis développez-vous en utilisant l’amélioration continue pour revoir et améliorer régulièrement vos processus en fonction des commentaires et des mesures.</li>'}, {'', '<h3>Et ensuite ?</h3>'}, {'', '<p>Les opérations en tant que code représentent la prochaine frontière de la gestion informatique, offrant la promesse de cohérence, d’efficacité et de fiabilité dans les tâches opérationnelles. En tirant parti de Terraform, des pipelines CI/CD et d’outils robustes, vous pouvez amener vos équipes à adopter cette approche transformatrice. Bien que des défis existent, ils sont surmontables grâce à une planification, une exécution et une amélioration continue minutieuses. Les opérations en tant que code peuvent être une pierre angulaire de l’excellence opérationnelle, permettant à vos équipes de passer d’un monde de travail et de dépannage à la création de capacités qui vous aideront à gagner sur le marché, à mieux servir vos équipes et, surtout, vos clients.</p>'}]"
Stratégie de migration vers le cloud AWS,"[{'', '<p>La migration vers le cloud consiste à déplacer des données, des applications ou d’autres services vers un environnement de cloud computing à partir d’un centre de données sur site. Migrer votre projet signifie déplacer vos données du centre de données sur site vers le cloud. Dans ce cas, le cloud est la virtualisation utilisée sur un centre de données pour rendre les fonctionnalités plus flexibles. De nombreuses entreprises comme GoDaddy, Expedia, Netflix, des startups, etc., ont récemment déplacé leur activité vers le cloud. La migration est une tâche importante et AWS facilite la migration par phases.</p>'}, {'', '<h3>Besoin de migration</h3>'}, {'', '<p>Les affaires ne sont pas une tâche facile lorsqu’il s’agit de gérer des situations telles que la sécurité, la mise à l’échelle à la hausse ou à la baisse, etc. Examinons quelques scénarios dans lesquels la migration AWS pourrait être un meilleur recours.</p>'}, {'', ""<ol>De plus en plus d'utilisateurs arrivent dans un contexte de performances élevées. Des besoins de mise en œuvre et de déploiement plus rapides. Une base de données croissante coûteuse à gérer. Un incident dans le centre de données s'est produit.</ol>""}, {'', ""<li>De plus en plus d'utilisateurs arrivent dans un contexte de performances élevées</li>""}, {'', '<li>Besoins de mise en œuvre et de déploiement plus rapides</li>'}, {'', '<li>Coûteux de gérer une base de données croissante</li>'}, {'', ""<li>Un incident s'est produit dans un centre de données</li>""}, {'', '<p>Nous pouvons conclure que les raisons principales pour migrer vers le cloud sont les suivantes\xa0:</p>'}, {'', '<li>Coût d’investissement nul sur les infrastructures</li>'}, {'', '<li>Sécurité</li>'}, {'', '<li>Évolutif (verticalement et horizontalement)</li>'}, {'', '<li>Productivité</li>'}, {'', '<li>Payez ce que vous utilisez</li>'}, {'', '<li>Des services faciles à gérer</li>'}, {'', '<li>Coûts opérationnels réduits</li>'}, {'', '<p>Si vous migrez vers le cloud, les problèmes mentionnés ci-dessus seront traités automatiquement. Allons plus loin et comprenons ce qu’est la migration.</p>'}, {'', '<p>Phases de la migration vers AWS</p>'}, {'', ""<p>La migration vers le cloud AWS est un processus étape par étape qui comprend la planification, la préparation et la découverte, la migration et l'optimisation. Cependant, il est plus complexe car le processus implique différentes phases. Permettez-moi maintenant de parler des différentes phases de la migration.</p>""}, {'', '<h3>Phase 1 : Préparation de la migration et planification des activités</h3>'}, {'', '<p>Au cours de cette phase, vous identifiez le besoin, l’objectif et les avantages de la migration vers le cloud. Vous effectuez une preuve de concept et vérifiez le résultat pour comparer les performances et les coûts. Il existe également des moments où vous n’avez pas besoin de déplacer l’ensemble de votre entreprise vers le cloud. C’est là que la séparation est importante. Vous devez identifier les applications qui peuvent être migrées et celles qui ne le peuvent pas. C’est l’objet de la première phase.</p>'}, {'<h3>Phase 2 : Évaluation — Choix de votre méthode de migration (découverte, analyse et évaluation)</h3>', ''}, {'', ""<p>La première étape consiste à identifier les actifs qui doivent être migrés vers le cloud : rassemblez les détails tels que le nombre de serveurs à migrer, la feuille de route de la migration, l'enregistrement des activités critiques pour l'entreprise, la taille du serveur, l'utilisation de la mémoire, l'utilisation du processeur, le type d'environnement et les données. Vous pouvez utiliser le service « AWS Application Discovery Service » pour vérifier les détails techniques en l'installant sur votre serveur. En fonction des données, AWS propose différentes manières de migrer votre application, par exemple AWS Snowball, AWS Snowmobile, AWS Direct Connect, etc. Évaluez également le coût de la location des serveurs, du stockage, du réseau et de l'assistance. Préparez la liste de contrôle pour l'évaluation réglementaire et de conformité et effectuez l'évaluation de la sécurité.</p>""}, {'', '<h3>Phase 3\xa0: Preuve de concept (POC) pour le stockage/la conception AWS</h3>'}, {'', ""<p>Une fois que vous savez comment et ce que vous migrez, vous devez ensuite planifier vos priorités de livraison et concevoir le processus de migration vers le cloud AWS. Vous identifierez les dépendances (composants en amont et en aval). Définissez la stratégie de l'organisation ou du compte pour les utilisateurs, la stratégie VPC et le modèle de cloud qu'elle adoptera (hybride/entièrement cloud) et d'autres paramètres tels que la sécurité.</p>""}, {'<h3>Phase 4 : Migration vers AWS</h3>', ''}, {'', ""<p>Maintenant que vous disposez de tous les prérequis tels que le plan directeur, les outils de migration, une liste d'affectations, les sauvegardes et sa synchronisation avec vos référentiels de données sur site, vous pouvez enfin migrer votre projet/application vers AWS Cloud. Une fois que vous avez migré votre projet vers le cloud, la fiabilité et la durabilité sont les avantages supplémentaires dont vous bénéficiez. Pour la migration du code d'application/du serveur, utilisez AWS Server Migration Service. Pour la base de données, vous pouvez utiliser AWS Database Migration Service et d'autres services d'importation/exportation AWS.</p>""}, {'', ""<h3>Phase 5\xa0: Opérations de cloud d'entreprise (exploitation et optimisation)</h3>""}, {'', ""<p>À ce stade, vous avez déjà migré vers AWS et cela apportera des mises à jour que vous devrez intégrer à votre architecture existante. Optimisez votre architecture, préparez votre équipe de support à rechercher les nouvelles fonctionnalités et l'optimisation des coûts, améliorez les applications à l'aide de la technologie sans serveur, utilisez les services de surveillance et de journalisation AWS et implémentez CloudFormation et DevOps.</p>""}, {'', ""<h3>Stratégies de migration d'applications « Les 6 R »</h3>""}, {'', ""<p>La complexité de la migration des applications existantes varie en fonction de l'architecture. Amazon a mis au point différentes stratégies qu'ils ont communément appelées les 6 R. Examinons chacune d'elles :</p>""}, {''}, {'', '<p>Avantages de la migration vers AWS</p>'}, {'', ""<ol>Élasticité : l'ajout et la suppression de capacité à chaque fois que cela est nécessaire constituent le principal avantage de la reprise après sinistre : avec une disponibilité garantie de 95 %, les entreprises peuvent être sûres que leurs données seront toujours disponibles. Gestion améliorée des coûts : la plateforme IaaS offre deux avantages majeurs. Tout d'abord, un IaaS tel qu'Amazon Web Services est disponible sous forme de service mensuel. Ensuite, il élimine le besoin de continuer à acheter et à entretenir du matériel physique.</ol>""}, {'', '<li>Élasticité : Ajouter et supprimer de la capacité chaque fois que cela est nécessaire est le plus grand avantage de</li>'}, {'', '<li>Reprise après sinistre : avec une disponibilité garantie de 95 %, les entreprises peuvent être sûres que leurs données seront toujours disponibles.</li>'}, {'', ""<li>Gestion des coûts améliorée : la plateforme IaaS offre deux avantages majeurs. Tout d'abord, un IaaS tel qu'Amazon Web Services est disponible sous forme de service mensuel. Ensuite, il élimine le besoin de continuer à acheter et à entretenir du matériel physique.</li>""}, {'', '<p>Services pour la migration AWS</p>'}, {'', '<p>Parmi les nombreux outils fournis par Amazon pour automatiser la migration de données, je parlerai des plus couramment utilisés.</p>'}, {''}, {'', '<p>AWS Migration Hub\xa0: AWS fournit un emplacement unique pour le suivi du processus de migration. Migration Hub vous donne la liberté de choisir votre partenaire de migration et les outils qui correspondent à vos besoins.</p>'}, {''}, {'', ""<p>AWS Server Migration Service (SMS) : AWS SMS est un service sans agent qui permet de migrer facilement et plus rapidement des charges de travail sur site vers AWS. Il vous permet d'automatiser la migration et de suivre la réplication du serveur. Il facilite la coordination avec votre migration de serveur à grande échelle.</p>""}, {''}, {'', '<p>Accélération du transfert Amazon S3\xa0: cela rend le transfert de fichiers sur une longue distance vers le compartiment AWS S3 plus rapide et plus sécurisé.</p>'}, {''}, {'', ""<p>AWS Snowball\xa0: il s'agit d'une solution de transfert de données à l'échelle du pétaoctet qui utilise des périphériques sécurisés pour transférer une grande quantité de données vers et depuis AWS.</p>""}, {''}, {'', ""<p>AWS Snowmobile : il s'agit d'une solution de transfert de données à l'échelle de l'exaoctet permettant de déplacer une quantité extrêmement importante de données vers AWS. Snowmobile facilite le transfert de volumes massifs de données.</p>""}, {''}, {'', ""<p>Amazon Kinesis Firehose\xa0: c'est la méthode la plus simple de toutes. Elle peut capturer et charger automatiquement des données en streaming dans Amazon S3. Vous pouvez analyser les données en temps réel pour obtenir des informations actualisées sur la migration.</p>""}, {'', '<p>Maintenant que vous savez tout sur AWS et AWS Migration, laissez-moi vous montrer un cas d’utilisation dans lequel vous allez migrer un système d’exploitation virtuel de ma machine locale vers AWS Cloud.</p>'}, {'', '<h3>Migration de données AWS</h3>'}, {'', ""<p>Les données peuvent être déplacées du site vers le centre de données AWS à l'aide des moyens suivants\xa0:</p>""}, {'', ""<li>Accélération du transfert Amazon S3 : pour un transfert plus rapide sur Internet, modifiez le point de terminaison que vous utilisez avec votre compartiment S3 et l'accélération est automatiquement appliquée.</li>""}, {'', ""<li>AWS Snowball — une solution de transport de données à l'échelle du pétaoctet qui utilise des appareils sécurisés pour transférer de grandes quantités de données vers et depuis AWS.</li>""}, {'', '<li>AWS Direct Connect : vous établissez une connexion réseau dédiée entre votre réseau et l’un des emplacements AWS Direct Connect pour le transfert.</li>'}, {'', ""<h3>Démonstration\xa0: Mise en œuvre de la migration à l'aide de la méthode d'importation/exportation</h3>""}, {'', '<p>Vous allez migrer un système d’exploitation virtuel sur site exécuté sur VMWare vers AWS à l’aide de la méthode d’importation/exportation (AWS Direct Connect).</p>'}, {'', '<h3>Condition préalable</h3>'}, {'', ""<ol>VMware Workstation installé sur votre machine localeExécution d'un compte AWSCréation d'un utilisateur IAM sur AWSAWS CLI configurée sur votre machine localeCréation d'un compartiment S3 sur AWS</ol>""}, {'', '<li>VMware Workstation installé sur votre machine locale</li>'}, {'', ""<li>Exécution d'un compte AWS</li>""}, {'', '<li>Utilisateur IAM créé sur AWS</li>'}, {'', '<li>AWS CLI configuré sur votre machine locale</li>'}, {'', ""<li>Création d'un bucket S3 sur AWS</li>""}, {'', ""<p>Créez un fichier d'exportation .vmdk pour Ubuntu 14.04.</p>""}, {''}, {'', '<ol>Télécharger et configurer AWS</ol>'}, {'', '<li>Télécharger et configurer AWS</li>'}, {''}, {'', '<ol>Créer un IAM</ol>'}, {'', '<li>Créer un IAM</li>'}, {'', '<p>IAM – Utilisateur — Ajouter un utilisateur — Type d’accès (accès par programmation) — Joindre une stratégie d’accès administrateur — Créer un utilisateur.</p>'}, {'', ""<ol>Créez un compartiment S3 et ajoutez un fichier (fichier image du système d'exploitation). Vous pouvez télécharger le fichier à l'aide de l'AWS CLI aws s3 cp «\xa0chemin source\xa0» <s3 bucket>Une fois le fichier téléchargé, créez une AMI pour l'image importée</ol>""}, {'', ""<li>Créez un bucket S3 et ajoutez un fichier (fichier image du système d'exploitation).</li>""}, {'', ""<li>Vous pouvez télécharger le fichier à l'aide de l'AWS CLI aws s3 cp «\xa0chemin source\xa0» <s3 bucket></li>""}, {'', ""<li>Une fois le fichier téléchargé, créez une AMI pour l'image importée</li>""}, {'', '<p>Aws ec2 import-image –description « AWS Linux » –disk-containers fichier://containers.json</p>'}, {'', ""<ol>Votre AMI est désormais disponible et vous pouvez la voir sous AMI dans EC2 et l'exécuter au fur et à mesure</ol>""}, {'', ""<li>Votre AMI est désormais disponible et vous pouvez la voir sous AMI dans EC2 et l'exécuter au fur et à mesure</li>""}, {'', '<p>Nous avons ainsi migré avec succès un système d’exploitation virtuel sur AWS.</p>'}, {'', '<p>Vous devriez maintenant avoir une bonne idée de la stratégie de migration vers le cloud AWS et du processus, des outils et des mécanismes de transfert disponibles.</p>'}]"
La mise à jour de Broadcom Virtual Cloud Foundation rationalise les flux de travail DevOps,"[{'', ""<p>Lors de la conférence VMware Explore 2024, Broadcom a mis à jour aujourd'hui sa plateforme VMware Cloud Foundation (VCF) pour fournir à la fois des niveaux d'intégration plus approfondis, des capacités de multi-location et une console unique via laquelle tous les composants VCF peuvent être gérés.</p>""}, {'', ""<p>Paul Turner, vice-président des produits pour la division VCF de Broadcom, a déclaré que VCF 9 réduira considérablement le coût total de possession (TCO), par exemple en éliminant le besoin de plus d'une douzaine de consoles qui étaient auparavant nécessaires.</p>""}, {'', '<p>L’objectif global est de simplifier la gestion par les équipes informatiques des clouds privés qui peuvent être déployés soit dans un environnement informatique sur site, soit sur un service de cloud public, a déclaré Turner.</p>'}, {'', '<p>Dans le cadre de cet effort, Broadcom propose également des services d’évaluation de la maturité du cloud privé virtuel (VPC), de formation et de démarrage.</p>'}, {'', ""<p>De plus, Broadcom ajoute à VCF 9 une fonctionnalité Advanced Memory Tiering avec NVMe qui rend l'exécution d'applications gourmandes en données plus rentable. VCF 9 permet également de gérer des flottes de machines virtuelles, y compris des opérations de sécurité augmentées par l'intelligence artificielle générative (IA), et de déployer des machines virtuelles jusqu'à 61 % plus rapidement.</p>""}, {'', ""<p>Parallèlement, les éléments de virtualisation réseau de la plateforme basée sur VMware NSX sont mieux intégrés à VMware ESXi pour rationaliser les opérations, a noté Turner. Les clouds privés virtuels (VPC) peuvent désormais également tirer parti de services tels que VMware vDefend pour activer les groupes de sécurité et VMware Avi Load Balancer pour permettre le déploiement en un clic des fonctions d'équilibrage de charge. L'hyperviseur VMware peut désormais également servir de menace pour vDefend.</p>""}, {'', ""<p>Un outil d'importation VCF facilitera l'importation de VMware NSX, VMware vDefend, VMware Avi Load Balancer et de topologies de stockage plus complexes dans les environnements VCF existants. VCF 9 prend désormais également en charge la protection des données native vSAN vers vSAN avec des snapshots profonds pour améliorer la résilience.</p>""}, {'', ""<p>Enfin, Broadcom facilite le déploiement de VMware Private AI Foundation avec les fonctionnalités NVIDIA sur VCF 9 en ajoutant la prise en charge de la visibilité du profil de l'unité de processeur graphique virtuelle (vGPU), des réservations de GPU, d'un service d'indexation et de récupération de données et d'un service de création d'agents d'IA.</p>""}, {'', '<p>On ne sait pas exactement combien d’entreprises informatiques adhèrent à VCF. Broadcom fournit des fonctionnalités supplémentaires pour rendre VCF attrayant, mais de nombreuses entreprises informatiques ont déjà investi dans d’autres plates-formes de réseau, de stockage et de sécurité. En outre, de nombreuses équipes DevOps ont adopté des plates-formes alternatives pour accélérer la création et le déploiement d’applications qui ne dépendent pas nécessairement de la machine virtuelle utilisée.</p>'}, {'', ""<p>De nombreuses entreprises informatiques tentent encore de déterminer le coût total d'une licence par abonnement par rapport au modèle de licence perpétuelle pour VMware, qui n'est plus une option. Le principal problème n'est pas tant la migration des charges de travail, mais plutôt la mesure dans laquelle il est économiquement judicieux de déployer des charges de travail supplémentaires sur les plates-formes VMware.</p>""}, {'', '<p>À l’heure où les entreprises informatiques sont de plus en plus sensibles au coût total de l’informatique, la consolidation s’impose. Le défi consiste à convaincre les équipes informatiques qu’elles ne dépendront pas d’un seul fournisseur susceptible d’augmenter les frais de licence à l’avenir, une fois qu’elles n’auront plus d’alternatives viables.</p>'}]"
Apica ajoute la possibilité de gérer de manière centralisée les données de télémétrie collectées via plusieurs agents,"[{'', ""<p>Apica a ajouté aujourd'hui la possibilité de centraliser la gestion des données de télémétrie collectées auprès de plusieurs types d'agents à sa plateforme d'observabilité Apica Ascent.</p>""}, {'', ""<p>Ranjan Parthasarathy, directeur des produits et de la technologie chez Apica, a déclaré que l'ajout de Fleet Data Management simplifiera également la normalisation des données de télémétrie collectées dans des environnements d'application de plus en plus complexes, quel que soit le type d'agent utilisé pour collecter les données.</p>""}, {'', ""<p>Les équipes DevOps peuvent utiliser les métadonnées pour identifier les journaux, les traces et les mesures au sein de ce pool de données normalisées, a-t-il ajouté. Cette approche permet également aux équipes DevOps de réduire le coût total de l'observabilité car les données de télémétrie ne sont pas collectées sans discernement, a ajouté Parthasarathy.</p>""}, {'', ""<p>Enfin, les équipes DevOps gagnent en flexibilité car Fleet Data Manager peut également être utilisé pour configurer automatiquement les agents. Par conséquent, les équipes DevOps ne sont pas liées à des configurations rigides, qui entraînent des erreurs et une baisse des performances, ou enfermées dans des instances propriétaires de logiciels d'agent, a déclaré Parthasarathy.</p>""}, {'', ""<p>Fleet Data Management prend désormais en charge des agents tels que OpenTelemetry Collector, Fluent-bit, OpenTelemetry Kubernetes Collector et Telegraf couvrant des environnements informatiques hybrides, qui peuvent tous être configurés automatiquement. C'est crucial, car plus il devient simple d'instrumenter les applications à l'aide d'agents, plus il est probable qu'elles adopteront l'observabilité, a noté Parthasarathy.</p>""}, {''}, {'', ""<p>Au cœur de la plateforme Apica Ascent se trouve un moteur d'indexation construit sur une plateforme Kubernetes qui regroupe des données telles que des journaux, des traces et des paquets réseau provenant de plusieurs sources. Conçue pour être déployée n'importe où, la plateforme réduit les coûts de stockage en supprimant les données excédentaires qui peuvent être stockées dans un lac de données qu'elle fournit ou, si un client le préfère, dans un lac de données tiers. L'année dernière, Apica a acquis Logic.ai pour ajouter une capacité de data fabric à sa plateforme principale.</p>""}, {'', '<p>Alors que de plus en plus d’équipes DevOps ne se contentent plus de surveiller un ensemble de mesures prédéfinies, elles sont confrontées à des défis allant de la gestion des agents aux coûts de stockage qui échappent à tout contrôle. En outre, les équipes DevOps peuvent ne pas avoir les connaissances et l’expertise nécessaires pour élaborer les requêtes nécessaires pour déterminer la cause profonde d’un problème. En théorie, les algorithmes d’apprentissage automatique feront automatiquement apparaître les alertes critiques, mais il sera toujours nécessaire de comprendre comment un environnement applicatif est construit si une équipe DevOps espère l’optimiser ou, si nécessaire, résoudre un problème.</p>'}, {'', '<p>Malgré ces défis, ce n’est qu’une question de temps avant que la plupart des organisations informatiques commencent à unifier les données d’observabilité au sein d’une seule plateforme. Cela simplifiera la collaboration entre les équipes et devrait entraîner moins de perturbations, car les problèmes informatiques sont détectés plus rapidement. Les plateformes d’observabilité constituent la première étape essentielle pour éliminer à terme bon nombre des silos qui, aujourd’hui, rendent la gestion informatique plus difficile que quiconque ne le souhaiterait.</p>'}, {'', '<p>L’enjeu, comme toujours, n’est pas seulement de trouver un moyen de financer l’acquisition d’une plateforme d’observabilité, mais également d’acquérir les compétences et l’expertise nécessaires pour la maîtriser.</p>'}]"
Changer le visage de la sécurité du développement logiciel : CodeOps,"[{'', '<p>Le développement de logiciels est devenu un véritable champ de mines. Tout au long du cycle de développement de logiciels, environ 60 % des entreprises américaines seront confrontées à des attaques de sécurité de la chaîne d’approvisionnement. Selon Gartner, cela représente une augmentation à trois chiffres de ce type d’attaques au cours des dernières années. Pour atténuer ce problème, les entreprises ont opté pour des stratégies DevSecOps, en s’orientant vers des principes qui implémentent des protocoles de sécurité dans le logiciel le plus tôt possible dans le processus.</p>'}, {'', ""<p>Avec cette mentalité de sécurité, un cadre de développement logiciel étroitement lié à DevSecOps a émergé : CodeOps. Pour garantir le plus haut niveau de sécurité possible à chaque étape du processus de développement, les organisations doivent savoir ce qu'est CodeOps, comment il fonctionne et comment il contribue à sécuriser les logiciels nouveaux et en développement.</p>""}, {'', ""<h3>Qu'est-ce que CodeOps ?</h3>""}, {'', ""<p>CodeOps est une approche de pointe du développement logiciel qui utilise du code réutilisable et détenu en interne pour rationaliser le processus de développement. Basé sur l'IA générative (GenAI) et exploitant le codage modulaire, CodeOps accélère le développement et encourage l'innovation. Grâce à une efficacité et une sécurité accrues, les produits numériques peuvent évoluer et s'améliorer en permanence.</p>""}, {'', ""<p>Les grandes entreprises et organisations telles que l'US Air Force, confrontées à des problèmes de sécurité à haut risque, ont commencé à utiliser cette technologie innovante pour créer rapidement des applications et des logiciels essentiels à leur mission. L'exploitation de GenAI pour générer des composants de code basés sur des spécifications préexistantes et l'utilisation de concepts de codage modulaires permettent aux organisations de créer du code réutilisable et interne, qui constitue la base de CodeOps.</p>""}, {'', '<p>Pour la plupart des entreprises, l’aspect le plus attrayant est l’efficacité. Les spécifications réutilisables et les composants de code pré-certifiés permettent aux entreprises de réduire considérablement le temps de développement, ce qui garantit une mise sur le marché plus rapide des produits numériques. La flexibilité garantit un processus de développement rationalisé sans compromettre la sécurité, ce qui permet aux équipes de s’adapter rapidement aux nouvelles technologies et aux nouvelles demandes à mesure de leur évolution. Grâce à l’automatisation, une grande partie du codage répétitif a été retirée des épaules d’une entreprise, les développeurs peuvent désormais se concentrer sur l’idéation et la conception de niveau supérieur.</p>'}, {'', '<h3>Comment CodeOps sécurise-t-il les logiciels\xa0?</h3>'}, {'', '<p>Si une sécurité rigoureuse semble être une exigence évidente pour les applications bancaires contenant des données financières importantes, les applications de santé contenant des informations médicales sensibles ou les logiciels militaires traitant des renseignements hautement classifiés, elle n’est pas moins cruciale pour les applications à vocation plus générale. Toutes les applications traitent des données sous une forme ou une autre ; par conséquent, toutes les applications doivent fonctionner avec une sécurité rigoureuse qui protège l’intégrité des données et la confidentialité des utilisateurs. Même dans les entreprises dont les données sont moins sensibles, les répercussions des failles de sécurité peuvent être extrêmement coûteuses, tant sur le plan financier que sur la réputation de la marque.</p>'}, {'', '<p>Voici les moyens par lesquels CodeOps peut garantir une meilleure sécurité\xa0:</p>'}, {'', ""<li>Tirer parti des modules de code éprouvés. L'un des aspects clés de CodeOps est de s'appuyer sur le code existant pour gagner du temps, et l'utilisation du code existant signifie que tous les modules sont pré-approuvés et conformes aux directives de sécurité existantes. Tous les nouveaux composants doivent être sécurisés, pré-sélectionnés et pré-testés sur l'ensemble de la pile (échafaudages, écrans, connecteurs, modèles, déployeurs, etc.), ce qui signifie qu'il n'est pas nécessaire de recertifier individuellement les morceaux de code pour les nouveaux projets avant qu'ils ne soient utilisés.</li>""}, {'', ""<li>Audits de code de routine. Lors du développement de logiciels avec CodeOps, l'intégralité de la base de code est entièrement vérifiable, ce qui permet aux équipes de sécurité de déterminer que le code déployé : est conforme aux mesures de sécurité internes telles que les configurations de ports réseau et le principe du moindre accès ; ne porte atteinte à aucun droit de propriété intellectuelle ; empêche les problèmes de sécurité connus via des bibliothèques importées ou autres de corrompre les logiciels.</li>""}, {'', '<li>Conforme aux mesures de sécurité internes telles que les configurations de ports réseau et le principe du moindre accès</li>'}, {'', '<li>Ne porte atteinte à aucun droit de propriété intellectuelle</li>'}, {'', '<li>Empêche les problèmes de sécurité connus via des bibliothèques importées ou similaires de corrompre les logiciels.</li>'}, {'', '<li>Utiliser les mesures de sécurité les plus récentes. Les meilleures pratiques en matière de développement de logiciels doivent évoluer au rythme des nouvelles technologies. De même, à mesure que de nouvelles tendances et techniques émergent dans le domaine de la sécurité logicielle, les modules de code sont mis à jour pour permettre aux organisations de mettre en œuvre plus facilement les dernières exigences de sécurité. Cela garantit une conformité cohérente et fiable aux normes de sécurité, protégeant les données à chaque étape du développement logiciel.</li>'}, {'', '<p>En fin de compte, avec l’adoption rapide de GenAI, le code sera de plus en plus écrit à la demande par des machines plutôt que par des humains. Même si cela peut sembler peu sûr à première vue, le fait est que CodeOps permet une telle approche. CodeOps est axé sur la réutilisation, pas seulement des composants logiciels sur mesure, mais la réutilisation de tout logiciel antérieur. Il permet aux organisations d’auditer, d’approuver et de stocker le code créé à partir de n’importe quelle source dans leurs catalogues privés. Ce code peut ensuite être facilement découvert au stade de la spécification afin que les efforts ne soient pas dupliqués.</p>'}, {'', '<p>Essentiellement, CodeOps et GenAI sont alignés : CodeOps traite le code produit à l’aide de GenAI comme une autre source de code et se concentre sur la résolution des problèmes de sécurité et de découvrabilité inhérents à la réutilisation des actifs existants.</p>'}]"
GitHub oriente Copilot Autofix vers l'œil de la tempête de sécurité de l'IA,"[{'', ""<p>GitHub, société de gestion de versions distribuées et de plateforme de collaboration, a présenté son nouvel outil Copilot Autofix. Ce service logiciel basé sur l'IA s'adresse aux développeurs qui doivent remédier aux vulnérabilités logicielles dans le code destiné aux applications traditionnelles et dans celles imprégnées d'une injection de nouvelles ou existantes races d'IA.</p>""}, {'', '<p>Copilot Autofix fait partie du groupe de produits de la plateforme GitHub Advanced Security (GHAS). Mentionnée pour la première fois au printemps de cette année, la technologie était au stade de la version bêta publique avant de passer ce mois-ci à une version complète.</p>'}, {'', '<p>Ce produit finalisé de première version intègre le moteur d’analyse de code CodeQL de GitHub, un moteur d’analyse de code développé par GitHub pour automatiser les contrôles de sécurité afin que les développeurs puissent analyser et afficher les résultats sous forme d’alertes d’analyse de code. Il intègre également GPT-4o, un modèle de langage multimodal de grande taille qui offre des fonctionnalités de conversation en temps réel et de génération de texte. Copilot Autofix propose également des heuristiques et la technologie dispose de son propre ensemble d’API pour permettre aux équipes d’implémenter son ensemble d’outils et de créer des suggestions de code (et des extraits de code) pour corriger et remédier aux vulnérabilités. Les développeurs peuvent accepter, modifier ou rejeter les suggestions de code proposées.</p>'}, {'<h3>Trouvé oui, corrigé peut-être</h3>', ''}, {'', '<p>« Les outils d’analyse de code détectent les vulnérabilités, mais ils ne s’attaquent pas au problème fondamental [de la réparation des logiciels] : la correction nécessite une expertise en sécurité et du temps, deux ressources précieuses dont on manque cruellement. En d’autres termes, le problème n’est pas de trouver les vulnérabilités, mais de les corriger », a déclaré Mike Hanley, responsable de la sécurité et vice-président senior de l’ingénierie chez GitHub.</p>'}, {'', ""<p>En utilisant son nouveau slogan « Trouvé signifie corrigé » pour aborder ce point précis, Hanley explique que Copilot Autofix analyse les vulnérabilités dans le code, explique pourquoi elles sont importantes… et propose des suggestions de code qui aident les développeurs à corriger les vulnérabilités aussi rapidement qu'elles sont découvertes.</p>""}, {'', '<blockquote>« Au cours de la version bêta publique, nous avons constaté que les développeurs corrigeaient les vulnérabilités du code plus de trois fois plus rapidement que ceux qui le faisaient manuellement, ce qui constitue un exemple frappant de la manière dont les agents d’IA peuvent radicalement simplifier et accélérer le développement de logiciels sécurisés », s’enthousiasme Hanley. « Les développeurs peuvent éviter les nouvelles vulnérabilités dans leur code grâce à Copilot Autofix dans la demande d’extraction, et désormais également réduire l’arriéré de la dette de sécurité en générant des correctifs pour les vulnérabilités existantes. »</blockquote>'}, {'', '<p>« Au cours de la version bêta publique, nous avons constaté que les développeurs corrigeaient les vulnérabilités du code plus de trois fois plus rapidement que ceux qui le faisaient manuellement, ce qui constitue un exemple frappant de la manière dont les agents d’IA peuvent radicalement simplifier et accélérer le développement de logiciels sécurisés », s’enthousiasme Hanley. « Les développeurs peuvent éviter les nouvelles vulnérabilités dans leur code grâce à Copilot Autofix dans la demande d’extraction, et désormais également réduire l’arriéré de la dette de sécurité en générant des correctifs pour les vulnérabilités existantes. »</p>'}, {'', '<p>La division GHAS de GitHub a déclaré qu’elle avait de grands projets pour Copilot Autofix et ses ensembles d’outils de plateforme associés. Elle s’efforce d’améliorer la portée et la précision de « l’analyse secrète » actuelle. L’organisation définit l’analyse secrète comme une fonctionnalité de sécurité qui permet de détecter et d’empêcher l’inclusion accidentelle d’informations sensibles telles que les clés API, les mots de passe, les jetons et autres secrets dans le référentiel de code d’une équipe DevOps. L’approche de GitHub ici signifie que l’analyse secrète analyse les validations de code dans les référentiels pour les types de secrets connus afin que les administrateurs du référentiel puissent être alertés en cas de détection. Hanley de GitHub a également déclaré que l’équipe développe de nouveaux flux de travail qui font évoluer Copilot Autofix pour les organisations ayant un volume élevé de dettes de sécurité, le tout sur des plateformes de développement familières.</p>'}, {'', ""<p>Avec une prise en charge initiale de JavaScript, TypeScript, Java et Python, Copilot Autofix étend désormais également la prise en charge à C#, C/C++, Go, Kotlin, Swift et Ruby. Disponible gratuitement pour les développeurs travaillant sur des projets open source, les clients GitHub Enterprise Cloud payants qui s'abonnent à GHAS trouveront Copilot Autofix activé par défaut dans leurs paramètres GHAS.</p>""}, {""<h3>L'IA pour le bien, pour le bien ?</h3>"", ''}, {'', '<p>Compte tenu de la vitesse à laquelle les équipes DevOps modernes doivent tenter de créer des offres logicielles fonctionnelles et du fait que tous les groupes ne disposent pas d’un gourou de la sécurité, GitHub affirme que le marché est mûr pour un outil alimenté par l’IA capable de remédier à ce niveau. À une époque où les professionnels moins techniques sont « préoccupés par l’impact de l’IA », il s’agit peut-être d’un bon exemple d’intelligence d’automatisation utilisée pour résoudre des problèmes plutôt que pour en créer de nouveaux.</p>'}]"
Libérez-vous des ransomwares : sécurisez votre CI/CD contre les RaaS,"[{'', ""<p>Pour les développeurs, peu de choses sont plus précieuses que leur base de code. Pourtant, une tendance effrayante émerge : les attaques de type Ransomware-as-a-Service (RaaS) ciblant les pipelines CI/CD, prenant en otage du code précieux. En 2023, le nombre d'attaques de ransomware a augmenté de près de 70 %, transformant cette menace autrefois théorique en une réalité bien réelle pour les équipes de développement de logiciels du monde entier.</p>""}, {'', '<p>Avant de nous plonger dans les tactiques RaaS qui exploitent les pipelines CI/CD, prenons du recul et comprenons le cycle de vie typique des ransomwares.</p>'}, {'', ""<li>Infection\xa0: les attaquants s'infiltrent dans votre système, souvent via des dépendances malveillantes, des vulnérabilités de script ou des serveurs de build compromis</li>""}, {'', '<li>Mouvement latéral : ils explorent furtivement votre réseau, à la recherche de ressources critiques et de référentiels de code</li>'}, {'', '<li>Cryptage : une fois intégrée, la charge utile du ransomware entre en action, cryptant vos fichiers et bases de code précieux</li>'}, {'', ""<li>Extorsion : le rideau se lève sur le véritable motif : une demande de rançon, souvent accompagnée d'un compte à rebours et de menaces terribles de perte permanente de données ou d'exposition publique.</li>""}, {'', '<h3>Comment les ransomwares en tant que service ciblent les pipelines CI/CD</h3>'}, {'', '<p>Cette vulnérabilité résulte d’une confluence de facteurs :</p>'}, {'', ""<ol>Dépendance accrue aux dépendances tierces : le développement de logiciels modernes repose largement sur un vaste écosystème de bibliothèques tierces open source et commerciales. Bien que cela soit pratique, cela introduit une surface d'attaque critique. Les acteurs malveillants peuvent intégrer des charges utiles de ransomware dans des dépendances apparemment légitimes, les transformant en chevaux de Troie pour infiltrer des communautés de développeurs entières. Un rapport récent de Sonatype a révélé que le nombre de packages open source malveillants a triplé en 2023, soulignant le risque généralisé posé par les dépendances compromises.</ol>""}, {'', ""<li>Dépendance accrue aux dépendances tierces : le développement de logiciels modernes repose largement sur un vaste écosystème de bibliothèques tierces open source et commerciales. Bien que cela soit pratique, cela introduit une surface d'attaque critique. Les acteurs malveillants peuvent intégrer des charges utiles de ransomware dans des dépendances apparemment légitimes, les transformant en chevaux de Troie pour infiltrer des communautés de développeurs entières. Un rapport récent de Sonatype a révélé que le nombre de packages open source malveillants a triplé en 2023, soulignant le risque généralisé posé par les dépendances compromises.</li>""}, {''}, {'', ""<li>Paquets trojanisés : les attaquants injectent des charges utiles de ransomware dans des logiciels open source ou commerciaux populaires ou largement utilisés. L'attaque de la chaîne d'approvisionnement de SolarWinds en 2023, où les attaquants ont compromis un serveur de création de logiciels pour injecter du code malveillant dans les mises à jour, en sert d'exemple.</li>""}, {'', ""<li>Dépendances invisibles : 80 % du code des applications modernes est du code open source et, comme le rapportent les résultats, 95 % des vulnérabilités se trouvent dans les dépendances transitives. La plupart des menaces de sécurité, y compris les vulnérabilités connues, se cachent dans la mer de dépendances transitives. Le problème est que les développeurs ont rarement une visibilité sur leur arborescence de dépendances ou sur sa profondeur. Log4Shell est un exemple célèbre qui a eu un impact sur 93 % des environnements cloud d'entreprise !</li>""}, {'', ""<ol>Vulnérabilités d'automatisation : les pipelines CI/CD prospèrent grâce à l'automatisation, mais les scripts et configurations d'automatisation peuvent contenir des erreurs de syntaxe, des paramètres de configuration non sécurisés et des contrôles d'accès inadéquats qui peuvent fournir des points d'appui aux attaquants pour compromettre les processus de construction et manipuler le code. En 2022, GitHub a signalé une vulnérabilité dans sa plateforme Actions qui permettait un accès non autorisé aux référentiels, illustrant l'impact potentiel de telles failles de script. Attaques par injection : les attaquants exploitent les erreurs de syntaxe ou les paramètres de configuration non sécurisés dans les scripts CI/CD pour injecter du code malveillant ou manipuler la construction. Escalade des privilèges : en exploitant les vulnérabilités des outils de construction ou de l'infrastructure sous-jacente, les attaquants peuvent augmenter leurs privilèges au sein du pipeline, accédant ainsi à des ressources sensibles et</ol>""}, {'', ""<li>Vulnérabilités d'automatisation : les pipelines CI/CD prospèrent grâce à l'automatisation, mais les scripts et configurations d'automatisation peuvent contenir des erreurs de syntaxe, des paramètres de configuration non sécurisés et des contrôles d'accès inadéquats qui peuvent fournir des points d'appui aux attaquants pour compromettre les processus de construction et manipuler le code. En 2022, GitHub a signalé une vulnérabilité dans sa plateforme Actions qui permettait un accès non autorisé aux référentiels, illustrant l'impact potentiel de telles failles de script. Attaques par injection : les attaquants exploitent les erreurs de syntaxe ou les paramètres de configuration non sécurisés dans les scripts CI/CD pour injecter du code malveillant ou manipuler la construction. Escalade des privilèges : en exploitant les vulnérabilités des outils de construction ou de l'infrastructure sous-jacente, les attaquants peuvent augmenter leurs privilèges au sein du pipeline, accédant ainsi à des ressources sensibles et</li>""}, {'', '<li>Attaques par injection : les attaquants exploitent les erreurs de syntaxe ou les paramètres de configuration non sécurisés dans les scripts CI/CD pour injecter du code malveillant ou manipuler la build.</li>'}, {'', ""<li>Escalade des privilèges : en exploitant les vulnérabilités des outils de création ou de l'infrastructure sous-jacente, les attaquants peuvent élever leurs privilèges au sein du pipeline, accédant ainsi à des ressources sensibles et</li>""}, {''}, {'', ""<ol>Visibilité et contrôle limités : la nature rapide des déploiements CI/CD dépasse souvent les outils traditionnels de surveillance de la sécurité et d'analyse des journaux qui ont du mal à suivre le rythme des processus de construction en constante évolution, créant ainsi des angles morts que les attaquants peuvent exploiter. 68 % des organisations rencontrent des vulnérabilités dans leurs applications pendant les phases de construction et de déploiement, ce qui reflète les défis liés à la sécurisation de ces environnements dynamiques.</ol>""}, {'', ""<li>Visibilité et contrôle limités : la nature rapide des déploiements CI/CD dépasse souvent les outils traditionnels de surveillance de la sécurité et d'analyse des journaux qui ont du mal à suivre le rythme des processus de construction en constante évolution, créant ainsi des angles morts que les attaquants peuvent exploiter. 68 % des organisations rencontrent des vulnérabilités dans leurs applications pendant les phases de construction et de déploiement, ce qui reflète les défis liés à la sécurisation de ces environnements dynamiques.</li>""}, {''}, {'', '<li>Exploits zero-day : les attaquants exploitent les vulnérabilités zero-day des serveurs de build ou des plateformes CI/CD pour obtenir un accès non autorisé et déployer des charges utiles de ransomware</li>'}, {'', ""<li>Mouvement latéral : une fois qu'un point d'appui est établi, les attaquants se déplacent latéralement au sein de l'infrastructure CI/CD, compromettant ainsi des ressources et du code supplémentaires</li>""}, {''}, {'', ""<p>Ces tactiques mettent en évidence la sophistication croissante des acteurs du RaaS. Leur impact peut être dévastateur, allant du cryptage des données et des demandes de rançon à l'atteinte à la réputation et à la paralysie opérationnelle.</p>""}, {'', ""<h3>Création de pipelines à l'épreuve des rançons</h3>""}, {'', '<p>En adoptant une mentalité DevSecOps proactive, nous pouvons repousser les attaques RaaS et protéger notre code. Voici votre boîte à outils\xa0:</p>'}, {'', '<ol>Shift Security Left : n’attendez pas le déploiement pour resserrer les vis. Intégrez la sécurité tout au long du cycle de vie du développement logiciel (SDLC). Tirez parti de l’analyse de la composition logicielle (SCA) et de la création de la nomenclature logicielle (SBOM), qui vous aident à examiner les dépendances pour détecter les vulnérabilités et à conserver un enregistrement transparent de chaque composant logiciel de votre pipeline.</ol>'}, {'', '<li>Shift Security Left : n’attendez pas le déploiement pour resserrer les vis. Intégrez la sécurité tout au long du cycle de vie du développement logiciel (SDLC). Tirez parti de l’analyse de la composition logicielle (SCA) et de la création de la nomenclature logicielle (SBOM), qui vous aident à examiner les dépendances pour détecter les vulnérabilités et à conserver un enregistrement transparent de chaque composant logiciel de votre pipeline.</li>'}, {''}, {'', '<ol>Vigilance continue : vos pipelines ne sont pas des entités statiques, ce sont des écosystèmes vivants qui exigent une surveillance et une journalisation constantes de l’activité des pipelines. Recherchez les anomalies, les comportements suspects et les tentatives d’accès non autorisées. Considérez cela comme un faucon de la cybersécurité qui surveille en permanence vos pipelines, détectant les menaces avant qu’elles ne prennent racine.</ol>'}, {'', '<li>Vigilance continue : vos pipelines ne sont pas des entités statiques, ce sont des écosystèmes vivants qui exigent une surveillance et une journalisation constantes de l’activité des pipelines. Recherchez les anomalies, les comportements suspects et les tentatives d’accès non autorisées. Considérez cela comme un faucon de la cybersécurité qui surveille en permanence vos pipelines, détectant les menaces avant qu’elles ne prennent racine.</li>'}, {''}, {'', ""<ol>Forteresse de contrôle d'accès\xa0: minimisez les accès inutiles à votre environnement CI/CD. Appliquez des contrôles d'accès stricts basés sur les rôles et le principe du moindre privilège. Utilisez des outils de contrôle d'accès pour gérer étroitement les rôles et les autorisations des utilisateurs, en veillant à ce que seuls les utilisateurs autorisés puissent interagir avec les ressources sensibles. N'oubliez pas que la vulnérabilité GitHub de 2022 a exposé les dangers d'un contrôle d'accès laxiste dans les environnements CI/CD.</ol>""}, {'', ""<li>Forteresse de contrôle d'accès\xa0: minimisez les accès inutiles à votre environnement CI/CD. Appliquez des contrôles d'accès stricts basés sur les rôles et le principe du moindre privilège. Utilisez des outils de contrôle d'accès pour gérer étroitement les rôles et les autorisations des utilisateurs, en veillant à ce que seuls les utilisateurs autorisés puissent interagir avec les ressources sensibles. N'oubliez pas que la vulnérabilité GitHub de 2022 a exposé les dangers d'un contrôle d'accès laxiste dans les environnements CI/CD.</li>""}, {''}, {'', '<ol>Automatisation avec garde-fous : l’automatisation est votre amie, mais ne la laissez pas de côté. Sécurisez vos pratiques de création de scripts en adoptant des outils d’analyse statique et testez vos scripts de build pour détecter les vulnérabilités. Mettez en œuvre des systèmes automatisés de détection des anomalies, par exemple pour détecter les écarts suspects dans le comportement du pipeline avant qu’ils ne s’aggravent.</ol>'}, {'', '<li>Automatisation avec garde-fous : l’automatisation est votre amie, mais ne la laissez pas de côté. Sécurisez vos pratiques de création de scripts en adoptant des outils d’analyse statique et testez vos scripts de build pour détecter les vulnérabilités. Mettez en œuvre des systèmes automatisés de détection des anomalies, par exemple pour détecter les écarts suspects dans le comportement du pipeline avant qu’ils ne s’aggravent.</li>'}, {''}, {'', ""<ol>Infrastructure immuable : envisagez des pratiques d'infrastructure immuables, où l'infrastructure est provisionnée et configurée sous forme de code, puis jamais modifiée. Cette approche, défendue par des outils comme Terraform, garantit des environnements cohérents et sécurisés en minimisant l'intervention humaine et les erreurs de configuration potentielles.</ol>""}, {'', ""<li>Infrastructure immuable : envisagez des pratiques d'infrastructure immuables, où l'infrastructure est provisionnée et configurée sous forme de code, puis jamais modifiée. Cette approche, défendue par des outils comme Terraform, garantit des environnements cohérents et sécurisés en minimisant l'intervention humaine et les erreurs de configuration potentielles.</li>""}, {''}, {'', '<p>L’émergence du RaaS ciblant les pipelines CI/CD exige un changement fondamental dans les pratiques de développement logiciel. Les mesures de sécurité réactives traditionnelles ne suffisent plus dans ce nouveau paysage de menaces. Au lieu de cela, une approche DevSecOps proactive, pilotée par des outils et des méthodologies robustes, est primordiale. Si aucun des points de cet article n’était suffisamment alarmant, terminons avec quelques citations de Christopher Wray, directeur du FBI : « Dans le cyberespace, les menaces ne semblent qu’évoluer et les enjeux n’ont jamais été aussi élevés. Et au cours des dernières années, nous avons vu de plus en plus de cybercriminels utiliser des ransomwares contre le secteur des infrastructures critiques aux États-Unis. Les victimes ciblées par le groupe Hive ont renforcé ce que nous savons : les groupes de ransomware ne font pas de discrimination. Ils s’en sont pris aux grandes et aux petites entreprises. »</p>'}]"
Ne bougez pas à gauche sans plateforme,"[{'', ""<p>Le terme shifting left (décalage vers la gauche) est un terme créé par Larry Smith en 2001. Il fait référence au cycle de vie de l'ingénierie logicielle. À l'époque, il était normal de tout construire, puis de tout tester et enfin de tout publier. Larry Smith a fait valoir qu'il était préférable d'effectuer les tests à un stade antérieur du pipeline du projet, d'où le terme shifting left (décalage vers la gauche).</p>""}, {'', ""<p>À partir de là, les choses ont commencé à devenir intéressantes. En 2007, le mouvement DevOps a pris forme. Une conséquence évidente de DevOps a été de faire avancer le déploiement plus tôt dans le pipeline du projet, en d'autres termes, de le déplacer vers la gauche. Vers 2015, la sécurité est entrée dans la mêlée, ce qui a donné naissance à DevSecOps.</p>""}, {'', ""<p>La tendance est claire. De plus en plus d'objets ont été retirés des mains des spécialistes, transformés en facilitateurs (nous y reviendrons plus loin), pour être confiés à des ingénieurs, qui ont dû rapidement devenir des généralistes.</p>""}, {'', '<h3>Alors, pourquoi est-ce un problème ?</h3>'}, {'', '<p>Commençons par une approche traditionnelle de l’ingénierie logicielle. Les développeurs construisent l’itération dans son intégralité. Les testeurs la testent et trouvent des bugs. Il y a beaucoup d’allers-retours. Ensuite, l’équipe de sécurité analyse et l’équipe de bases de données crée des requêtes et stocke les procédures avant que l’équipe d’exploitation ne les mette en production. Cela présente l’inconvénient majeur de nécessiter une ingénierie par lots.</p>'}, {'', ""<p>Des lots plus petits sont synonymes d'un meilleur flux, et un meilleur flux est synonyme d'une livraison logicielle plus prévisible. Par conséquent, nous devons diviser notre travail en incréments beaucoup plus petits. Cependant, maintenant, au lieu de traiter une seule demande importante, l'équipe de test est inondée de dizaines de demandes provenant de chaque équipe d'application qui construit un composant différent du système. Cela va rapidement devenir un goulot d'étranglement.</p>""}, {'', ""<h3>Le problème du goulot d'étranglement des spécialistes</h3>""}, {'', ""<p>Lorsqu'une seule personne sait comment réaliser une tâche, à mesure que l'entreprise évolue, cette personne devient la principale contrainte à la productivité. On les appelle des points de défaillance uniques (SPOF). Ils doivent être présents à chaque révision, à chaque version et constituent un élément clé de toute session de conception.</p>""}, {'', '<p>Les spécialistes sont essentiels pour une organisation, mais leur incapacité à évoluer constitue un énorme problème, ce qui pousse les organisations à se concentrer sur des généralistes en raison du potentiel d’économies (une personne peut faire cinq choses) et de la promesse de flexibilité, donc de l’absence de contraintes strictes. Ce n’est pas une mauvaise stratégie, à condition que vous n’ayez pas besoin de spécialistes. Mais comme le dit le dicton, on ne sait pas qu’on a besoin d’un plombier jusqu’à ce que sa maison soit inondée.</p>'}, {'', '<h3>Du spécialiste au facilitateur</h3>'}, {'', '<p>Team Topologies décrit quatre types d’équipes différentes, dont l’une est l’équipe d’activation. Cette équipe élimine les obstacles et permet aux équipes de se lancer. La manière dont elles s’y prennent peut être décomposée en plusieurs options : facilitation, collaboration ou « X en tant que service ». Nous pouvons y voir l’état d’esprit fertile d’une plateforme.</p>'}, {'', '<p>Sans plateforme, nous sommes un groupe d’individus possédant des connaissances spécialisées, cherchant tous à collaborer et, par conséquent, contraints au rythme de notre lien le plus lent. Avec une plateforme, nous élaborons chacun des solutions pour multiplier notre présence dans l’organisation. Notre objectif est d’amplifier l’impact de nos connaissances. À quoi cela ressemble-t-il ?</p>'}, {'<h3>Des opérations à la SRE</h3>', ''}, {'', ""<p>Plutôt qu'une équipe opérationnelle qui connaît les particularités du déploiement et gère tout personnellement, une équipe SRE élabore des solutions qui permettent aux équipes de déployer en toute sécurité. Ils mettent en œuvre des garde-fous, déploient des stratégies de test, créent des mécanismes d'auto-réparation et bien plus encore. Toutes ces choses font qu'elles font partie de toutes les équipes à la fois.</p>""}, {'', ""<h3>De l'InfoSec à la SecOps</h3>""}, {'', '<p>Au lieu d’une armée d’employés armés de presse-papiers et dont le travail consiste à surveiller les activités des équipes, il serait plus efficace d’avoir un groupe de personnes soucieuses de la conformité et de l’ingénierie, qui recherchent constamment les vulnérabilités, conseillent les équipes, créent des fonctionnalités et contribuent à la plateforme partagée. Cela, encore une fois, augmente considérablement la portée de l’équipe et réduit le nombre de réunions nécessaires pour mettre un produit en production.</p>'}, {'', '<h3>Un exemple classique</h3>'}, {'', '<p>Imaginons une organisation qui gère Kubernetes. Elle regroupe des professionnels de la sécurité, des professionnels des opérations, des ingénieurs, etc. Plutôt que de travailler en vase clos, en exigeant des réunions les uns avec les autres pour garantir la conformité avec un cadre, ils collaborent à la création d’un pipeline. Ce pipeline garantit plusieurs choses :</p>'}, {'', '<li>Tout le code a une couverture de test minimale (généralement 80\xa0%)</li>'}, {'', '<li>Toutes les dépendances sont exemptes de CVE connus</li>'}, {'', '<li>Le code lui-même ne présente aucune vulnérabilité connue, comme une attaque par injection SQL</li>'}, {'', ""<li>Le fichier conteneur ne contient aucun CVE ou il s'agit d'une image provenant d'un fichier pré-numérisé</li>""}, {'', ""<p>Ensuite, l'application est déployée à l'aide d'un organigramme partagé, qui comporte un autre ensemble de responsabilités\xa0:</p>""}, {'', '<li>Les politiques réseau garantissent que seuls les ports nécessaires sont ouverts</li>'}, {'', ""<li>Les souillures, les cordons, les affinités et les sélecteurs de nœuds sont définis pour garantir que cette charge de travail s'exécute sur le bon serveur</li>""}, {'', ""<li>L'application dispose d'un contrôle de santé qui est utilisé pour les contrôles de vivacité</li>""}, {'', ""<li>L'application dispose d'un HorizontalPodAutoscaler pour garantir qu'elle peut évoluer pour répondre aux besoins</li>""}, {'', '<p>Plutôt que de tenir des réunions interminables pour garantir la conformité de chaque version, chaque équipe de parties prenantes doit renforcer la confiance dans le diagramme de gestion. Cette confiance se traduit par une confiance, et cette confiance ouvre un tout nouveau niveau de collaboration.</p>'}, {'', '<h3>Le déplacement vers la gauche nécessite une multiplication des forces</h3>'}, {'', '<p>Si vous souhaitez déplacer vos processus vers la gauche, réduire la taille des lots et améliorer le flux, le défi que vous devez relever est le suivant : comment conserver la rigueur des spécialistes lorsque les choses sont gérées par des généralistes ? La réponse est une plateforme.</p>'}, {'', ""<p>La réflexion sur les plateformes transforme un millier de réunions en quelques centaines de lignes de code, et si vous souhaitez vous déplacer vers la gauche sans sacrifier la perspicacité et la sagesse de vos spécialistes, c'est le seul moyen de garantir que chaque équipe bénéficie de leurs connaissances sans contraintes excessives sur sa capacité à fournir des logiciels.</p>""}]"
DevSecOps : Intégration de la sécurité dans le cycle de vie DevOps,"[{'', ""<p>Tout comme un livre captivant transformé en film se déroule scène par scène, DevSecOps intègre la sécurité dans le processus DevOps, en dressant un tableau vivant de la protection à chaque étape. Intégrer la sécurité dans le flux DevOps ne consiste pas seulement à cocher des cases ; il s'agit d'intégrer la protection dans la structure de la création de logiciels, du début à la fin.</p>""}, {'', '<p>DevSecOps intègre la sécurité dans le flux de travail DevOps. Il intervient pour remédier aux failles de sécurité sur lesquelles le DevOps traditionnel trébuche souvent, en intégrant des mesures de protection du début à la fin. DevSecOps ne se contente pas de cocher des cases ; il s’agit d’intégrer la sécurité dans la création de logiciels du début à la fin, ce qui non seulement renforce la protection, mais accélère également les choses et nous permet de rester du bon côté des réglementations. Les consultants DevOps jouent un rôle clé dans ce processus, en fournissant une expertise et des conseils pour garantir que les mesures de sécurité s’alignent parfaitement sur la stratégie DevOps globale, favorisant ainsi un environnement de développement harmonieux et sécurisé.</p>'}, {'', '<h3>Comprendre DevSecOps</h3>'}, {'', ""<p>DevSecOps est une approche stratégique qui intègre la sécurité dans le pipeline DevOps plutôt que de la traiter comme une réflexion ultérieure ou un processus séparé. Passer à DevSecOps signifie que nous intégrons la sécurité directement dans le mix de développement, de sorte qu'il ne s'agit pas d'une simple vérification de dernière minute, mais d'une partie intégrante de chaque étape du déploiement de nouveaux éléments. Le principal avantage de DevSecOps est la création d'un produit final plus sécurisé, car la sécurité est une préoccupation constante tout au long du processus de développement, plutôt qu'un obstacle final à franchir.</p>""}, {'', '<h3>Principes clés de DevSecOps</h3>'}, {'', '<p>Le cœur de DevSecOps est la « sécurité en tant que code », un principe qui impose d’intégrer la sécurité dans le processus de développement logiciel. Pour que chaque version soit rigoureusement sécurisée, nous intégrons ces pratiques au cœur de notre flux CI/CD. L’automatisation est ici essentielle, car elle simplifie l’ensemble du processus de sécurité dans notre processus de développement, garantissant que nous sommes en sécurité dès le départ sans nous ralentir.</p>'}, {'', '<p>Le modèle de responsabilité partagée est un autre pilier de DevSecOps. La sécurité n’est plus le domaine exclusif d’une équipe de sécurité distincte, mais une préoccupation partagée par toutes les équipes impliquées dans le cycle de développement. En travaillant ensemble, la sécurité n’est pas simplement appliquée à la fin, mais intégrée à chaque étape du début à la fin.</p>'}, {'', '<h3>Mise en œuvre de DevSecOps dans le cycle de vie du développement</h3>'}, {'', ""<p>L'intégration de la sécurité à chaque phase du cycle de vie DevOps nécessite des outils et des pratiques spécifiques :</p>""}, {'', '<li>Planification et codage : les exigences de sécurité sont identifiées en amont et des pratiques de codage sécurisées sont adoptées. Des outils tels que les tests de sécurité statique des applications (SAST) peuvent être utilisés pour analyser le code source à la recherche de vulnérabilités de sécurité.</li>'}, {'', ""<li>Création et test : des outils de test de sécurité automatisés tels que les tests de sécurité dynamique des applications (DAST) sont intégrés au processus de création pour détecter les vulnérabilités d'exécution.</li>""}, {'', ""<li>Publication et déploiement\xa0: des contrôles de sécurité sont intégrés à l'étape de prépublication pour garantir la sécurité de l'environnement de déploiement. Des contrôles de conformité automatisés peuvent également être intégrés ici.</li>""}, {'', ""<li>Exploitation et surveillance : des outils de surveillance continue sont utilisés pour détecter et répondre aux menaces de sécurité en temps réel, garantissant ainsi une protection continue de l'application déployée.</li>""}, {'', ""<h3>Surmonter les défis liés à l'adoption de DevSecOps</h3>""}, {'', '<p>Adopter DevSecOps n’est pas sans poser quelques défis. Passer à DevSecOps signifie que nous devons abattre les murs qui ont longtemps séparé nos développeurs, nos opérateurs et nos équipes de sécurité. Il peut être difficile de trouver un équilibre entre la nécessité d’un déploiement rapide et les considérations de sécurité. Pour réussir DevSecOps, les équipes doivent améliorer leurs compétences grâce à des formations ciblées. Associer des systèmes éprouvés à des tactiques DevSecOps de pointe nécessite une approche stratégique pointue.</p>'}, {'', '<h3>Tendances futures en matière de DevSecOps</h3>'}, {'', '<p>À mesure que DevSecOps gagne en maturité, plusieurs tendances clés émergent qui devraient façonner son avenir :</p>'}, {'', '<li>IA et Machine Learning (ML) dans la sécurité automatisée : l’intégration de l’IA et du Machine Learning dans DevSecOps devient de plus en plus répandue. Les systèmes de sécurité automatisés peuvent désormais détecter et arrêter les menaces plus rapidement. Tirant parti des leçons des erreurs de sécurité précédentes, les systèmes d’IA évoluent pour se protéger de manière proactive contre les menaces émergentes avec une précision accrue.</li>'}, {'', '<li>Accent accru sur la conformité et la gouvernance : avec des réglementations de protection des données et des normes industrielles plus strictes, la conformité et la gouvernance occupent une place centrale dans DevSecOps. Des contrôles de conformité automatisés et des outils de gouvernance sont intégrés au pipeline CI/CD pour garantir que le logiciel répond aux exigences réglementaires à chaque étape du développement.</li>'}, {'', ""<li>Tout comme un film doit rester fidèle au livre dont il est tiré, la sécurité cloud-native doit adhérer étroitement aux principes DevSecOps pour vraiment fonctionner. Alors que le cloud computing continue de dominer, les pratiques de sécurité cloud-native deviennent partie intégrante de DevSecOps. L'adaptation des mesures de sécurité à la configuration cloud en constante évolution et évolutive est essentielle pour maintenir la cohérence dans un monde axé sur le cloud.</li>""}, {'', '<h3>Conclusion</h3>'}, {'', ""<p>DevSecOps facilite le développement de logiciels plus sûrs et plus efficaces. L'adoption de DevSecOps renforce la sécurité tout en améliorant l'efficacité et la fiabilité du développement logiciel. Adopter DevSecOps, c'est comme se préparer pour un marathon : il s'agit de rester agile, de s'adapter aux nouvelles technologies à la volée et de suivre le rythme d'un environnement commercial en constante évolution.</p>""}, {'', '<p>À l’avenir, DevSecOps continuera de façonner la manière dont nous créons des logiciels, restant crucial dans notre monde axé sur la technologie. En intégrant la sécurité directement dans le flux de travail de développement, DevSecOps garantit que nos logiciels sont non seulement robustes et rapides, mais également sécurisés et conformes, tout cela grâce à l’utilisation intelligente de l’IA. À l’ère numérique effrénée dans laquelle nous vivons, l’adoption de DevSecOps est essentielle pour créer une technologie robuste qui résiste à l’épreuve du temps.</p>'}]"
AWS clôture la sélection d'outils de code cloud pour les nouveaux clients,"[{'', ""<p>Le cloud est un marché de services, c'est clair. En tant que tel, les fournisseurs de services cloud sont libres de vendre certains services dans des régions spécifiques. De même, ils peuvent choisir de vendre plusieurs services définis dans le cadre de certains forfaits contractuels, tout en vendant d'autres services à des clients nouveaux ou existants (mais pas toujours aux deux), comme bon leur semble.</p>""}, {'', '<p>Alors que le secteur des services financiers, les fournisseurs d’accès Internet à domicile et peut-être une sélection d’opérateurs de télécommunications et de détaillants proposent leurs offres les plus alléchantes aux « nouveaux clients uniquement » dans une sorte de manœuvre d’appât et d’échange, le secteur des technologies de l’information fonctionne généralement dans l’autre sens. Pour encourager les nouveaux clients à suivre la feuille de route à long terme d’un fournisseur de plateforme, il est plus courant de voir une marque informatique de la taille d’un fournisseur ERP ou d’un hyperscaler détourner les nouveaux clients des produits plus anciens.</p>'}, {'', '<p>Cela peut expliquer ce qui se passe chez Amazon Web Services (AWS).</p>'}, {'', '<h3>Cloud9 et AWS CodeCommit</h3>'}, {'', ""<p>Après une déclaration initiale et un indicateur de produit apparus en juillet, en août 2024, les nouveaux clients ne pourront plus s'inscrire à AWS Cloud9 (un environnement de développement intégré basé sur le cloud) et au service de référentiel Git privé AWS CodeCommit.</p>""}, {'', '<p>C’est ce qui bouge, alors comment ces technologies sont-elles utilisées, quelles sont les alternatives et qu’est-ce qu’AWS a à dire sur sa stratégie et ses tactiques ici ?</p>'}, {'', ""<p>AWS Cloud9 IDE est un service d'édition de code destiné aux développeurs centrés sur le cloud, qui prend en charge plusieurs langages de programmation et débogueurs d'exécution. Il dispose d'un terminal intégré et d'un ensemble d'outils qui peuvent être utilisés pour coder, créer, exécuter, tester et déboguer des logiciels cloud. AWS CodeCommit est un service de contrôle de code source géré qui héberge des référentiels Git privés et stocke tout, du code aux binaires. Avec AWS en arrière-plan, il s'agit d'un magasin de code pour les équipes qui ne veulent pas se soucier de la mise à l'échelle de l'infrastructure.</p>""}, {'', '<h3>Également disponible maintenant, non</h3>'}, {'', ""<p>Bien que Cloud9 et AWS CodeCommit soient les services initialement mis en avant, d'autres services AWS qui ne sont pas actuellement disponibles pour les nouveaux clients incluent Amazon S3 Select (un outil permettant d'utiliser des instructions SQL pour filtrer et récupérer des sous-ensembles de données à partir d'un objet Amazon S3), Amazon S3 Glacier Select (un outil de requête pour AWS Glacier, un service de stockage de données à faible coût et plus lent), Amazon CloudSearch (un service géré permettant de configurer une solution de recherche pour un site Web), Amazon Forecast (un service entièrement géré qui utilise des algorithmes statistiques et d'apprentissage automatique pour fournir des prévisions de séries chronologiques très précises) et AWS Data Pipeline (un service Web que les développeurs cloud et les ingénieurs de données peuvent utiliser pour automatiser le mouvement et la transformation des données).</p>""}, {'', ""<p>Cloud Native Now s'est entretenu cette semaine avec le commandement central d'AWS dans la région EMEA et a reçu la déclaration suivante d'un porte-parole d'AWS.</p>""}, {'', '<blockquote>« Après mûre réflexion, nous avons décidé de fermer l’accès à quelques services pour les nouveaux clients afin de pouvoir nous concentrer sur la fourniture des innovations les plus appréciées par les clients. Ces services resteront disponibles pour les clients existants et nous continuerons d’améliorer la sécurité, la disponibilité et les performances [de ces outils], mais nous ne prévoyons pas d’introduire de nouvelles fonctionnalités. Nous continuerons à soutenir nos clients, qu’ils continuent à utiliser ces services ou qu’ils migrent vers d’autres offres AWS ou des solutions tierces alternatives », a déclaré AWS.</blockquote>'}, {'', '<p>« Après mûre réflexion, nous avons décidé de fermer l’accès à quelques services pour les nouveaux clients afin de pouvoir nous concentrer sur la fourniture des innovations les plus appréciées par les clients. Ces services resteront disponibles pour les clients existants et nous continuerons d’améliorer la sécurité, la disponibilité et les performances [de ces outils], mais nous ne prévoyons pas d’introduire de nouvelles fonctionnalités. Nous continuerons à soutenir nos clients, qu’ils continuent à utiliser ces services ou qu’ils migrent vers d’autres offres AWS ou des solutions tierces alternatives », a déclaré AWS.</p>'}, {'', ""<h3>L'« œil » d'AWS dans le ciel</h3>""}, {'', '<p>C’est donc ce que nous avions suggéré au début de cet article, n’est-ce pas ? À mesure que les nouvelles technologies de plateforme évoluent (et soyons honnêtes, à l’ère de l’IA générative et de l’âge d’or actuel de la science des données, beaucoup de choses changent), AWS utilise son « œil » interne pour voir plus loin que n’importe quel défenseur ou évangéliste externe du cloud ne pourrait même être en mesure de voir.</p>'}, {'', '<p>AWS a promis de fournir des conseils détaillés sur les étapes que les clients peuvent suivre pour assurer une transition en douceur vers tout nouveau service de leur choix.</p>'}, {'', '<p>« Nous prenons également en charge les migrations vers d’autres solutions AWS ou tierces mieux adaptées à vos besoins en constante évolution. Continuez à nous faire part de vos commentaires. Nous sommes toujours à l’écoute », a déclaré Jeff Barr, évangéliste en chef chez Amazon Web Services sur X, anciennement Twitter.</p>'}, {'', '<h3>Situations migratoires</h3>'}, {'', '<p>À titre d’exemple pratique (ou deux) des types de migrations dont nous parlons ici, AWS fournit des conseils détaillés sur la manière dont les clients peuvent effectuer un transfert d’Amazon CloudSearch vers Amazon OpenSearch et sur la manière dont les clients peuvent migrer leur référentiel AWS CodeCommit vers un autre fournisseur Git.</p>'}, {'', ""<p>En outre, AWS indique que les utilisateurs peuvent utiliser des techniques de clonage ou de mise en miroir pour effectuer leurs migrations. Dans le cas d'AWS Cloud9, AWS préconise une migration vers AWS IDE Toolkits et AWS CloudShell, un outil autonome à usage général que les ingénieurs cloud peuvent utiliser pour exécuter des commandes sur AWS.</p>""}, {'', '<p>Bien que certains utilisateurs puissent être naturellement perturbés – en particulier ceux qui étaient peut-être au stade final d’une phase de planification de projet qui comportait certaines des technologies désormais « héritées vivantes » évoquées ici – la plupart des ateliers informatiques cloud-native en évolution rapide auront (sans doute) évolué et oublié ces changements d’ici Noël.</p>'}]"
Cinq excellentes opportunités d'emploi DevOps,"[{'', ""<p>DevOps.com fournit désormais un rapport hebdomadaire sur les emplois DevOps à travers lequel les opportunités pour les professionnels DevOps seront mises en évidence dans le cadre d'un effort visant à mieux servir notre public.</p>""}, {'', '<p>Notre objectif en ces temps économiques difficiles est de permettre aux professionnels DevOps de faire progresser leur carrière plus facilement.</p>'}, {'', '<p>Bien entendu, le vivier de talents DevOps disponibles est encore relativement limité. Ainsi, lorsqu’un professionnel DevOps assume un nouveau rôle, cela a tendance à créer une opportunité pour les autres.</p>'}, {'', '<p>Les cinq offres d’emploi partagées cette semaine sont sélectionnées en fonction de l’entreprise qui cherche à embaucher, du segment industriel vertical et, bien entendu, de l’échelle salariale proposée.</p>'}, {'', '<p>Nous nous engageons également à fournir des informations supplémentaires sur l’état du marché du travail DevOps. En attendant, à votre attention.</p>'}, {'', '<p>CareerBuilder.com</p>'}, {'', '<p>EYDallasDevOps – Ingénieur de plateforme156\xa0600 $</p>'}, {'', '<p>SimplyHired.com</p>'}, {'', '<p>BAE SystemsQuantico, VAIngénieur DevOps140\xa0690 $ à 239\xa0140 $</p>'}, {'', '<p>Indeed.com</p>'}, {'', '<p>CrowdStrikeRemoteIngénieur CI/CD senior135\xa0000 à 210\xa0000 $</p>'}, {'', '<p>Dés.com</p>'}, {'', '<p>HII Mission TechnologiesFort Belvoir, VAIngénieur DevOps senior129\xa0620 $ à 240\xa0000 $</p>'}, {'', '<p>LinkedIn</p>'}, {'', '<p>Publix Super MarketsRemoteIngénieur Cloud senior – Client numérique110\xa0400 $ à 165\xa0600 $</p>'}]"
DevOps et réussite de la phase de test,"[{'', ""<p>Dans le cadre de DevOps, la capacité à publier des logiciels rapidement, parfois plusieurs fois par jour, est impérative. Les développeurs doivent être prêts à effectuer et à terminer les tests en quelques minutes, afin de déterminer si le logiciel est prêt à passer à la phase suivante ou si les équipes doivent abandonner le projet. La détection et la résolution des bugs avant qu'une version n'entre en production sont essentielles au cycle de vie du développement logiciel (SDLC) — tout comme l'intégration d'une phase de test dans vos processus.</p>""}, {'', '<p>Pour cela, des outils de test automatisés sont nécessaires, bien que leur type varie en fonction de l’application en cours de création. Par exemple, des outils comme Newman sont parfaits pour tester les méthodes publiques d’API. JUnit ou Jest sont les meilleurs pour les tests unitaires de code et de composants. Playwright ou Cypress sont idéaux pour les implémentations de tests E2E (de bout en bout) complètes. Vous pouvez également utiliser des outils de gestion des tests comme TestRail, qui peuvent fournir des rapports qui tiendront les parties prenantes informées de la progression et de la maturité d’une application.</p>'}, {'', '<p>Quel que soit l’outil utilisé, l’entreprise doit se concentrer sur la qualité. Les tests ne sont plus la seule responsabilité de l’assurance qualité (QA) ; l’ensemble du département d’ingénierie doit être impliqué et responsable. Ce modèle de responsabilité partagée produit les résultats les plus fiables, corrigeant les problèmes avant qu’ils ne s’installent et ne gaspillent les investissements par la suite. Il accélère également les cycles de livraison de logiciels plus continus, les tests automatisés réduisant le risque que les humains ne parviennent pas à détecter et à résoudre les problèmes.</p>'}, {'', '<h3>La pyramide de test</h3>'}, {'', ""<p>Un concept populaire dans le développement de logiciels est la pyramide de tests, un cadre utilisé pour guider les processus. Elle se compose de plusieurs couches de tests qui traitent des aspects de fonctionnalité, de performance et de fiabilité. La liste suivante examine les couches clés, les types de tests les plus utilisés et les avantages qu'ils offrent.</p>""}, {'', '<li>Tests unitaires : ces tests se concentrent sur une seule unité de travail, généralement une méthode ou un composant. Ils sont faciles à réaliser, peu coûteux et offrent une première ligne de défense en matière de qualité du code. Idéalement, ces tests doivent être effectués pendant la phase de construction.</li>'}, {'', ""<li>Tests d'intégration et API : ces tests valident la capacité du logiciel en cours de développement à s'intégrer aux systèmes, sinon il sera inutilisable. Cela relève généralement du domaine des développeurs et de l'assurance qualité, mais peut varier en fonction de la structure de l'entreprise.</li>""}, {'', ""<li>Tests UI E2E : ce sont les tests les plus complets, nécessitant l'intégration complète de vos systèmes, y compris le front-end, le back-end, la base de données et le réseau. Ils sont généralement rédigés par des personnes de l'assurance qualité, qui travaillent en étroite collaboration avec les différents secteurs d'activité et les propriétaires de produits. Ce sont les tests les plus coûteux et nécessitent plus de temps et de maintenance, en particulier à mesure que les besoins de l'entreprise et les scénarios de test évoluent. L'accent doit rester mis sur les tests E2E. S'ils sont surprovisionnés, les équipes peuvent retomber dans les tests API ou unitaires et la pyramide pourrait s'inverser, ce qui augmenterait considérablement les coûts globaux.</li>""}, {'', ""<h3>Attribution d'automatisation</h3>""}, {'', '<p>Il n’est pas facile de tester manuellement des applications. Il est quasiment impossible de garantir que le test est effectué correctement à chaque fois tout en évitant les erreurs humaines, sans parler du temps et des dépenses considérables. C’est pourquoi l’automatisation est désormais utilisée tout au long du processus de test, depuis l’orchestration de l’infrastructure jusqu’au code de test.</p>'}, {'', ""<p>Les développeurs doivent être chargés de rédiger des tests unitaires et des tests d'intégration. Les professionnels de la qualité doivent se charger de la rédaction des tests d'interface utilisateur de bout en bout. Les scénarios de test doivent être créés par les propriétaires de produits. La phase de découverte des tests, généralement effectuée manuellement, serait un prérequis à la phase d'automatisation.</p>""}, {'', '<h3>Mise en œuvre en action</h3>'}, {'', '<p>La phase de test peut être réalisée avec différents outils et services existants. À titre d’exemple, examinons l’une des offres les plus populaires disponibles, Amazon Web Services (AWS).</p>'}, {'', ""<p>Cette entité propose AWS CodePipeline, un service de livraison continue (CD) entièrement géré pour créer des pipelines, orchestrer et intégrer des mises à jour dans votre infrastructure et vos applications. Sans surprise, il fonctionne avec leurs autres services DevOps, notamment AWS CodeDeploy, AWS CodeCommit et AWS CodeBuild. Il fonctionne également bien avec des fournisseurs d'actions tiers comme Jenkins et Github.</p>""}, {'', ""<p>Par conséquent, les fonctionnalités d'AWS CodePipeline incluent des fonctionnalités telles que\xa0:</p>""}, {'', ""<li>Option de détection\xa0: cela démarre un pipeline basé sur l'emplacement source des artefacts, un sous-produit du développement utilisé pour des tâches allant des descriptions de fonctions aux évaluations des risques. AWS recommande d'utiliser les webhooks Github, ainsi que les événements Amazon CloudWatch pour les artefacts stockés.</li>""}, {'', '<li>Désactiver la transition\xa0: une fonctionnalité de transition relie les étapes d’un pipeline et peut être activée par défaut. Si vous ne souhaitez pas passer automatiquement à de nouvelles étapes, cliquez simplement sur le bouton «\xa0Désactiver la transition\xa0» pour interrompre l’exécution en cours du pipeline.</li>'}, {'', ""<li>Ajout d'étapes\xa0: avec AWS CodePipeline, vous pouvez modifier un pipeline pour introduire une nouvelle étape, mettre à jour une étape existante ou supprimer entièrement une étape. Une page de modification vous permet d'ajouter des actions en série ou en parallèle à des activités existantes. Cette fonctionnalité rend un pipeline plus flexible et plus facile à développer.</li>""}, {'', ""<li>Action d'approbation\xa0: l'utilisation de cette fonctionnalité permet de gérer les étapes du pipeline. Par exemple, si vous attendez l'approbation d'une personne pendant le déploiement, cela suspendra le pipeline jusqu'à ce que l'approbation soit accordée.</li>""}, {'', '<h3>Pas de repos pour les tests</h3>'}, {'', '<p>Aucune application ne devrait être mise sur le marché sans une phase de test. Lors du développement de cette dernière, minimisez l’interaction humaine dans les processus, faites appel à l’automatisation et recherchez les outils qui permettront de tout réaliser. N’oubliez pas que l’ensemble de l’écosystème de développement (codeurs, ingénieurs, personnel d’assurance qualité) doit être impliqué et avoir un intérêt dans les tests. Il appartient à chacun de s’assurer que la phase de test est en place, flexible et que les résultats sont à toute épreuve.</p>'}]"
Endor Labs ajoute des outils d'analyse et de correctifs pour sécuriser les logiciels open source,"[{'', ""<p>Endor Labs a révélé aujourd'hui, lors de la conférence Black Hat USA 2024, qu'il avait ajouté la capacité de déterminer à quel point il pourrait s'avérer difficile de mettre à niveau un progiciel open source, y compris son potentiel à casser une application, à sa plateforme de sécurisation des chaînes d'approvisionnement en logiciels.</p>""}, {'', ""<p>De plus, la société ajoute des correctifs Endor Magic pour permettre aux équipes DevSecOps d'appliquer les correctifs créés dans une version ultérieure à une version précédente du module si elles déterminent que la mise à niveau de ce module serait trop difficile.</p>""}, {'', ""<p>Jenn Gile, directrice du marketing produit chez Endor Labs, a déclaré que ces capacités d'analyse fourniront aux équipes DevSecOps le contexte nécessaire pour déterminer quand mettre à niveau un module en fonction du niveau potentiel de perturbation qu'un nouveau module pourrait introduire dans un environnement informatique.</p>""}, {''}, {'', ""<p>Les outils d'analyse de la composition logicielle (SCA) existants sont certes capables d'identifier les vulnérabilités, mais ils ne disposent pas des conseils de correction essentiels dont les entreprises ont besoin pour prendre la décision de procéder à une mise à niveau, a-t-elle ajouté. Endor Labs, au contraire, utilise des analyses appliquées au moment de la création d'une version pour déterminer exactement quelles dépendances tierces sont utilisées et comment elles interagissent avec le code de l'application, a noté Gile.</p>""}, {'', '<p>En conséquence, la plupart des organisations choisiront de ne pas mettre à niveau un module en supposant que le niveau de risque pour l’entreprise est trop élevé, a-t-elle ajouté.</p>'}, {'', '<p>À l’inverse, les équipes DevSecOps peuvent décider de procéder à une mise à niveau pour découvrir que l’effort requis était bien plus important que prévu. Il n’est pas rare que les équipes DevSecOps annulent une mise à niveau une fois ce niveau de difficulté déterminé.</p>'}, {'', '<p>Les dernières fonctionnalités ajoutées à la plateforme Endor Labs fournissent les renseignements exploitables dont les équipes DevSecOps travaillant en collaboration avec leurs collègues de cybersécurité ont besoin pour prendre des décisions de mise à niveau plus éclairées, a déclaré Gile.</p>'}, {'', ""<p>S'il est déterminé que la mise à niveau d'un module est trop difficile, les équipes DevSecOps ont alors la possibilité de déployer le code source et les correctifs associés, y compris les étapes de test, de création et de déploiement, à l'aide des correctifs magiques Endor.</p>""}, {'', '<p>La sécurité des logiciels open source est devenue un enjeu majeur, car les entreprises qui s’appuient sur ces applications qui incluent du code open source ne peuvent pas facilement développer un correctif à chaque fois qu’une nouvelle vulnérabilité est découverte. Dans de nombreux cas, comme dans le cas de la célèbre vulnérabilité du shell Log4J qui a affecté les applications Java, les équipes informatiques découvrent que le code affecté est maintenu par une petite équipe de bénévoles non rémunérés qui n’ont ni le temps ni la motivation pour développer rapidement un correctif.</p>'}, {'', '<p>Alors que la communauté open source met en commun ses ressources pour relever ce type de défi à l’avenir, de nombreuses organisations informatiques d’entreprise réévaluent leurs applications dans l’intention de devenir moins dépendantes des logiciels open source qui ne sont pas maintenus par une grande équipe de développeurs contribuant activement au projet.</p>'}, {'', '<p>D’une manière ou d’une autre, il arrivera certainement un moment où une nouvelle vulnérabilité zero-day sera découverte et les équipes DevSecOps devront immédiatement la corriger avant que les cybercriminels ne l’exploitent. C’est particulièrement important à une époque où le temps entre la divulgation d’une vulnérabilité et les premières attaques qui l’exploitent se mesure désormais en heures.</p>'}]"
L'alignement de Microsoft sur GitHub favorise la naissance du développeur d'IA,"[{'', '<p>Microsoft souhaite que les développeurs évoluent vers des développeurs d’IA. C’est une évolution assez prévisible : au début, les développeurs étaient des développeurs mainframe, puis ils sont devenus des développeurs PC, des développeurs mobiles, puis des développeurs cloud. Ensuite, logiquement, les ingénieurs logiciels devront devenir des développeurs d’IA centrés sur l’automatisation, plus proches des modèles et des moteurs qui permettent nos nouveaux services d’intelligence.</p>'}, {'', '<p>Pour faciliter cette transition, Redmond promeut bien sûr l’utilisation des services d’IA Microsoft Azure comme moyen de créer des applications d’IA personnalisées. Pour atteindre la masse critique qu’elle recherche tant, Microsoft s’associe à GitHub pour permettre à ses plus de 100 millions de développeurs de créer des applications d’IA directement sur la plateforme GitHub.</p>'}, {'', ""<p>Bien que Microsoft ait acquis GitHub en 2018 et que la plateforme ait ensuite été renforcée par des outils de pipeline CI/CD et des capacités de sécurité plus larges, elle reste une entité relativement indépendante dans le style d'un Red Hat au sein d'IBM.</p>""}, {'', '<h3>Place de marché des modèles GitHub</h3>'}, {'', '<p>Microsoft a désormais mis à disposition des développeurs les fonctions de sélection de modèles Azure AI Studio via GitHub Models, un portail « marketplace » où les ingénieurs logiciels qui souhaitent développer une application d’IA générative peuvent trouver et expérimenter gratuitement des modèles d’IA avant d’envisager un paiement par jeton une fois l’application passée en production. Les efforts de Microsoft dans ce domaine incluent également la fourniture d’API conçues pour permettre la mise en production d’applications d’IA.</p>'}, {'', '<p>GitHub Marketplace existe en tant que « terrain de jeu gratuit » où les programmeurs peuvent ajuster les paramètres du modèle et soumettre des invites pour voir comment un modèle d’IA réagit. Ce travail comprend également des intégrations avec Codespaces et Microsoft Visual Studio Code. GitHub Codespaces fournit un environnement de développement sécurisé avec une sélection de ressources intégrées prédéfinies qui bénéficient d’une intégration native avec la plateforme GitHub.</p>'}, {'', ""<p>GitHub Models a été créé en août de cette année pour permettre l'essor de l'ingénieur en IA avec accès aux principaux modèles de langage, grands et petits.</p>""}, {'', ""<h3>L'essor de l'ingénieur en IA</h3>""}, {'', '<p>« Alors que l’innovation en matière de modèles d’IA s’accélère, Azure reste déterminé à proposer la meilleure sélection de modèles et la plus grande diversité de modèles pour répondre aux besoins uniques des développeurs d’IA en matière de coût, de latence, de conception et de sécurité. Aujourd’hui, nous proposons la bibliothèque de modèles la plus vaste et la plus complète du marché, comprenant les derniers modèles d’OpenAI, Meta, Mistral et Cohere ainsi que des mises à jour de notre propre famille Phi-3 de petits modèles de langage », note Asha Sharma, vice-présidente de la plateforme d’IA chez Microsoft.</p>'}, {'', ""<p>Avec GitHub Models, Sharma suggère aux développeurs d'explorer les derniers modèles ainsi que d'autres innovations en matière d'IA telles que les « modèles de frontière » de nouvelle génération, c'est-à-dire les modèles à grande échelle avec de nombreux paramètres et une logique algorithmique complexe tels que Google Gemini, Claude d'Anthropic, DeepMind d'AlphaFold et même GPT-4 d'OpenAI.</p>""}, {'', '<p>« Pour la plupart d’entre nous, l’apprentissage du développement ne s’est pas fait de manière linéaire en classe. Il a fallu beaucoup de pratique, de jeu et d’apprentissage par l’expérimentation. Il en va de même aujourd’hui pour les modèles d’IA. Dans le nouveau terrain de jeu interactif, les étudiants, les amateurs et les startups peuvent explorer les modèles privés et ouverts les plus populaires de Meta, Mistral, Azure OpenAI Service Microsoft et d’autres en quelques clics et frappes de touches », a écrit Thomas Dohmke, PDG de GitHub.</p>'}, {'', '<h3>De nombreux modes et modaux de modèles</h3>'}, {'', '<p>Cette opération de rapprochement est probablement le résultat de l’ampleur et de la portée du développement actuel des modèles d’IA. Associée à la nécessité de chevaucher les surfaces d’attaque des déploiements couvrant le cloud, la périphérie, les applications à usage général et peut-être les déploiements d’applications et de services de données plus spécifiques à une tâche ou à un secteur, la complexité actuelle est vertigineuse. GitHub Models vise à ouvrir la porte aux développeurs pour expérimenter plusieurs modèles, en simplifiant l’expérimentation et la sélection de modèles dans le catalogue Azure AI afin que les ingénieurs puissent comparer les modèles, les paramètres et les invites.</p>'}, {'', '<p>En faisant d’Azure AI une plateforme ouverte et modulaire, Microsoft entend aider ses clients à passer rapidement de l’idée au code et au cloud, explique Sharma. Avec Azure AI sur GitHub, les développeurs peuvent utiliser Codespaces pour configurer un prototype ou utiliser l’extension Prompty pour générer du code avec des modèles GitHub directement dans Microsoft Visual Studio Code.</p>'}, {'', '<p>« La sélection accrue de modèles offre aux développeurs la plus large gamme d’options pour les applications individuelles qu’ils créent. Mais chaque modèle apporte naturellement une complexité accrue. Pour contrer ce problème, nous facilitons considérablement l’expérimentation de différents modèles par chaque développeur via l’API d’inférence de modèles Azure AI. Grâce à cette API unique, les développeurs GitHub peuvent désormais accéder à un ensemble commun de fonctionnalités pour comparer les performances d’un ensemble diversifié de modèles fondamentaux de manière uniforme et cohérente, en passant facilement d’un modèle à l’autre pour comparer les performances sans modifier le code sous-jacent », a déclaré Sharma.</p>'}, {'', '<p>Microsoft a annoncé qu’elle allait étendre encore davantage son intégration. Ce que cela signifie, c’est l’engagement de Redmond à apporter le langage, la vision et les services multimodaux d’Azure AI à GitHub, ainsi que d’autres éléments de la chaîne d’outils Azure AI. Les organisations disposant d’un abonnement Azure existant peuvent acheter des produits GitHub en libre-service, directement auprès du service commercial Microsoft ou via des fournisseurs de solutions tiers et peuvent ajuster le nombre de postes GitHub selon les besoins.</p>'}]"
Une enquête révèle que la vitesse de déploiement des logiciels dépasse la sécurité,"[{'', ""<p>Une enquête menée auprès de 5 315 contributeurs individuels et dirigeants du développement, des opérations informatiques et de la sécurité révèle que les deux tiers (66 %) d'entre eux publient des logiciels plus rapidement qu'il y a un an.</p>""}, {'', ""<p>Réalisée par le cabinet d'études de marché Omdia pour le compte de GitLab, l'enquête révèle également que parmi les 40 % des développeurs ayant participé à l'enquête, 24 % ont déclaré qu'ils poussaient du code dans des environnements de production au moins une fois par jour, et 13 % poussaient du code plusieurs fois par jour.</p>""}, {'', '<p>Dans le même temps, plus des trois quarts (78 %) des personnes interrogées ont déclaré qu’elles utilisaient déjà l’intelligence artificielle (IA) ou qu’elles prévoyaient de l’utiliser dans les deux prochaines années pour développer des logiciels. Plus de la moitié (55 %) ont reconnu que l’introduction de l’IA dans le cycle de développement logiciel était risquée, la confidentialité et la sécurité des données étant la principale préoccupation.</p>'}, {'', '<p>Malheureusement, l’enquête suggère également que les meilleures pratiques en matière de sécurisation des logiciels sont encore en retard. L’enquête révèle que parmi les 27 % de personnes interrogées qui travaillent dans le domaine de la sécurité, seulement 38 % déclarent avoir transféré la responsabilité de la sécurité des applications aux développeurs, et seulement 34 % déclarent proposer une formation à la sécurité aux développeurs.</p>'}, {'', '<p>Du côté positif, 32 % prévoient de commencer à proposer cette formation cette année, et plus d’un quart (26 %) de ceux qui utilisent actuellement l’IA pour le développement d’applications ont identifié l’amélioration de la sécurité comme l’un des principaux avantages de l’IA. Plus de la moitié (52 %) ont déclaré qu’ils étaient intéressés par l’utilisation ou prévoyaient de s’appuyer sur les explications de l’IA sur les vulnérabilités de sécurité pour améliorer le code.</p>'}, {'', ""<p>Dans l'ensemble, plus des deux tiers des développeurs (67 %) ont déclaré que plus d'un quart du code sur lequel ils travaillent provient de bibliothèques open source, et 40 % d'entre eux ont indiqué que les composants logiciels open source constituaient plus de la moitié du code de leur application. À l'heure actuelle, cependant, seuls 20 % d'entre eux travaillent pour des organisations utilisant des nomenclatures de logiciels (SBOM).</p>""}, {'', ""<p>Plus inquiétant encore, seulement 34 % déclarent utiliser des outils de test de sécurité des applications dynamiques (DAST), suivis de près par 33 % utilisant des outils de test de sécurité des applications statiques (SAST), l'analyse des conteneurs (29 %) et la détection des secrets (24 %).</p>""}, {'', '<p>Plus de la moitié des personnes interrogées dans le domaine de la sécurité ont également déclaré avoir du mal à convaincre les équipes de développement de donner la priorité à la correction des vulnérabilités, et 52 % ont indiqué que la bureaucratie ralentit souvent leurs efforts pour corriger rapidement les vulnérabilités.</p>'}, {'', ""<p>Josh Lemos, responsable de la sécurité des systèmes d'information de GitLab, a déclaré que même si des progrès ont été réalisés en termes d'adoption des meilleurs processus DevSecOps, il reste encore beaucoup à faire. La sécurité des applications ne fera que devenir un problème plus urgent à mesure que la quantité de code créée à l'aide d'outils d'IA augmentera, a-t-il ajouté.</p>""}, {'', '<h3>Le rythme de développement des logiciels</h3>'}, {'', '<p>Plutôt que de simplement traiter la sécurité des applications comme une autre porte à ajouter à un flux de travail DevOps, les organisations doivent, autant que possible, fournir aux développeurs le contexte dont ils ont besoin pour résoudre les problèmes de sécurité au moment où ils écrivent du code, a déclaré Lemos.</p>'}, {'', '<p>Le défi, bien sûr, est de trouver des moyens de continuer à créer des logiciels plus sûrs sans ralentir le rythme auquel ils sont créés, a-t-il noté.</p>'}, {'', ""<p>Chaque organisation devra naturellement déterminer elle-même le rythme auquel elle souhaite développer ses logiciels. Cependant, plus une entreprise dépend des logiciels pour générer des revenus, plus la pression pour créer de nouvelles applications tout en mettant à jour en permanence les applications existantes ne cesse d'augmenter.</p>""}]"
ZEST Security apporte l'IA générative à la sécurité de l'infrastructure cloud,"[{'', ""<p>ZEST Security a lancé une plateforme qui exploite l'intelligence artificielle générative (IA) pour corriger le code créé à l'aide d'outils d'infrastructure en tant que code (IaC).</p>""}, {'', ""<p>Fraîchement sorti d'une levée de fonds de 5 millions de dollars, le PDG de l'entreprise, Snir Ben Shimol, a déclaré que la plateforme ZEST permettra à la fois de corréler et d'identifier la cause profonde d'un risque de vulnérabilité et de générer des recommandations et du code pour permettre aux équipes DevOps de résoudre le problème.</p>""}, {'', ""<p>Aujourd'hui, il faut en moyenne 30 à 60 jours pour corriger manuellement, par exemple, une mauvaise configuration du cloud. La plateforme ZEST réduira ce temps à quelques heures, a déclaré Ben Shimol.</p>""}, {'', '<p>Cela permet également de s’assurer que les mêmes erreurs ne se reproduisent pas en identifiant les erreurs avant que le code ne soit utilisé pour provisionner l’infrastructure informatique, a-t-il ajouté. C’est essentiel car 80 % des problèmes résolus réapparaissent lorsque les développeurs commettent les mêmes erreurs plusieurs fois, a noté Ben Shimol.</p>'}, {'', '<p>Plus difficile encore, les tactiques et techniques employées par les cybercriminels évoluent en permanence, a-t-il ajouté. Une infrastructure cloud qui semblait parfaitement sécurisée peut soudainement devenir vulnérable à une vulnérabilité zero-day, a déclaré Ben Shimol.</p>'}, {'', '<p>Au cœur de la plateforme ZEST se trouve un grand modèle de langage (LLM) qui a été personnalisé pour identifier les problèmes IaC et générer du code Terraform, Pulumi ou AWS CloudFormation.</p>'}, {'', '<p>Bien que les plateformes cloud soient généralement plus sécurisées que l’environnement informatique sur site moyen, la manière dont elles sont provisionnées par les développeurs conduit souvent à des erreurs de configuration qui sont facilement exploitées par les cybercriminels.</p>'}, {''}, {'', '<p>Malheureusement, le développeur moyen manque d’expertise en cybersécurité, ce qui explique le nombre extrêmement élevé de ces erreurs de configuration potentielles. L’une des raisons pour lesquelles tant d’applications résident encore dans des environnements informatiques sur site est que de nombreux responsables informatiques et de cybersécurité craignent que les processus utilisés pour créer et déployer des applications cloud, dans le cadre du modèle actuel de responsabilité partagée exigé par les fournisseurs de services cloud, soient profondément défaillants.</p>'}, {'', '<p>On ne sait pas exactement dans quelle mesure les fournisseurs de services cloud s’efforcent de répondre à ces préoccupations en offrant aux développeurs un accès à davantage de formations, mais en attendant, des LLM formés pour générer des types de code spécifiques émergent. Le problème est désormais de trouver un moyen d’examiner systématiquement toutes les instances cloud qui ont été précédemment invoquées par programmation à l’aide d’outils IaC.</p>'}, {'', '<p>En attendant, la qualité globale du code utilisé pour provisionner les environnements cloud devrait s’améliorer régulièrement grâce à l’essor de l’IA. C’est essentiel à l’heure où le nombre d’applications déployées continue d’augmenter régulièrement. Il est moins évident de savoir dans quelle mesure le financement de ces plateformes d’IA sera assuré par les équipes de cybersécurité qui sont tenues responsables de la sécurité des applications par rapport aux équipes DevOps qui provisionnent l’infrastructure cloud.</p>'}, {'', '<p>D’une manière ou d’une autre, cependant, les problèmes de sécurité du cloud seront inévitablement imposés à mesure que les réglementations à venir exigeront des organisations qu’elles verrouillent leurs chaînes d’approvisionnement en logiciels, dont beaucoup dépendent aujourd’hui partiellement ou totalement de l’infrastructure cloud.</p>'}, {'', '<p>En fin de compte, une meilleure sécurité du cloud profite à tous. Le problème est que lorsque la sécurité du cloud est la responsabilité de tous, personne n’est tenu spécifiquement responsable de sa mise en œuvre et de son maintien.</p>'}]"
Les risques de l’oligopole du cloud,"[{'', '<p>Le marché mondial des technologies est de plus en plus dominé par quelques entreprises, chacune luttant pour renforcer sa position concurrentielle. Pour illustrer clairement cette tendance, une analyse récente montre qu’Amazon Web Services (AWS), Microsoft Azure et Google Cloud contrôlent plus des deux tiers de l’ensemble du secteur du cloud public. Dans ce que l’on appelle désormais « l’oligopole du cloud », la consolidation du marché a créé des défis inattendus pour les entreprises. Dans leur quête d’une activité distribuée et plus résiliente, de nombreuses organisations se sont exposées par inadvertance au risque de concentration, la plupart des produits SaaS sur lesquels elles s’appuient étant hébergés chez les mêmes fournisseurs de cloud public.</p>'}, {'', '<p>Les régulateurs ont exprimé à plusieurs reprises leurs inquiétudes quant à ce risque et au risque que les entreprises deviennent « prisonnières » de leurs fournisseurs. L’Ofcom, par exemple, a publié en 2023 un rapport faisant état de ses inquiétudes quant aux pratiques d’entreprises comme AWS et Microsoft. Il s’agit notamment des frais de sortie élevés, des obstacles techniques à l’interopérabilité et de l’épée à double tranchant des remises sur les dépenses engagées.</p>'}, {'', '<p>Dans un contexte où les marchés sont en pleine mutation et où une réglementation stricte n’est pas susceptible d’entrer en vigueur prochainement, il incombe aux entreprises qui investissent dans le cloud public d’évaluer leur propre exposition à ces risques. À cette fin, nous avons examiné de plus près la dynamique et l’avenir de l’oligopole du cloud, ainsi que les mesures que les entreprises peuvent prendre pour protéger leurs intérêts.</p>'}, {'', '<h3>Le risque du cloud et de la concentration</h3>'}, {'', '<p>Le risque de concentration est à son maximum lorsqu’une organisation dépend d’un nombre limité de fournisseurs, ce qui peut entraîner des perturbations importantes en cas de problèmes. Dans le contexte du cloud, le recours à plusieurs fournisseurs peut créer l’illusion d’une diversité de la chaîne d’approvisionnement si leur hébergeur cloud partagé tombe soudainement en panne – cette illusion se brise.</p>'}, {'', '<p>En pratique, le risque de concentration rend les entreprises plus vulnérables aux défaillances de leurs fournisseurs. Si un fournisseur de cloud subit une interruption inattendue – qu’il s’agisse d’une faille de sécurité, d’une pénurie dans la chaîne d’approvisionnement ou d’une catastrophe naturelle –, toutes les entreprises qui dépendent de ses services en subiront également les conséquences. Bien entendu, une interruption potentielle peut avoir de graves conséquences financières ou en termes de réputation.</p>'}, {'', '<p>En outre, le risque de concentration ne concerne pas uniquement les entreprises individuelles, il peut avoir des répercussions sur des secteurs entiers. Si une part importante d’entreprises similaires dépend d’un seul fournisseur de cloud, une perturbation pourrait se propager à l’ensemble du marché avec des conséquences généralisées et imprévisibles.</p>'}, {'', ""<h3>Éviter la dépendance vis-à-vis d'un fournisseur</h3>""}, {'', ""<p>Le verrouillage du fournisseur est également un effet secondaire malheureux, bien que presque certainement intentionnel, de l'oligopole du cloud. Le verrouillage du cloud se produit lorsqu'une entreprise devient dépendante d'un seul fournisseur de cloud en raison de coûts cachés, de problèmes d'interopérabilité ou d'incitations financières qui peuvent rendre difficile le passage à un autre fournisseur.</p>""}, {'', '<p>Par exemple, les principaux fournisseurs fixent des frais de sortie (c’est-à-dire des frais pour transférer des données hors d’un cloud), ce qui rend le changement de fournisseur coûteux. Les restrictions techniques peuvent également rendre difficile la configuration des données et des applications pour qu’elles fonctionnent sur différents clouds et ancrer davantage les clients dans l’écosystème d’un seul fournisseur.</p>'}, {'', '<p>De plus, les fournisseurs de cloud sont désireux d’offrir des remises sur les dépenses engagées. Si ces dernières offrent un avantage considérable en termes d’économies, elles peuvent également inciter les clients à concentrer leur utilisation du cloud auprès d’un seul fournisseur, renforçant ainsi la dépendance vis-à-vis du fournisseur.</p>'}, {'', '<h3>Renforcer la résilience dans un marché concentré</h3>'}, {'', '<p>Pour atténuer les risques associés à l’oligopole du cloud, les entreprises doivent adopter des stratégies qui renforcent la résilience et réduisent la dépendance à un fournisseur unique. Voici quelques approches clés :</p>'}, {'', '<ol>Audits réguliers et évaluations des risques</ol>'}, {'', '<li>Audits réguliers et évaluations des risques</li>'}, {'', '<p>Les entreprises doivent procéder à des audits réguliers de leurs chaînes d’approvisionnement afin d’identifier les vulnérabilités. Cela implique non seulement d’évaluer les fournisseurs directs, mais aussi d’examiner les chaînes d’approvisionnement de ces fournisseurs. Comprendre où se situent les risques permet aux entreprises de prendre des mesures proactives pour y faire face.</p>'}, {'', '<ol>Diversification des fournisseurs de cloud</ol>'}, {'', '<li>Diversification des fournisseurs de cloud</li>'}, {'', ""<p>Même si le recours à plusieurs fournisseurs de cloud présente des défis techniques et financiers, la diversification est essentielle pour la résilience. Le recours à différents fournisseurs pour les systèmes de production et les sauvegardes peut contribuer à atténuer le risque d'un point de défaillance unique.</p>""}, {'', ""<p>Le basculement immédiat vers un autre cloud peut ne pas être pratique ou rentable, mais disposer de copies de données en dehors de l'environnement de production est également essentiel à la résilience. Au minimum, la configuration de sauvegardes vers un autre cloud est une pratique peu coûteuse mais utile.</p>""}, {'', '<ol>Construction sur plusieurs zones et régions de disponibilité</ol>'}, {'', '<li>Construction sur plusieurs zones et régions de disponibilité</li>'}, {'', ""<p>Les entreprises doivent structurer leurs environnements cloud sur plusieurs zones de disponibilité au sein d'une même région afin de limiter les impacts des pannes localisées. Le déploiement sur différentes régions géographiques peut également contribuer à améliorer la résilience, bien qu'il s'agisse d'une opération plus coûteuse et plus complexe.</p>""}, {'', ""<ol>Adopter l'interopérabilité et les conteneurs</ol>""}, {'', ""<li>Adopter l'interopérabilité et les conteneurs</li>""}, {'', ""<p>Les technologies telles que les conteneurs peuvent faciliter la résilience multicloud en facilitant le déplacement des applications et des données entre différents environnements cloud. Les conteneurs encapsulent les applications et leurs dépendances, leur permettant de s'exécuter de manière cohérente sur différentes plateformes cloud.</p>""}, {'', '<p>Les méthodes et outils d’infrastructure en tant que code (IaC) permettent également la création et la gestion d’infrastructures qui peuvent être répliquées rapidement et efficacement dans différents environnements cloud.</p>'}, {'', ""<h3>L'avenir du cloud et de l'IA</h3>""}, {'', '<p>Alors que l’adoption et la complexité de l’intelligence artificielle (IA) continuent de s’accélérer, l’influence de l’oligopole du cloud se fait déjà sentir. Le stockage, l’infrastructure et le calcul dans le cloud sont essentiels au développement de l’IA, car ils fournissent les moyens de développer et de former les modèles d’IA les plus avancés.</p>'}, {'', '<p>Cela signifie toutefois que les principaux fournisseurs de cloud exercent un contrôle considérable sur l’avenir de l’IA, ce qui risque d’étouffer la concurrence et l’innovation dans ce domaine. L’IA devenant de plus en plus essentielle à divers secteurs, cette dépendance pourrait exacerber les risques de concentration existants.</p>'}, {'', '<p>Les décideurs politiques et les acteurs de l’industrie doivent donc envisager des mesures visant à promouvoir la concurrence et à garantir un environnement sain et concurrentiel pour le développement de l’IA.</p>'}, {'<h3>Réflexions finales</h3>', ''}, {'', '<p>L’oligopole du cloud, dans la mesure où il étouffe la concurrence, représente un domaine potentiellement négligé dans lequel les entreprises doivent évaluer leurs engagements en matière de chaîne d’approvisionnement. Si la croissance de ces fournisseurs a rendu le cloud public plus accessible que jamais, leur ascension a finalement exposé les entreprises et le marché dans son ensemble à de nouvelles vulnérabilités.</p>'}, {'', '<p>Dans ces circonstances, les entreprises doivent adopter des stratégies pour diversifier leur utilisation du cloud, renforcer leur résilience et atténuer les risques. Heureusement, les organismes de réglementation commencent déjà à s’en rendre compte et joueront un rôle crucial pour garantir que le marché reste compétitif et équitable.</p>'}, {'', '<p>Alors que nous nous tournons vers l’avenir, notamment avec l’essor de l’IA, il deviendra encore plus crucial de répondre à ces questions pour garantir que chaque entreprise maintienne sa continuité et sa résilience dans un paysage technologique en évolution rapide.</p>'}]"
Réussir dans DevSecOps nécessite une grille d'évaluation pour la gouvernance du SDLC,"[{'', ""<p>En janvier 2024, le New York Times a connu une faille importante en raison d'un jeton GitHub exposé, entraînant le vol de 270 Go de données sensibles, notamment de la documentation informatique et des outils d'infrastructure. Cet incident n'est qu'un exemple parmi tant d'autres qui soulignent le besoin urgent de mesures de sécurité robustes tout au long du cycle de vie du développement logiciel (SDLC).</p>""}, {'', '<p>Dans le monde d’aujourd’hui, les développeurs de logiciels sont notre source de production ; ils sont les bêtes de somme d’une économie numérique en plein essor. Cependant, les exigences de productivité imposées aux développeurs conduisent souvent à une sous-priorité des mesures de sécurité du cycle de développement logiciel. Par exemple, lorsque les vice-présidents de l’ingénierie accordent à de grandes équipes de développeurs des autorisations étendues sur un référentiel de code dans un souci de rapidité, ils peuvent par inadvertance créer une vaste surface d’attaque vulnérable à un seul compte compromis.</p>'}, {'', ""<p>DevSecOps, un terme qui englobe les efforts combinés de développement, de sécurité et d'exploitation agissant comme une seule unité, est une exigence de sécurité urgente pour tout RSSI. En décomposant DevSecOps en quatre piliers essentiels (gouvernance des identités, gouvernance CI/CD, gouvernance du code et conformité SDLC), les organisations disposent d'une grille d'évaluation pour prévenir les violations SDLC. Ce cadre permet également de définir une nouvelle catégorie de sécurité pour les environnements de développement, appelée gouvernance SDLC.</p>""}, {'', '<h3>Les quatre piliers de la gouvernance du cycle de vie du développement logiciel</h3>'}, {'', '<p>La sécurisation d’un cycle de développement logiciel (SDLC) est une tâche monumentale. Il ne s’agit pas d’un simple changement d’infrastructure cloud. Un environnement SDLC comprend plutôt des référentiels de code, des environnements d’exécution, des référentiels d’artefacts, des scanners de code open source, des gestionnaires de secrets, des outils CI/CD et bien plus encore. Voici quatre piliers pour bien cerner l’ensemble des problèmes de gouvernance du SDLC :</p>'}, {'', ""<li>Gouvernance des identités : assurez-vous que chaque développeur et compte de service dispose des droits nécessaires pour effectuer son travail, ni plus, ni moins. La mise en œuvre de politiques strictes de gouvernance des identités limite l'accès à ce qui est nécessaire, réduisant ainsi le risque d'accès non autorisé pouvant conduire à des violations. En outre, il est essentiel de veiller à ce que les risques internes et les comportements à risque en matière d'identité externe soient rapidement détectés et contenus afin d'atténuer les risques d'identité liés au SDLC.</li>""}, {'', ""<li>Gouvernance CI/CD : Maintenez des postures de sécurité cohérentes pour les outils d'intégration et de déploiement en fonction du contexte dans lequel l'application est déployée. Cela signifie comprendre les besoins spécifiques de chaque application pour laquelle les outils CI/CD appliquent des procédures d'intégration et de déploiement personnalisées.</li>""}, {'', ""<li>Gouvernance du code\xa0: attribuez une identité, une date et une heure pour l'importation de chaque package dans le référentiel. Cela inclut la surveillance des fuites de secrets, les vulnérabilités des tests de sécurité des applications statiques (SAST) et la réalisation d'analyses Terraform d'infrastructure en tant que code (IaC).</li>""}, {'', ""<li>Conformité : automatisez la détection et la correction continues de la conformité conformément aux normes en vigueur. En détectant automatiquement les manquements à la conformité et en les corrigeant en temps réel, les organisations peuvent se conformer aux normes du secteur et éviter d'éventuelles amendes ou pénalités.</li>""}, {'', ""<p>Avant de mettre en œuvre un modèle de gouvernance SDLC efficace, il est essentiel d'impliquer les parties prenantes de l'ensemble de l'organisation. Les RSSI doivent collaborer avec les responsables de l'ingénierie, les responsables AppSec, les responsables DevOps et les responsables de la conformité sur les politiques de sécurité et de gouvernance SDLC afin de garantir que tous les aspects de la sécurité sont pris en compte et traités.</p>""}, {'', '<h3>Mise en œuvre du cadre et résolution des défis</h3>'}, {'', '<p>Un leadership fort de la part du RSSI est essentiel pour mettre en œuvre ce cadre avec succès. Cependant, la plupart des défis auxquels les RSSI sont confrontés ne sont pas des problèmes technologiques, mais des problèmes culturels. En tant que principal défenseur de la sécurité, le RSSI doit mener des recherches approfondies pour comprendre les besoins et les défis spécifiques de l’organisation en matière de sécurité. En impliquant le personnel et les responsables fonctionnels, le RSSI doit présenter des preuves des menaces potentielles et élaborer des politiques qui équilibrent la productivité avec des mesures de sécurité robustes.</p>'}, {'', '<p>L’un des principaux défis est la culture dans laquelle les RSSI peuvent s’en remettre à des fonctions où l’informatique est une compétence clé, en présumant souvent qu’ils comprennent les risques de sécurité que prennent les équipes. Un autre défi découle de la nature des entreprises technologiques à forte croissance, où les objectifs de croissance agressifs poussent les équipes d’ingénierie à privilégier la vitesse au détriment de la sécurité. Cette mentalité du « go, go, go » conduit à ce que les mesures de gouvernance et de sécurité deviennent des préoccupations secondaires.</p>'}, {'', '<h3>Adopter la gouvernance SDLC pour un avenir sûr</h3>'}, {'', '<p>Même si les défis liés à la mise en œuvre de la gouvernance SDLC peuvent être décourageants, les succès obtenus par les RSSI dans la sécurisation de l’infrastructure cloud démontrent ce qui est possible. Songez à l’évolution rapide de la sécurité sur site, qui a pris des décennies à perfectionner, à l’adoption transparente de solutions de cloud public et privé en quelques années seulement. Les RSSI ont joué un rôle déterminant dans cette transition, en apportant de nombreux outils de développement et en aidant à promouvoir rapidement les produits. Sans oublier que les RSSI ne sont jamais reconnus pour toutes les violations qui ne se produisent pas, mais ils restent sur leurs gardes et sont prêts à discuter avec les innovateurs pour trouver des solutions. Grâce à leur travail, je crois vraiment que ce secteur peut tout résoudre.</p>'}, {'', '<p>Les entreprises de tous les secteurs dépendent désormais fortement du développement logiciel. Il n’a jamais été aussi urgent d’intégrer la sécurité dans le processus DevOps. Les attaquants malveillants continuent d’expérimenter et d’exploiter les faiblesses des infrastructures critiques. La récente série d’attaques axées sur le cycle de vie du développement logiciel met en évidence la nécessité de mesures de sécurité robustes dans la manière dont les logiciels sont créés. Il ne suffit plus de protéger uniquement le cloud dans lequel l’application est hébergée.</p>'}, {'', '<p>Si vous attendez que le marché ASPM se déplace vers la gauche, je dirais que la gouvernance SDLC est trop complexe pour être réalisée de manière exhaustive par un fournisseur de sécurité cloud classique. DevSecOps est sa compétence principale et nécessite des outils et services spécialisés pour la sécuriser. Il est désormais temps d’impliquer les parties prenantes de votre SDLC et de commencer à sécuriser vos pipelines de développement : l’avenir de la sécurité de votre organisation en dépend. La mise en œuvre d’un cadre de gouvernance SDLC qui englobe l’identité, le CI/CD et la gouvernance du code, ainsi que la conformité, garantira que la sécurité fait partie intégrante de votre processus de développement, protégeant les actifs critiques et favorisant une culture de responsabilité partagée.</p>'}]"
Migration massive de Java Open Source vers Survey Surfaces,"[{'', ""<p>Une enquête mondiale menée auprès de 663 professionnels de l'informatique possédant une expertise Java révèle que 86 % d'entre eux migrent une partie de leurs applications vers une édition open source de la plateforme Java ou ont déjà migré (25 %).</p>""}, {'', ""<p>Réalisée par Dimensional Research pour le compte d'Azul Systems, un fournisseur d'une plateforme pour les applications basées sur OpenJDK, une distribution open source de Java, l'enquête révèle également que la principale raison du changement est le coût de l'abonnement Oracle Java (53 %), suivi de près par une préférence générale pour les logiciels open source.</p>""}, {'', ""<p>De plus, 38 % des personnes interrogées ont déclaré craindre des changements supplémentaires en termes de prix, de licences et de support de la part d'Oracle, suivis par un quart (25 %) qui s'inquiète des audits d'utilisation de Java qui pourraient être exigés par Oracle. De plus, 24 % ont déclaré que le niveau de support Java d'Oracle fourni ne répondait pas aux attentes.</p>""}, {'', '<p>Scott Sellers, PDG d’Azul Systems, a déclaré que même si Oracle est en droit de demander des audits, il est difficile pour les entreprises de s’assurer que les développeurs et les équipes DevOps qui les soutiennent ne dépassent pas par inadvertance les limites d’un accord de licence Oracle Java. Plutôt que de risquer de se voir facturer des licences supplémentaires qui pourraient s’élever à des millions de dollars, il est plus simple pour les entreprises de s’appuyer sur une distribution OpenJDK basée sur Java Platform, Standard Edition (SE), a-t-il noté.</p>'}, {'', ""<p>L'enquête révèle que parmi les répondants qui travaillent pour des organisations ayant déjà migré, les trois quarts (75 %) ont terminé ces projets en moins de 12 mois, et 23 % en moins de trois mois. Dans l'ensemble, 84 % ont déclaré que le processus était plus facile (41 %) ou s'était déroulé comme prévu.</p>""}, {'', ""<p>De plus, parmi les répondants qui ont migré, un quart (25 %) ont déclaré qu'Oracle Java était significativement plus cher, contre 41 % qui ont déclaré qu'Oracle Java était un peu plus cher.</p>""}, {'', ""<p>Parmi ceux qui prévoient de migrer, plus de la moitié (52 %) estiment que ces efforts prendront un à deux ans, contre 23 % qui prévoient des projets dans trois à quatre ans. Seuls 14 % s'attendent à ce que leur migration prenne moins d'un an.</p>""}, {'', ""<p>En général, les migrations vers les éditions open source de Java se sont accélérées à la suite d'un changement dans les licences Oracle Java qui a augmenté les coûts pour de nombreuses entreprises en basant la tarification sur le nombre d'employés accédant à une application Java plutôt que sur le nombre de développeurs impliqués ou de cœurs de processeur requis, a noté Sellers. Jusqu'à présent, cependant, seulement 36 % des répondants qui utilisent Oracle Java ont adopté le nouveau modèle de tarification basé sur les employés, selon l'enquête.</p>""}, {'', '<p>Dans l’ensemble, l’enquête révèle que seulement 14 % des répondants n’ont pas de projet de migration, et 36 % d’entre eux déclarent être satisfaits du support et des tarifs d’Oracle.</p>'}, {'', '<p>À l’heure où de plus en plus d’entreprises sont attentives au coût total de leurs technologies de l’information, il est évident que les frais de licence augmentent et qu’elles doivent évaluer leurs options. Après tout, ce qui peut sembler être un changement relativement mineur dans un modèle de licence peut rapidement augmenter les coûts d’une manière que de nombreuses entreprises ne sont pas prêtes à absorber lors du prochain audit.</p>'}]"
Cinq leçons apprises à la dure face à l’écran bleu mondial de la mort,"[{'', '<p>En tant que professionnel de la gestion de produits, je suis formé pour comprendre les difficultés des clients, en particulier ceux qui rencontrent des problèmes de temps d’arrêt en raison d’une mauvaise qualité des logiciels. Vendredi dernier, j’ai vécu le problème de première main lorsque je suis arrivé tôt à l’aéroport d’Heathrow, pour découvrir que la panne de Microsoft avait cloué au sol des vols dans le monde entier. Mon vol de Londres n’a été retardé que de quelques heures, mais mon vol de correspondance a été complètement annulé. Comme c’était le dernier vol de la journée vers ma ville natale, j’ai dû trouver une chambre d’hôtel pour la nuit, mais toutes les chambres dans un rayon de 50 kilomètres de l’aéroport étaient complètes. Tout cela est dû à un bug dans un petit fichier pas beaucoup plus gros que cet article, qui a eu un impact sur des millions de personnes à travers le monde.</p>'}, {'', '<p>La plupart des versions de logiciels contiennent des défauts. Je n’ai jamais vu de code de production qui ne présentait aucun problème, en particulier dans les applications d’entreprise. Les découvertes qui sont publiées en production sont généralement des bugs cosmétiques ou mineurs qui affectent une fonctionnalité rarement utilisée. Cependant, des défauts comme celui qui a fait tomber les compagnies aériennes sont une toute autre situation. Il est maintenant temps de réfléchir aux leçons que nous pouvons tirer de cet événement.</p>'}, {'', '<p>Leçon 1 : le décalage vers la droite n’est PAS facultatif. Le décalage vers la droite est le concept que vous devez tester en production, après la sortie. On peut se demander si cela a été fait dans le cas de la panne de la semaine dernière. Même si les tests ont été effectués dans les environnements de pré-version, vous ne pouvez pas être sûr que vos environnements de pré-version sont identiques à ceux de production. Vous devez au moins effectuer des tests de détection en production. Mieux encore, exécutez régulièrement des tests de régression. Même lors de tests en production, tester un seul processus ou composant peut ne pas suffire. Ce qui nous amène à la leçon suivante.</p>'}, {'', ""<p>Deuxième leçon : l'intégration et les tests de bout en bout du système sont indispensables. La plupart des applications d'entreprise actuelles sont construites à partir d'un ensemble complexe de services et de systèmes interconnectés. Chaque service isolé peut fonctionner exactement comme prévu, mais lorsque vous le testez avec tous les autres éléments mobiles, vous pouvez constater que de petits retards ici et là perturbent le processus de bout en bout. Ces problèmes liés au temps sont exacerbés par des volumes inhabituellement élevés, ce qui nous amène à la leçon suivante.</p>""}, {'', ""<p>Troisième leçon : les tests de performance et de charge sont également importants. Les tests de performance et de charge peuvent ne pas sembler importants dans des circonstances normales, mais lors de la reprise après une panne, on assiste généralement à une vague d'utilisateurs essayant de poursuivre le travail retardé. On pourrait considérer cela comme un effet en cascade, mais la situation se présente souvent à la fin du mois et du trimestre pour certaines entreprises.</p>""}, {'', '<p>Dans le cas de l’incident de la semaine dernière, un grand nombre de vols annulés a entraîné une augmentation du volume sur le système de réservation des compagnies aériennes. Un système qui n’a peut-être pas été affecté par le bug réel est affecté par un niveau d’activité inhabituel. Les tests de charge ne sont pas une tâche facile et il n’est peut-être pas nécessaire de les effectuer régulièrement, mais vous ne devez pas attendre qu’une catastrophe se produise pour apprendre qu’un volume supérieur de 20 % à la moyenne rendra un système inutilisable.</p>'}, {'', '<p>Quatrième leçon : vous n’êtes pas obligé de diffuser le logiciel à tout le monde en même temps. Deux concepts auraient permis d’éviter une crise mondiale. Le premier est le « dogfooding », c’est-à-dire le fait de manger sa propre nourriture pour chien. Il s’agit de la pratique consistant à utiliser son propre logiciel avant de le diffuser à d’autres. De cette façon, en cas de problèmes majeurs, vous les détectez rapidement et vous limitez les souffrances au sein de votre propre organisation.</p>'}, {'', ""<p>Le deuxième concept est appelé « versions canari ». Tout comme les mineurs emmenaient autrefois des canaris dans les mines de charbon pour les avertir d'une condition mortelle, vous pouvez également diffuser du code à un petit sous-ensemble d'utilisateurs externes pendant une courte période pour garantir qu'une version fonctionne en dehors de votre organisation. Une fois encore, vous limitez l'impact négatif.</p>""}, {'', '<p>Cinquième leçon : le coût des pannes de système a des répercussions en cascade. Il ne s’agit pas seulement de la perte de productivité pendant le temps d’arrêt, mais aussi de l’effet domino. Par exemple, un temps d’arrêt dans le système de saisie des commandes peut entraîner des problèmes avec le système de gestion des stocks, ce qui peut retarder les expéditions et avoir un impact sur le chiffre d’affaires et les résultats financiers. La perte de revenus et de bénéfices peut amener les actionnaires et les clients à voter avec leurs pieds.</p>'}, {'', ""<p>La perte de productivité est aggravée par la nécessité de faire des heures supplémentaires pour rattraper le retard. Pour les travailleurs horaires, cela représente un coût direct pour l'entreprise. Pour les employés salariés, comme les développeurs et les testeurs, cela signifie un impact négatif sur leur satisfaction au travail et sur le temps passé loin de leur famille. L'augmentation du turnover dans le secteur informatique a également un coût.</p>""}, {'', ""<p>La baisse de la satisfaction client est un autre coût indirect de la mauvaise qualité qui n'est pas toujours directement lié à une panne. Même si vos concurrents ont été touchés par le même incident, vos clients ne le savent peut-être pas. Ils voyageaient avec vous ce jour-là.</p>""}, {'', ""<p>La prochaine fois que vous négocierez un budget pour les outils et les ressources de test, n'oubliez pas qu'il y a non seulement des coûts importants liés aux temps d'arrêt, mais aussi de nombreux coûts indirects. Parfois, je pense que le coût des outils et du personnel de test devrait être classé dans la catégorie des assurances, car c'est le cas. Le fait que les bugs entraînent très rarement des temps d'arrêt signifie que la douleur n'est généralement pas aiguë. Nous vivons avec cela. Nos clients vivent avec cela. Jusqu'au jour où cela vous met à terre.</p>""}, {'', '<p>N’attendez pas que la douleur soit insupportable. Investissez dans des personnes, des processus et des outils pour vous assurer de ne jamais avoir à expliquer pourquoi un bug dans un petit fichier a eu un impact sur le monde entier.</p>'}]"
Solutions basées sur l'IA pour les pipelines Azure DevOps,"[{'', '<p>Le paysage technologique est de plus en plus concurrentiel et, pour que les entreprises prospèrent, le dernier recours consiste à exploiter les technologies modernes. L’intelligence artificielle (IA) n’est plus une chose du passé : elle est là et change rapidement le paysage du développement logiciel. Vous serez gagnant si vous trouvez cet article en recherchant l’intégration de l’IA dans vos pipelines Azure DevOps.</p>'}, {'', '<p>Ce guide complet révélera comment l’IA peut augmenter le processus de livraison de logiciels de haute qualité plus rapidement et plus efficacement.</p>'}, {'', '<h3>Comprendre les pipelines Azure DevOps</h3>'}, {'', '<p>Définition</p>'}, {'', ""<p>Les pipelines Azure DevOps sont la pierre angulaire du développement logiciel moderne. Ils servent de workflows automatisés qui coordonnent l'ensemble du processus de livraison de logiciels. Considérez-les comme des chaînes de montage numériques, où votre code est compilé, testé et empaqueté dans des artefacts déployables. Ces pipelines rationalisent et normalisent les étapes impliquées dans la création, le test et le déploiement d'applications, garantissant ainsi la cohérence et la fiabilité dans différents environnements.</p>""}, {'', '<p>Composants clés</p>'}, {'', '<li>Build\xa0: la première étape consiste à créer un code source, à minimiser les dépendances et enfin à le créer. Cette étape consiste à compiler votre code source, à résoudre les dépendances et à former des packages réalisables. Les pipelines Azure DevOps offrent diverses tâches de build pour différents langages et frameworks de programmation.</li>'}, {'', ""<li>Test : les tests sont essentiels pour une automatisation efficace des fonctionnalités et des performances des applications. Les pipelines Azure DevOps ont une capacité unique à s'intégrer à plusieurs frameworks de test. Cela permet d'intégrer différents types de tests, notamment des tests unitaires, d'intégration et d'interface utilisateur, dans le cadre de votre pipeline global.</li>""}, {'', '<li>Déploiement\xa0: une fois votre application créée et testée avec succès, l’étape de déploiement prend le relais. Les pipelines Azure DevOps vous permettent de déployer vos artefacts sur différentes cibles, telles que des machines virtuelles, des clusters Kubernetes ou des services cloud.</li>'}, {'', '<h3>Les défis de l’optimisation traditionnelle des pipelines</h3>'}, {'', ""<p>Bien que les pipelines Azure DevOps offrent un cadre robuste pour l'automatisation, leur optimisation traditionnelle peut s'avérer difficile. Les problèmes courants incluent\xa0:</p>""}, {'', '<li>Configuration manuelle : la configuration et la maintenance de pipelines complexes nécessitent souvent une configuration manuelle approfondie, qui peut prendre du temps et être sujette aux erreurs.</li>'}, {'', ""<li>Boucles de rétroaction longues\xa0: l'identification et la résolution des problèmes dans un pipeline peuvent prendre du temps, en particulier lorsque l'on s'appuie sur des tests et un déploiement manuels. Cela peut entraîner des retards dans les versions et des équipes frustrées.</li>""}, {'', ""<li>Visibilité limitée : obtenir des informations sur les performances du pipeline, les goulots d'étranglement et les défaillances potentielles peut être difficile sans outils de surveillance et d'analyse avancés.</li>""}, {'', ""<li>Défis d'évolutivité : à mesure que les projets gagnent en complexité et en ampleur, la gestion et l'optimisation des pipelines deviennent de plus en plus difficiles, nécessitant des ressources et une expertise supplémentaires.</li>""}, {'', '<p>Notre approche de l’optimisation des pipelines a complètement changé avec l’arrivée de l’IA. En automatisant les tâches répétitives, en analysant de vastes quantités de données et en faisant des prévisions intelligentes, l’IA peut permettre aux équipes de surmonter ces défis et d’atteindre de nouveaux niveaux d’efficacité, de qualité et d’agilité dans leurs processus de livraison de logiciels.</p>'}, {""<h3>Optimiser les pipelines Azure DevOps avec l'IA\xa0: un guide étape par étape</h3>"", ''}, {'', '<p>En suivant les étapes décrites ci-dessous, vous pouvez exploiter la puissance de l’IA pour optimiser les pipelines Azure DevOps. Explorons cela en détail.</p>'}, {'', ""<p>Étape 1 : Surveillance et analyse basées sur l'IA</p>""}, {'', '<p>Visibilité en temps réel : une évaluation appropriée de l’état de santé, des performances et des indicateurs clés de votre pipeline est essentielle pour un flux de travail fluide. Les outils de surveillance basés sur l’IA sont d’une grande aide à cet égard. Vous pouvez mettre en œuvre des solutions telles qu’Azure Application Insights ou des plateformes similaires pour obtenir une visibilité complète et ciblée sur le fonctionnement du pipeline.</p>'}, {'', ""<p>Détection d'anomalies\xa0: les algorithmes d'apprentissage automatique (ML) sont un excellent moyen de détecter les comportements inhabituels dans votre pipeline. Les exemples incluent les contraintes de ressources et les tests en échec. Le principal avantage de cette surveillance assistée par l'IA est l'intervention précoce pour éliminer le problème.</p>""}, {'', ""<p>Analyse des causes profondes : une fois que les moniteurs d'IA ont détecté l'anomalie, ils agissent rapidement pour identifier la cause profonde car ils peuvent facilement inspecter des données et des journaux volumineux. Cela permet également de réduire le temps de réponse et de lancer des processus de dépannage.</p>""}, {'', '<p>Étape 2 : Tests intelligents et automatisation</p>'}, {'', ""<p>Génération de cas de test pilotés par l'IA : avec des outils basés sur l'IA, des cas de test peuvent être générés après toute modification de code ou cas d'utilisation. Cela présente deux avantages majeurs : cela réduit l'effort requis pour tester en minimisant le travail manuel et en permettant une analyse complète des tests.</p>""}, {'', '<p>Exécution de tests automatisée : les cadres de test pilotés par l’IA peuvent effectuer des tests sur plusieurs cas de test et configurations. Ces solutions de tests automatisés augmentent la vitesse et l’efficacité du flux de travail et réduisent les erreurs humaines.</p>'}, {'', ""<p>Analyse intelligente des résultats des tests : après les tests, les algorithmes d'IA peuvent également analyser les résultats des tests et fournir des informations utiles telles que des comportements anormaux et des modèles similaires. L'un des avantages de cette fonctionnalité est la suite de tests améliorée et les déploiements soutenus en toute confiance.</p>""}, {'', '<p>Étape 3\xa0: Déploiement et livraison continus</p>'}, {'', ""<p>Orchestration des versions pilotée par l'IA : exploitez l'IA pour automatiser le processus d'orchestration des versions, y compris les approbations de déploiement, les contrôles d'environnement et les procédures de restauration. Cela simplifie le processus de déploiement, minimise les interventions manuelles et réduit le risque d'erreurs.</p>""}, {'', ""<p>Déploiements Canary\xa0: implémentez des déploiements Canary basés sur l'IA, où de nouvelles fonctionnalités ou modifications de code sont progressivement déployées auprès d'un sous-ensemble d'utilisateurs. L'IA peut analyser l'impact de ces modifications sur les performances, l'expérience utilisateur et d'autres indicateurs clés, ce qui vous permet de prendre des décisions basées sur les données concernant un déploiement plus large.</p>""}, {'', ""<p>Restaurations automatiques : en cas d'échec après le déploiement, l'IA peut déclencher des restaurations automatiques vers une version stable antérieure de votre application. Une récupération rapide et un impact minimal sur les utilisateurs sont les avantages de cette capacité.</p>""}, {'', '<p>Étape 4 : Boucles de rétroaction et amélioration continue</p>'}, {'', ""<p>Surveillance et analyse continues : les informations basées sur l'IA peuvent aider à identifier les domaines à améliorer et à maintenir une boucle de rétroaction constante en surveillant et en analysant en permanence les données de votre pipeline, par exemple en optimisant les temps de création, en affinant les stratégies de test ou en peaufinant les processus de déploiement.</p>""}, {'', '<p>Apprentissage adaptatif : implémentez des algorithmes d’IA capables d’apprendre des exécutions de pipeline passées et d’ajuster automatiquement les configurations ou les paramètres pour améliorer les performances et la fiabilité au fil du temps.</p>'}, {'', '<p>Partage des connaissances : créer un environnement de coordination et de partage d’informations entre les équipes de développement et d’exploitation est essentiel au bon fonctionnement d’un projet. Utilisez des outils basés sur l’IA pour faciliter la communication, documenter les meilleures pratiques et créer une culture d’apprentissage et d’amélioration continue.</p>'}, {'', '<h3>Outils et technologies pour Azure DevOps</h3>'}, {'', ""<ol>Azure Machine Learning : un outil puissant et efficace qui permet de créer, de former et de déployer des frameworks ML facilement intégrés à vos pipelines Azure DevOps. De plus, il est basé sur le cloud, ce qui augmente ses fonctionnalités sur diverses plateformes et réseaux. Vous pouvez l'exploiter pour la détection d'anomalies, l'analyse prédictive, les tests intelligents et d'autres optimisations pilotées par l'IA. Azure DevOps Extensions : un vaste marché d'extensions offre des fonctionnalités basées sur l'IA pour divers aspects de votre pipeline. Ces extensions peuvent aider à l'analyse de code, à l'automatisation des tests, à la gestion des versions et plus encore, facilitant ainsi l'intégration de l'IA dans vos flux de travail existants. Opsera : en exploitant l'IA pour automatiser et optimiser la distribution de logiciels, Opsera donne une longueur d'avance à DevOps. Il fournit des fonctionnalités telles que l'optimisation intelligente des pipelines, l'analyse prédictive et les tests automatisés, permettant aux équipes de rationaliser les flux de travail et d'obtenir des versions plus rapides. Harness : cette plateforme de livraison continue intègre l'IA pour automatiser la vérification du déploiement, optimiser les coûts du cloud et garantir la conformité. Il utilise le ML pour analyser les modèles de déploiement et prédire les problèmes potentiels, permettant aux équipes de livrer des logiciels en toute confiance. GitHub Copilot : cet outil de saisie semi-automatique de code basé sur l'IA peut améliorer considérablement la qualité du code et la productivité des développeurs. Cependant, il est important de savoir que GitHub Copilot n'est actuellement pas intégré à Azure DevOps. Il utilise le ML pour suggérer des extraits de code, des lignes complètes et même générer des fonctions entières, ce qui permet aux développeurs de gagner un temps précieux et de réduire les erreurs.</ol>""}, {'', ""<li>Azure Machine Learning : un outil puissant et efficace qui permet de créer, de former et de déployer des frameworks ML facilement intégrés à vos pipelines Azure DevOps. De plus, il est basé sur le cloud, ce qui augmente ses fonctionnalités sur différentes plateformes et réseaux. Vous pouvez l'exploiter pour la détection d'anomalies, l'analyse prédictive, les tests intelligents et d'autres optimisations pilotées par l'IA.</li>""}, {'', ""<li>Extensions Azure DevOps\xa0: un vaste marché d'extensions offre des fonctionnalités optimisées par l'IA pour divers aspects de votre pipeline. Ces extensions peuvent vous aider dans l'analyse de code, l'automatisation des tests, la gestion des versions et bien plus encore, facilitant ainsi l'intégration de l'IA dans vos flux de travail existants.</li>""}, {'', ""<li>Opsera : En exploitant l'IA pour automatiser et optimiser la distribution de logiciels, Opsera donne une longueur d'avance à DevOps. Il fournit des fonctionnalités telles que l'optimisation intelligente du pipeline, l'analyse prédictive et les tests automatisés, permettant aux équipes de rationaliser les flux de travail et d'obtenir des versions plus rapides.</li>""}, {'', ""<li>Harness : cette plateforme de livraison continue intègre l'IA pour automatiser la vérification du déploiement, optimiser les coûts du cloud et garantir la conformité. Elle utilise le ML pour analyser les modèles de déploiement et prédire les problèmes potentiels, permettant ainsi aux équipes de livrer des logiciels en toute confiance.</li>""}, {'', ""<li>GitHub Copilot : cet outil de saisie semi-automatique de code basé sur l'IA peut améliorer considérablement la qualité du code et la productivité des développeurs. Cependant, il est important de savoir que GitHub Copilot n'est actuellement pas intégré à Azure DevOps. Il utilise le ML pour suggérer des extraits de code, des lignes complètes et même générer des fonctions entières, ce qui permet aux développeurs de gagner un temps précieux et de réduire les erreurs.</li>""}, {'', '<h3>Études de cas</h3>'}, {'', '<p>Microsoft : en tant que créateur d’Azure DevOps, Microsoft est à l’avant-garde de l’intégration de l’IA dans ses pipelines. L’entreprise utilise Azure Machine Learning pour analyser de vastes quantités de données de télémétrie issues de ses processus de développement et de déploiement, ce qui lui permet de signaler les obstacles, d’estimer les échecs et de répartir les ressources. Cela a considérablement amélioré les fréquences de déploiement et la qualité globale des logiciels.</p>'}, {'', '<p>Adobe : Adobe a adopté l’IA pour améliorer ses pipelines DevOps pour les applications cloud créatives. L’entreprise utilise des modèles ML pour analyser les modifications de code et prédire les impacts potentiels sur les performances et la stabilité. Cela lui permet de résoudre les problèmes de manière proactive avant qu’ils n’affectent les utilisateurs, ce qui se traduit par une expérience plus fluide et plus fiable pour des millions de professionnels de la création dans le monde entier.</p>'}, {'', '<h3>Conclusion</h3>'}, {'', '<p>En conclusion, l’intégration de l’IA dans les pipelines Azure DevOps transforme le processus manuel de développement et de livraison de logiciels. En adoptant l’IA, les organisations peuvent atteindre des niveaux d’efficacité, de qualité et d’agilité sans précédent, ce qui leur permet d’innover et d’obtenir un avantage concurrentiel dans le monde en constante évolution du développement de logiciels.</p>'}]"
Une enquête révèle des signes inquiétants d'insécurité dans la chaîne d'approvisionnement des logiciels,"[{'', ""<p>Une enquête mondiale menée auprès de 1 224 professionnels de l'ingénierie logicielle travaillant dans des organisations de plus de 1 000 employés suggère qu'un écart important est apparu entre ce que les cadres supérieurs estiment être fait pour améliorer la sécurité des applications et ce qui se passe réellement.</p>""}, {'', ""<p>Réalisée par Atomik Research pour le compte de JFrog, fournisseur d'une plateforme DevSecOps, l'enquête révèle que 67 % des 331 cadres et managers interrogés estiment que des analyses de sécurité au niveau du code sont effectuées régulièrement, mais seulement 41 % des développeurs sont du même avis.</p>""}, {'', '<p>88 % des dirigeants estiment également que l’intelligence artificielle (IA) et les outils d’apprentissage automatique sont utilisés pour les processus d’analyse et de correction de la sécurité. Cependant, seules 60 % des équipes DevSecOps ont déclaré utiliser réellement ces outils. Un peu plus de 90 % des dirigeants pensent également qu’ils utilisent des modèles d’apprentissage automatique dans leurs applications logicielles, alors que seulement 63 % des développeurs confirment cette affirmation.</p>'}, {'', '<p>De même, 92 % des dirigeants affirment que leurs organisations disposent d’outils pour détecter les packages open source malveillants, alors que seulement 70 % des développeurs sont du même avis.</p>'}, {'', ""<p>Paul Davis, responsable de la sécurité des systèmes d'information chez JFrog Field, a déclaré que le fossé entre les dirigeants et les développeurs de base est probablement dû au simple fait que de nombreuses organisations sont encore en train de repenser leurs workflows DevSecOps. Le défi est que chaque organisation commence ce voyage à partir d'un point de départ différent et progresse donc à des rythmes différents, souvent difficiles à quantifier.</p>""}, {'', '<p>Le fait que seulement 30 % des personnes interrogées aient identifié la nécessité de remédier aux vulnérabilités de leur chaîne d’approvisionnement de logiciels comme une préoccupation majeure en matière de sécurité complique encore les choses. On ne comprend pas vraiment pourquoi moins d’un tiers des personnes interrogées s’inquiètent des vulnérabilités malgré leur prévalence, mais les résultats de l’enquête suggèrent qu’il existe soit un niveau de confiance excessif, soit que d’autres problèmes plus urgents pourraient prendre le pas.</p>'}, {'', ""<h3>Sécuriser les chaînes d'approvisionnement de logiciels avec ou sans réglementation</h3>""}, {'', '<p>On ne sait pas exactement dans quelle mesure la réglementation pourrait inciter les entreprises à mieux sécuriser leurs chaînes d’approvisionnement en logiciels. L’Union européenne (UE), par exemple, fait avancer le Cyber \u200b\u200bResilience Act (CRA) qui oblige les entreprises à sécuriser la chaîne d’approvisionnement de tout logiciel vendu, mais d’autres pays n’ont pas encore suivi l’exemple. En l’absence d’une réglementation réelle, il appartient à chaque entreprise de déterminer la meilleure façon de sécuriser sa chaîne d’approvisionnement en logiciels contre les cyberattaques.</p>'}, {'', '<p>La forme la plus courante de ces attaques consiste généralement à voler les identifiants nécessaires pour accéder à un environnement de développement d’applications. Une fois que les cybercriminels y ont accès, ils tentent pendant une longue période d’intégrer des logiciels malveillants dans un flux de travail dans l’espoir que ces derniers parviennent à s’infiltrer dans plusieurs applications en aval. Dans le même temps, les cybercriminels ciblent également les référentiels de logiciels open source en se faisant passer pour des contributeurs intéressés, avant d’injecter de la même manière des logiciels malveillants dans un outil ou un composant logiciel susceptible d’être utilisé par des milliers d’organisations.</p>'}, {'', '<p>Espérons que les progrès de l’intelligence artificielle (IA) permettront bientôt d’identifier et de remédier plus facilement à ce type de menaces. En attendant, le fossé entre les développeurs et les responsables quant aux mesures prises pour garantir la sécurité des chaînes d’approvisionnement en logiciels est pour le moins déconcertant.</p>'}]"
4 raisons pour lesquelles les leaders technologiques devraient donner la priorité à la phase de test et de simulation pour un meilleur développement,"[{'', ""<p>L'enquête Stack Overflow Developer Survey 2023 révèle que 60 % des développeurs utilisent des tests automatisés. Mais la vérité est que les tests automatisés et les simulations doivent être le domaine le plus prioritaire de votre SDLC pour éliminer les frictions pour votre équipe de développeurs.</p>""}, {'', '<p>Il ne s’agit pas seulement de disposer des outils nécessaires, mais de cultiver une culture dans laquelle les tests sont considérés comme une partie intégrante du processus de développement, et non comme une considération secondaire. Ce changement de mentalité peut considérablement améliorer la qualité des logiciels, réduire les coûts et améliorer les performances globales de l’entreprise. Voici les quatre raisons pour lesquelles les tests devraient être le domaine le plus prioritaire de votre cycle de développement logiciel.</p>'}, {'', '<h3>1. Les tests garantissent un logiciel de haute qualité</h3>'}, {'', ""<p>En tant qu'ancien développeur devenu PDG, je suis convaincu que la pierre angulaire d'une expérience utilisateur exceptionnelle est de garantir la qualité de nos logiciels. Notre priorité absolue est, et doit toujours être, d'améliorer la satisfaction et la confiance de nos utilisateurs finaux, ce qui signifie que la mise en œuvre d'un cadre de test robuste est cruciale dans notre quête de qualité.</p>""}, {'', ""<p>Ce cadre constitue votre première ligne de défense. Il vous aide à identifier et à corriger les bugs dès le début du processus de développement, minimisant ainsi le risque de problèmes majeurs lors de la mise en service du logiciel. Il s'agit de garantir que vos produits fonctionnent parfaitement et répondent aux attentes des utilisateurs finaux, ce qui est essentiel pour maintenir la fiabilité et augmenter la satisfaction des clients.</p>""}, {'', ""<p>Il existe également un effet de feed-forward, qui consiste à maintenir des normes élevées de qualité du code et de fonctionnalité. Avec les tests intégrés, le code est toujours au plus haut niveau requis, tout code inférieur échoue aux tests et est rejeté. Ainsi, votre base de code devient, par définition, de haut niveau, et tout nouveau développeur de votre équipe peut voir la norme et ce qui est nécessaire pour l'atteindre.</p>""}, {'', '<p>En mettant l’accent sur l’importance de tester, de simuler et de résoudre les problèmes avant qu’ils n’affectent vos utilisateurs, vous protégez votre réputation et améliorez la fidélisation de vos clients. Cette approche proactive réduit considérablement les coûts qui seraient autrement engagés pour résoudre les problèmes après la sortie du produit.</p>'}, {'', '<p>En résumé, en tant que leaders technologiques, notre engagement envers des tests rigoureux est non négociable. Il ne s’agit pas seulement de trouver des bugs ; il s’agit d’instaurer la confiance, de garantir la fiabilité et de favoriser une culture d’excellence qui imprègne tous les niveaux de notre organisation.</p>'}, {'', '<h3>2. Tests automatisés -> Des développeurs plus heureux</h3>'}, {'', '<p>Les tests dans le développement de logiciels peuvent parfois être considérés comme une tâche ardue par les développeurs, principalement parce que les méthodes de test traditionnelles nécessitent une intervention manuelle importante. Les développeurs se retrouvent souvent à écrire ou à mettre à jour de nombreux tests, même pour des modifications mineures du code, et la responsabilité de maintenir ces tests peut être écrasante et générer des frictions.</p>'}, {'', ""<p>Cependant, lorsque les tests sont correctement abordés avec l'automatisation, ils se transforment en un outil puissant qui améliore la productivité des développeurs et crée un environnement de codage sans friction. En donnant la priorité aux tests automatisés, les développeurs ne redoutent plus le processus mais l'adoptent. Cela vous permet d'éliminer les obstacles et de réduire les aspects fastidieux du processus de développement, permettant aux développeurs de se concentrer sur ce qu'ils font le mieux : créer et améliorer des logiciels.</p>""}, {'', '<p>Pour commencer, les tests automatisés inspirent confiance aux développeurs quant aux changements qu’ils mettent en œuvre. La peur d’introduire des bugs dans la production peut amener les développeurs à passer trop de temps à vérifier manuellement leur travail. Les tests automatisés agissent comme un filet de sécurité fiable, offrant aux développeurs la liberté de refactoriser et d’innover sans craindre de perturber les fonctionnalités existantes. Cette confiance permet d’adopter des approches plus créatives et plus audacieuses en matière de développement de produits.</p>'}, {'', ""<p>De plus, l'automatisation des tests améliore considérablement l'efficacité du débogage. Lorsque des problèmes surviennent, les frameworks de tests automatisés permettent d'identifier le problème rapidement et avec précision. Ce processus simplifié réduit non seulement le temps consacré au débogage, mais minimise également les temps d'arrêt, permettant ainsi aux développeurs de se concentrer davantage sur l'amélioration du produit plutôt que sur sa réparation.</p>""}, {'', '<h3>3. De meilleurs tests soutiennent davantage les pratiques Agile et DevOps</h3>'}, {'', ""<p>Le feedback continu est l'élément vital du développement agile. Les tests et simulations automatisés s'intègrent aux pipelines CI/CD pour un feedback rapide pour l'équipe. Cela signifie que les problèmes sont détectés tôt, souvent quelques minutes après la validation du code.</p>""}, {'', '<p>Il ne s’agit pas seulement de trouver des bugs, mais de créer un environnement de développement où la qualité est constamment surveillée et améliorée. Ce développement itératif devient possible grâce à des pratiques de test robustes. Sans tests complets, chaque itération risque d’introduire de nouveaux bugs ou des régressions. Grâce à cela, les équipes peuvent déployer des mises à jour en toute confiance, sachant que les fonctionnalités de base resteront intactes.</p>'}, {'', '<p>Ces pratiques favorisent une culture DevOps dans laquelle le développement et les opérations travaillent main dans la main et où ces départements traditionnellement cloisonnés s’alignent sur ce qui constitue un « logiciel fonctionnel ». En donnant la priorité aux tests et aux simulations automatisés dans vos pratiques agiles et DevOps, vous créez un écosystème dans lequel la qualité est construite dès le départ, les développeurs sont habilités à innover rapidement et votre organisation peut s’adapter rapidement aux besoins changeants de l’entreprise.</p>'}, {'', '<h3>4. Les tests améliorent la collaboration et le partage des connaissances</h3>'}, {'', '<p>Cela peut paraître étrange. Comment l’ajout de tests peut-il aider les développeurs à collaborer et à partager leurs connaissances\xa0?</p>'}, {'', ""<p>C'est facile. Les tests sont de la documentation. Ils constituent le meilleur type de documentation car ils représentent le résultat attendu de toute fonction ou composant. Ils constituent une documentation en direct du comportement du code, aidant ainsi les nouveaux membres de l'équipe à comprendre le système. Les tests fournissent une documentation claire sur la manière dont les différentes parties du système doivent fonctionner, servant de point de référence fiable pour les développeurs.</p>""}, {'', '<p>Cette compréhension commune favorisée par des tests complets peut réduire considérablement les malentendus et les conflits au sein de l’équipe, ce qui conduit à une collaboration plus fluide. Lorsque les développeurs écrivent des tests, ils découvrent également des cas limites ou des problèmes potentiels qui n’auraient peut-être pas été apparents autrement, améliorant ainsi encore les connaissances collectives de l’équipe sur les subtilités du système.</p>'}, {'', ""<p>Les tests contribuent également à créer un environnement de développement cohérent au sein de votre organisation technique. Les simulations créent un environnement prévisible pour le développement et les tests, améliorant ainsi la collaboration en équipe. Cette cohérence permet aux développeurs de travailler en toute confiance sur différentes parties du système, sachant que leurs modifications ne risquent pas de perturber de manière inattendue d'autres composants.</p>""}, {'', ""<p>Lorsque de nouveaux membres de l'équipe rejoignent ou que les développeurs changent de projet, des tests et des simulations complets réduisent considérablement la courbe d'apprentissage, leur permettant de devenir plus productifs et de contribuer efficacement à la base de code.</p>""}, {'', '<h3>Conclusion</h3>'}, {'', '<p>Si je dois vous laisser une chose, c’est qu’il est important de noter que les tests sont nécessaires pour toute architecture logicielle moderne. Les microservices et les applications cloud natives sont si complexes aujourd’hui que les tests sont nécessaires pour garantir que chaque service fonctionne et interagit comme prévu.</p>'}, {'', ""<p>Il est essentiel de donner la priorité aux tests et aux simulations tout au long du cycle de développement (et de les déplacer vers la gauche) : cela transcende les bonnes pratiques pour devenir un impératif concurrentiel. Les leaders technologiques qui prônent une culture de test rigoureuse permettent à leurs organisations de produire des produits de qualité supérieure, de s'adapter rapidement aux changements du marché et de développer des systèmes durables et évolutifs.</p>""}, {'', '<p>La véritable question n’est plus de savoir si vous pouvez vous permettre de donner la priorité aux tests, mais plutôt de savoir si vous pouvez vous permettre de les négliger.</p>'}]"
Mise en œuvre de la modélisation des menaces dans un workflow DevOps,"[{'', ""<p>La sécurité reste primordiale dans le paysage numérique contemporain, en particulier dans le flux de travail DevOps. Les pratiques DevOps visent à rationaliser le cycle de vie du développement logiciel (SDLC) en favorisant la collaboration entre les équipes de développement et d'exploitation, garantissant ainsi un SDLC sécurisé. Cependant, le rythme rapide de DevOps peut parfois négliger des problèmes de sécurité critiques. L'intégration de la modélisation des menaces dans le flux de travail DevOps est essentielle pour identifier et atténuer les menaces de sécurité potentielles. Ce guide complet explore comment la modélisation des menaces peut être efficacement mise en œuvre dans un cadre DevOps pour améliorer la sécurité.</p>""}, {'', ""<p>La modélisation des menaces est une approche structurée permettant d'identifier, d'évaluer et d'atténuer les menaces de sécurité pesant sur un système. Elle consiste à anticiper les attaques potentielles, à comprendre leur impact et à élaborer des stratégies pour s'en défendre. Cette approche proactive de la sécurité garantit que les vulnérabilités sont identifiées dès le début du processus de développement, réduisant ainsi le risque de failles de sécurité après le déploiement.</p>""}, {'', '<h3>Éléments clés de la modélisation des menaces</h3>'}, {'', '<li>Identification des actifs : déterminer ce qui doit être protégé, y compris les données, les systèmes et les services</li>'}, {'', '<li>Identification des menaces : Reconnaître les menaces potentielles qui pourraient exploiter les vulnérabilités</li>'}, {'', '<li>Identification des vulnérabilités : identifier les faiblesses du système qui pourraient être exploitées</li>'}, {'', '<li>Stratégies d’atténuation : Élaboration de contre-mesures pour répondre aux menaces et vulnérabilités identifiées.</li>'}, {'', ""<p>L'intégration de la modélisation des menaces dans un flux de travail DevOps implique l'intégration de pratiques de sécurité tout au long du cycle de vie du développement et des opérations. Cette approche garantit une évaluation et une amélioration continues de la sécurité, conformément aux principes DevOps d'intégration continue et de déploiement continu (CI/CD).</p>""}, {'', '<h3>1. Shift-Left Security : intégration au début du SDLC</h3>'}, {'', '<p>L’un des principes fondamentaux de DevOps est le concept de sécurité « shift-left », qui signifie intégrer les pratiques de sécurité dès le début du cycle de développement logiciel. En intégrant la modélisation des menaces dès les premières étapes, les menaces de sécurité potentielles peuvent être identifiées avant qu’elles ne soient profondément ancrées dans la base de code. Cette intégration précoce favorise un cycle de développement logiciel sécurisé, garantissant que les mesures de sécurité sont prises en compte dès le début et tout au long du processus de développement.</p>'}, {'', '<h4>Avantages de la sécurité Shift-Left</h4>'}, {'', ""<li>Détection précoce des vulnérabilités : l'identification précoce des problèmes de sécurité réduit le coût et les efforts nécessaires pour les résoudre ultérieurement</li>""}, {'', '<li>Amélioration de la qualité du code : encourager les développeurs à écrire du code sécurisé dès le début</li>'}, {'', '<li>Délai de mise sur le marché réduit : réduction du risque de retards causés par des problèmes de sécurité découverts tardivement dans le processus de développement.</li>'}, {'', '<h3>2. Modélisation continue des menaces</h3>'}, {'', ""<p>La modélisation continue des menaces s'aligne sur le principe DevOps d'intégration et de déploiement continus. Cette pratique implique la mise à jour régulière des modèles de menaces pour refléter les changements dans l'application, l'infrastructure et le paysage des menaces. Les outils automatisés peuvent aider à maintenir à jour les modèles de menaces, garantissant ainsi que les évaluations de sécurité sont effectuées de manière cohérente.</p>""}, {'', '<h4>Aspects clés de la modélisation des menaces continues</h4>'}, {'', ""<li>Analyses de sécurité automatisées : intégration d'outils d'analyse de sécurité dans le pipeline CI/CD pour détecter les vulnérabilités en continu</li>""}, {'', ""<li>Mises à jour régulières\xa0: révision et mise à jour des modèles de menaces à mesure que l'application évolue</li>""}, {'', '<li>Surveillance et rétroaction : utilisation de boucles de rétroaction pour améliorer les modèles de menaces en fonction des nouvelles informations et des menaces détectées.</li>'}, {'', ""<h3>3. Collaboration entre les équipes de développement, d'exploitation et de sécurité</h3>""}, {'', ""<p>Une modélisation efficace des menaces dans un environnement DevOps nécessite une collaboration étroite entre les équipes de développement, d'exploitation et de sécurité. Cette collaboration interfonctionnelle garantit que la sécurité est prise en compte à chaque étape du cycle de développement.</p>""}, {'', '<h4>Stratégies pour améliorer la collaboration</h4>'}, {'', ""<li>Champions de la sécurité : nommer des champions de la sécurité au sein des équipes de développement et d'exploitation pour défendre les meilleures pratiques de sécurité</li>""}, {'', ""<li>Formation régulière en matière de sécurité : proposer une formation continue en matière de sécurité aux développeurs et au personnel d'exploitation pour les sensibiliser et renforcer leurs compétences</li>""}, {'', '<li>Responsabilité partagée : promouvoir une culture de responsabilité partagée en matière de sécurité au sein de toutes les équipes.</li>'}, {'', '<h3>4. Intégration des outils de modélisation des menaces</h3>'}, {'', ""<p>De nombreux outils sont disponibles pour faciliter la modélisation des menaces au sein d'un workflow DevOps. Ces outils peuvent automatiser divers aspects de la modélisation des menaces, facilitant ainsi l'identification et l'atténuation des menaces de sécurité potentielles.</p>""}, {'', '<h4>Outils de modélisation des menaces les plus répandus</h4>'}, {'', '<li>OWASP Threat Dragon : un outil open source pour créer des modèles de menaces et visualiser les menaces potentielles</li>'}, {'', '<li>Outil de modélisation des menaces Microsoft\xa0: fournit une approche systématique de la modélisation des menaces avec des modèles prédéfinis</li>'}, {'', ""<li>IriusRisk : une plateforme qui s'intègre aux pipelines CI/CD pour automatiser la modélisation des menaces et l'évaluation des risques</li>""}, {'<h3>5. Études de cas et statistiques</h3>', ''}, {'', '<p>L’intégration de la modélisation des menaces dans un flux de travail DevOps est essentielle pour identifier et atténuer les menaces de sécurité potentielles avant qu’elles ne deviennent des problèmes majeurs. Des études de cas réelles et des preuves statistiques peuvent mettre en lumière les avantages tangibles de cette approche. Cette section se penche sur des études de cas notables, telles que les violations de données de Capital One et Equifax, pour souligner l’importance cruciale de la modélisation des menaces et de la surveillance continue dans les pratiques DevOps.</p>'}, {'', '<p>Violation de données chez Capital One</p>'}, {'', '<p>En 2019, Capital One a connu l’une des violations de données les plus importantes de l’histoire récente, exposant les informations personnelles de plus de 100 millions de clients. La violation a été causée par un pare-feu d’application Web (WAF) mal configuré, qui a permis à un attaquant d’accéder à l’environnement cloud Amazon Web Services (AWS) de Capital One. Cet incident met en évidence plusieurs points critiques concernant l’importance de la modélisation et de la surveillance continues des menaces dans un flux de travail DevOps.</p>'}, {'', '<p>La faille de sécurité de Capital One souligne la nécessité d’une gestion et d’une surveillance rigoureuses de la configuration. Une mauvaise configuration, comme celle qui s’est produite dans ce cas, peut avoir des conséquences dévastatrices si elle n’est pas détectée et corrigée rapidement. Une modélisation continue des menaces aurait pu identifier cette mauvaise configuration à un stade précoce, permettant à Capital One de résoudre le problème avant qu’il ne soit exploité. En outre, des évaluations de sécurité régulières et des outils de surveillance automatisés auraient pu alerter l’équipe de sécurité de la vulnérabilité à temps pour empêcher la faille.</p>'}, {'', '<p>La faille de sécurité de Capital One illustre également l’importance d’une approche globale de la sécurité qui inclut non seulement des mesures techniques, mais aussi des pratiques organisationnelles. En favorisant une culture de sensibilisation à la sécurité et en mettant en œuvre des pratiques de sécurité robustes tout au long du cycle de vie DevOps, les organisations peuvent réduire le risque d’incidents similaires. La modélisation continue des menaces garantit que la sécurité est une considération continue, plutôt qu’une tâche ponctuelle, et contribue à créer un environnement dans lequel les menaces potentielles sont identifiées et traitées de manière proactive.</p>'}, {'', '<p>Violation de données chez Equifax</p>'}, {'', ""<p>La violation de données d'Equifax de 2017 a exposé les informations personnelles d'environ 147 millions de personnes, ce qui en fait l'une des violations les plus importantes et les plus dommageables de l'histoire. La cause principale de la violation était une vulnérabilité non corrigée dans Apache Struts, un framework d'application Web largement utilisé. Cet incident met en évidence le besoin crucial d'intégrer des outils d'analyse de sécurité automatisés dans le pipeline CI/CD, un aspect clé de la modélisation des menaces dans DevOps.</p>""}, {'', ""<p>La faille de sécurité d'Equifax aurait pu être évitée si la vulnérabilité avait été identifiée et corrigée à temps. Des outils d'analyse de sécurité automatisés, intégrés au pipeline CI/CD, peuvent surveiller en permanence les vulnérabilités connues et alerter les développeurs de la nécessité d'appliquer des correctifs. En intégrant de tels outils, les entreprises peuvent s'assurer que leurs applications sont toujours à jour avec les derniers correctifs de sécurité, réduisant ainsi le risque d'exploitation par des attaquants.</p>""}, {'', '<p>De plus, la faille de sécurité d’Equifax souligne l’importance d’une posture de sécurité proactive. Dans un environnement DevOps, où le développement et le déploiement rapides sont la norme, il est essentiel de disposer de systèmes automatisés capables de suivre le rythme des changements. La modélisation continue des menaces et l’analyse automatisée de la sécurité offrent un moyen d’y parvenir, permettant aux organisations d’identifier et d’atténuer les vulnérabilités au fur et à mesure qu’elles apparaissent, plutôt qu’après qu’elles aient été exploitées.</p>'}, {'', ""<h4>Statistiques à l'appui de la modélisation des menaces dans DevOps</h4>""}, {'', '<p>Les données statistiques renforcent encore davantage l’intérêt d’intégrer la modélisation des menaces dans les pratiques DevOps. Selon Gartner, « d’ici 2022, 90 % des projets de développement logiciel affirmeront suivre les pratiques DevSecOps, contre 40 % en 2019 ». Cette augmentation significative reflète une reconnaissance croissante de l’importance d’intégrer la sécurité dans le flux de travail DevOps. Les organisations qui adoptent les pratiques DevSecOps, y compris la modélisation continue des menaces, sont mieux placées pour identifier et traiter les menaces de sécurité en amont, réduisant ainsi le risque de violations et d’autres incidents de sécurité.</p>'}, {'', ""<p>Le rapport State of DevOps 2021 de Puppet fournit des informations supplémentaires. Le rapport indique que les organisations qui intègrent la sécurité dans leurs pratiques DevOps ont 2,4 fois plus de chances de détecter les incidents de sécurité avant qu'ils ne causent des dommages importants. Ce résultat souligne l'efficacité des mesures de sécurité proactives, telles que la modélisation continue des menaces, pour identifier et atténuer les menaces avant qu'elles ne puissent avoir un impact négatif.</p>""}, {'', ""<p>Une enquête menée par le SANS Institute a révélé que 64 % des personnes interrogées ont constaté une amélioration de leur posture de sécurité après avoir mis en œuvre les pratiques DevSecOps. Cette amélioration est probablement due à la nature continue de la modélisation des menaces et des évaluations de sécurité dans un environnement DevOps, qui aide les organisations à garder une longueur d'avance sur les menaces émergentes. En mettant à jour en permanence les modèles de menaces et en intégrant des outils de sécurité automatisés, les organisations peuvent maintenir une posture de sécurité robuste qui s'adapte aux nouveaux défis et vulnérabilités.</p>""}, {'', ""<h4>Avantages de l'intégration de la modélisation des menaces dans DevOps</h4>""}, {'', '<p>Les études de cas et les statistiques présentées mettent en évidence plusieurs avantages clés de l’intégration de la modélisation des menaces dans un flux de travail DevOps\xa0:</p>'}, {'', '<li>Détection précoce des vulnérabilités : la modélisation continue des menaces et les outils d’analyse de sécurité automatisés permettent aux organisations d’identifier les vulnérabilités au début du processus de développement, réduisant ainsi le risque d’exploitation.</li>'}, {'', ""<li>Posture de sécurité proactive : en évaluant et en mettant à jour en permanence les modèles de menaces, les organisations peuvent maintenir une posture de sécurité proactive qui s'adapte aux nouvelles menaces et vulnérabilités.</li>""}, {'', '<li>Sensibilisation améliorée à la sécurité : l’intégration de la modélisation des menaces dans DevOps favorise une culture de sensibilisation à la sécurité, encourageant tous les membres de l’équipe à donner la priorité à la sécurité dans leur travail.</li>'}, {'', ""<li>Risque réduit de violations : la modélisation continue des menaces aide les organisations à identifier et à traiter les menaces de sécurité avant qu'elles ne deviennent des problèmes importants, réduisant ainsi le risque de violations de données et d'autres incidents de sécurité.</li>""}, {'', '<h4>Statistiques</h4>'}, {'', '<li>Selon un rapport de Gartner, « d’ici 2022, 90 % des projets de développement de logiciels prétendront suivre les pratiques DevSecOps, contre 40 % en 2019. »</li>'}, {'', ""<li>Le rapport State of DevOps 2021 de Puppet indique que les organisations qui intègrent la sécurité dans leurs pratiques DevOps sont 2,4 fois plus susceptibles de détecter les incidents de sécurité avant qu'ils ne causent des dommages importants.</li>""}, {'', '<li>Une enquête du SANS Institute a révélé que 64 % des personnes interrogées ont constaté une amélioration de leur posture de sécurité après avoir mis en œuvre les pratiques DevSecOps.</li>'}, {'', '<h3>6. Bonnes pratiques pour la mise en œuvre de la modélisation des menaces dans DevOps</h3>'}, {'', ""<p>La mise en œuvre de la modélisation des menaces dans un workflow DevOps est essentielle pour garantir que la sécurité est intégrée dès le départ au cycle de vie du développement logiciel. En suivant les meilleures pratiques, les organisations peuvent optimiser l'efficacité de leurs efforts de modélisation des menaces, ce qui conduit à des applications plus sécurisées et plus résilientes. Les sections suivantes développent les meilleures pratiques pour la mise en œuvre de la modélisation des menaces dans un environnement DevOps.</p>""}, {'', '<h4>Intégration précoce et continue</h4>'}, {'', '<p>Intégrer la modélisation des menaces dès le début</p>'}, {'', '<p>Le concept de sécurité « shift-left » souligne l’importance d’intégrer des mesures de sécurité dès les premières étapes du cycle de vie du développement logiciel. En intégrant la modélisation des menaces dès les premières phases de développement, les problèmes de sécurité potentiels peuvent être identifiés avant qu’ils ne soient profondément ancrés dans la base de code. L’intégration précoce de la modélisation des menaces garantit que les considérations de sécurité constituent un élément fondamental du processus de conception et d’architecture.</p>'}, {'', '<p>Mise à jour continue des modèles de menaces</p>'}, {'', ""<p>Les modèles de menaces ne doivent pas être des documents statiques créés une fois pour toutes puis oubliés. Ils doivent plutôt être des artefacts vivants qui sont continuellement mis à jour pour refléter les changements dans l'application, l'infrastructure et l'évolution du paysage des menaces. La mise à jour continue garantit que les modèles de menaces restent pertinents et efficaces pour identifier et atténuer les menaces potentielles pour la sécurité.</p>""}, {'', '<p>Utiliser des outils automatisés</p>'}, {'', ""<p>Les outils automatisés jouent un rôle crucial dans la facilitation de la modélisation continue des menaces et des évaluations de sécurité. Des outils tels que OWASP Threat Dragon, Microsoft Threat Modeling Tool et IriusRisk peuvent automatiser divers aspects de la modélisation des menaces, facilitant ainsi l'intégration de ces pratiques dans le pipeline CI/CD. L'automatisation permet de garantir que la modélisation des menaces est effectuée de manière cohérente et efficace, réduisant ainsi la charge de travail des équipes de développement et de sécurité.</p>""}, {'', '<h4>Approche collaborative</h4>'}, {'', '<p>Favoriser la collaboration interfonctionnelle</p>'}, {'', ""<p>Une modélisation efficace des menaces nécessite une collaboration étroite entre les équipes de développement, d'exploitation et de sécurité. Cette approche interfonctionnelle garantit que la sécurité est prise en compte sous différents angles et tout au long du cycle de développement. La collaboration peut être favorisée par des réunions régulières, des ateliers communs et une documentation partagée.</p>""}, {'', '<p>Nommer des champions de la sécurité</p>'}, {'', ""<p>La nomination de champions de la sécurité au sein des équipes de développement et d'exploitation peut contribuer à promouvoir une mentalité axée sur la sécurité. Ces personnes agissent en tant que défenseurs des meilleures pratiques de sécurité, en veillant à ce que les considérations de sécurité soient prioritaires et intégrées dans les flux de travail quotidiens. Les champions de la sécurité servent également de points de contact pour les questions et problèmes liés à la sécurité, comblant ainsi le fossé entre les différentes équipes.</p>""}, {'', '<p>Assurer une formation régulière en matière de sécurité</p>'}, {'', ""<p>Une formation continue en matière de sécurité est essentielle pour tenir tous les membres de l'équipe informés des dernières menaces, vulnérabilités et meilleures pratiques. La formation doit couvrir les principes fondamentaux de sécurité, les menaces spécifiques liées aux technologies utilisées et des exercices pratiques de modélisation et d'atténuation des menaces. Des formations régulières contribuent à créer une culture de sensibilisation à la sécurité et garantissent que tous les membres de l'équipe disposent des connaissances et des compétences nécessaires pour contribuer aux efforts de modélisation des menaces.</p>""}, {'', '<h4>Tirer parti des outils automatisés</h4>'}, {'', ""<p>Automatisez l'identification et l'atténuation des menaces</p>""}, {'', ""<p>Les outils de modélisation des menaces peuvent automatiser l'identification et l'atténuation des menaces de sécurité potentielles, rendant le processus plus efficace et cohérent. Des outils tels que OWASP Threat Dragon, Microsoft Threat Modeling Tool et IriusRisk offrent des fonctionnalités telles que la visualisation des modèles de menaces, la suggestion d'atténuations et l'intégration aux pipelines CI/CD. L'automatisation réduit l'effort manuel requis pour la modélisation des menaces et permet de garantir que les évaluations de sécurité sont effectuées régulièrement.</p>""}, {'', ""<p>Intégrer les outils d'analyse de sécurité dans le pipeline CI/CD</p>""}, {'', ""<p>L'intégration d'outils d'analyse de sécurité dans le pipeline CI/CD permet une détection continue des vulnérabilités. Des outils tels que Snyk, Veracode et Checkmarx peuvent analyser automatiquement le code, les dépendances et les configurations à la recherche de vulnérabilités dans le cadre du processus de création et de déploiement. L'analyse continue permet d'identifier les problèmes de sécurité de manière précoce, en fournissant aux développeurs un retour d'information immédiat et en leur permettant de corriger les vulnérabilités avant qu'elles n'atteignent la production.</p>""}, {'', '<h4>Examens et mises à jour réguliers</h4>'}, {'', '<p>Examiner et mettre à jour régulièrement les modèles de menaces</p>'}, {'', '<p>Des révisions et mises à jour régulières des modèles de menaces sont essentielles pour garantir leur efficacité continue. À mesure que les applications évoluent et que de nouvelles fonctionnalités sont ajoutées, les modèles de menaces doivent être revus pour tenir compte de ces changements. Des révisions régulières doivent être programmées à des étapes clés du cycle de vie du développement logiciel, par exemple avant les versions majeures ou après des modifications architecturales importantes.</p>'}, {'', '<p>Mettre en œuvre des boucles de surveillance et de rétroaction</p>'}, {'', ""<p>Les boucles de surveillance et de rétroaction sont essentielles pour améliorer en permanence les modèles de menace. En collectant des données sur les incidents de sécurité, les vulnérabilités et l'efficacité des modèles de menace, les organisations peuvent obtenir des informations sur les domaines à améliorer. Les boucles de rétroaction doivent impliquer des débriefings réguliers et des analyses post-mortem pour comprendre ce qui a bien fonctionné et ce qui pourrait être amélioré. Cette approche itérative permet d'affiner les modèles de menace et de garantir qu'ils restent pertinents et efficaces.</p>""}, {'', '<h4>Mesures et rapports</h4>'}, {'', '<p>Suivre les indicateurs de sécurité clés</p>'}, {'', ""<p>Le suivi des indicateurs de sécurité clés est essentiel pour mesurer l'efficacité des efforts de modélisation des menaces. Des indicateurs tels que le nombre de vulnérabilités identifiées et corrigées, le temps nécessaire pour résoudre les problèmes de sécurité et l'impact des incidents de sécurité peuvent fournir des informations précieuses sur la posture de sécurité de l'organisation. Ces indicateurs doivent être suivis au fil du temps pour identifier les tendances et mesurer les progrès.</p>""}, {'', '<p>Utilisez des indicateurs pour identifier les domaines à améliorer</p>'}, {'', '<p>Les indicateurs peuvent être utilisés pour identifier les domaines à améliorer dans la modélisation des menaces et les pratiques de sécurité globales. Par exemple, si certains types de vulnérabilités sont systématiquement identifiés, cela peut indiquer un besoin de formation supplémentaire ou de modifications des pratiques de développement. Les indicateurs peuvent également aider à hiérarchiser les efforts de sécurité, en veillant à ce que les ressources soient concentrées sur les domaines les plus critiques.</p>'}, {'', '<p>Démontrer la valeur aux parties prenantes</p>'}, {'', ""<p>Les rapports sur les indicateurs de sécurité clés permettent de démontrer la valeur de la modélisation des menaces aux parties prenantes. En montrant les avantages tangibles de la modélisation des menaces, tels que la réduction des vulnérabilités et des délais de résolution plus rapides, les organisations peuvent obtenir le soutien nécessaire pour poursuivre les investissements dans les pratiques de sécurité. Des rapports réguliers permettent également de tenir les parties prenantes informées de la posture de sécurité de l'organisation et des menaces émergentes.</p>""}, {'', '<p>La mise en œuvre de la modélisation des menaces dans un workflow DevOps est essentielle pour identifier et atténuer de manière proactive les menaces potentielles pour la sécurité. En suivant les meilleures pratiques (intégration précoce et continue, encouragement de la collaboration interfonctionnelle, utilisation d’outils automatisés, examen et mise à jour réguliers des modèles de menaces et suivi des indicateurs de sécurité clés), les entreprises peuvent améliorer leur posture de sécurité et créer des applications plus résilientes. À mesure que le paysage numérique continue d’évoluer, l’importance des mesures de sécurité proactives telles que la modélisation des menaces ne peut être surestimée. En intégrant la sécurité à chaque étape du SDLC, les entreprises peuvent s’assurer que leurs applications restent sécurisées et résilientes face aux menaces émergentes.</p>'}]"
Surveillance en temps réel des API tierces : avantages et mise en œuvre,"[{'', ""<p>Les entreprises s'appuient largement sur des API tierces pour étendre leurs fonctionnalités, améliorer l'expérience utilisateur et rationaliser leurs opérations. Cependant, cette dépendance introduit un besoin critique de surveillance en temps réel de ces API pour garantir une intégration transparente, des performances élevées et une prestation de services ininterrompue. Cet article explore les avantages de la surveillance des API en temps réel pour les intégrations tierces et fournit un guide pour sa mise en œuvre efficace.</p>""}, {'', '<h3>Avantages de la surveillance en temps réel</h3>'}, {'', '<p>La surveillance en temps réel des API tierces offre de nombreux avantages, améliorant considérablement l’efficacité, la fiabilité et la sécurité globales de vos applications. Voici un aperçu détaillé des principaux avantages :</p>'}, {'', '<p>Détection et réponse immédiates aux problèmes</p>'}, {'', ""<p>La surveillance en temps réel vous permet de détecter les problèmes dès qu'ils surviennent plutôt qu'après qu'ils aient déjà eu un impact sur vos utilisateurs. Cette fonctionnalité offre plusieurs avantages\xa0:</p>""}, {'', '<li>Identification rapide : la détection immédiate des anomalies (telles que des temps de réponse lents, des taux d’erreur accrus ou des temps d’arrêt complets de l’API) permet un dépannage rapide.</li>'}, {'', ""<li>Réponse rapide : grâce aux alertes en temps réel, votre équipe peut réagir rapidement aux problèmes, réduisant ainsi le délai moyen de résolution (MTTR) et minimisant l'impact sur l'utilisateur.</li>""}, {'', ""<li>Satisfaction de l'utilisateur : la résolution rapide des problèmes garantit une expérience utilisateur cohérente, préservant ainsi la confiance et la satisfaction des clients.</li>""}, {'', '<p>Performances et fiabilité améliorées</p>'}, {'', '<p>La surveillance des API en temps réel permet de maintenir et d’améliorer leurs performances et leur fiabilité :</p>'}, {'', ""<li>Surveillance continue : suivez en permanence les indicateurs de performance clés tels que les temps de réponse, la latence, le débit et les taux d'erreur pour garantir le fonctionnement optimal des API.</li>""}, {'', ""<li>Optimisation des performances : utilisez des données en temps réel pour identifier et résoudre les problèmes de performances, ce qui permet d'obtenir des réponses API plus rapides et plus fiables.</li>""}, {'', '<li>Stabilité du système : en gérant et en optimisant de manière proactive les performances de l’API, vous pouvez garantir la stabilité et la robustesse globales de votre application.</li>'}, {'', '<p>Maintenance proactive</p>'}, {'', '<p>La surveillance en temps réel permet une approche proactive de la maintenance des API\xa0:</p>'}, {'', '<li>Système d’alerte précoce : ce système détecte les problèmes potentiels avant qu’ils ne deviennent critiques, permettant ainsi une maintenance préventive et évitant les pannes majeures.</li>'}, {'', ""<li>Maintenance planifiée : planifiez et exécutez les activités de maintenance en fonction des données de performance en temps réel, minimisant ainsi les interruptions et les temps d'arrêt.</li>""}, {'', '<li>Gestion des ressources : allouez efficacement les ressources et gérez la capacité pour gérer les charges de pointe, garantissant des performances API cohérentes.</li>'}, {'', '<p>Meilleure posture de sécurité</p>'}, {'', ""<p>La sécurité est un problème majeur lorsqu'il s'agit d'intégrations tierces. La surveillance en temps réel améliore votre posture de sécurité en\xa0:</p>""}, {'', ""<li>Détection d'anomalies : identifiez des modèles inhabituels pouvant indiquer des menaces de sécurité, telles que des tentatives d'accès non autorisées.</li>""}, {'', '<li>Réponse immédiate : réagissez rapidement aux incidents de sécurité, en atténuant les dommages potentiels et en protégeant les données sensibles.</li>'}, {'', '<li>Surveillance de la conformité : assurez la conformité aux normes et réglementations de sécurité en surveillant en permanence les interactions API pour détecter les écarts.</li>'}, {'', '<p>Conformité SLA</p>'}, {'', '<p>Le respect des accords de niveau de service (SLA) est essentiel pour maintenir les relations commerciales et la confiance. La surveillance en temps réel permet de :</p>'}, {'', ""<li>Suivi des performances : surveillez en permanence les indicateurs de performances des API pour vous assurer qu'ils respectent les SLA convenus.</li>""}, {'', '<li>Assurance de disponibilité : suivez la disponibilité et le temps de disponibilité des API en temps réel, garantissant ainsi le respect des garanties de disponibilité.</li>'}, {'', '<li>Rapports de conformité : générez des rapports en temps réel et historiques pour fournir la preuve de la conformité aux SLA, aidant ainsi à gérer les relations avec les fournisseurs et les attentes des clients.</li>'}, {'', '<p>Informations basées sur les données</p>'}, {'', '<p>La surveillance en temps réel fournit des données précieuses qui peuvent éclairer les décisions commerciales\xa0:</p>'}, {'', ""<li>Modèles d'utilisation : analysez les données d'utilisation en temps réel pour comprendre comment les clients interagissent avec vos API, en identifiant les fonctionnalités populaires et les tendances d'utilisation.</li>""}, {'', '<li>Développement de produits : utilisez les informations issues de données en temps réel pour guider le développement de produits et hiérarchiser les améliorations en fonction du comportement des utilisateurs.</li>'}, {'', '<li>Planification stratégique : exploitez les données pour prendre des décisions stratégiques éclairées, telles que la mise à l’échelle de l’infrastructure, l’amélioration de l’expérience utilisateur et l’optimisation de l’allocation des ressources.</li>'}, {'', '<h3>Mise en œuvre de la surveillance en temps réel</h3>'}, {'', '<p>La mise en œuvre d’une surveillance en temps réel des API tierces implique plusieurs étapes importantes. Chacune de ces étapes est essentielle pour garantir l’efficacité de votre stratégie de surveillance. Voici un aperçu détaillé de chaque étape\xa0:</p>'}, {'', '<p>Définir les objectifs de surveillance</p>'}, {'', '<p>Avant de vous lancer dans la mise en œuvre technique, il est essentiel de définir des objectifs clairs pour votre suivi. Tenez compte des aspects suivants :</p>'}, {'', ""<li>Indicateurs de performance : identifiez les indicateurs de performance essentiels pour votre entreprise. Les indicateurs courants incluent le temps de réponse, la disponibilité, les taux d'erreur et le débit.</li>""}, {'', ""<li>Impact sur l'entreprise\xa0: comprenez l'impact des performances de l'API sur vos opérations commerciales et l'expérience utilisateur. Cette compréhension vous aidera à hiérarchiser les indicateurs à surveiller de près.</li>""}, {'', '<li>Exigences de conformité : si vous avez des SLA ou des exigences réglementaires, assurez-vous que vos objectifs de surveillance correspondent à ces obligations.</li>'}, {'', '<p>Choisissez les bons outils</p>'}, {'', ""<p>Il est essentiel de sélectionner les outils appropriés pour la surveillance en temps réel. Envisagez des outils offrant des fonctionnalités complètes, une évolutivité et une facilité d'intégration. Voici quelques options :</p>""}, {'', ""<li>New Relic\xa0: fournit des informations détaillées sur les performances de l'API et propose des tableaux de bord personnalisables et des fonctionnalités d'alerte.</li>""}, {'', ""<li>Datadog : offre une surveillance et des analyses en temps réel pour les API, avec des capacités d'intégration robustes et une interface conviviale.</li>""}, {'', '<li>Pingdom : spécialisé dans la surveillance de la disponibilité et les tests de performances, ce qui le rend idéal pour garantir la disponibilité des API.</li>'}, {'', ""<p>Configurer l'infrastructure de surveillance</p>""}, {'', '<p>La mise en place de l’infrastructure de surveillance implique plusieurs étapes techniques :</p>'}, {'', ""<li>Intégration d'API\xa0: intégrez l'outil de surveillance à vos API. Ce processus peut impliquer l'ajout d'agents de surveillance à votre environnement serveur ou la configuration de points de terminaison d'API pour envoyer des données à l'outil de surveillance.</li>""}, {'', ""<li>Collecte de données : assurez-vous que l'outil collecte des points de données pertinents en temps réel. Ces données peuvent inclure les temps de réponse, les codes d'erreur et les volumes de transactions.</li>""}, {'', ""<li>Stockage des données : configurez le stockage des données historiques pour permettre l'analyse des tendances et la création de rapports.</li>""}, {'', '<p>Mettre en place des mécanismes d’alerte</p>'}, {'', ""<p>La surveillance en temps réel n'est efficace que si vous pouvez réagir rapidement aux problèmes. La mise en place de mécanismes d'alerte implique :</p>""}, {'', ""<li>Seuils\xa0: définissez des seuils pour les indicateurs clés. Par exemple, vous pouvez définir une alerte pour les temps de réponse supérieurs à 200 millisecondes ou un taux d'erreur supérieur à 1\xa0%.</li>""}, {'', ""<li>Notifications\xa0: choisissez la manière dont vous souhaitez recevoir les alertes. Les canaux courants incluent les e-mails, les SMS et l'intégration avec des outils de communication comme Slack ou Microsoft Teams.</li>""}, {'', ""<li>Stratégies d'escalade : définissez des stratégies d'escalade pour traiter rapidement les alertes critiques. Par exemple, les utilisateurs peuvent transmettre une alerte à une équipe d'assistance de niveau supérieur s'ils n'en accusent pas réception dans un certain délai.</li>""}, {'', '<p>Créer des tableaux de bord et des rapports</p>'}, {'', '<p>Les tableaux de bord et les rapports sont essentiels pour visualiser et analyser les données de performances des API. Tenez compte des éléments suivants\xa0:</p>'}, {'', '<li>Tableaux de bord personnalisés\xa0: créez des tableaux de bord personnalisables qui affichent des mesures en temps réel dans un format facile à comprendre. Des outils comme Grafana peuvent être intégrés à vos solutions de surveillance pour créer des tableaux de bord détaillés.</li>'}, {'', ""<li>Rapports automatisés\xa0: configurez des mises à jour régulières sur les performances de l'API. Ces rapports peuvent être programmés quotidiennement ou hebdomadairement.</li>""}, {'', ""<li>Analyse des tendances : utilisez des tableaux de bord et des rapports pour réaliser une analyse des tendances. L'identification de tendances au fil du temps peut contribuer aux efforts de maintenance et d'optimisation proactifs.</li>""}, {'', '<p>Réviser et optimiser régulièrement</p>'}, {'', ""<p>L'amélioration continue est essentielle pour maintenir une surveillance efficace en temps réel. Examinez régulièrement les données collectées et prenez des mesures pour optimiser les performances :</p>""}, {'', ""<li>Optimisation des performances\xa0: utilisez les informations issues de la surveillance en temps réel pour affiner les performances de l'API. Ce processus peut impliquer l'optimisation du code, la mise à niveau de l'infrastructure ou l'ajustement des configurations.</li>""}, {'', '<li>Examen des incidents : Effectuer des analyses post-mortem après avoir résolu les incidents pour trouver la cause principale et prévenir de futures occurrences.</li>'}, {'', ""<li>Boucle de rétroaction\xa0: le partage des informations issues de la surveillance peut contribuer à améliorer l'architecture globale du système et les stratégies de déploiement. Créez une boucle de rétroaction avec vos équipes de développement et d'exploitation.</li>""}, {'', ""<li>Planification de l'évolutivité : à mesure que votre entreprise se développe, assurez-vous que votre infrastructure de surveillance évolue en conséquence. Cela implique d'évaluer régulièrement la capacité de vos outils de surveillance et d'effectuer les mises à niveau nécessaires.</li>""}, {'', '<h3>Conclusion</h3>'}, {'', ""<p>La surveillance en temps réel des API tierces est essentielle pour les entreprises qui s'appuient sur des services externes pour offrir de la valeur à leurs clients. En mettant en œuvre une surveillance efficace en temps réel, les organisations peuvent garantir une détection immédiate des problèmes, améliorer les performances et la fiabilité, maintenir une posture de sécurité solide et se conformer aux SLA. L'adoption des bons outils et stratégies de surveillance des API en temps réel protège vos opérations et permet à votre entreprise d'offrir des expériences utilisateur supérieures dans un paysage numérique concurrentiel.</p>""}]"
Cloud Canaries émerge pour fournir une alternative aux plateformes d'observabilité,"[{'', ""<p>Cloud Canaries est sorti de la clandestinité aujourd'hui pour fournir un ensemble d'agents légers qui exploitent un réseau neuronal pour détecter les problèmes, surveiller les performances en temps réel et identifier comment résoudre les problèmes d'ingénierie logicielle.</p>""}, {'', ""<p>Mark Callahan, PDG de l'entreprise, a déclaré que les Intelligent Canaries sont des agents légers que les équipes DevOps peuvent déployer et qui font appel à une plateforme Cloud Intelligence développée par l'entreprise. Plutôt que de devoir instrumenter chaque environnement informatique, les Intelligent Canaries sont conçus pour être déployés selon les besoins, a-t-il ajouté.</p>""}, {'', '<p>Cette approche élimine le besoin de s’appuyer sur une plateforme d’observabilité qui doit collecter en permanence des quantités massives de données de log pour faire émerger des informations exploitables. Le défi est que le coût total de cette approche d’observabilité nécessite des investissements majeurs. En revanche, Intelligent Canaries est une alternative plus légère qui exploite la technologie de réseau neuronal intégrée dans un service cloud pour une fraction du coût d’une plateforme d’observabilité, a déclaré Callahan.</p>'}, {'', ""<p>Les canaris intelligents fournissent le même retour d'informations en temps réel sur les performances du système qu'une plate-forme d'observabilité que les équipes DevOps utilisent pour surveiller les indicateurs clés et garantir que les systèmes fonctionnent dans des paramètres acceptables.</p>""}, {'', ""<p>Cependant, les équipes DevOps peuvent également utiliser Intelligent Canaries pour affiner de manière autonome les processus, les performances et la fiabilité des flux de travail afin d'apporter des améliorations progressives une fois la cause première des problèmes ou anomalies potentiels identifiée pour isoler et résoudre les problèmes rapidement.</p>""}, {'', '<p>De plus, les équipes DevOps peuvent plus facilement utiliser Intelligent Canaries pour surveiller l’impact des modifications apportées sur l’environnement informatique.</p>'}, {''}, {'', '<p>On ne sait pas exactement dans quelle mesure les équipes DevOps ont adopté les plateformes d’observabilité. En théorie, l’observabilité a toujours été un principe fondamental de DevOps, mais la plupart des équipes informatiques s’appuient sur des outils de surveillance qui leur permettent de suivre un ensemble de mesures prédéfinies. Cependant, ces mesures ne donnent pas accès aux données de journal que les équipes DevOps peuvent interroger pour déterminer la cause première d’un problème.</p>'}, {'', '<p>Le défi est que la collecte des données de journal nécessite que les équipes informatiques instrumentent d’abord les applications pour collecter les données qu’elles peuvent interroger. Une fois ces données collectées, elles doivent ensuite disposer de l’expertise nécessaire pour élaborer les types de requêtes susceptibles de révéler la cause profonde d’un problème. Cette dernière tâche devient plus simple avec l’avènement des algorithmes d’apprentissage automatique capables de suggérer les requêtes à exécuter, mais elle nécessite toujours une bonne dose d’expertise DevOps pour tirer pleinement parti de l’observabilité.</p>'}, {'', ""<p>Cloud Canaries défend une approche alternative à l'agent de levier que les équipes DevOps peuvent facilement déployer pour identifier la cause première d'un problème, qui auparavant aurait pu prendre des jours ou des semaines à déterminer.</p>""}, {'', '<p>Chaque organisation doit déterminer la meilleure voie d’observabilité qui lui convient le mieux. Cependant, à mesure que les environnements applicatifs deviennent plus complexes, le besoin d’observabilité devient un problème plus pressant. De nombreux environnements applicatifs ne peuvent plus être gérés efficacement sans l’aide de technologies avancées telles que les réseaux neuronaux, les algorithmes d’apprentissage automatique et d’autres techniques de science des données. La question est désormais de savoir comment parvenir à l’observabilité à un moment où, grâce à d’autres avancées en matière d’intelligence artificielle (IA), le nombre d’applications exécutées dans des environnements de production va bientôt augmenter de manière exponentielle.</p>'}]"
Pourquoi et comment s'engager à fond dans GitOps,"[{'', '<p>GitOps est comme l’exercice physique : un peu est bon pour vous, mais plus c’est mieux. Tout comme courir quelques kilomètres toutes les deux semaines ne vous fera pas de mal mais n’améliorera pas considérablement votre santé, gérer environ 30 ou 40 % de vos processus à l’aide de GitOps ne boostera pas votre approche des opérations informatiques. Si vous voulez en tirer tous les avantages, vous devez vous y mettre à fond.</p>'}, {'', '<p>Je ne suis pas qualifié pour donner des conseils sur la mise en place d’un programme d’exercices sains. Mais je connais une chose ou deux sur GitOps et comment en tirer le meilleur parti. À cette fin, voici un aperçu des raisons pour lesquelles se lancer à fond dans GitOps peut être difficile, comment vous pouvez surmonter les défis et pourquoi cela en vaut la peine.</p>'}, {'', '<p>Bien que vous n’ayez pas strictement besoin de GitOps pour tirer parti des passerelles API, GitOps rationalise et fait évoluer la gestion de cette infrastructure clé dans les architectures d’applications modernes.</p>'}, {'', '<p>Pour ancrer la discussion, je me concentrerai aujourd’hui sur un cas d’utilisation courant de GitOps : son utilisation pour gérer les API en conjonction avec une passerelle API, qui permet de déployer, d’observer et de sécuriser les API.</p>'}, {'', '<h3>GitOps : définition des principes de base</h3>'}, {'', ""<p>GitOps est l'utilisation de Git, le système de contrôle de version open source, comme source unique de vérité pour la gestion des configurations d'infrastructure. L'objectif de GitOps est de remplacer la gestion manuelle des configurations par des flux de travail automatisés et pilotés par code.</p>""}, {'', '<p>En procédant ainsi, vous bénéficiez de nombreux avantages, tels que :</p>'}, {'', ""<li>Une source centralisée de vérité — sous la forme de référentiels Git — pour la configuration de l'infrastructure</li>""}, {'', ""<li>La capacité de gérer l'infrastructure à l'aide de configurations déclaratives et d'appliquer les modifications automatiquement et en continu pour garantir que l'état actuel n'a pas divergé de l'état déclaré</li>""}, {'', ""<li>Collaboration simple et efficace, puisque toutes les personnes impliquées dans la configuration et la gestion de l'infrastructure peuvent opérer à partir d'un hub centralisé</li>""}, {'', ""<li>La possibilité de suivre les modifications apportées aux configurations d'infrastructure au fil du temps à l'aide des fonctionnalités de contrôle de version intégrées de Git</li>""}, {'', '<li>Retour rapide et facile à une configuration antérieure en cas de problème</li>'}, {'', ""<li>La capacité de valider des configurations à l'aide d'agents logiciels, fréquemment appelés linters.</li>""}, {'', ""<p>Par exemple, si vous exploitez une passerelle API que vous pouvez gérer à l'aide d'une approche d'infrastructure en tant que code (IaC), vous pouvez utiliser GitOps pour configurer la passerelle à l'aide du code que vous stockez dans un référentiel Git. Cela permet d'analyser et de valider le code avant d'appliquer la configuration. De plus, vous pouvez suivre les modifications apportées à votre configuration via Git, et tous les membres de votre équipe peuvent facilement surveiller et collaborer autour de la gestion de la passerelle API en utilisant Git comme source de vérité centralisée.</p>""}, {'', ""<h3>L'état de l'adoption de GitOps</h3>""}, {'', '<p>Il est facile de parler des avantages de GitOps. L’adoption de GitOps peut toutefois s’avérer plus difficile. Selon une récente enquête de la CNCF, plus de 90 % des entreprises déclarent utiliser GitOps dans une certaine mesure, mais beaucoup ne l’ont mis en œuvre que dans une fraction de leurs déploiements cloud natifs. Les entreprises sont impatientes d’adopter GitOps, mais l’étendre à l’ensemble de leurs environnements informatiques s’avère beaucoup plus difficile.</p>'}, {'', '<p>C’est dommage, car la valeur de GitOps augmente de manière exponentielle lorsque vous l’implémentez de manière systématique et cohérente dans toute votre organisation. Lorsque vous pouvez gérer chaque déploiement cloud natif à l’aide d’une approche GitOps, les processus déclaratifs et automatisés s’intègrent alors dans votre culture et transforment votre entreprise.</p>'}, {'', '<p>En revanche, une implémentation partielle de GitOps peut améliorer l’efficacité de quelques processus, mais elle n’améliorera pas vos opérations globales. Elle vous laissera également avec des processus incohérents, car certains sont gérés via GitOps tandis que d’autres s’appuient sur des techniques manuelles. Et vos équipes se retrouveront coincées entre deux ensembles d’outils disparates, l’un pour activer GitOps et l’autre conçu pour les opérations héritées.</p>'}, {'', '<p>C’est pourquoi GitOps devrait idéalement être une affaire de tous les instants. Mettre en œuvre GitOps ici et là est mieux que ne pas l’utiliser du tout, mais vous ne pourrez pas exploiter tout le potentiel de GitOps tant qu’il ne s’agira pas d’un processus cohérent et systématique qui s’étend à toutes les facettes de votre entreprise et de vos opérations.</p>'}, {'', '<h3>Surmonter les obstacles à la mise en œuvre de GitOps</h3>'}, {'', ""<p>Mais encore une fois, se lancer à fond dans GitOps est plus facile à dire qu'à faire. Voici les principaux défis auxquels les entreprises sont confrontées, ainsi que les stratégies pour les surmonter.</p>""}, {'', '<p>Modifications du flux de travail</p>'}, {'', '<p>Le défi le plus évident dans la mise en œuvre de GitOps est peut-être la nécessité de faire évoluer les flux de travail des approches manuelles traditionnelles vers des approches ancrées dans Git. Vous devez vous assurer que toutes les parties prenantes savent comment travailler avec Git et sont prêtes à en faire leur solution de référence pour la gestion des configurations.</p>'}, {'', '<p>Par exemple, si votre objectif est de gérer une passerelle API à l’aide de GitOps, vos ingénieurs doivent être capables de gérer les configurations de la passerelle à l’aide de code et de savoir comment gérer ce code dans Git.</p>'}, {'', '<p>Il n’existe pas de solution miracle pour résoudre ce problème du jour au lendemain, mais la formation est essentielle. Les organisations qui souhaitent adopter complètement GitOps doivent former les développeurs, les ingénieurs informatiques et toute autre personne jouant un rôle dans la gestion de la configuration au fonctionnement de Git et à l’utilisation de plateformes complémentaires, telles que GitHub ou GitLab, que l’entreprise pourrait utiliser pour stocker le code de configuration. En outre, les initiatives de formation doivent expliquer pourquoi GitOps est si précieux, afin d’obtenir l’adhésion à une adoption à grande échelle de GitOps.</p>'}, {'', '<p>Trouver des outils compatibles avec GitOps</p>'}, {'', ""<p>Étant donné que GitOps est un concept relativement nouveau, tous les outils ne fonctionnent pas de manière transparente avec une approche de gestion de configuration basée sur GitOps. La mise en œuvre de GitOps est particulièrement difficile dans les cas où vous travaillez avec des outils qui ont une prise en charge native limitée ou inexistante de la gestion de configuration déclarative (c'est-à-dire la capacité de décrire un état souhaité à l'aide de code et de l'activer automatiquement). Par exemple, même si vous utilisez une plateforme comme Kubernetes (qui prend en charge les configurations déclaratives), vous pouvez l'associer à une passerelle API héritée au sein de votre pile qui ne prend pas en charge nativement une approche GitOps, car elles ne sont pas conçues pour être gérées à l'aide de code et de configurations déclaratives.</p>""}, {'', ""<p>Dans certains cas, vous pouvez contourner ce problème en installant des modules complémentaires ou en utilisant des outils supplémentaires pour activer une approche déclarative dans des environnements qui ne la prennent pas en charge. Cependant, cette solution nécessite beaucoup d'efforts pour être mise en œuvre et augmente également la complexité de vos opérations en ajoutant des outils supplémentaires.</p>""}, {'', '<p>Une meilleure solution à ce problème consiste à migrer vers des outils conçus pour prendre en charge la gestion de configuration déclarative de manière native. Aujourd’hui, vous pouvez trouver des outils compatibles avec GitOps pour pratiquement toutes les couches d’une pile de déploiement cloud native. Il n’y a donc plus aucune raison de se contenter de logiciels qui n’offrent pas une prise en charge complète des configurations déclaratives.</p>'}, {'', '<p>Gérer les secrets</p>'}, {'', ""<p>La gestion des secrets, tels que les clés qui permettent de sécuriser le trafic circulant via une passerelle API, peut s'avérer difficile lors de la mise en œuvre de GitOps, car vous ne souhaitez idéalement pas stocker les secrets en texte brut dans le même code que celui que vous utilisez pour gérer les configurations. Si vous le faites, toute personne pouvant consulter votre code de configuration pourra également accéder à vos systèmes et ressources.</p>""}, {'', ""<p>Heureusement, il existe des solutions simples à ce problème. L'une d'entre elles consiste à utiliser un gestionnaire de secrets, qui vous permet de stocker des secrets en dehors de vos référentiels de code tout en les rendant accessibles aux demandes d'authentification. Une autre solution consiste à intégrer des secrets dans votre code de configuration, mais à vous assurer qu'ils sont chiffrés pour empêcher tout accès non autorisé. L'une ou l'autre approche vous permet de connecter des secrets de manière sécurisée à un outil que vous gérez de manière déclarative.</p>""}, {'', '<p>Gérer la complexité</p>'}, {'', '<p>Si GitOps simplifie et fait évoluer les opérations de certaines manières, il peut également rendre les processus plus complexes en ajoutant de nouveaux types de ressources (comme des référentiels de code) aux workflows. C’est l’une des raisons pour lesquelles les équipes sont parfois tentées de s’en tenir aux passerelles API héritées qui ne prennent pas en charge la configuration déclarative : au départ, ces solutions peuvent sembler plus faciles à gérer car elles ne nécessitent pas l’apprentissage d’un tout nouveau workflow et d’outils supplémentaires. Cependant, l’organisation finit par en payer le prix avec un manque de rapidité et d’agilité à long terme.</p>'}, {'', ""<p>Les meilleures pratiques pour garder les choses gérables incluent la dénomination cohérente des ressources, le maintien de structures de dossiers cohérentes pour le code Git et l'utilisation d'outils comme Kustomize pour appliquer les configurations de manière cohérente dans des environnements disparates.</p>""}, {'', ""<p>L'analyse du code de configuration (c'est-à-dire la détection automatique des problèmes de formatage) et l'établissement de processus de révision systématique peuvent également être utiles.</p>""}, {'', '<p>Collaboration et communication</p>'}, {'', ""<p>Pour offrir une valeur maximale, GitOps doit s'accompagner d'un changement culturel au sein de votre organisation. Vous pouvez y parvenir en encourageant activement les équipes à exploiter GitOps comme moyen de rationaliser la collaboration et la communication. Soulignez également comment GitOps augmente la transparence et l'efficacité.</p>""}, {'', ""<p>Au fil du temps, les parties prenantes ont tendance à apprécier la valeur de GitOps de manière organique. Mais vous devrez peut-être lancer le processus pour faire évoluer votre état d'esprit et votre culture vers une approche centrée sur GitOps.</p>""}, {'', '<p>Reprise après sinistre</p>'}, {'', ""<p>GitOps simplifie la reprise après sinistre dans le sens où vous pouvez facilement restaurer des configurations basées sur le code Git. Lorsque les choses tournent mal (par exemple, lorsque la configuration de votre passerelle API entraîne une augmentation de la latence), il vous suffit de rétablir la modification problématique à l'état de fonctionnement précédent et vous avez maintenant le temps d'enquêter et d'identifier la cause première.</p>""}, {'', ""<p>Pour tirer parti de cette fonctionnalité, vous devez toutefois intégrer les opérations basées sur Git dans les plans de reprise après sinistre et de réponse. La plupart des organisations qui ont mis en place des stratégies de reprise après sinistre n'ont pas conçu ces stratégies en tenant compte de GitOps. Par conséquent, vous devez mettre à jour vos manuels et former les ingénieurs pour vous assurer qu'ils sont prêts à faire de Git un pilier essentiel des efforts de reprise après sinistre. Le retour à un état antérieur ne doit pas être un événement effrayant, mais un exercice de confiance.</p>""}, {'', '<p>Gestion des API</p>'}, {'', ""<p>GitOps va de pair avec une approche API-first des workflows. Les normes API, telles que la spécification OpenAPI (OAS), offrent une approche cohérente pour définir et gérer les API à toutes les étapes du cycle de vie des API. De cette façon, elles apportent plus de cohérence et d'ordre à GitOps.</p>""}, {'', '<p>Suivi et observabilité</p>'}, {'', ""<p>La capacité de surveiller et d'observer les pipelines GitOps est essentielle pour anticiper les problèmes susceptibles de perturber les flux de travail GitOps. Par exemple, si quelqu'un supprime accidentellement un code de configuration essentiel dont dépend votre passerelle API, vous souhaiterez le savoir rapidement afin de pouvoir annuler la modification.</p>""}, {'', ""<p>Comme pour la gestion des API, les normes ouvertes sont également utiles ici. En particulier, OpenTelemetry, ou OTel, facilite la collecte cohérente de mesures, de traces et de journaux à partir de n'importe quelle application ou outil.</p>""}, {'', '<p>GitOps pour les systèmes hérités</p>'}, {'', '<p>Un dernier défi auquel vous pouvez être confronté lors de la mise en œuvre de GitOps est de constater que votre parc informatique comprend des systèmes hérités qui ne peuvent tout simplement pas être mis à jour pour prendre en charge une approche déclarative.</p>'}, {'', '<p>La solution idéale consiste bien sûr à mettre à jour des systèmes modernes entièrement compatibles avec GitOps. Mais si la migration n’est pas possible, il existe toujours des moyens d’apporter certains des avantages de GitOps aux environnements existants. Par exemple, les applications monolithiques existantes qui s’exécutent actuellement directement sur des serveurs peuvent être redéployées à l’aide de conteneurs et de Kubernetes. Elles resteront des monolithes, mais vous pourrez gérer leur déploiement de manière déclarative.</p>'}, {'', '<h3>Conclusion : miser à fond sur GitOps</h3>'}, {'', '<p>Il est facile de reconnaître la valeur de GitOps. La mise en œuvre systématique de GitOps dans une entreprise peut être beaucoup plus difficile. Mais comme je l’ai expliqué ci-dessus, il est possible de surmonter chacun des principaux défis que les entreprises rencontrent généralement au cours de leur parcours GitOps. Ne vous attendez pas à vous lancer à fond dans GitOps du jour au lendemain, mais avec la bonne planification et les bons outils, vous pouvez tout à fait transformer votre organisation grâce à GitOps.</p>'}]"
