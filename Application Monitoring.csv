Title,Content
SBOM comme pierre angulaire du développement de logiciels sécurisés,"[{'', ""<p>Dans le monde en constante évolution du développement logiciel, la sécurité est devenue plus critique que jamais. Les applications sont désormais plus complexes et interconnectées, ce qui signifie qu'il existe de nombreux points d'entrée potentiels pour les vulnérabilités. Ces faiblesses peuvent être cachées au plus profond du logiciel, passant souvent inaperçues jusqu'à ce qu'elles causent un problème important. C'est là qu'entre en jeu la nomenclature logicielle (SBOM). Une SBOM agit comme un inventaire détaillé de tous les composants, bibliothèques et modules utilisés pour créer une application logicielle, offrant une transparence inégalée dans la chaîne d'approvisionnement logicielle.</p>""}, {'', '<p>Imaginez une SBOM comme une liste complète des ingrédients d’une application logicielle. Tout comme une liste qui répertorie tous les ingrédients d’une recette, une SBOM inclut tous les composants impliqués dans le logiciel, allant des bibliothèques tierces et des modules open source au code propriétaire. Par exemple, si une application logicielle utilise une bibliothèque open source populaire comme OpenSSL, la SBOM la répertorie avec sa version spécifique. Ce niveau de détail est similaire à une nomenclature dans la fabrication, qui détaille toutes les pièces nécessaires à la fabrication d’un produit physique, comme une automobile ou un smartphone. De la même manière, une SBOM fournit une vue claire de ce qui constitue le logiciel, garantissant que chaque composant est pris en compte et correctement géré.</p>'}, {'', '<h3>Pourquoi les SBOM sont essentiels pour la sécurité des logiciels</h3>'}, {'', '<p>L’un des principaux avantages d’un SBOM est la transparence qu’il offre. En répertoriant tous les composants, les entreprises peuvent retracer chaque partie de leur logiciel jusqu’à son origine. Cela garantit une connaissance précise des composants du logiciel et de son origine, qu’il s’agisse d’un module propriétaire développé en interne ou d’une bibliothèque externe provenant d’un référentiel public. Par exemple, imaginez découvrir qu’une version particulière d’une bibliothèque utilisée dans votre logiciel présente une vulnérabilité de sécurité. Avec un SBOM, il est facile d’identifier les applications concernées, ce qui permet une réponse rapide. Cette traçabilité est essentielle dans la gestion de la sécurité des logiciels, en particulier lorsque des vulnérabilités sont découvertes après le déploiement.</p>'}, {'', ""<p>La gestion des vulnérabilités logicielles est un défi permanent qui nécessite une surveillance et des mises à jour continues. Un SBOM aide les entreprises à suivre tous les composants utilisés dans leurs logiciels, ce qui facilite la détection des problèmes potentiels avant qu'ils ne deviennent graves. Par exemple, si une vulnérabilité critique est découverte dans une bibliothèque open source largement utilisée, le fait de disposer d'un SBOM permet aux équipes de sécurité d'identifier rapidement les composants logiciels concernés et de prendre des mesures correctives, telles que l'application de correctifs ou de mises à jour. Cette approche proactive de la gestion des vulnérabilités permet de prévenir les failles de sécurité et de réduire le risque d'exploitation.</p>""}, {'', '<p>Il ne s’agit peut-être pas seulement de trouver et de corriger les vulnérabilités. Un SBOM peut changer la donne en cas de problème. Il fournit une carte détaillée des composants du logiciel et de leurs versions, ce qui permet aux équipes de sécurité d’identifier, d’isoler et de réparer rapidement les parties affectées. Cela accélère non seulement la réponse aux incidents, mais minimise également les dommages potentiels. De plus, l’intégration des pratiques SBOM dans le cycle de vie du développement logiciel encourage une culture de sécurité et de responsabilité. Les développeurs deviennent plus conscients des composants qu’ils utilisent et des risques associés, ce qui se traduit par des logiciels plus sûrs et plus robustes. Cette prise de conscience accrue est inestimable pour créer un état d’esprit axé sur la sécurité au sein des équipes de développement.</p>'}, {'<h3>Comment mettre en œuvre les pratiques SBOM dans votre organisation</h3>', ''}, {'', '<p>La mise en œuvre d’un SBOM dans votre organisation peut sembler intimidante, mais ce n’est pas forcément le cas. Plusieurs outils et stratégies peuvent contribuer à rationaliser ce processus. Les générateurs SBOM automatisés, par exemple, analysent la base de code et les dépendances du logiciel pour générer automatiquement un SBOM. Des outils comme CycloneDX ou SPDX peuvent analyser les fichiers d’un projet et compiler une liste complète de toutes les dépendances et de leurs versions, ce qui facilite la mise à jour d’un inventaire.</p>'}, {'', ""<p>En plus des générateurs automatisés, les scanners de vulnérabilité jouent un rôle crucial dans le maintien d'un SBOM efficace. Ces outils analysent les composants à la recherche de vulnérabilités connues et mettent à jour le SBOM si nécessaire. Un outil comme OWASP Dependency-Check peut vérifier régulièrement les vulnérabilités et garantir que le SBOM est mis à jour pour refléter les changements. Cette surveillance continue est essentielle pour maintenir la sécurité des logiciels et traiter rapidement toute vulnérabilité nouvellement détectée.</p>""}, {'', ""<p>Les gestionnaires de dépendances sont un autre outil essentiel pour gérer un SBOM. Ils aident à gérer les dépendances logicielles et à maintenir le SBOM à jour. Des outils comme Maven ou npm gèrent non seulement les dépendances, mais peuvent également aider à automatiser la mise à jour d'un SBOM chaque fois qu'un nouveau composant est ajouté ou mis à jour. Cette automatisation réduit la charge de travail manuelle des équipes de développement et garantit que le SBOM reste précis et à jour.</p>""}, {'', ""<p>Pour mettre en œuvre efficacement un SBOM dans votre organisation, il est important de l'intégrer au cycle de vie du développement logiciel (SDLC). Cela implique de mettre à jour régulièrement le SBOM pour refléter les changements apportés au logiciel, comme l'ajout de nouvelles bibliothèques ou la mise à jour des bibliothèques existantes. Il est également essentiel de garantir l'exactitude des informations fournies par les fournisseurs. Tous les fournisseurs tiers doivent fournir des informations précises et à jour sur leurs composants, y compris sur les vulnérabilités connues. Il est également essentiel d'éduquer et de former les développeurs et les principales parties prenantes sur l'importance de maintenir un SBOM précis et son rôle dans la sécurisation de la chaîne d'approvisionnement logicielle pour une mise en œuvre réussie.</p>""}, {'', '<p>L’un des plus grands défis des SBOM est de les maintenir à jour. Les composants logiciels sont fréquemment mis à jour et de nouvelles vulnérabilités sont constamment découvertes. Pour résoudre ce problème, les organisations doivent automatiser la génération des SBOM dans la mesure du possible. L’utilisation d’outils qui génèrent et mettent à jour automatiquement les SBOM dans le cadre du processus de développement peut faire gagner du temps et réduire le risque d’erreur humaine. La mise en œuvre d’une surveillance continue des nouvelles vulnérabilités et la mise à jour régulière du SBOM en conséquence constituent une autre bonne pratique qui peut contribuer à sécuriser votre logiciel.</p>'}, {'', ""<p>Des audits réguliers du SBOM sont également importants pour garantir qu'il reste précis et à jour. Cela implique de réviser périodiquement le SBOM pour s'assurer que tous les composants sont pris en compte et que toutes les modifications ou mises à jour ont été correctement documentées. En restant vigilantes et proactives, les organisations peuvent maintenir un SBOM efficace et mieux protéger leurs logiciels contre les menaces potentielles.</p>""}, {'', ""<h3>L'avenir des SBOM dans le développement et la sécurité des logiciels</h3>""}, {'', '<p>La sécurité de la chaîne d’approvisionnement des logiciels devenant de plus en plus importante, l’adoption des SBOM devrait augmenter. Plusieurs tendances clés sont à l’origine de cette croissance. Les exigences réglementaires en sont une, les gouvernements et les organismes de réglementation commençant à imposer l’utilisation des SBOM dans des secteurs critiques comme la santé et la finance pour garantir une sécurité et une responsabilité accrues. Les efforts de normalisation facilitent également l’adoption des pratiques SBOM par les organisations. Des organisations comme le National Institute of Standards and Technology (NIST) travaillent à la normalisation des formats et du contenu des SBOM, simplifiant ainsi le processus d’adoption pour les entreprises.</p>'}, {'', '<p>L’intégration avec les workflows DevOps est une autre tendance qui facilite l’utilisation des SBOM. En intégrant les pratiques SBOM aux workflows DevOps, les organisations peuvent faciliter la gestion continue et automatisée des SBOM, améliorant ainsi la sécurité globale. Cette intégration garantit que la sécurité est prise en compte à chaque étape du processus de développement, réduisant ainsi le risque de vulnérabilités et facilitant la gestion et la maintenance d’un SBOM.</p>'}, {'', '<h3>Conclusion</h3>'}, {'', ""<p>Un SBOM est un outil puissant pour améliorer la sécurité des applications. En offrant une transparence sur la chaîne d'approvisionnement des logiciels, un SBOM aide les organisations à identifier et à gérer les vulnérabilités des composants tiers et open source. La mise en œuvre des pratiques SBOM peut favoriser une culture de sécurité et de responsabilité, améliorer la réponse aux incidents et garantir une intégration sécurisée. Bien que des défis existent, les meilleures pratiques et les outils automatisés peuvent aider les organisations à gérer et à maintenir efficacement les SBOM. À mesure que les exigences réglementaires et les efforts de normalisation progressent, les SBOM deviendront un élément essentiel des stratégies de développement et de sécurité des logiciels, garantissant la résilience et la sécurité des applications dans un paysage numérique de plus en plus complexe.</p>""}]"
F5 tient sa promesse de NGINX One Platform,"[{'', ""<p>F5 a mis à disposition cette semaine une plateforme de mise en réseau d'applications intégrée qui centralise la gestion de l'équilibrage de charge, des serveurs Web et d'applications, des passerelles d'interface de programmation d'applications (API) et de la cybersécurité.</p>""}, {'', '<p>Shawn Wormke, vice-président et directeur général de NGINX chez F5, a déclaré que NGINX One permet de gérer à la fois les instances F5 NGINX et NGINX Open Source via une plate-forme de logiciel en tant que service (SaaS) via une seule console.</p>'}, {'', ""<p>NGINX One rationalise la mise en œuvre de NGINX Plus, NGINX Open Source, NGINX Unit, NGINX Gateway Fabric et du contrôleur d'entrée Kubernetes que la société fournit d'une manière qui unifie la mise en réseau des applications entre les applications monolithiques et basées sur des microservices qui ont été déployées dans un environnement de cloud computing hybride, a noté Wormke.</p>""}, {'', ""<p>De plus, NGINX One donne accès à des données de télémétrie supplémentaires et à des capacités d'intelligence artificielle (IA) qui facilitent le déploiement et la gestion d'applications à grande échelle, a-t-il noté.</p>""}, {''}, {'', ""<p>En général, la mise en réseau des applications est devenue plus complexe avec l'essor des applications cloud natives basées sur des microservices. En réduisant la charge cognitive nécessaire à la mise en réseau de ces applications, il devient plus simple de faire des opérations de réseau et de sécurité une extension plus naturelle des flux de travail DevOps, a déclaré Wormke.</p>""}, {'', '<p>Par exemple, une équipe d’ingénierie de plateforme peut créer un ensemble de modèles pour intégrer des applications que les équipes de développement d’applications individuelles peuvent ensuite personnaliser pour répondre à des exigences spécifiques, a-t-il ajouté.</p>'}, {'', '<p>Dans d’autres cas, une équipe d’exploitation réseau peut mettre en œuvre ces modèles pour le compte d’une équipe DevOps, a noté Wormke. Quelle que soit l’approche, les organisations informatiques doivent être en mesure de répondre de manière flexible à plusieurs cas d’utilisation selon les besoins, a-t-il noté.</p>'}, {'', '<p>La plupart des organisations ont une certaine expérience de la mise en réseau des applications à l’aide de serveurs proxy, mais aujourd’hui, le besoin de passerelles et de contrôleurs d’entrée pour fournir ces fonctionnalités à grande échelle est devenu plus évident. Par conséquent, de plus en plus d’organisations informatiques commencent à revoir leur structure. Il peut toujours y avoir un besoin de spécialistes réseau dédiés pour gérer la sous-couche réseau physique, mais à mesure que d’autres services réseau s’intègrent davantage aux flux de travail DevOps, la responsabilité de certains services réseau commence à se déplacer davantage vers les équipes DevOps qui déploieront elles-mêmes ces fonctionnalités ou tireront parti d’une plate-forme SaaS telle que NGINX One. L’objectif global est de pouvoir provisionner dynamiquement des services de mise en réseau des applications sans avoir à attendre qu’un administrateur réseau les provisionne.</p>'}, {'', '<p>Quelle que soit l’évolution des réseaux d’applications, la rigidité qui caractérise la fourniture de services réseau depuis des décennies devrait enfin commencer à s’estomper. Aujourd’hui, en dehors d’un environnement de cloud computing, il est encore courant que les équipes informatiques provisionnent des machines virtuelles ou des pods Kubernetes en quelques heures, pour ensuite attendre des jours, voire des semaines, avant que la connectivité réseau ne soit disponible. À mesure que les environnements informatiques deviennent plus hybrides et que de plus en plus de charges de travail sont poussées vers la périphérie, il devient évident qu’une approche plus agile est nécessaire.</p>'}, {'', '<p>Chaque organisation devra bien entendu décider de la meilleure approche à adopter pour la mise en réseau des applications. Cependant, à mesure que les environnements informatiques deviennent plus complexes, il apparaît de plus en plus évident que les approches traditionnelles de mise en réseau des applications ne répondront pas aux besoins des environnements informatiques modernes.</p>'}]"
Automatiser les tests de sécurité des applications Web pour lutter contre les cybermenaces,"[{'', '<p>Pour la plupart des professionnels de la cybersécurité, les tests de sécurité peuvent sembler une évidence, presque comme un pari risqué. Pourtant, rien n’est plus faux. Malgré les centaines d’applications Web et d’API exposées dans nos surfaces d’attaque, de nombreux actifs restent dangereusement non testés et vulnérables aux cyberattaques. Avec l’essor de l’IA, ce nombre ne fera qu’augmenter.</p>'}, {'', '<p>Cela n’a rien de surprenant. Une enquête récente menée auprès de plus de 100 professionnels de la cybersécurité au Royaume-Uni a révélé que les menaces pesant sur leurs applications Web étaient très préoccupantes. Pourtant, la plupart des équipes de sécurité ne parviennent à tester ces applications qu’une fois par mois, ce qui laisse une part importante d’entre elles vulnérables, ce qui met en évidence une lacune critique dans nos programmes de cybersécurité.</p>'}, {'', '<p>Alors pourquoi n’arrivons-nous pas à réaliser des tests corrects ?</p>'}, {'', '<h3>Les surfaces d’attaque deviennent ingérables</h3>'}, {'', ""<p>Les surfaces d'attaque ont toujours été des cibles mouvantes. Elles fluctuent à mesure que les organisations développent leurs piles technologiques et s'intègrent aux systèmes d'autres clients et partenaires. Mais à long terme, elles ne font que croître, ce qui rend difficile de suivre le rythme.</p>""}, {'', ""<p>Les mêmes professionnels de la cybersécurité britanniques ont révélé que leurs organisations ont du mal à faire face au volume considérable et à la nature dynamique des applications Web. En fait, 54,2 % des répondants ont admis que le nombre d'applications Web dans leur environnement est trop important pour permettre des tests adéquats.</p>""}, {'', '<p>D’autres obstacles importants incluent le nombre d’API à tester et le temps nécessaire pour tester chaque application Web, cités respectivement par 59,8 % et 55,1 % des répondants.</p>'}, {'', ""<p>Le sondage a également révélé un fait choquant : ces organisations subissent chaque trimestre des événements de sécurité importants liés à leur application Web, dont la résolution peut prendre jusqu'à huit heures.</p>""}, {'', '<h3>Alors, où sont les tests ?</h3>'}, {'', '<p>Les organisations utilisent diverses méthodes, notamment DAST, IAST et les tests de pénétration, pour identifier les vulnérabilités, les erreurs de configuration et autres faiblesses des applications Web.</p>'}, {'', '<p>Pourtant, plus d’un quart des personnes interrogées ont admis ne pas disposer d’un processus formel pour tester la sécurité de leurs applications Web. Près de la moitié d’entre elles ont déclaré qu’elles utilisaient rarement des outils ou des méthodes de test de sécurité pour découvrir les vulnérabilités de leurs applications Web.</p>'}, {'', '<p>Raisons des tests peu fréquents et de la couverture limitée :</p>'}, {'', ""<li>Trop d'applications et d'API : le nombre d'applications et d'API à tester peut être écrasant</li>""}, {'', '<li>Manque de temps : les contraintes de temps empêchent des tests approfondis et fréquents</li>'}, {'', ""<li>Mises à jour et modifications fréquentes des applications\xa0: les mises à jour et les modifications constantes des applications rendent difficile le maintien d'un calendrier de tests cohérent</li>""}, {'', '<li>Insuffisance de personnel : manque de personnel qualifié pour effectuer des tests approfondis</li>'}, {'', '<li>Limitations budgétaires : les contraintes financières limitent la capacité à investir dans des outils et des ressources de test complets.</li>'}, {'', ""<h3>S'attaquer au problème</h3>""}, {'', ""<p>Au-delà des contraintes de temps et de ressources, l'amélioration de la fréquence et de l'efficacité des tests n'est pas négociable. Voici quelques bonnes pratiques :</p>""}, {'', ""<li>Surveillance continue : la visibilité continue sur la surface d'attaque permet aux organisations d'être proactives et de guider efficacement les activités de correction. La surveillance continue permet d'identifier les vulnérabilités à un stade précoce, réduisant ainsi le risque d'attaques réussies.</li>""}, {'', ""<li>Automatisation : près des trois quarts des dirigeants britanniques interrogés ont déclaré qu'ils prévoyaient d'accroître l'automatisation de leurs flux de tests de sécurité des applications Web. L'automatisation peut permettre d'économiser du temps, de l'argent et des efforts, à condition qu'elle ne crée pas de problèmes supplémentaires tels que la génération de faux positifs. Elle peut rationaliser le processus de test, permettant des évaluations plus fréquentes et plus complètes.</li>""}, {'', ""<li>Tests de production : les tests effectués dans l'environnement de production, plutôt que dans des sandbox ou hors ligne, garantissent que tous les éléments affectant une application Web, y compris les bases de données, les bibliothèques open source et les mécanismes d'authentification, sont pris en compte. Cette approche fournit une représentation plus précise des vulnérabilités potentielles et de leurs impacts.</li>""}, {'', ""<li>Investissement dans DevSecOps : pour accélérer les cycles de développement et améliorer les délais de mise sur le marché, les entreprises ont investi dans des logiciels DevOps pour publier du code plus rapidement. Cependant, elles n'ont pas investi dans des logiciels de sécurité (DevSecOps). L'intégration de la sécurité dans le pipeline DevOps est essentielle pour garantir qu'un développement rapide ne se fasse pas au détriment d'une sécurité compromise.</li>""}, {'', '<h3>Prendre du recul</h3>'}, {'', '<p>Le message principal ici est que les organisations sont de plus en plus exposées et que les méthodes de test sont inadéquates pour sécuriser les environnements.</p>'}, {'', '<p>Les méthodes de test automatisées ne sont pas une mince affaire. Un changement rapide peut fournir une couverture plus complète, une identification plus rapide des vulnérabilités et un processus de correction plus rapide.</p>'}, {'', ""<p>Il peut rationaliser les processus manuels à forte intensité de main-d'œuvre en effectuant des tests continus ou fréquents de toutes les applications Web et des API associées dans l'environnement, en identifiant avec précision les risques et en filtrant les problèmes ou événements de faible priorité.</p>""}, {'', '<p>Non seulement cela améliorera considérablement la posture de sécurité d’une organisation, mais cela soulagera également une partie de la pression sur l’équipe de sécurité.</p>'}]"
Gearset acquiert Clayton pour ajouter Salesforce Code Analytics aux workflows DevOps,"[{'', ""<p>Gearset a révélé cette semaine avoir acquis Clayton, un fournisseur d'une plate-forme d'analyse de code pour les applications logicielles en tant que service (SaaS) exécutées sur le service cloud Salesforce.</p>""}, {'', '<p>La plateforme Clayton identifie les anti-modèles et les vulnérabilités dans le cycle de vie de développement logiciel des applications personnalisées qui étendent les fonctionnalités de base fournies par les applications Salesforce.</p>'}, {'', ""<p>Le PDG de Gearset, Kevin Boyle, a déclaré que l'ajout de Clayton à la plateforme DevOps fournie par Gearset pour créer et déployer ces applications permettra aux organisations de déployer plus facilement des applications sécurisées sans impacter négativement la vitesse à laquelle elles sont créées et déployées.</p>""}, {'', ""<p>Clayton a déjà conclu une alliance avec Gearset et continuera à fonctionner comme une unité commerciale. La société fusionnée travaillera en même temps à renforcer l'intégration entre les deux plateformes, a déclaré Boyle. Les conditions financières de l'accord n'ont pas été divulguées.</p>""}, {'', ""<p>En général, une récente enquête de Gearset révèle que plus de la moitié des entreprises qui créent des applications personnalisées sur la plateforme Salesforce ont adopté des plateformes d'intégration continue/déploiement continu (CI/CD), et que 28 % d'entre elles prévoient de les adopter. L'enquête révèle également que 59 % des répondants travaillent pour des entreprises qui ont déjà adopté ou prévoient d'adopter le contrôle de version, tandis que 54 % ont mis en œuvre des tests automatisés.</p>""}, {'', ""<p>Globalement, plus de la moitié des répondants (54 %) ont déclaré que les équipes DevOps avaient amélioré la qualité des versions, tandis que 46 % ont signalé une amélioration de la collaboration. Cependant, seuls 38 % et 33 % ont déclaré que des versions plus fréquentes ou des délais de livraison réduits, respectivement, étaient un facteur justifiant leur retour sur investissement (ROI) dans les workflows DevOps. En effet, les organisations signalent qu'elles sont beaucoup plus préoccupées par la qualité des applications que par la vitesse à laquelle cet objectif est atteint.</p>""}, {'', '<p>Il est plus simple d’atteindre cet objectif en utilisant une plateforme CI/CD dédiée, car la création d’applications Salesforce nécessite la maîtrise d’un mélange exclusif de langages de programmation, de formats et de métadonnées. Le défi auquel sont confrontées les organisations qui créent ces applications est que les développeurs qui les créent ont des niveaux d’expertise variables, y compris les développeurs dits citoyens qui ont peu d’expertise en programmation. Par conséquent, les risques que les applications soient mal configurées ou contiennent des vulnérabilités susceptibles d’être exploitées par des cybercriminels sont assez élevés. Gearset fournit une plateforme qui simplifie l’application des meilleures pratiques DevSecOps lors de la création et du déploiement de ces applications.</p>'}, {'', '<p>À long terme, il est déjà évident que l’intelligence artificielle générative (IA) accélérera également le rythme de développement de bon nombre de ces applications. À mesure que ce rythme s’accélère, le besoin de meilleurs workflows DevOps pour gérer efficacement ce rythme de développement accru ne fera qu’augmenter. Le défi et l’opportunité, comme toujours, consistent à déterminer comment garantir que ces applications soient de la plus haute qualité possible, car la fréquence à laquelle elles sont créées et déployées ne cesse d’augmenter.</p>'}, {'', '<p>En fin de compte, les entreprises qui adoptent DevSecOps vont systématiquement créer et déployer des applications de meilleure qualité, a déclaré Boyle. À l’heure actuelle, cependant, on ne sait pas vraiment combien d’entreprises qui créent des applications SaaS sur la plateforme Salesforce savent qu’il existe un ensemble de bonnes pratiques pour les créer.</p>'}]"
"L'adoption de la modernisation des applications s'accélère, ce qui encourage l'intégration de l'IA","[{'', '<p>À l’ère de la modernisation technologique rapide, le recours à des applications héritées sous-optimales constitue le principal obstacle à une transition rapide et réussie vers un avenir cloud natif.</p>'}, {'', '<p>Une étude de Futurum intitulée « Navigating Innovation in AI, Application Development, and Observability » révèle que plus des deux tiers (88 %) des applications déployées dans les entreprises sont des actifs hérités. Ces produits logiciels, construits sur une architecture obsolète et maintenus à l’aide de processus en cascade, ont dépassé leur apogée et ne sont pas adaptés aux exigences des entreprises modernes.</p>'}, {'', '<p>Cependant, il est difficile de mettre hors service et de remplacer un ensemble d’applications obsolètes dans l’entreprise. De nombreuses fonctions commerciales critiques sont prises en charge par ces applications. Dans des secteurs comme la banque, par exemple, les applications héritées constituent toujours le pivot des portefeuilles de services des organisations.</p>'}, {'', '<p>« Un pourcentage important d’applications qui sont encore à l’état patrimonial tentent désormais d’en sortir », explique Paul Nashawaty, analyste principal et responsable de la pratique chez The Futurum Group.</p>'}, {'', ""<p>Lors d'une conférence Ignite organisée par Tech Field Day lors de l'AppDev Field Day, Nashawaty s'est concentré sur la manière dont les entreprises peuvent tirer parti des nouvelles technologies en mettant à niveau leurs parcs patrimoniaux.</p>""}, {'', ""<p>Le marché de la modernisation des applications connaît une croissance de 16,8 % et devrait atteindre 24,8 milliards de dollars d'ici la fin de l'année prochaine. Selon une étude d'Infosys réalisée auprès de 1 500 dirigeants et cadres du secteur technologique, seul un dixième des applications existantes sera encore disponible d'ici la fin de 2027.</p>""}, {'', '<h3>Les cadres communs de modernisation des applications</h3>'}, {'', ""<p>Les entreprises mettent à niveau les applications vieillissantes qui sont toujours adaptées à leur objectif vers un état prêt pour le cloud ou natif du cloud en utilisant une gamme de stratégies invasives et non invasives telles que les mises à niveau de l'architecture interne, l'intégration de nouvelles fonctionnalités, l'exposition des fonctions via des API et la maintenance en utilisant des principes modernes comme DevOps et CI/CD.</p>""}, {'', ""<p>« Cloud-ready et cloud-native sont deux choses différentes », souligne Nashawaty. « Cloud-ready signifie que le système fonctionne dans le cloud. Cloud-native signifie qu'il est entièrement élastique et capable d'utiliser les technologies cloud. »</p>""}, {'', '<p>Aujourd’hui, on observe une poignée de modèles de modernisation d’applications axés sur la plateforme, l’architecture et les fonctionnalités. Ils se répartissent en deux grandes catégories : les programmes de type « big-bang » et les programmes de type « low-disruption ».</p>'}, {'', '<p>En fonction du cas commercial spécifique, des objectifs de modernisation et de la capacité d’investissement, une entreprise adopte une ou plusieurs des stratégies suivantes.</p>'}, {'', '<p>Réhébergement – \u200b\u200bCela permet de déplacer des applications d’un environnement existant vers un nouvel environnement dans son état d’origine.</p>'}, {'', '<p>Reconstruction – Lorsqu’une tâche plus lourde est requise, des processus plus intensifs comme la reconstruction permettent aux entreprises de modifier les fonctionnalités des anciennes applications en remplaçant l’intégralité de la base de code par une nouvelle version.</p>'}, {'', '<p>Réécriture – La réécriture d’une application implique la réécriture du code à partir de zéro, ce qui entraîne moins de limitations et une meilleure conception.</p>'}, {'', ""<p>D'autres techniques telles que la re-plateforme, la réarchitecture, la refactorisation et l'encapsulation peuvent également préparer les applications à l'utilisation avec les technologies cloud avec des modifications de code légères à modérées.</p>""}, {'', '<p>Les risques, les coûts et les impacts de chacun de ces processus dépendent de la planification et de l’exécution globales. Des études montrent que les projets échelonnés et coexistants ont le taux de réussite le plus élevé. Comparés à un programme de remplacement, ils sont moins perturbateurs et plus propices à la continuité des activités.</p>'}, {'', '<p>Dans ce contexte, on évoque souvent une approche bien connue, appelée « strangler pattern » (modèle de l’étrangleur). Cette approche permet de démanteler les applications pièce par pièce en commençant par les fruits à portée de main ou les parties les plus vulnérables, au lieu de tout faire en une seule fois.</p>'}, {'', ""<h3>L'application stratégique de l'IA et de l'automatisation dans les processus de modernisation des applications garantit l'agilité</h3>""}, {'', ""<p>Les incitations à la modernisation des applications sont nombreuses. Les applications de microservices sont petites, peu compressées et donc plus faciles à gérer et à prendre en charge. Étant hautement composables, elles peuvent également être mises à l'échelle de manière indépendante et rentable.</p>""}, {'', '<p>En revanche, les applications héritées sont sujettes aux pannes, présentent de graves failles de sécurité et constituent une dette technique pour les organisations.</p>'}, {'', '<p>Cependant, la mise à niveau des applications héritées vers de nouvelles applications et plateformes présente de nombreux points de friction.</p>'}, {'', '<p>« Le principal défi auquel les entreprises sont confrontées en matière de modernisation est la complexité. Une grande partie de ce problème de complexité est liée au manque de compétences », explique Nashawaty. « Les entreprises ne disposent pas des compétences nécessaires ni des personnes nécessaires pour accomplir le travail. »</p>'}, {'', '<p>L’enquête Infosys susmentionnée montre que les pénuries de compétences et de talents sont les principaux obstacles à la modernisation des applications, suivies des dépassements de coûts et des risques de perturbation des activités.</p>'}, {'', ""<p>Un autre obstacle de taille est l'imprévisibilité du retour sur investissement. Un projet de modernisation peut ou non offrir le retour sur investissement estimé une fois terminé.</p>""}, {'', ""<p>« Si vous prenez une application, la refactorisez et obtenez les mêmes performances, voire de moins bonnes, pourquoi dépenser du temps, de l'énergie et de l'argent ? »</p>""}, {'', ""<p>Dans une enquête du groupe Futurum menée auprès de 848 personnes, 24 % ont déclaré qu'ils souhaitaient publier du code toutes les heures, et seulement 8 % ont déclaré qu'ils pouvaient le faire avec succès.</p>""}, {'', '<p>« La raison est qu’ils ne sont pas assez agiles parce qu’ils ne disposent pas d’automatisation », explique Nashawaty. « Ils doivent avoir un humain dans la boucle à chaque étape du pipeline CI/CD. »</p>'}, {'', '<p>L’élimination des contraintes des processus manuels nécessite une utilisation stratégique de l’automatisation. Un modèle de maturité intégrant l’automatisation et l’IA permet de s’attaquer aux tâches redondantes et fastidieuses, ce qui permet de rationaliser les opérations.</p>'}, {'', ""<p>Les efforts de modernisation continue, selon les experts, doivent être guidés par une compréhension claire des moteurs et des objectifs. Lorsque les impacts possibles de la modernisation des applications existantes sur les capacités de l'entreprise sont pleinement analysés et cartographiés, ils mettent en évidence les risques et les opportunités qui permettent de déterminer l'adéquation du projet avec l'entreprise.</p>""}, {'', '<p>Actuellement, les organisations consacrent 33 % de leur temps à l’innovation et 66 % à la maintenance.</p>'}, {'', '<p>« À mesure que de plus en plus d’applications sont créées, nous approchons d’un point critique où la gestion des applications sans IA et automatisation deviendra impossible. Les applications seront vulnérables si elles ne sont pas gérées par une IA capable de fournir des informations exploitables. »</p>'}]"
L’enthousiasme des développeurs pour l’IA générative augmente : enquête,"[{'', '<p>Les développeurs sont de plus en plus positifs quant aux effets de l’IA générative sur leur travail et leurs entreprises en général, un changement dans une communauté qui, par le passé, semblait le craindre, selon un rapport publié cette semaine par le développeur d’API cloud Kong.</p>'}, {'', '<p>En outre, ils affirment que les API joueront un rôle de plus en plus important dans le développement continu de l’IA, l’intégration avec l’IA étant l’aspect le plus critique de la gestion des API au cours des un à deux ans à venir.</p>'}, {'', '<p>« Pour les entreprises, l’intégration de l’IA dans la gestion des API est essentielle pour automatiser et optimiser les opérations, renforcer la sécurité et offrir des expériences utilisateur plus personnalisées », écrivent les auteurs du « API Impact Report 2024 » de Kong. « Elle peut permettre la détection des anomalies en temps réel, l’ajustement dynamique des ressources pour des performances optimales et le développement de services innovants. »</p>'}, {'', ""<p>Dans une enquête menée auprès de 747 professionnels de l'informatique et chefs d'entreprise, 92 % des répondants ont déclaré que l'IA était une priorité pour leur entreprise et 83 % ont déclaré que les investissements en IA dans leur organisation ont ouvert des opportunités pour de nouveaux produits ou services au cours de l'année écoulée.</p>""}, {'', '<p>Cela dit, la tendance des utilisateurs à contourner les politiques de leur organisation en matière d’utilisation de l’IA exerce une pression sur les entreprises pour qu’elles mettent en place des outils tels que des passerelles d’IA et des solutions de prévention des pertes de données (DLP) afin de garantir la gouvernance des données et la conformité réglementaire, d’autant plus que les gouvernements mettent de plus en plus en place de nouvelles lois pour rendre l’utilisation et l’innovation de l’IA générative plus sûres, selon Kong.</p>'}, {'<h3>Les directives d’utilisation de l’IA sont importantes</h3>', ''}, {'', '<p>Les auteurs du rapport ont noté que les organisations et les individus adoptent l’IA à un rythme beaucoup plus rapide que lors des précédentes avancées technologiques, « ce qui signifie que les organisations doivent évoluer plus vite que jamais pour rester à la pointe de la vague d’innovation en matière d’IA ». Environ 80 % des développeurs interrogés ont déclaré que leurs organisations avaient mis en place des directives ou des restrictions en matière d’utilisation de l’IA, et environ 20 % n’en avaient aucune. Environ 2 % ont déclaré que leurs organisations interdisaient purement et simplement les outils d’IA générative.</p>'}, {'', '<p>Près de 60 % des personnes interrogées ont déclaré qu’elles parvenaient à contourner les restrictions pour accéder aux outils d’IA dont elles ont besoin. Cependant, la confidentialité, la sécurité et la gouvernance des données figurent également en tête de liste (près de 60 %) des défis rencontrés lors de l’intégration des services d’IA aux infrastructures de microservices existantes.</p>'}, {'', '<p>« Nous avons atteint le moment où l’adoption de l’IA est impérative. Cependant, le succès à long terme dépendra de la stratégie adoptée », a déclaré Marco Palladino, cofondateur et directeur technique de Kong. « Notre rapport souligne la nécessité d’une infrastructure sous-jacente aux applications d’IA qui assure une gouvernance solide tout en améliorant les performances de l’IA. »</p>'}, {'', ""<h3>Se familiariser avec l'IA générative</h3>""}, {'', '<p>Le rapport de l’entreprise fait écho aux conclusions d’autres fournisseurs en matière de développeurs et d’IA. Une enquête réalisée à la fin de l’année dernière par Kobiton a révélé que la plupart des développeurs et testeurs d’applications mobiles utilisent des outils d’IA générative pour diverses tâches et pour relever les défis des opérations de développement et d’assurance qualité. En outre, ils utilisent cette technologie lorsqu’ils disposent de ressources financières limitées ou qu’ils ne disposent pas de suffisamment de programmeurs qualifiés.</p>'}, {'', ""<p>En avril, Docker a déclaré dans un rapport que les ingénieurs logiciels s'appuient de plus en plus sur l'IA pour créer leurs applications, avec des cas d'utilisation allant du codage et de la documentation à la recherche, au dépannage et au débogage.</p>""}, {'', '<p>De même, l’enquête de Kong a révélé que les développeurs s’intéressent de plus en plus à l’IA générative, 60 % d’entre eux déclarant qu’ils étaient passionnés par le travail avec cette technologie émergente et 57 % affirmant qu’elle faciliterait leur travail.</p>'}, {'', '<p>D’autres s’inquiètent néanmoins de cette situation : 35 % des sondés estiment que l’adoption de l’IA progresse trop vite. Ils sont également inquiets de l’avenir : 18 % pensent que l’utilisation d’outils d’IA entraînera des licenciements dans leur entreprise. Près d’un sur dix déclare ne pas vouloir travailler avec l’IA, certains estimant que l’IA dévalorisera leur travail ou augmentera leur charge de travail.</p>'}, {'', '<p>« En passant de la perspective personnelle à l’opinion sur l’impact sur l’entreprise, la moitié des personnes interrogées estiment que l’IA améliorera la productivité et l’innovation », écrivent les auteurs. « 26 % d’entre elles estiment que l’IA créera de nouveaux postes ou de nouvelles opportunités dans leurs organisations. »</p>'}, {'', '<h3>Les API joueront un rôle central</h3>'}, {'', '<p>Selon Kong, les API vont devenir de plus en plus importantes dans le développement et l’utilisation de l’IA. L’entreprise a noté que les API en général sont largement utilisées, avec 85 % des entreprises du Fortune 100 les utilisant comme élément central de leurs opérations informatiques et près de la moitié des développeurs et des gestionnaires déclarant que leurs entreprises doivent être plus conscientes de la valeur commerciale que les API apportent.</p>'}, {'', '<p>Elles joueront également un rôle central dans l’IA, les auteurs du rapport écrivant que « les API permettent la communication entre les humains et les systèmes d’IA, ainsi qu’entre les systèmes d’IA et d’autres outils numériques. Les API servent de mains, d’yeux et d’oreilles à l’IA. Et à mesure que l’utilisation de l’IA augmente, le nombre d’API qui les rendent possibles augmentera également. »</p>'}, {'', '<p>Ils ont souligné une prévision de Gartner selon laquelle d’ici 2026, plus de 30 % de l’augmentation de la demande d’API proviendra de l’IA et des outils utilisant des modèles de langage volumineux (LLM). Au-delà de cela, Kong prédit que la valeur des API permettant l’IA augmentera de 170 % d’ici 2030.</p>'}, {'', '<p>Selon les recherches de Kong, l’impact économique des API aux États-Unis atteindra 3 400 milliards de dollars et l’impact mondial atteindra 17 300 milliards de dollars. En s’appuyant sur les données du Fonds monétaire international sur la croissance prévue du PIB mondial et l’économie numérique mondiale, l’impact économique des API devrait passer de 12,7 % du PIB mondial cette année à 14 % en 2030.</p>'}, {'', '<p>« Un simple calcul mathématique permet de comprendre pourquoi une stratégie API est si essentielle pour les entreprises d’aujourd’hui : dans peu de temps, les consommateurs de nos sites Web ou de nos produits numériques seront plus susceptibles d’être des IA que des humains », écrivent les auteurs. « Que nous utilisions l’IA, que nous la formions ou que nous laissions interagir l’IA avec les API pour effectuer des opérations, une API est impliquée. C’est pourquoi le contrôle de l’utilisation de l’IA au niveau de l’interface est un problème d’API. »</p>'}]"
Pipelines de télémétrie : le chaînon manquant essentiel dans la surveillance des applications modernes et la gestion des performances,"[{'', ""<p>En matière de données de télémétrie (à savoir les journaux, les mesures, les traces et autres informations que les ingénieurs utilisent pour surveiller les applications, gérer les performances et résoudre les pannes), plus il y a de données, mieux c'est. Mais il y a un inconvénient majeur : si vous ne parvenez pas à gérer efficacement les données de télémétrie, celles-ci peuvent rapidement créer plus de problèmes qu'elles n'en résolvent, ce qui peut entraîner des problèmes tels que des coûts de stockage plus élevés, des difficultés à trouver les bonnes informations lors de la réponse aux pannes et le risque d'exposer des informations sensibles à un accès non autorisé.</p>""}, {'', '<p>C’est pourquoi les pipelines de télémétrie sont devenus un élément essentiel des stratégies modernes d’observabilité des applications et de gestion des performances, ainsi qu’une ressource essentielle lors du déploiement d’outils tels que les plateformes de gestion des informations et des événements de sécurité (SIEM). Les pipelines de télémétrie permettent aux entreprises de collecter, de traiter, d’acheminer et de stocker les données de télémétrie de manière efficace et à grande échelle.</p>'}, {'', '<p>Voilà au moins un aperçu de haut niveau de ce que font les pipelines de télémétrie et de leur importance d’un point de vue technique et commercial. Pour une analyse plus approfondie, continuez à lire pendant que nous nous appuyons sur notre expérience collective en matière de conception, de mise en œuvre et de gestion de pipelines de télémétrie pour expliquer pourquoi ils sont si précieux et ce qu’il faut rechercher lors de la création d’un pipeline de télémétrie adapté aux besoins de votre organisation.</p>'}, {'<h3>Que sont les pipelines de télémétrie et pourquoi sont-ils importants\xa0?</h3>', ''}, {'', ""<p>Les pipelines de télémétrie sont un type de solution qui collecte, traite et achemine les données de télémétrie, c'est-à-dire les journaux, les métriques, les traces et tout autre type d'informations offrant une visibilité sur les performances des applications.</p>""}, {'', ""<p>Les données de télémétrie sont importantes depuis des décennies, étant donné le rôle central qu'elles jouent pour permettre aux organisations de surveiller les performances des applications, de détecter les problèmes et de les résoudre de manière à minimiser l'impact sur les utilisateurs. Mais jusqu'à récemment, peu d'organisations avaient mis en place des stratégies délibérées pour gérer ces données. Au lieu de cela, elles s'appuyaient sur des approches ad hoc pour collecter les données à partir des lieux d'origine et les transférer vers les outils sur lesquels elles s'appuyaient pour les analyser.</p>""}, {'', '<p>Cette approche fonctionnait bien dans la plupart des cas lorsque le volume de journaux, de mesures et de traces gérés par une entreprise était relativement faible. Mais dans le monde actuel des architectures logicielles distribuées, la quantité de données de télémétrie à gérer par une entreprise type a explosé. Au lieu de devoir collecter un seul ensemble de fichiers journaux et de mesures pour chaque application, comme cela aurait été le cas à l’ère des applications monolithiques, il est courant aujourd’hui qu’une seule application soit composée d’une douzaine ou plus de microservices, chacun générant ses propres journaux et mesures. Si l’on ajoute à cela le fait que les applications d’aujourd’hui s’exécutent souvent sur une infrastructure distribuée qui peut se composer de centaines ou de milliers de serveurs individuels, il est facile de comprendre pourquoi il y a tellement plus de données de télémétrie à gérer aujourd’hui.</p>'}, {'', '<p>Pour gérer efficacement ce volume, la plupart des organisations ont besoin de plus qu’une solution ad hoc pour collecter, traiter et acheminer les données. Elles ont besoin d’une solution sur mesure qui extrait systématiquement les données des différentes sources d’où elles proviennent, les normalise et les transforme si nécessaire et les livre aux endroits où elles sont analysées ou stockées. En d’autres termes, elles ont besoin d’un pipeline de télémétrie.</p>'}, {'', '<h3>Les avantages des pipelines de télémétrie</h3>'}, {'', ""<p>En apportant cohérence et ordre à la gestion des données de télémétrie, les pipelines de télémétrie produisent une série d'avantages commerciaux. Les plus importants sont les suivants\xa0:</p>""}, {'', ""<li>Réduction des coûts : les pipelines peuvent aider à réduire le coût de traitement et de stockage des données de télémétrie grâce à des fonctionnalités telles que la déduplication (qui supprime les données redondantes, réduisant ainsi les volumes et les coûts de stockage) et la minimisation des données (qui réduit la quantité de données ingérées dans les outils d'analyse, ce qui entraîne une baisse des coûts d'exploitation des outils dont le prix est basé sur le total des données ingérées).</li>""}, {'', '<li>Confidentialité et sécurité des données\xa0: les données de télémétrie peuvent contenir des informations sensibles, telles que des informations personnelles identifiables (PII) stockées dans des fichiers journaux. En fournissant des fonctionnalités telles que le chiffrement des données en mouvement, les pipelines de télémétrie contribuent à protéger les données sensibles et à respecter les obligations de conformité.</li>'}, {'', ""<li>Performances applicatives améliorées : plus vous pouvez déplacer rapidement et de manière fiable les données de télémétrie de leur lieu d'origine vers l'endroit où vous les analysez, mieux vous serez en mesure de détecter et de résoudre les problèmes de performances logicielles avant qu'ils n'entraînent une panne.</li>""}, {'', '<li>Contrôle et visibilité centralisés : un pipeline de télémétrie vous offre une vue consolidée de toutes vos données de télémétrie. Cela signifie que vous saurez toujours quelles sources de données sont disponibles, comment vous les utilisez et ce que vous pouvez faire pour rendre votre processus de télémétrie encore plus efficace.</li>'}, {'', ""<li>Flexibilité opérationnelle : une fois que vous avez créé un pipeline de télémétrie, vous pouvez facilement échanger des sources et des destinations de données en fonction des besoins. Cela signifie que vous pouvez connecter, déployer des applications ou des outils d'analyse à volonté, sans avoir à mettre en œuvre des processus de gestion de télémétrie personnalisés pour chacun.</li>""}, {'', '<li>Liberté de verrouillage : Dans la même optique, les pipelines de télémétrie permettent de garantir que les entreprises peuvent facilement migrer vers différents outils d’analyse ou de gestion des performances des applications sans être liées à la pile d’un fournisseur particulier en raison du défi que représente la mise à jour de processus complexes de gestion des données de télémétrie.</li>'}, {'', '<p>Pour offrir les avantages que nous venons d’évoquer, chaque solution de pipeline de télémétrie digne de ce nom doit fournir un ensemble de fonctionnalités de base, notamment\xa0:</p>'}, {'', ""<li>Collecte, c'est-à-dire la capacité d'extraire des données des différents endroits où elles proviennent.</li>""}, {'', ""<li>Traitement qui transforme les données de diverses manières afin qu'elles soient parfaitement adaptées à l'utilisation par des outils d'analyse ou de gestion des performances des applications.</li>""}, {'', ""<li>Le routage, ou la livraison des données traitées aux différents outils qu'une organisation utilise pour les analyser ou les interpréter. Le routage peut également fournir des données à des référentiels de stockage à long terme si l'organisation doit conserver les données.</li>""}, {'', ""<p>Il s'agit toutefois des fonctionnalités minimales que les pipelines de télémétrie doivent prendre en charge. Pour obtenir une efficacité et une flexibilité optimales du pipeline, les organisations doivent rechercher plusieurs fonctionnalités supplémentaires clés.</p>""}, {'', '<h3>Le projet OpenTelemetry</h3>'}, {'', '<p>OpenTelemetry (ou OTel en abrégé) est un cadre ouvert (régi par la CNCF) qui offre une approche standardisée pour la collecte, le traitement et la transmission de données de télémétrie.</p>'}, {'', ""<p>OTel est devenu une norme pratiquement universelle, ses outils étant téléchargés plus de 30 millions de fois par mois. Cela signifie que tant que votre pipeline prend en charge OTel, vous pourrez l'utiliser pour connecter presque n'importe quelle source de données à n'importe quel outil d'analyse ou de gestion de données.</p>""}, {'', '<h3>La valeur d’un cadre ouvert</h3>'}, {'', ""<p>La prise en charge d'OTel garantit qu'un pipeline de télémétrie fonctionnera avec n'importe quelle source de données ou outil compatible avec OTel. Cependant, pour maximiser la flexibilité de votre pipeline et minimiser le risque de dépendance vis-à-vis d'un fournisseur, vous pouvez aller plus loin dans l'ouverture en créant un pipeline qui inclut un minimum de composants propriétaires.</p>""}, {'', ""<p>Lorsque votre logiciel de pipeline est ouvert, c'est-à-dire qu'il est basé sur des composants transparents et standardisés, vous n'avez pas à vous soucier d'être enfermé dans votre logiciel de pipeline lui-même ou d'être redevable à un fournisseur particulier pour prendre en charge l'outil dont vous dépendez pour travailler avec les données de télémétrie.</p>""}, {'', '<h3>Traitement des aperçus</h3>'}, {'', ""<p>Les aperçus de traitement vous permettent de prédire comment les routines de traitement de données que vous avez configurées dans un pipeline modifieront vos données. Cela est important, car vous ne voulez pas découvrir après coup que vous avez traité des données d'une manière qui les a rendues inutilisables ou qui a introduit des erreurs ou des problèmes de formatage. Grâce aux aperçus, vous pouvez adopter une approche plus proactive de la gestion des données et anticiper les problèmes de données en temps réel.</p>""}, {'<h3>Résumés des données</h3>', ''}, {'', ""<p>En plus de visualiser les données au fur et à mesure qu'elles circulent dans les différents pipelines, la possibilité de résumer toutes les données de votre pipeline vous aide à suivre la quantité de données que vous traitez et ce que vous en faites. À leur tour, ces informations vous permettent d'identifier les tendances à long terme concernant vos données de télémétrie. Elles peuvent également vous aider à suivre les coûts de télémétrie et à trouver des opportunités de rationalisation des flux de données.</p>""}, {'', '<p>Une fois que vous avez décidé ce que vous souhaitez que votre pipeline de télémétrie fasse et que vous avez trouvé le logiciel qui le fait, vous devez implémenter le pipeline lui-même, ce qui peut être une tâche difficile, étant donné la complexité des pipelines modernes et des données de télémétrie.</p>'}, {'', '<p>Nous n’allons pas passer en revue ici toutes les étapes du processus de mise en œuvre, car les spécificités varient en fonction du logiciel de pipeline de télémétrie que vous utilisez. Nous aimerions toutefois mentionner les défis de mise en œuvre et d’exploitation que les équipes négligent parfois, tels que :</p>'}, {'', ""<li>Migration d'agent\xa0: vous disposez peut-être déjà d'agents de surveillance de logiciels qui collectent des données à partir d'applications ou de services. Plutôt que de reconfigurer ces agents, vous pourrez idéalement les migrer dans votre pipeline, ce qui vous fera gagner du temps et réduira les efforts nécessaires à la mise en œuvre d'un nouveau pipeline.</li>""}, {'', ""<li>Observabilité du pipeline\xa0: vous aurez besoin d'un moyen de surveiller et d'observer votre pipeline lui-même pour détecter d'éventuels problèmes de performances ou erreurs.</li>""}, {'', ""<li>Évolutivité du pipeline : il est fort probable que le volume de données de télémétrie auquel les entreprises doivent faire face ne fera qu'augmenter dans les années à venir. C'est pourquoi il est important de veiller à ce que votre pipeline puisse évoluer pour prendre en charge des volumes de données toujours plus importants, ainsi qu'une augmentation du nombre de sources de données et d'outils pris en charge.</li>""}, {'', '<h3>Conclusion : Transformer la gestion des performances des applications et le SIEM avec les pipelines de télémétrie</h3>'}, {'', '<p>En conclusion, pour de nombreuses entreprises, les approches traditionnelles de gestion des données de télémétrie ne suffisent plus. Elles sont trop lentes, coûteuses et difficiles à mettre en œuvre.</p>'}, {'', ""<p>La solution consiste à adopter une approche délibérée et cohérente des données de télémétrie, de leur traitement et de leur livraison en créant un pipeline pour garantir que chaque source de données atteigne sa destination prévue, prête à prendre en charge son cas d'utilisation prévu. En procédant ainsi, vous avez préparé votre organisation à un succès à long terme à l'ère des ensembles de données de télémétrie de plus en plus volumineux et complexes.</p>""}]"
Les équipes AppSec et DevOps confrontées à des problèmes de sécurité,"[{'', ""<p>Selon un rapport d'ESG, les équipes de sécurité des applications (AppSec) sont confrontées à une pression accrue à mesure que les organisations accélèrent leur adoption des pratiques DevSecOps.</p>""}, {'', '<p>La tendance vers l’intégration des équipes de développement, de sécurité et d’exploitation se développe rapidement, l’adoption de DevSecOps devant passer de 38 % aujourd’hui à 48 % au cours des deux prochaines années.</p>'}, {'', '<p>Cependant, ce rythme accéléré crée une pression considérable sur des équipes de sécurité déjà surchargées et sous-dotées en ressources.</p>'}, {'', '<p>Le rapport indique également que l’essor de l’IA générative (GenAI) ajoute une couche supplémentaire de complexité, l’intégration posant des défis en matière de sécurité.</p>'}, {'', ""<p>La majorité (97 %) des organisations interrogées ont déclaré qu'elles utilisaient ou prévoyaient de mettre en œuvre l'IA générative dans leurs processus de développement logiciel, mais les équipes de sécurité expriment de profondes inquiétudes quant à la protection de l'utilisation de l'IA.</p>""}, {'', '<h3>Manque de visibilité</h3>'}, {'', '<p>Un problème clé qui exacerbe ces défis est le manque de visibilité entre les équipes de sécurité et de développement.</p>'}, {'', ""<p>L'enquête a révélé que 42 % des personnes interrogées peuvent tester et corriger leur code de manière indépendante, sans faire appel aux équipes de sécurité, ce qui entraîne des failles de sécurité et souligne le besoin urgent d'une meilleure intégration.</p>""}, {'', ""<p>Melinda Marks, directrice de la pratique de la cybersécurité pour ESG, a encouragé les équipes AppSec à discuter avec les développeurs et les équipes DevOps pour en savoir plus sur leurs flux de travail, leurs processus, leur sensibilisation à la sécurité et les tests de sécurité qu'ils ont mis en place, et à s'aligner sur des objectifs pour sécuriser leurs applications.</p>""}, {'', ""<p>« Ces éléments devraient se chevaucher dans des domaines tels que la disponibilité des applications, la garantie du service client et la protection des données de l'entreprise et des clients », a-t-elle déclaré.</p>""}, {'', '<p>Deuxièmement, réfléchissez aux moyens d’intégrer les outils et processus de sécurité dans la façon dont les développeurs travaillent, notamment en définissant des politiques et en automatisant les tests dès le début des processus de développement.</p>'}, {'', ""<p>« Troisièmement, assurez-vous que la sécurité dispose du contrôle et de la visibilité nécessaires pour déployer des outils et des processus de sécurité qui soutiennent le développement afin qu'ils puissent gérer efficacement les risques et résoudre les problèmes de sécurité », a déclaré Marks.</p>""}, {'', ""<p>Karthik Swarnam, responsable de la sécurité et de la confiance chez ArmorCode, a déclaré que pour gérer efficacement le rythme et l'échelle croissants de DevSecOps, même avec des ressources limitées, les équipes AppSec devraient exploiter les capacités de l'IA pour des tests de sécurité améliorés.</p>""}, {'', '<p>« L’automatisation du pipeline DevSecOps est essentielle pour maintenir l’efficacité sans sacrifier la sécurité », a-t-il déclaré. « Les équipes doivent également privilégier l’utilisation d’outils qui offrent une visibilité sur les risques de sécurité et aident à hiérarchiser les efforts de remédiation. »</p>'}, {'', ""<p>Il a déclaré qu'il est essentiel que les équipes restent concentrées sur les tâches critiques, telles que l'identification des vulnérabilités nécessitant une attention immédiate, la détermination des déficiences spécifiques à corriger et l'identification des domaines dans lesquels les développeurs ont besoin d'une formation ciblée, évitant ainsi l'inefficacité de la formation sur tous les sujets possibles.</p>""}, {'', ""<p>Marks a déclaré que la sécurité veut aider à permettre l'utilisation de l'IA car elle contribuera à la productivité, mais elle doit s'assurer qu'elle est sécurisée car en cas d'incident, comme un partage inapproprié de données ou l'introduction de vulnérabilités par l'utilisation de l'IA, cela peut constituer un revers pour l'adoption de l'IA.</p>""}, {'', '<p>« Nous ne voulons pas non plus que les mesures de sécurité et les mesures informatiques bloquent l’utilisation de l’IA, car cela entrave la capacité des entreprises à utiliser ses avantages », a-t-elle déclaré.</p>'}, {'', '<p>Swarnam a déclaré que pour combler efficacement les failles de sécurité, les entreprises doivent améliorer la visibilité et la communication entre les équipes de sécurité et de développement.</p>'}, {'', '<p>« Une stratégie clé consiste à intégrer ces équipes dans un programme complet de gestion des vulnérabilités », a-t-il déclaré.</p>'}, {'', ""<p>La fourniture de tableaux de bord au niveau de l'entreprise et de la direction, ainsi que de rapports de visibilité qui mesurent l'état de la sécurité, permet une priorisation claire des efforts de correction.</p>""}, {'', '<p>Ces rapports doivent mettre en évidence « ce qui compte » le plus, garantissant que les failles de sécurité critiques sont rapidement traitées.</p>'}, {'', ""<p>« L'intégration de ces outils dans les systèmes de gestion des flux de travail, associée à des capacités de priorisation qui mettent l'accent sur l'efficacité, garantit que les équipes de sécurité et de développement sont alignées dans leurs efforts pour combler les lacunes et renforcer la posture de sécurité globale de l'organisation », a déclaré Swarnam.</p>""}]"
Backslash Security ajoute des outils de simulation et d'IA générative à sa plateforme DevSecOps,"[{'', ""<p>Backslash Security a ajouté aujourd'hui la possibilité de simuler des mises à niveau vers une version supérieure d'une application à sa plate-forme de sécurité des applications pour analyser le code et créer des nomenclatures de logiciels (SBOM).</p>""}, {'', ""<p>De plus, l'entreprise utilise désormais également de grands modèles de langage (LLM) pour fournir aux équipes DevSecOps des conseils pour résoudre les problèmes sans jamais exposer de code au LLM.</p>""}, {'', ""<p>Amit Bismut, responsable produit chez Backslash Security, a déclaré que l'un des défis auxquels les équipes DevSecOps sont régulièrement confrontées est qu'une mise à jour d'un package ou d'un module tiers inclus dans une application peut introduire des vulnérabilités supplémentaires. La fonctionnalité Fix Simulation ajoutée à la plateforme SaaS (Software-as-a-Service) Backslash permet aux équipes DevSecOps de vérifier l'impact de ce logiciel sur leurs applications sans avoir à le déployer au préalable dans leurs environnements informatiques, a déclaré Bismut.</p>""}, {'', '<p>C’est essentiel car les équipes DevSecOps débattent souvent de la mesure dans laquelle une mise à jour mineure ou majeure d’un progiciel ou d’un composant pourrait être nécessaire pour sécuriser définitivement une application, a noté Bismut.</p>'}, {'', ""<p>Parallèlement, les équipes DevSecOps peuvent tirer parti de la plateforme BackSlash pour exposer les métadonnées d'un environnement d'application collectées par Backslash à un LLM, qui peut ensuite générer des recommandations de correction du chemin d'attaque pour corriger les vulnérabilités. Cette approche élimine la nécessité d'exposer le code créé par un développeur d'application directement à un LLM, a déclaré Bismut.</p>""}, {'', '<h3>Gestion de la posture de sécurité des applications</h3>'}, {'', ""<p>La plateforme de gestion de la posture de sécurité des applications (ASPM) Backslash identifie et hiérarchise les vulnérabilités en fonction de leur facilité d'exploitation et d'accès. Cette approche permet de modéliser visuellement les menaces dans le contexte de l'architecture réelle de l'application, jusqu'aux lignes de code spécifiques et au développeur qui les a écrites.</p>""}, {'', ""<p>Les fonctionnalités incluent une plate-forme d'échange d'exploitabilité de vulnérabilité (VEX) intégrée à des outils d'analyse de la composition logicielle (SCA), de tests de sécurité des applications statiques (SAST), de détection de secrets et de création de SBOM.</p>""}, {''}, {'', '<p>La rationalisation des flux de travail DevSecOps est devenue cruciale à une époque où, grâce à l’essor de l’intelligence artificielle (IA), la quantité de code écrit et déployé est sur le point d’augmenter de manière exponentielle. Malheureusement, les développeurs ne consacrent encore qu’environ 10 % de leur temps à la correction des vulnérabilités. Les organisations doivent s’assurer que le temps est consacré à la résolution des problèmes qui pourraient avoir le plus d’impact sur l’entreprise. Sinon, un certain niveau de fatigue s’installe, ce qui conduit les développeurs à répéter sans cesse les mêmes erreurs de cybersécurité. La plupart des vulnérabilités découvertes dans les applications aujourd’hui sont les mêmes problèmes qui ont tourmenté les équipes d’ingénierie logicielle au cours de la dernière décennie.</p>'}, {'', '<p>Il existe bien sûr de nombreuses options DevSecOps, allant de la fourniture aux développeurs d’outils supplémentaires qui détectent les vulnérabilités et écrivent le code, à l’ajout de portes supplémentaires aux flux de travail DevOps existants pour garantir que les vulnérabilités ne sont pas intégrées à une version logicielle. Quelle que soit l’approche adoptée, il est clair que les cybercriminels deviennent de plus en plus habiles à exploiter les vulnérabilités logicielles connues. Ainsi, ce n’est qu’une question de temps avant que la prochaine fois qu’une de ces vulnérabilités sera exploitée, on pourra remonter jusqu’au développeur qui l’a créée.</p>'}]"
Secure Code Warrior dévoile un agent pour gérer les autorisations de validation,"[{'', ""<p>Secure Code Warrior (SCW) a ajouté aujourd'hui un agent à son portefeuille de sécurité des applications qui évalue la compétence en matière de sécurité des développeurs lorsqu'ils valident du code dans un référentiel.</p>""}, {'', ""<p>L'agent de confiance SCW est basé sur une capacité à évaluer l'expertise en sécurité des développeurs que SCW a déployée plus tôt cette année.</p>""}, {'', ""<p>L'agent de confiance SCW peut être déployé sur n'importe quel référentiel de code basé sur Git, notamment Github, Gitlab et Atlassian Bitbucket, afin de garantir que seuls les développeurs ayant atteint une note de sécurité spécifique sont autorisés à effectuer un commit. Ces notes sont basées sur plus de 20 millions de points de données collectés auprès de 250\xa0000 développeurs du monde entier.</p>""}, {'', '<p>En fonction de la criticité du projet, les équipes DevSecOps peuvent personnaliser la configuration des politiques en fonction du niveau de risque que représente chaque projet d’application.</p>'}, {'', ""<p>Matias Madou, directeur technique de SCW, a déclaré que l'objectif global est non seulement de rendre les bases de code plus sûres, mais également de permettre aux organisations d'identifier plus facilement les développeurs d'applications qui ont besoin d'une formation supplémentaire en matière de sécurité.</p>""}, {'', '<p>De plus, restreindre les membres d’une équipe de développement d’applications qui sont réellement autorisés à effectuer une validation permet également de rationaliser tout audit qui pourrait être effectué à une date ultérieure.</p>'}, {'', '<p>Pour garantir la sécurité des chaînes d’approvisionnement de logiciels, il faut non seulement fournir aux développeurs les meilleurs outils possibles, mais aussi verrouiller les processus. Dans la course à la création d’applications plus rapides, de nombreuses entreprises ont négligé la nécessité de s’assurer que les commits dans les référentiels sont d’abord examinés par des développeurs expérimentés. Cela n’empêchera peut-être pas toutes les erreurs commises d’être enregistrées dans un référentiel, mais cela devrait au moins en réduire la fréquence.</p>'}, {'', '<p>L’un des problèmes les plus exaspérants rencontrés par les équipes DevSecOps est sans doute le fait que les développeurs d’applications continuent de commettre la même erreur de sécurité de base plusieurs fois. Les vulnérabilités les plus courantes trouvées dans le code sont à peu près les mêmes depuis une décennie, a noté Madou.</p>'}, {'', '<p>On ne sait pas encore dans quelle mesure les entreprises seront obligées de déployer des applications moins vulnérables. L’Union européenne a adopté une loi sur la cyber-résilience qui oblige les entreprises à créer des logiciels plus sécurisés. Cependant, une récente décision de la Cour suprême des États-Unis dans l’affaire Chevron a limité l’autorité des agences fédérales et étendu les statuts en vertu desquels elles ont été créées pour appliquer de nouvelles réglementations qui n’ont pas été spécifiquement approuvées par le Congrès américain. Il est donc de moins en moins probable que les exigences de conformité relatives à la sécurité des logiciels soient appliquées dans un avenir proche.</p>'}, {'', '<p>Toutefois, les entreprises sont susceptibles d’être tenues responsables devant les tribunaux pour avoir déployé des logiciels présentant des vulnérabilités connues. La seule chose qui devra être déterminée dans chaque cas est le degré auquel le déploiement de logiciels présentant des vulnérabilités connues est considéré comme imprudent par le tribunal. Plus un acte est imprudent, plus les sanctions généralement appliquées sont élevées.</p>'}, {'', '<p>Par conséquent, quelles que soient les réglementations de conformité, les organisations de toutes tailles ont désormais un intérêt plus grand à s’assurer qu’un maximum de vulnérabilités connues ne se retrouvent pas dans les environnements de production.</p>'}]"
Solutions basées sur l'IA pour les pipelines Azure DevOps,"[{'', '<p>Le paysage technologique est de plus en plus concurrentiel et, pour que les entreprises prospèrent, le dernier recours consiste à exploiter les technologies modernes. L’intelligence artificielle (IA) n’est plus une chose du passé : elle est là et change rapidement le paysage du développement logiciel. Vous serez gagnant si vous trouvez cet article en recherchant l’intégration de l’IA dans vos pipelines Azure DevOps.</p>'}, {'', '<p>Ce guide complet révélera comment l’IA peut augmenter le processus de livraison de logiciels de haute qualité plus rapidement et plus efficacement.</p>'}, {'', '<h3>Comprendre les pipelines Azure DevOps</h3>'}, {'', '<p>Définition</p>'}, {'', ""<p>Les pipelines Azure DevOps sont la pierre angulaire du développement logiciel moderne. Ils servent de workflows automatisés qui coordonnent l'ensemble du processus de livraison de logiciels. Considérez-les comme des chaînes de montage numériques, où votre code est compilé, testé et empaqueté dans des artefacts déployables. Ces pipelines rationalisent et normalisent les étapes impliquées dans la création, le test et le déploiement d'applications, garantissant ainsi la cohérence et la fiabilité dans différents environnements.</p>""}, {'', '<p>Composants clés</p>'}, {'', '<li>Build\xa0: la première étape consiste à créer un code source, à minimiser les dépendances et enfin à le créer. Cette étape consiste à compiler votre code source, à résoudre les dépendances et à former des packages réalisables. Les pipelines Azure DevOps offrent diverses tâches de build pour différents langages et frameworks de programmation.</li>'}, {'', ""<li>Test : les tests sont essentiels pour une automatisation efficace des fonctionnalités et des performances des applications. Les pipelines Azure DevOps ont une capacité unique à s'intégrer à plusieurs frameworks de test. Cela permet d'intégrer différents types de tests, notamment des tests unitaires, d'intégration et d'interface utilisateur, dans le cadre de votre pipeline global.</li>""}, {'', '<li>Déploiement\xa0: une fois votre application créée et testée avec succès, l’étape de déploiement prend le relais. Les pipelines Azure DevOps vous permettent de déployer vos artefacts sur différentes cibles, telles que des machines virtuelles, des clusters Kubernetes ou des services cloud.</li>'}, {'', '<h3>Les défis de l’optimisation traditionnelle des pipelines</h3>'}, {'', ""<p>Bien que les pipelines Azure DevOps offrent un cadre robuste pour l'automatisation, leur optimisation traditionnelle peut s'avérer difficile. Les problèmes courants incluent\xa0:</p>""}, {'', '<li>Configuration manuelle : la configuration et la maintenance de pipelines complexes nécessitent souvent une configuration manuelle approfondie, qui peut prendre du temps et être sujette aux erreurs.</li>'}, {'', ""<li>Boucles de rétroaction longues\xa0: l'identification et la résolution des problèmes dans un pipeline peuvent prendre du temps, en particulier lorsque l'on s'appuie sur des tests et un déploiement manuels. Cela peut entraîner des retards dans les versions et des équipes frustrées.</li>""}, {'', ""<li>Visibilité limitée : obtenir des informations sur les performances du pipeline, les goulots d'étranglement et les défaillances potentielles peut être difficile sans outils de surveillance et d'analyse avancés.</li>""}, {'', ""<li>Défis d'évolutivité : à mesure que les projets gagnent en complexité et en ampleur, la gestion et l'optimisation des pipelines deviennent de plus en plus difficiles, nécessitant des ressources et une expertise supplémentaires.</li>""}, {'', '<p>Notre approche de l’optimisation des pipelines a complètement changé avec l’arrivée de l’IA. En automatisant les tâches répétitives, en analysant de vastes quantités de données et en faisant des prévisions intelligentes, l’IA peut permettre aux équipes de surmonter ces défis et d’atteindre de nouveaux niveaux d’efficacité, de qualité et d’agilité dans leurs processus de livraison de logiciels.</p>'}, {'', ""<h3>Optimiser les pipelines Azure DevOps avec l'IA\xa0: un guide étape par étape</h3>""}, {'', '<p>En suivant les étapes décrites ci-dessous, vous pouvez exploiter la puissance de l’IA pour optimiser les pipelines Azure DevOps. Explorons cela en détail.</p>'}, {'', ""<p>Étape 1 : Surveillance et analyse basées sur l'IA</p>""}, {'', '<p>Visibilité en temps réel : une évaluation appropriée de l’état de santé, des performances et des indicateurs clés de votre pipeline est essentielle pour un flux de travail fluide. Les outils de surveillance basés sur l’IA sont d’une grande aide à cet égard. Vous pouvez mettre en œuvre des solutions telles qu’Azure Application Insights ou des plateformes similaires pour obtenir une visibilité complète et ciblée sur le fonctionnement du pipeline.</p>'}, {'', ""<p>Détection d'anomalies\xa0: les algorithmes d'apprentissage automatique (ML) sont un excellent moyen de détecter les comportements inhabituels dans votre pipeline. Les exemples incluent les contraintes de ressources et les tests en échec. Le principal avantage de cette surveillance assistée par l'IA est l'intervention précoce pour éliminer le problème.</p>""}, {'', ""<p>Analyse des causes profondes : une fois que les moniteurs d'IA ont détecté l'anomalie, ils agissent rapidement pour identifier la cause profonde car ils peuvent facilement inspecter des données et des journaux volumineux. Cela permet également de réduire le temps de réponse et de lancer des processus de dépannage.</p>""}, {'', '<p>Étape 2 : Tests intelligents et automatisation</p>'}, {'', ""<p>Génération de cas de test pilotés par l'IA : avec des outils basés sur l'IA, des cas de test peuvent être générés après toute modification de code ou cas d'utilisation. Cela présente deux avantages majeurs : cela réduit l'effort requis pour tester en minimisant le travail manuel et en permettant une analyse complète des tests.</p>""}, {'', '<p>Exécution de tests automatisée : les cadres de test pilotés par l’IA peuvent effectuer des tests sur plusieurs cas de test et configurations. Ces solutions de tests automatisés augmentent la vitesse et l’efficacité du flux de travail et réduisent les erreurs humaines.</p>'}, {'', ""<p>Analyse intelligente des résultats des tests : après les tests, les algorithmes d'IA peuvent également analyser les résultats des tests et fournir des informations utiles telles que des comportements anormaux et des modèles similaires. L'un des avantages de cette fonctionnalité est la suite de tests améliorée et les déploiements soutenus en toute confiance.</p>""}, {'', '<p>Étape 3\xa0: Déploiement et livraison continus</p>'}, {'', ""<p>Orchestration des versions pilotée par l'IA : exploitez l'IA pour automatiser le processus d'orchestration des versions, y compris les approbations de déploiement, les contrôles d'environnement et les procédures de restauration. Cela simplifie le processus de déploiement, minimise les interventions manuelles et réduit le risque d'erreurs.</p>""}, {'', ""<p>Déploiements Canary\xa0: implémentez des déploiements Canary basés sur l'IA, où de nouvelles fonctionnalités ou modifications de code sont progressivement déployées auprès d'un sous-ensemble d'utilisateurs. L'IA peut analyser l'impact de ces modifications sur les performances, l'expérience utilisateur et d'autres indicateurs clés, ce qui vous permet de prendre des décisions basées sur les données concernant un déploiement plus large.</p>""}, {'', ""<p>Restaurations automatiques : en cas d'échec après le déploiement, l'IA peut déclencher des restaurations automatiques vers une version stable antérieure de votre application. Une récupération rapide et un impact minimal sur les utilisateurs sont les avantages de cette capacité.</p>""}, {'', '<p>Étape 4 : Boucles de rétroaction et amélioration continue</p>'}, {'', ""<p>Surveillance et analyse continues : les informations basées sur l'IA peuvent aider à identifier les domaines à améliorer et à maintenir une boucle de rétroaction constante en surveillant et en analysant en permanence les données de votre pipeline, par exemple en optimisant les temps de création, en affinant les stratégies de test ou en peaufinant les processus de déploiement.</p>""}, {'', '<p>Apprentissage adaptatif : implémentez des algorithmes d’IA capables d’apprendre des exécutions de pipeline passées et d’ajuster automatiquement les configurations ou les paramètres pour améliorer les performances et la fiabilité au fil du temps.</p>'}, {'', '<p>Partage des connaissances : créer un environnement de coordination et de partage d’informations entre les équipes de développement et d’exploitation est essentiel au bon fonctionnement d’un projet. Utilisez des outils basés sur l’IA pour faciliter la communication, documenter les meilleures pratiques et créer une culture d’apprentissage et d’amélioration continue.</p>'}, {'', '<h3>Outils et technologies pour Azure DevOps</h3>'}, {'', ""<ol>Azure Machine Learning : un outil puissant et efficace qui permet de créer, de former et de déployer des frameworks ML facilement intégrés à vos pipelines Azure DevOps. De plus, il est basé sur le cloud, ce qui augmente ses fonctionnalités sur diverses plateformes et réseaux. Vous pouvez l'exploiter pour la détection d'anomalies, l'analyse prédictive, les tests intelligents et d'autres optimisations pilotées par l'IA. Azure DevOps Extensions : un vaste marché d'extensions offre des fonctionnalités basées sur l'IA pour divers aspects de votre pipeline. Ces extensions peuvent aider à l'analyse de code, à l'automatisation des tests, à la gestion des versions et plus encore, facilitant ainsi l'intégration de l'IA dans vos flux de travail existants. Opsera : en exploitant l'IA pour automatiser et optimiser la distribution de logiciels, Opsera donne une longueur d'avance à DevOps. Il fournit des fonctionnalités telles que l'optimisation intelligente des pipelines, l'analyse prédictive et les tests automatisés, permettant aux équipes de rationaliser les flux de travail et d'obtenir des versions plus rapides. Harness : cette plateforme de livraison continue intègre l'IA pour automatiser la vérification du déploiement, optimiser les coûts du cloud et garantir la conformité. Il utilise le ML pour analyser les modèles de déploiement et prédire les problèmes potentiels, permettant aux équipes de livrer des logiciels en toute confiance. GitHub Copilot : cet outil de saisie semi-automatique de code basé sur l'IA peut améliorer considérablement la qualité du code et la productivité des développeurs. Cependant, il est important de savoir que GitHub Copilot n'est actuellement pas intégré à Azure DevOps. Il utilise le ML pour suggérer des extraits de code, des lignes complètes et même générer des fonctions entières, ce qui permet aux développeurs de gagner un temps précieux et de réduire les erreurs.</ol>""}, {'', ""<li>Azure Machine Learning : un outil puissant et efficace qui permet de créer, de former et de déployer des frameworks ML facilement intégrés à vos pipelines Azure DevOps. De plus, il est basé sur le cloud, ce qui augmente ses fonctionnalités sur différentes plateformes et réseaux. Vous pouvez l'exploiter pour la détection d'anomalies, l'analyse prédictive, les tests intelligents et d'autres optimisations pilotées par l'IA.</li>""}, {'', ""<li>Extensions Azure DevOps\xa0: un vaste marché d'extensions offre des fonctionnalités optimisées par l'IA pour divers aspects de votre pipeline. Ces extensions peuvent vous aider dans l'analyse de code, l'automatisation des tests, la gestion des versions et bien plus encore, facilitant ainsi l'intégration de l'IA dans vos flux de travail existants.</li>""}, {'', ""<li>Opsera : En exploitant l'IA pour automatiser et optimiser la distribution de logiciels, Opsera donne une longueur d'avance à DevOps. Il fournit des fonctionnalités telles que l'optimisation intelligente du pipeline, l'analyse prédictive et les tests automatisés, permettant aux équipes de rationaliser les flux de travail et d'obtenir des versions plus rapides.</li>""}, {'', ""<li>Harness : cette plateforme de livraison continue intègre l'IA pour automatiser la vérification du déploiement, optimiser les coûts du cloud et garantir la conformité. Elle utilise le ML pour analyser les modèles de déploiement et prédire les problèmes potentiels, permettant ainsi aux équipes de livrer des logiciels en toute confiance.</li>""}, {'', ""<li>GitHub Copilot : cet outil de saisie semi-automatique de code basé sur l'IA peut améliorer considérablement la qualité du code et la productivité des développeurs. Cependant, il est important de savoir que GitHub Copilot n'est actuellement pas intégré à Azure DevOps. Il utilise le ML pour suggérer des extraits de code, des lignes complètes et même générer des fonctions entières, ce qui permet aux développeurs de gagner un temps précieux et de réduire les erreurs.</li>""}, {'', '<h3>Études de cas</h3>'}, {'', '<p>Microsoft : en tant que créateur d’Azure DevOps, Microsoft est à l’avant-garde de l’intégration de l’IA dans ses pipelines. L’entreprise utilise Azure Machine Learning pour analyser de vastes quantités de données de télémétrie issues de ses processus de développement et de déploiement, ce qui lui permet de signaler les obstacles, d’estimer les échecs et de répartir les ressources. Cela a considérablement amélioré les fréquences de déploiement et la qualité globale des logiciels.</p>'}, {'', '<p>Adobe : Adobe a adopté l’IA pour améliorer ses pipelines DevOps pour les applications cloud créatives. L’entreprise utilise des modèles ML pour analyser les modifications de code et prédire les impacts potentiels sur les performances et la stabilité. Cela lui permet de résoudre les problèmes de manière proactive avant qu’ils n’affectent les utilisateurs, ce qui se traduit par une expérience plus fluide et plus fiable pour des millions de professionnels de la création dans le monde entier.</p>'}, {'', '<h3>Conclusion</h3>'}, {'', '<p>En conclusion, l’intégration de l’IA dans les pipelines Azure DevOps transforme le processus manuel de développement et de livraison de logiciels. En adoptant l’IA, les organisations peuvent atteindre des niveaux d’efficacité, de qualité et d’agilité sans précédent, ce qui leur permet d’innover et d’obtenir un avantage concurrentiel dans le monde en constante évolution du développement de logiciels.</p>'}]"
4 raisons pour lesquelles les leaders technologiques devraient donner la priorité à la phase de test et de simulation pour un meilleur développement,"[{'', ""<p>L'enquête Stack Overflow Developer Survey 2023 révèle que 60 % des développeurs utilisent des tests automatisés. Mais la vérité est que les tests automatisés et les simulations doivent être le domaine le plus prioritaire de votre SDLC pour éliminer les frictions pour votre équipe de développeurs.</p>""}, {'', '<p>Il ne s’agit pas seulement de disposer des outils nécessaires, mais de cultiver une culture dans laquelle les tests sont considérés comme une partie intégrante du processus de développement, et non comme une considération secondaire. Ce changement de mentalité peut considérablement améliorer la qualité des logiciels, réduire les coûts et améliorer les performances globales de l’entreprise. Voici les quatre raisons pour lesquelles les tests devraient être le domaine le plus prioritaire de votre cycle de développement logiciel.</p>'}, {'', '<h3>1. Les tests garantissent un logiciel de haute qualité</h3>'}, {'', ""<p>En tant qu'ancien développeur devenu PDG, je suis convaincu que la pierre angulaire d'une expérience utilisateur exceptionnelle est de garantir la qualité de nos logiciels. Notre priorité absolue est, et doit toujours être, d'améliorer la satisfaction et la confiance de nos utilisateurs finaux, ce qui signifie que la mise en œuvre d'un cadre de test robuste est cruciale dans notre quête de qualité.</p>""}, {'', ""<p>Ce cadre constitue votre première ligne de défense. Il vous aide à identifier et à corriger les bugs dès le début du processus de développement, minimisant ainsi le risque de problèmes majeurs lors de la mise en service du logiciel. Il s'agit de garantir que vos produits fonctionnent parfaitement et répondent aux attentes des utilisateurs finaux, ce qui est essentiel pour maintenir la fiabilité et augmenter la satisfaction des clients.</p>""}, {'', ""<p>Il existe également un effet de feed-forward, qui consiste à maintenir des normes élevées de qualité du code et de fonctionnalité. Avec les tests intégrés, le code est toujours au plus haut niveau requis, tout code inférieur échoue aux tests et est rejeté. Ainsi, votre base de code devient, par définition, de haut niveau, et tout nouveau développeur de votre équipe peut voir la norme et ce qui est nécessaire pour l'atteindre.</p>""}, {'', '<p>En mettant l’accent sur l’importance de tester, de simuler et de résoudre les problèmes avant qu’ils n’affectent vos utilisateurs, vous protégez votre réputation et améliorez la fidélisation de vos clients. Cette approche proactive réduit considérablement les coûts qui seraient autrement engagés pour résoudre les problèmes après la sortie du produit.</p>'}, {'', '<p>En résumé, en tant que leaders technologiques, notre engagement envers des tests rigoureux est non négociable. Il ne s’agit pas seulement de trouver des bugs ; il s’agit d’instaurer la confiance, de garantir la fiabilité et de favoriser une culture d’excellence qui imprègne tous les niveaux de notre organisation.</p>'}, {'', '<h3>2. Tests automatisés -> Des développeurs plus heureux</h3>'}, {'', '<p>Les tests dans le développement de logiciels peuvent parfois être considérés comme une tâche ardue par les développeurs, principalement parce que les méthodes de test traditionnelles nécessitent une intervention manuelle importante. Les développeurs se retrouvent souvent à écrire ou à mettre à jour de nombreux tests, même pour des modifications mineures du code, et la responsabilité de maintenir ces tests peut être écrasante et générer des frictions.</p>'}, {'', ""<p>Cependant, lorsque les tests sont correctement abordés avec l'automatisation, ils se transforment en un outil puissant qui améliore la productivité des développeurs et crée un environnement de codage sans friction. En donnant la priorité aux tests automatisés, les développeurs ne redoutent plus le processus mais l'adoptent. Cela vous permet d'éliminer les obstacles et de réduire les aspects fastidieux du processus de développement, permettant aux développeurs de se concentrer sur ce qu'ils font le mieux : créer et améliorer des logiciels.</p>""}, {'', '<p>Pour commencer, les tests automatisés inspirent confiance aux développeurs quant aux changements qu’ils mettent en œuvre. La peur d’introduire des bugs dans la production peut amener les développeurs à passer trop de temps à vérifier manuellement leur travail. Les tests automatisés agissent comme un filet de sécurité fiable, offrant aux développeurs la liberté de refactoriser et d’innover sans craindre de perturber les fonctionnalités existantes. Cette confiance permet d’adopter des approches plus créatives et plus audacieuses en matière de développement de produits.</p>'}, {'', ""<p>De plus, l'automatisation des tests améliore considérablement l'efficacité du débogage. Lorsque des problèmes surviennent, les frameworks de tests automatisés permettent d'identifier le problème rapidement et avec précision. Ce processus simplifié réduit non seulement le temps consacré au débogage, mais minimise également les temps d'arrêt, permettant ainsi aux développeurs de se concentrer davantage sur l'amélioration du produit plutôt que sur sa réparation.</p>""}, {'', '<h3>3. De meilleurs tests soutiennent davantage les pratiques Agile et DevOps</h3>'}, {'', ""<p>Le feedback continu est l'élément vital du développement agile. Les tests et simulations automatisés s'intègrent aux pipelines CI/CD pour un feedback rapide pour l'équipe. Cela signifie que les problèmes sont détectés tôt, souvent quelques minutes après la validation du code.</p>""}, {'', '<p>Il ne s’agit pas seulement de trouver des bugs, mais de créer un environnement de développement où la qualité est constamment surveillée et améliorée. Ce développement itératif devient possible grâce à des pratiques de test robustes. Sans tests complets, chaque itération risque d’introduire de nouveaux bugs ou des régressions. Grâce à cela, les équipes peuvent déployer des mises à jour en toute confiance, sachant que les fonctionnalités de base resteront intactes.</p>'}, {'', '<p>Ces pratiques favorisent une culture DevOps dans laquelle le développement et les opérations travaillent main dans la main et où ces départements traditionnellement cloisonnés s’alignent sur ce qui constitue un « logiciel fonctionnel ». En donnant la priorité aux tests et aux simulations automatisés dans vos pratiques agiles et DevOps, vous créez un écosystème dans lequel la qualité est construite dès le départ, les développeurs sont habilités à innover rapidement et votre organisation peut s’adapter rapidement aux besoins changeants de l’entreprise.</p>'}, {'', '<h3>4. Les tests améliorent la collaboration et le partage des connaissances</h3>'}, {'', '<p>Cela peut paraître étrange. Comment l’ajout de tests peut-il aider les développeurs à collaborer et à partager leurs connaissances\xa0?</p>'}, {'', ""<p>C'est facile. Les tests sont de la documentation. Ils constituent le meilleur type de documentation car ils représentent le résultat attendu de toute fonction ou composant. Ils constituent une documentation en direct du comportement du code, aidant ainsi les nouveaux membres de l'équipe à comprendre le système. Les tests fournissent une documentation claire sur la manière dont les différentes parties du système doivent fonctionner, servant de point de référence fiable pour les développeurs.</p>""}, {'', '<p>Cette compréhension commune favorisée par des tests complets peut réduire considérablement les malentendus et les conflits au sein de l’équipe, ce qui conduit à une collaboration plus fluide. Lorsque les développeurs écrivent des tests, ils découvrent également des cas limites ou des problèmes potentiels qui n’auraient peut-être pas été apparents autrement, améliorant ainsi encore les connaissances collectives de l’équipe sur les subtilités du système.</p>'}, {'', ""<p>Les tests contribuent également à créer un environnement de développement cohérent au sein de votre organisation technique. Les simulations créent un environnement prévisible pour le développement et les tests, améliorant ainsi la collaboration en équipe. Cette cohérence permet aux développeurs de travailler en toute confiance sur différentes parties du système, sachant que leurs modifications ne risquent pas de perturber de manière inattendue d'autres composants.</p>""}, {'', ""<p>Lorsque de nouveaux membres de l'équipe rejoignent ou que les développeurs changent de projet, des tests et des simulations complets réduisent considérablement la courbe d'apprentissage, leur permettant de devenir plus productifs et de contribuer efficacement à la base de code.</p>""}, {'', '<h3>Conclusion</h3>'}, {'', '<p>Si je dois vous laisser une chose, c’est qu’il est important de noter que les tests sont nécessaires pour toute architecture logicielle moderne. Les microservices et les applications cloud natives sont si complexes aujourd’hui que les tests sont nécessaires pour garantir que chaque service fonctionne et interagit comme prévu.</p>'}, {'', ""<p>Il est essentiel de donner la priorité aux tests et aux simulations tout au long du cycle de développement (et de les déplacer vers la gauche) : cela transcende les bonnes pratiques pour devenir un impératif concurrentiel. Les leaders technologiques qui prônent une culture de test rigoureuse permettent à leurs organisations de produire des produits de qualité supérieure, de s'adapter rapidement aux changements du marché et de développer des systèmes durables et évolutifs.</p>""}, {'', '<p>La véritable question n’est plus de savoir si vous pouvez vous permettre de donner la priorité aux tests, mais plutôt de savoir si vous pouvez vous permettre de les négliger.</p>'}]"
Surveillance en temps réel des API tierces : avantages et mise en œuvre,"[{'', ""<p>Les entreprises s'appuient largement sur des API tierces pour étendre leurs fonctionnalités, améliorer l'expérience utilisateur et rationaliser leurs opérations. Cependant, cette dépendance introduit un besoin critique de surveillance en temps réel de ces API pour garantir une intégration transparente, des performances élevées et une prestation de services ininterrompue. Cet article explore les avantages de la surveillance des API en temps réel pour les intégrations tierces et fournit un guide pour sa mise en œuvre efficace.</p>""}, {'', '<h3>Avantages de la surveillance en temps réel</h3>'}, {'', '<p>La surveillance en temps réel des API tierces offre de nombreux avantages, améliorant considérablement l’efficacité, la fiabilité et la sécurité globales de vos applications. Voici un aperçu détaillé des principaux avantages :</p>'}, {'', '<p>Détection et réponse immédiates aux problèmes</p>'}, {'', ""<p>La surveillance en temps réel vous permet de détecter les problèmes dès qu'ils surviennent plutôt qu'après qu'ils aient déjà eu un impact sur vos utilisateurs. Cette fonctionnalité offre plusieurs avantages\xa0:</p>""}, {'', '<li>Identification rapide : la détection immédiate des anomalies (telles que des temps de réponse lents, des taux d’erreur accrus ou des temps d’arrêt complets de l’API) permet un dépannage rapide.</li>'}, {'', ""<li>Réponse rapide : grâce aux alertes en temps réel, votre équipe peut réagir rapidement aux problèmes, réduisant ainsi le délai moyen de résolution (MTTR) et minimisant l'impact sur l'utilisateur.</li>""}, {'', ""<li>Satisfaction de l'utilisateur : la résolution rapide des problèmes garantit une expérience utilisateur cohérente, préservant ainsi la confiance et la satisfaction des clients.</li>""}, {'', '<p>Performances et fiabilité améliorées</p>'}, {'', '<p>La surveillance des API en temps réel permet de maintenir et d’améliorer leurs performances et leur fiabilité :</p>'}, {'', ""<li>Surveillance continue : suivez en permanence les indicateurs de performance clés tels que les temps de réponse, la latence, le débit et les taux d'erreur pour garantir le fonctionnement optimal des API.</li>""}, {'', ""<li>Optimisation des performances : utilisez des données en temps réel pour identifier et résoudre les problèmes de performances, ce qui permet d'obtenir des réponses API plus rapides et plus fiables.</li>""}, {'', '<li>Stabilité du système : en gérant et en optimisant de manière proactive les performances de l’API, vous pouvez garantir la stabilité et la robustesse globales de votre application.</li>'}, {'', '<p>Maintenance proactive</p>'}, {'', '<p>La surveillance en temps réel permet une approche proactive de la maintenance des API\xa0:</p>'}, {'', '<li>Système d’alerte précoce : ce système détecte les problèmes potentiels avant qu’ils ne deviennent critiques, permettant ainsi une maintenance préventive et évitant les pannes majeures.</li>'}, {'', ""<li>Maintenance planifiée : planifiez et exécutez les activités de maintenance en fonction des données de performance en temps réel, minimisant ainsi les interruptions et les temps d'arrêt.</li>""}, {'', '<li>Gestion des ressources : allouez efficacement les ressources et gérez la capacité pour gérer les charges de pointe, garantissant des performances API cohérentes.</li>'}, {'', '<p>Meilleure posture de sécurité</p>'}, {'', ""<p>La sécurité est un problème majeur lorsqu'il s'agit d'intégrations tierces. La surveillance en temps réel améliore votre posture de sécurité en\xa0:</p>""}, {'', ""<li>Détection d'anomalies : identifiez des modèles inhabituels pouvant indiquer des menaces de sécurité, telles que des tentatives d'accès non autorisées.</li>""}, {'', '<li>Réponse immédiate : réagissez rapidement aux incidents de sécurité, en atténuant les dommages potentiels et en protégeant les données sensibles.</li>'}, {'', '<li>Surveillance de la conformité : assurez la conformité aux normes et réglementations de sécurité en surveillant en permanence les interactions API pour détecter les écarts.</li>'}, {'', '<p>Conformité SLA</p>'}, {'', '<p>Le respect des accords de niveau de service (SLA) est essentiel pour maintenir les relations commerciales et la confiance. La surveillance en temps réel permet de :</p>'}, {'', ""<li>Suivi des performances : surveillez en permanence les indicateurs de performances des API pour vous assurer qu'ils respectent les SLA convenus.</li>""}, {'', '<li>Assurance de disponibilité : suivez la disponibilité et le temps de disponibilité des API en temps réel, garantissant ainsi le respect des garanties de disponibilité.</li>'}, {'', '<li>Rapports de conformité : générez des rapports en temps réel et historiques pour fournir la preuve de la conformité aux SLA, aidant ainsi à gérer les relations avec les fournisseurs et les attentes des clients.</li>'}, {'', '<p>Informations basées sur les données</p>'}, {'', '<p>La surveillance en temps réel fournit des données précieuses qui peuvent éclairer les décisions commerciales\xa0:</p>'}, {'', ""<li>Modèles d'utilisation : analysez les données d'utilisation en temps réel pour comprendre comment les clients interagissent avec vos API, en identifiant les fonctionnalités populaires et les tendances d'utilisation.</li>""}, {'', '<li>Développement de produits : utilisez les informations issues de données en temps réel pour guider le développement de produits et hiérarchiser les améliorations en fonction du comportement des utilisateurs.</li>'}, {'', '<li>Planification stratégique : exploitez les données pour prendre des décisions stratégiques éclairées, telles que la mise à l’échelle de l’infrastructure, l’amélioration de l’expérience utilisateur et l’optimisation de l’allocation des ressources.</li>'}, {'', '<h3>Mise en œuvre de la surveillance en temps réel</h3>'}, {'', '<p>La mise en œuvre d’une surveillance en temps réel des API tierces implique plusieurs étapes importantes. Chacune de ces étapes est essentielle pour garantir l’efficacité de votre stratégie de surveillance. Voici un aperçu détaillé de chaque étape\xa0:</p>'}, {'', '<p>Définir les objectifs de surveillance</p>'}, {'', '<p>Avant de vous lancer dans la mise en œuvre technique, il est essentiel de définir des objectifs clairs pour votre suivi. Tenez compte des aspects suivants :</p>'}, {'', ""<li>Indicateurs de performance : identifiez les indicateurs de performance essentiels pour votre entreprise. Les indicateurs courants incluent le temps de réponse, la disponibilité, les taux d'erreur et le débit.</li>""}, {'', ""<li>Impact sur l'entreprise\xa0: comprenez l'impact des performances de l'API sur vos opérations commerciales et l'expérience utilisateur. Cette compréhension vous aidera à hiérarchiser les indicateurs à surveiller de près.</li>""}, {'', '<li>Exigences de conformité : si vous avez des SLA ou des exigences réglementaires, assurez-vous que vos objectifs de surveillance correspondent à ces obligations.</li>'}, {'', '<p>Choisissez les bons outils</p>'}, {'', ""<p>Il est essentiel de sélectionner les outils appropriés pour la surveillance en temps réel. Envisagez des outils offrant des fonctionnalités complètes, une évolutivité et une facilité d'intégration. Voici quelques options :</p>""}, {'', ""<li>New Relic\xa0: fournit des informations détaillées sur les performances de l'API et propose des tableaux de bord personnalisables et des fonctionnalités d'alerte.</li>""}, {'', ""<li>Datadog : offre une surveillance et des analyses en temps réel pour les API, avec des capacités d'intégration robustes et une interface conviviale.</li>""}, {'', '<li>Pingdom : spécialisé dans la surveillance de la disponibilité et les tests de performances, ce qui le rend idéal pour garantir la disponibilité des API.</li>'}, {'', ""<p>Configurer l'infrastructure de surveillance</p>""}, {'', '<p>La mise en place de l’infrastructure de surveillance implique plusieurs étapes techniques :</p>'}, {'', ""<li>Intégration d'API\xa0: intégrez l'outil de surveillance à vos API. Ce processus peut impliquer l'ajout d'agents de surveillance à votre environnement serveur ou la configuration de points de terminaison d'API pour envoyer des données à l'outil de surveillance.</li>""}, {'', ""<li>Collecte de données : assurez-vous que l'outil collecte des points de données pertinents en temps réel. Ces données peuvent inclure les temps de réponse, les codes d'erreur et les volumes de transactions.</li>""}, {'', ""<li>Stockage des données : configurez le stockage des données historiques pour permettre l'analyse des tendances et la création de rapports.</li>""}, {'', '<p>Mettre en place des mécanismes d’alerte</p>'}, {'', ""<p>La surveillance en temps réel n'est efficace que si vous pouvez réagir rapidement aux problèmes. La mise en place de mécanismes d'alerte implique :</p>""}, {'', ""<li>Seuils\xa0: définissez des seuils pour les indicateurs clés. Par exemple, vous pouvez définir une alerte pour les temps de réponse supérieurs à 200 millisecondes ou un taux d'erreur supérieur à 1\xa0%.</li>""}, {'', ""<li>Notifications\xa0: choisissez la manière dont vous souhaitez recevoir les alertes. Les canaux courants incluent les e-mails, les SMS et l'intégration avec des outils de communication comme Slack ou Microsoft Teams.</li>""}, {'', ""<li>Stratégies d'escalade : définissez des stratégies d'escalade pour traiter rapidement les alertes critiques. Par exemple, les utilisateurs peuvent transmettre une alerte à une équipe d'assistance de niveau supérieur s'ils n'en accusent pas réception dans un certain délai.</li>""}, {'', '<p>Créer des tableaux de bord et des rapports</p>'}, {'', '<p>Les tableaux de bord et les rapports sont essentiels pour visualiser et analyser les données de performances des API. Tenez compte des éléments suivants\xa0:</p>'}, {'', '<li>Tableaux de bord personnalisés\xa0: créez des tableaux de bord personnalisables qui affichent des mesures en temps réel dans un format facile à comprendre. Des outils comme Grafana peuvent être intégrés à vos solutions de surveillance pour créer des tableaux de bord détaillés.</li>'}, {'', ""<li>Rapports automatisés\xa0: configurez des mises à jour régulières sur les performances de l'API. Ces rapports peuvent être programmés quotidiennement ou hebdomadairement.</li>""}, {'', ""<li>Analyse des tendances : utilisez des tableaux de bord et des rapports pour réaliser une analyse des tendances. L'identification de tendances au fil du temps peut contribuer aux efforts de maintenance et d'optimisation proactifs.</li>""}, {'', '<p>Réviser et optimiser régulièrement</p>'}, {'', ""<p>L'amélioration continue est essentielle pour maintenir une surveillance efficace en temps réel. Examinez régulièrement les données collectées et prenez des mesures pour optimiser les performances :</p>""}, {'', ""<li>Optimisation des performances\xa0: utilisez les informations issues de la surveillance en temps réel pour affiner les performances de l'API. Ce processus peut impliquer l'optimisation du code, la mise à niveau de l'infrastructure ou l'ajustement des configurations.</li>""}, {'', '<li>Examen des incidents : Effectuer des analyses post-mortem après avoir résolu les incidents pour trouver la cause principale et prévenir de futures occurrences.</li>'}, {'', ""<li>Boucle de rétroaction\xa0: le partage des informations issues de la surveillance peut contribuer à améliorer l'architecture globale du système et les stratégies de déploiement. Créez une boucle de rétroaction avec vos équipes de développement et d'exploitation.</li>""}, {'', ""<li>Planification de l'évolutivité : à mesure que votre entreprise se développe, assurez-vous que votre infrastructure de surveillance évolue en conséquence. Cela implique d'évaluer régulièrement la capacité de vos outils de surveillance et d'effectuer les mises à niveau nécessaires.</li>""}, {'', '<h3>Conclusion</h3>'}, {'', ""<p>La surveillance en temps réel des API tierces est essentielle pour les entreprises qui s'appuient sur des services externes pour offrir de la valeur à leurs clients. En mettant en œuvre une surveillance efficace en temps réel, les organisations peuvent garantir une détection immédiate des problèmes, améliorer les performances et la fiabilité, maintenir une posture de sécurité solide et se conformer aux SLA. L'adoption des bons outils et stratégies de surveillance des API en temps réel protège vos opérations et permet à votre entreprise d'offrir des expériences utilisateur supérieures dans un paysage numérique concurrentiel.</p>""}]"
Loi sur la cyber-résilience : une nouvelle ère pour les développeurs d'applications mobiles,"[{'', '<p>De nombreux fabricants d’applications mobiles pourraient connaître un réveil soudain dans un avenir pas si lointain. Le Cyber \u200b\u200bResilience Act (CRA) de l’Union européenne est une nouvelle loi qui devrait entrer en vigueur prochainement et qui sert de cadre conçu pour renforcer la sécurité des « produits comportant des éléments numériques ». Il est désormais clair que la nouvelle loi inclut les applications mobiles.</p>'}, {'', '<p>Au cours des longues discussions sur la nouvelle loi, la signification de l’expression « tous les appareils et logiciels numériques » n’a pas été entièrement comprise. Au début, le champ d’application semblait vague, mais le groupe PPE – le plus grand bloc du Parlement européen – a précisé que la CRA ne couvre pas seulement le matériel informatique tel que les ordinateurs portables et les téléphones mobiles, mais aussi les applications qu’ils exécutent. Cela ne laisse aucune place au doute : les fabricants d’applications mobiles doivent se conformer aux exigences de la CRA.</p>'}, {""<h3>Normes strictes au sein de l'ARC</h3>"", ''}, {'', '<p>Les normes de cybersécurité de la CRA sont particulièrement strictes. Le non-respect de ces normes comporte des risques financiers et de réputation considérables. Les sanctions en cas de non-conformité peuvent atteindre 15 millions d’euros ou 2,5 % du chiffre d’affaires mondial, selon le montant le plus élevé. Au-delà de l’impact financier immédiat, le non-respect pourrait avoir un impact négatif sur la confiance des consommateurs et la présence sur le marché.</p>'}, {'', '<p>L’objectif principal de la CRA est de lancer un ensemble uniforme d’exigences en matière de cybersécurité pour tous les produits et services numériques mis à disposition au sein de l’UE. Pour les développeurs d’applications mobiles, cela signifie une refonte potentielle des objectifs et des pratiques de développement afin de donner la priorité à la sécurité dès le départ. La loi exige que les applications soient développées en tenant compte de la sécurité et de la protection des données. Cela comprend la mise en œuvre du chiffrement, la garantie de l’intégrité des données et la fourniture de mises à jour de sécurité régulières.</p>'}, {'', '<p>Si les développeurs se concentrent sur la protection des données des utilisateurs et sur la garantie de la confidentialité et de la disponibilité des services, ils répondront probablement aux exigences du framework. Mais si l’on découvre que des failles de sécurité majeures ne sont pas corrigées, les développeurs pourraient se retrouver confrontés à de sérieux problèmes. Les applications mobiles sont vulnérables aux attaques telles que l’ingénierie inverse, la falsification et les logiciels malveillants mobiles qui peuvent entraîner la perte de données client sensibles et de propriété intellectuelle.</p>'}, {'', '<p>Toutes les applications mobiles utilisées dans l’UE sont concernées par le CRA, des grandes banques mondiales aux petites applications locales destinées à des applications de niche. L’impact est gigantesque pour le secteur des applications mobiles. Les petits développeurs indépendants travaillent souvent avec des ressources limitées, ce qui peut rendre les exigences du CRA insurmontables. L’obligation de mettre en œuvre des mesures de cybersécurité de pointe pourrait non seulement peser sur les budgets et les délais, mais aussi menacer l’innovation. Cependant, pour relever ces défis, les développeurs peuvent s’appuyer sur des outils open source, rechercher le soutien de la communauté et adopter une approche du développement d’applications qui permet des améliorations progressives de la sécurité.</p>'}, {'', '<h3>Les développeurs d’applications mobiles doivent collaborer</h3>'}, {'', '<p>La collaboration est essentielle pour que les développeurs d’applications mobiles se préparent à l’évaluation des vulnérabilités. Ils doivent d’abord procéder à un audit de sécurité approfondi de leurs applications, en identifiant et en corrigeant les vulnérabilités. Ensuite, ils voudront mettre en œuvre un plan structuré pour intégrer les fonctionnalités de sécurité nécessaires, en fonction de la liste de contrôle de l’évaluation des vulnérabilités. Il peut également être judicieux d’investir dans un partenariat avec des experts en cybersécurité qui peuvent fournir plus efficacement davantage d’informations et contribuer à rationaliser ce processus en général. On ne peut pas s’attendre à ce que les développeurs deviennent des experts en sécurité de premier ordre du jour au lendemain.</p>'}, {'', ""<p>En travaillant avec des entreprises de cybersécurité, des conseillers juridiques et des experts en conformité, on peut clarifier le CRA et simplifier le chemin vers la conformité et fournir des informations essentielles sur les meilleures pratiques, le jargon réglementaire et les solutions technologiques, garantissant que les applications répondent aux normes du CRA et maintiennent l'innovation.</p>""}, {'', '<p>Il est également important de noter que la tenue de registres complets des efforts de conformité est essentielle en vertu de la CRA. Les développeurs doivent établir un processus clair pour documenter les mesures de sécurité, les vulnérabilités traitées et les violations ou autres incidents qui ont été identifiés et corrigés. Il sera important de communiquer efficacement ces efforts aux organismes de réglementation pour démontrer la conformité et la diligence continue.</p>'}, {'', '<p>La loi CRA représente un changement important vers une présence numérique plus sûre et plus résiliente dans l’UE. En effet, nombreux sont ceux qui ne voulaient pas qu’elle prenne autant de temps à se concrétiser. Mais pour les développeurs d’applications mobiles, elle introduit un ensemble de défis ainsi que des opportunités pour améliorer la sécurité, différencier leur marque, renforcer la confiance des utilisateurs et promouvoir l’innovation. En comprenant les exigences de la loi, en adoptant un esprit de collaboration et en utilisant une technologie de pointe, les développeurs peuvent manœuvrer les complexités de la conformité et en sortir plus forts que jamais.</p>'}]"
Pourquoi et comment s'engager à fond dans GitOps,"[{'', '<p>GitOps est comme l’exercice physique : un peu est bon pour vous, mais plus c’est mieux. Tout comme courir quelques kilomètres toutes les deux semaines ne vous fera pas de mal mais n’améliorera pas considérablement votre santé, gérer environ 30 ou 40 % de vos processus à l’aide de GitOps ne boostera pas votre approche des opérations informatiques. Si vous voulez en tirer tous les avantages, vous devez vous y mettre à fond.</p>'}, {'', '<p>Je ne suis pas qualifié pour donner des conseils sur la mise en place d’un programme d’exercices sains. Mais je connais une chose ou deux sur GitOps et comment en tirer le meilleur parti. À cette fin, voici un aperçu des raisons pour lesquelles se lancer à fond dans GitOps peut être difficile, comment vous pouvez surmonter les défis et pourquoi cela en vaut la peine.</p>'}, {'', '<p>Bien que vous n’ayez pas strictement besoin de GitOps pour tirer parti des passerelles API, GitOps rationalise et fait évoluer la gestion de cette infrastructure clé dans les architectures d’applications modernes.</p>'}, {'', '<p>Pour ancrer la discussion, je me concentrerai aujourd’hui sur un cas d’utilisation courant de GitOps : son utilisation pour gérer les API en conjonction avec une passerelle API, qui permet de déployer, d’observer et de sécuriser les API.</p>'}, {'', '<h3>GitOps : définition des principes de base</h3>'}, {'', ""<p>GitOps est l'utilisation de Git, le système de contrôle de version open source, comme source unique de vérité pour la gestion des configurations d'infrastructure. L'objectif de GitOps est de remplacer la gestion manuelle des configurations par des flux de travail automatisés et pilotés par code.</p>""}, {'', '<p>En procédant ainsi, vous bénéficiez de nombreux avantages, tels que :</p>'}, {'', ""<li>Une source centralisée de vérité — sous la forme de référentiels Git — pour la configuration de l'infrastructure</li>""}, {'', ""<li>La capacité de gérer l'infrastructure à l'aide de configurations déclaratives et d'appliquer les modifications automatiquement et en continu pour garantir que l'état actuel n'a pas divergé de l'état déclaré</li>""}, {'', ""<li>Collaboration simple et efficace, puisque toutes les personnes impliquées dans la configuration et la gestion de l'infrastructure peuvent opérer à partir d'un hub centralisé</li>""}, {'', ""<li>La possibilité de suivre les modifications apportées aux configurations d'infrastructure au fil du temps à l'aide des fonctionnalités de contrôle de version intégrées de Git</li>""}, {'', '<li>Retour rapide et facile à une configuration antérieure en cas de problème</li>'}, {'', ""<li>La capacité de valider des configurations à l'aide d'agents logiciels, fréquemment appelés linters.</li>""}, {'', ""<p>Par exemple, si vous exploitez une passerelle API que vous pouvez gérer à l'aide d'une approche d'infrastructure en tant que code (IaC), vous pouvez utiliser GitOps pour configurer la passerelle à l'aide du code que vous stockez dans un référentiel Git. Cela permet d'analyser et de valider le code avant d'appliquer la configuration. De plus, vous pouvez suivre les modifications apportées à votre configuration via Git, et tous les membres de votre équipe peuvent facilement surveiller et collaborer autour de la gestion de la passerelle API en utilisant Git comme source de vérité centralisée.</p>""}, {'', ""<h3>L'état de l'adoption de GitOps</h3>""}, {'', '<p>Il est facile de parler des avantages de GitOps. L’adoption de GitOps peut toutefois s’avérer plus difficile. Selon une récente enquête de la CNCF, plus de 90 % des entreprises déclarent utiliser GitOps dans une certaine mesure, mais beaucoup ne l’ont mis en œuvre que dans une fraction de leurs déploiements cloud natifs. Les entreprises sont impatientes d’adopter GitOps, mais l’étendre à l’ensemble de leurs environnements informatiques s’avère beaucoup plus difficile.</p>'}, {'', '<p>C’est dommage, car la valeur de GitOps augmente de manière exponentielle lorsque vous l’implémentez de manière systématique et cohérente dans toute votre organisation. Lorsque vous pouvez gérer chaque déploiement cloud natif à l’aide d’une approche GitOps, les processus déclaratifs et automatisés s’intègrent alors dans votre culture et transforment votre entreprise.</p>'}, {'', '<p>En revanche, une implémentation partielle de GitOps peut améliorer l’efficacité de quelques processus, mais elle n’améliorera pas vos opérations globales. Elle vous laissera également avec des processus incohérents, car certains sont gérés via GitOps tandis que d’autres s’appuient sur des techniques manuelles. Et vos équipes se retrouveront coincées entre deux ensembles d’outils disparates, l’un pour activer GitOps et l’autre conçu pour les opérations héritées.</p>'}, {'', '<p>C’est pourquoi GitOps devrait idéalement être une affaire de tous les instants. Mettre en œuvre GitOps ici et là est mieux que ne pas l’utiliser du tout, mais vous ne pourrez pas exploiter tout le potentiel de GitOps tant qu’il ne s’agira pas d’un processus cohérent et systématique qui s’étend à toutes les facettes de votre entreprise et de vos opérations.</p>'}, {'', '<h3>Surmonter les obstacles à la mise en œuvre de GitOps</h3>'}, {'', ""<p>Mais encore une fois, se lancer à fond dans GitOps est plus facile à dire qu'à faire. Voici les principaux défis auxquels les entreprises sont confrontées, ainsi que les stratégies pour les surmonter.</p>""}, {'', '<p>Modifications du flux de travail</p>'}, {'', '<p>Le défi le plus évident dans la mise en œuvre de GitOps est peut-être la nécessité de faire évoluer les flux de travail des approches manuelles traditionnelles vers des approches ancrées dans Git. Vous devez vous assurer que toutes les parties prenantes savent comment travailler avec Git et sont prêtes à en faire leur solution de référence pour la gestion des configurations.</p>'}, {'', '<p>Par exemple, si votre objectif est de gérer une passerelle API à l’aide de GitOps, vos ingénieurs doivent être capables de gérer les configurations de la passerelle à l’aide de code et de savoir comment gérer ce code dans Git.</p>'}, {'', '<p>Il n’existe pas de solution miracle pour résoudre ce problème du jour au lendemain, mais la formation est essentielle. Les organisations qui souhaitent adopter complètement GitOps doivent former les développeurs, les ingénieurs informatiques et toute autre personne jouant un rôle dans la gestion de la configuration au fonctionnement de Git et à l’utilisation de plateformes complémentaires, telles que GitHub ou GitLab, que l’entreprise pourrait utiliser pour stocker le code de configuration. En outre, les initiatives de formation doivent expliquer pourquoi GitOps est si précieux, afin d’obtenir l’adhésion à une adoption à grande échelle de GitOps.</p>'}, {'', '<p>Trouver des outils compatibles avec GitOps</p>'}, {'', ""<p>Étant donné que GitOps est un concept relativement nouveau, tous les outils ne fonctionnent pas de manière transparente avec une approche de gestion de configuration basée sur GitOps. La mise en œuvre de GitOps est particulièrement difficile dans les cas où vous travaillez avec des outils qui ont une prise en charge native limitée ou inexistante de la gestion de configuration déclarative (c'est-à-dire la capacité de décrire un état souhaité à l'aide de code et de l'activer automatiquement). Par exemple, même si vous utilisez une plateforme comme Kubernetes (qui prend en charge les configurations déclaratives), vous pouvez l'associer à une passerelle API héritée au sein de votre pile qui ne prend pas en charge nativement une approche GitOps, car elles ne sont pas conçues pour être gérées à l'aide de code et de configurations déclaratives.</p>""}, {'', ""<p>Dans certains cas, vous pouvez contourner ce problème en installant des modules complémentaires ou en utilisant des outils supplémentaires pour activer une approche déclarative dans des environnements qui ne la prennent pas en charge. Cependant, cette solution nécessite beaucoup d'efforts pour être mise en œuvre et augmente également la complexité de vos opérations en ajoutant des outils supplémentaires.</p>""}, {'', '<p>Une meilleure solution à ce problème consiste à migrer vers des outils conçus pour prendre en charge la gestion de configuration déclarative de manière native. Aujourd’hui, vous pouvez trouver des outils compatibles avec GitOps pour pratiquement toutes les couches d’une pile de déploiement cloud native. Il n’y a donc plus aucune raison de se contenter de logiciels qui n’offrent pas une prise en charge complète des configurations déclaratives.</p>'}, {'', '<p>Gérer les secrets</p>'}, {'', ""<p>La gestion des secrets, tels que les clés qui permettent de sécuriser le trafic circulant via une passerelle API, peut s'avérer difficile lors de la mise en œuvre de GitOps, car vous ne souhaitez idéalement pas stocker les secrets en texte brut dans le même code que celui que vous utilisez pour gérer les configurations. Si vous le faites, toute personne pouvant consulter votre code de configuration pourra également accéder à vos systèmes et ressources.</p>""}, {'', ""<p>Heureusement, il existe des solutions simples à ce problème. L'une d'entre elles consiste à utiliser un gestionnaire de secrets, qui vous permet de stocker des secrets en dehors de vos référentiels de code tout en les rendant accessibles aux demandes d'authentification. Une autre solution consiste à intégrer des secrets dans votre code de configuration, mais à vous assurer qu'ils sont chiffrés pour empêcher tout accès non autorisé. L'une ou l'autre approche vous permet de connecter des secrets de manière sécurisée à un outil que vous gérez de manière déclarative.</p>""}, {'', '<p>Gérer la complexité</p>'}, {'', '<p>Si GitOps simplifie et fait évoluer les opérations de certaines manières, il peut également rendre les processus plus complexes en ajoutant de nouveaux types de ressources (comme des référentiels de code) aux workflows. C’est l’une des raisons pour lesquelles les équipes sont parfois tentées de s’en tenir aux passerelles API héritées qui ne prennent pas en charge la configuration déclarative : au départ, ces solutions peuvent sembler plus faciles à gérer car elles ne nécessitent pas l’apprentissage d’un tout nouveau workflow et d’outils supplémentaires. Cependant, l’organisation finit par en payer le prix avec un manque de rapidité et d’agilité à long terme.</p>'}, {'', ""<p>Les meilleures pratiques pour garder les choses gérables incluent la dénomination cohérente des ressources, le maintien de structures de dossiers cohérentes pour le code Git et l'utilisation d'outils comme Kustomize pour appliquer les configurations de manière cohérente dans des environnements disparates.</p>""}, {'', ""<p>L'analyse du code de configuration (c'est-à-dire la détection automatique des problèmes de formatage) et l'établissement de processus de révision systématique peuvent également être utiles.</p>""}, {'', '<p>Collaboration et communication</p>'}, {'', ""<p>Pour offrir une valeur maximale, GitOps doit s'accompagner d'un changement culturel au sein de votre organisation. Vous pouvez y parvenir en encourageant activement les équipes à exploiter GitOps comme moyen de rationaliser la collaboration et la communication. Soulignez également comment GitOps augmente la transparence et l'efficacité.</p>""}, {'', ""<p>Au fil du temps, les parties prenantes ont tendance à apprécier la valeur de GitOps de manière organique. Mais vous devrez peut-être lancer le processus pour faire évoluer votre état d'esprit et votre culture vers une approche centrée sur GitOps.</p>""}, {'', '<p>Reprise après sinistre</p>'}, {'', ""<p>GitOps simplifie la reprise après sinistre dans le sens où vous pouvez facilement restaurer des configurations basées sur le code Git. Lorsque les choses tournent mal (par exemple, lorsque la configuration de votre passerelle API entraîne une augmentation de la latence), il vous suffit de rétablir la modification problématique à l'état de fonctionnement précédent et vous avez maintenant le temps d'enquêter et d'identifier la cause première.</p>""}, {'', ""<p>Pour tirer parti de cette fonctionnalité, vous devez toutefois intégrer les opérations basées sur Git dans les plans de reprise après sinistre et de réponse. La plupart des organisations qui ont mis en place des stratégies de reprise après sinistre n'ont pas conçu ces stratégies en tenant compte de GitOps. Par conséquent, vous devez mettre à jour vos manuels et former les ingénieurs pour vous assurer qu'ils sont prêts à faire de Git un pilier essentiel des efforts de reprise après sinistre. Le retour à un état antérieur ne doit pas être un événement effrayant, mais un exercice de confiance.</p>""}, {'', '<p>Gestion des API</p>'}, {'', ""<p>GitOps va de pair avec une approche API-first des workflows. Les normes API, telles que la spécification OpenAPI (OAS), offrent une approche cohérente pour définir et gérer les API à toutes les étapes du cycle de vie des API. De cette façon, elles apportent plus de cohérence et d'ordre à GitOps.</p>""}, {'', '<p>Suivi et observabilité</p>'}, {'', ""<p>La capacité de surveiller et d'observer les pipelines GitOps est essentielle pour anticiper les problèmes susceptibles de perturber les flux de travail GitOps. Par exemple, si quelqu'un supprime accidentellement un code de configuration essentiel dont dépend votre passerelle API, vous souhaiterez le savoir rapidement afin de pouvoir annuler la modification.</p>""}, {'', ""<p>Comme pour la gestion des API, les normes ouvertes sont également utiles ici. En particulier, OpenTelemetry, ou OTel, facilite la collecte cohérente de mesures, de traces et de journaux à partir de n'importe quelle application ou outil.</p>""}, {'', '<p>GitOps pour les systèmes hérités</p>'}, {'', '<p>Un dernier défi auquel vous pouvez être confronté lors de la mise en œuvre de GitOps est de constater que votre parc informatique comprend des systèmes hérités qui ne peuvent tout simplement pas être mis à jour pour prendre en charge une approche déclarative.</p>'}, {'', '<p>La solution idéale consiste bien sûr à mettre à jour des systèmes modernes entièrement compatibles avec GitOps. Mais si la migration n’est pas possible, il existe toujours des moyens d’apporter certains des avantages de GitOps aux environnements existants. Par exemple, les applications monolithiques existantes qui s’exécutent actuellement directement sur des serveurs peuvent être redéployées à l’aide de conteneurs et de Kubernetes. Elles resteront des monolithes, mais vous pourrez gérer leur déploiement de manière déclarative.</p>'}, {'<h3>Conclusion : miser à fond sur GitOps</h3>', ''}, {'', '<p>Il est facile de reconnaître la valeur de GitOps. La mise en œuvre systématique de GitOps dans une entreprise peut être beaucoup plus difficile. Mais comme je l’ai expliqué ci-dessus, il est possible de surmonter chacun des principaux défis que les entreprises rencontrent généralement au cours de leur parcours GitOps. Ne vous attendez pas à vous lancer à fond dans GitOps du jour au lendemain, mais avec la bonne planification et les bons outils, vous pouvez tout à fait transformer votre organisation grâce à GitOps.</p>'}]"
Aider les développeurs à créer des API sécurisées dès le départ,"[{'', '<p>Les entreprises modernes gèrent des dizaines de milliers d’API. Si cette interconnexion favorise l’innovation et l’agilité, elle crée également une surface d’attaque coûteuse et tentaculaire. Rien qu’en 2022, on estime que les entreprises américaines ont perdu jusqu’à 23 milliards de dollars à cause de compromissions liées aux API, et 59 % des organisations ont été contraintes de ralentir le déploiement de nouvelles applications en production en raison de problèmes de sécurité des API.</p>'}, {'', ""<p>Bien que les développeurs soient en grande partie responsables de la création et de la gestion des API au sein d'une organisation, ils ont rarement accès aux outils réseau nécessaires pour les sécuriser ou les contrôler. Pour minimiser les risques liés aux API, les développeurs doivent avoir la possibilité de les créer en toute sécurité dès le départ. Voici les mesures et outils de sécurité nécessaires pour obtenir les meilleurs résultats.</p>""}, {'', '<h3>Bonnes pratiques en matière de sécurité des API</h3>'}, {'', '<p>Les API sont conçues pour aider les applications à communiquer entre elles. Elles ont donc souvent accès aux données de l’entreprise et des clients, notamment aux informations financières et aux enregistrements de transactions. Ce n’est pas un secret, et c’est pourquoi les pirates informatiques ciblent souvent les API, en particulier celles exposées aux requêtes réseau. Pour atténuer ce risque, les développeurs doivent mettre en œuvre les meilleures pratiques de sécurité dans chaque API qu’ils créent, notamment en ajoutant l’authentification et l’autorisation, les contrôles d’accès et le chiffrement pour toutes les requêtes et réponses d’API.</p>'}, {'', '<h3>Authentification et autorisation</h3>'}, {'', '<p>Une sécurité API efficace repose sur l’authentification (qui sont les utilisateurs) et l’autorisation (ce que les utilisateurs peuvent faire) pour contrôler l’accès aux ressources. Les applications côté client doivent inclure un jeton dans l’appel d’API, permettant au service de valider l’identité du client via l’authentification à l’aide d’une norme comme OAuth 2.0, OpenID Connect ou JSON Web Tokens. Ensuite, l’autorisation doit déterminer les actions autorisées pour le client authentifié. Cela garantit que les utilisateurs ou les appareils ne disposent que des autorisations minimales requises pour effectuer des tâches spécifiques, conformément au principe du moindre privilège.</p>'}, {'', ""<h3>Contrôles d'accès</h3>""}, {'', ""<p>Toute API qui accorde un accès tiers aux systèmes et données internes doit inclure des contrôles d'accès et une surveillance robustes. Les développeurs peuvent mettre en œuvre ces contrôles via un middleware pour vérifier l'identité de l'utilisateur, déterminer les actions autorisées et définir quand ces actions sont autorisées et pendant combien de temps. Le middleware peut également limiter le débit et appliquer des politiques de sécurité telles que le géorepérage, la validation du contenu des E/S et la désinfection des données. Pour une protection avancée, les développeurs doivent placer les API derrière une passerelle API, un pare-feu ou un pare-feu d'application Web utilisant HTTPS pour crypter la communication et filtrer le trafic malveillant.</p>""}, {'', '<h3>Cryptage</h3>'}, {'', ""<p>Le chiffrement des requêtes et des réponses API peut contribuer à protéger contre les écoutes clandestines et les violations de données. Les développeurs doivent utiliser HTTPS comme protocole de communication pour chiffrer tout le trafic et protéger les informations sensibles telles que les informations d'identification et les données échangées entre l'API et ses clients. Ils peuvent activer HTTP Strict Transport Security (HSTS) au lieu de rediriger pour garantir que les clients API utilisent toujours HTTPS, évitant ainsi les fuites d'informations accidentelles dues à des connexions non sécurisées. En intégrant le chiffrement au cœur de l'API, les développeurs peuvent garantir que les données sensibles restent protégées pendant la communication.</p>""}, {'', ""<h3>Amélioration de la sécurité des API grâce à l'accès des développeurs aux outils de mise en réseau</h3>""}, {'', '<p>Ces bonnes pratiques peuvent réduire considérablement le risque de failles de sécurité des API, mais de nombreux développeurs n’ont pas directement accès aux outils nécessaires pour les mettre en œuvre. Les passerelles API, qui sont utilisées pour gérer le trafic API, présentent un risque car les développeurs n’ont aucun contrôle sur elles. Cela augmente la probabilité d’une rupture de contrat entre les services API et la gestion du trafic, ce qui peut entraîner des temps d’arrêt et une sécurité affaiblie en raison de mauvaises configurations.</p>'}, {'', ""<p>Les passerelles API définies par les développeurs peuvent éliminer ces frictions et rationaliser le processus de déploiement. Elles permettent aux développeurs d'établir une connectivité sécurisée à leurs API en y intégrant directement les entrées. La déclaration des entrées, qui englobent des fonctionnalités telles que les hautes performances, la résilience, la sécurité et l'observabilité, directement dans l'application peut simplifier le processus de développement et réduire les incidents de sécurité et de performances. Les développeurs peuvent s'assurer que les requêtes API contiennent les paramètres exacts attendus par l'API pour éliminer le risque de rupture de contrat, d'accès non autorisé, etc.</p>""}, {'', '<p>En donnant aux développeurs l’accès à des outils de gestion du trafic robustes tels que les passerelles API, ils peuvent créer des API sécurisées dès le départ. En combinant cette fonctionnalité avec l’authentification et l’autorisation, les contrôles d’accès et le chiffrement, on peut réduire considérablement les cas de compromission liés aux API, tout en améliorant les délais de mise sur le marché et les performances. Les développeurs sont déjà chargés de créer et de gérer les API ; ils doivent également être en mesure de les sécuriser facilement.</p>'}]"
Boomi centralise la gestion et la gouvernance des API,"[{'', ""<p>Boomi a lancé cette semaine un plan de contrôle pour les interfaces de programmation d'applications (API) pour permettre aux organisations de les gérer et de les gouverner de manière centralisée, quelle que soit la manière dont elles sont créées ou la plate-forme sur laquelle elles s'exécutent.</p>""}, {'', ""<p>Jamie Ryan, vice-président de la gestion des produits chez Boomi, a déclaré que Boomi API Control Plane permet aux équipes informatiques de découvrir des API, y compris des API fantômes et malveillantes qui étaient jusque-là inconnues. C'est essentiel car le nombre d'API non gérées dans la plupart des organisations dépasse encore largement le nombre d'API gérées, a-t-il ajouté.</p>""}, {'', ""<p>Basé sur une plateforme que Boomi a acquise auprès d'APIIDA plus tôt cette année, l'objectif global est, via un ensemble d'agents de passerelle, de fournir la possibilité de découvrir des API, puis de centraliser la gestion sans obliger les organisations à migrer loin de toute plateforme sur laquelle elles ont déjà été déployées, a déclaré Ryan.</p>""}, {'', ""<p>Cette approche fédérée permettra aux équipes informatiques de gérer plus facilement la prolifération des API à une époque où elles sont créées plus rapidement que jamais, a-t-il ajouté. À mesure que les environnements applicatifs deviennent plus distribués, il devient de moins en moins probable que les entreprises souhaitent se standardiser sur une plateforme de gestion des API optimisée pour un environnement informatique spécifique lorsqu'il existe un plan de contrôle qui s'intègre à tous, a noté Ryan.</p>""}, {''}, {'', ""<p>Cela permet également aux équipes informatiques de mieux comprendre la manière dont les API sont utilisées, a noté Ryan. Il a ajouté qu'un trop grand nombre d'API ont été créées sans être utilisées aux fins prévues. À moins que ces API ne soient retirées, elles créent une autre surface d'attaque que les cybercriminels sont de plus en plus aptes à exploiter, a déclaré Ryan.</p>""}, {'', '<p>Les personnes chargées de gérer les API au sein d’une organisation informatique varient considérablement. Dans certains cas, ce sont les développeurs qui gèrent les API. Dans d’autres cas, cette responsabilité incombe à une équipe DevOps ou à une équipe d’ingénierie de plateforme. Une équipe de gestion de produits peut également être chargée de gérer les API. Le plan de contrôle des API Boomi fournit aux équipes un tableau de bord unique grâce auquel chaque membre de l’équipe peut participer à la gestion du portefeuille d’API de l’organisation, a noté Ryan.</p>'}, {'', '<p>Cela est particulièrement crucial à mesure que de plus en plus de modèles et de plateformes d’intelligence artificielle (IA) qui exposent des API supplémentaires s’intègrent aux environnements informatiques existants, a-t-il ajouté.</p>'}, {'', '<p>Quelle que soit la personne responsable au sein d’une organisation, la gestion du cycle de vie des API reste un défi. De nombreux développeurs, par exemple, créent des API pour une application interne et découvrent que cette API est désormais accessible par des applications externes. Le problème est que le niveau de contrôle de sécurité d’une API interne n’est pas toujours le même que celui d’une API externe. Sans que personne ne s’en rende compte, une API interne peut soudainement devenir accessible aux cybercriminels qui sont récemment devenus de plus en plus habiles à les utiliser pour exfiltrer des données et manipuler la logique métier à leurs propres fins néfastes. Boomi résout ce problème en permettant aux organisations d’évaluer les contrôles de sécurité relatifs appliqués à leurs API.</p>'}, {'', '<p>On ne sait pas encore dans quelle mesure la gestion des API et les outils nécessaires pour les sécuriser convergent, mais en général, les équipes informatiques s’orientent vers une centralisation de la gestion des opérations informatiques et de sécurité. La question est désormais de déterminer la meilleure façon d’atteindre cet objectif en incluant les API.</p>'}]"
Pourquoi vous risquez de supprimer des données mobiles clés de votre solution d'observabilité,"[{'', '<p>D</p>'}]"
Les trois piliers de la gestion moderne des API,"[{'', '<p>La prolifération des API est un véritable problème pour les entreprises modernes. Les entreprises gèrent des dizaines de milliers d’API dans leurs applications et services. Au-dessus se trouvent plusieurs passerelles d’API telles qu’Amazon API Gateway ou Google Apigee, ainsi que des courtiers d’événements tels que Solace, qui sont limités dans leur capacité à voir les autres passerelles. Il doit y avoir un endroit central où toutes les API sont sécurisées, détectables et gouvernables, quel que soit leur emplacement ou la passerelle d’API utilisée. Nous avons vu des tendances autour de la création d’un « panneau de verre unique » dans d’autres domaines du développement moderne (l’observabilité étant une catégorie qui me vient à l’esprit), mais la gestion des API reste un désordre complexe et cloisonné. Cela a des implications importantes pour la gouvernance des API.</p>'}, {'', ""<p>C'est là qu'entre en jeu la gestion des API fédérées.</p>""}, {'', '<p>Contrairement à l’approche plus traditionnelle, où la plupart des fonctionnalités de gestion des API étaient inextricablement liées à la passerelle API spécifique utilisée, la gestion fédérée des API soustrait une grande partie de la gestion et de la gouvernance de vos API à la passerelle API. Pour être clair, la passerelle API reste un composant crucial de l’infrastructure, mais les équipes doivent être habilitées à utiliser la passerelle API la mieux adaptée à leur cas d’utilisation tout en étant en mesure d’obtenir une gouvernance appropriée.</p>'}, {'', ""<p>Cette approche fédérée permet aux équipes de plateforme centrale de gérer chaque API sans avoir à basculer entre différents outils. Il s'agit d'une couche de gestion qui repose sur les passerelles sous-jacentes en tant que « wrapper » de gouvernance. Les équipes de plateforme doivent pouvoir tout faire, de la découverte et de l'inventaire des API à la mesure de la conformité des spécifications, en passant par la conception des flux d'abonnement et la publication des API sur un portail de développement pour la découverte et la consommation en libre-service côté consommateur, le tout avec une seule plateforme.</p>""}, {'', '<p>Et cela ne peut pas se limiter aux API synchrones. La gestion des API doit commencer à traiter les flux d’événements et les API d’événements comme des citoyens de première classe, au même titre que les API synchrones, à mesure que de plus en plus d’organisations introduisent et/ou redynamisent les initiatives de diffusion d’événements. Cela permettra aux équipes d’obtenir un meilleur retour sur investissement de leur investissement initial dans le streaming. Et bien sûr, tout système de gestion d’API moderne que vous envisagez doit être axé sur l’IA pour rester en phase avec les normes du secteur.</p>'}, {'', '<p>Pour être concis, voici les trois piliers que toute solution de gestion d’API doit couvrir.</p>'}, {'', '<h3>Les trois piliers</h3>'}, {'', ""<p>Multi-passerelle et multi-courtier\xa0: la gestion des API, en tant que pratique, doit prendre en charge la capacité des éditeurs d'API à gérer, sécuriser et gouverner les API et les services à partir de plusieurs passerelles d'API et courtiers d'événements différents.</p>""}, {'', ""<p>Événement natif\xa0: la gestion des API doit traiter les flux d'événements et les API d'événements comme des éléments de premier ordre, au même titre que les API synchrones, car de plus en plus d'organisations introduisent le streaming d'événements. Cela permettra aux équipes d'obtenir un meilleur retour sur investissement de leur investissement initial dans le streaming.</p>""}, {'', '<p>AI-Forward\xa0: les fournisseurs de gestion d’API doivent trouver des moyens d’améliorer leurs offres de gestion d’API grâce à l’IA et de mieux permettre aux organisations d’exploiter l’IA comme un multiplicateur de force.</p>'}, {'', '<p>Si la solution de gestion des API de votre entreprise est multi-passerelle, native des événements et axée sur l’IA, vous aurez une longueur d’avance sur la concurrence en termes de productivité et de sécurité. À l’heure actuelle, où des milliers d’API sont exploitées dans toute une organisation, il est évident de disposer d’une source unique de vérité où ces API sont détectables et gérées de manière adéquate. En bénéficiant d’une plus grande transparence et d’une meilleure visibilité, les développeurs peuvent évoluer plus rapidement tandis que les équipes de plateforme et de sécurité peuvent se sentir plus confiantes quant à leur posture de sécurité. C’est gagnant-gagnant.</p>'}]"
Une enquête révèle de nombreuses failles dans la chaîne d'approvisionnement des logiciels,"[{'', ""<p>Une enquête mondiale menée auprès de 900 professionnels de la sécurité des applications révèle que près des deux tiers d'entre eux travaillent pour des organisations dont la chaîne d'approvisionnement en logiciels a été compromise au cours des deux dernières années, 18 % d'entre eux ayant été victimes de cette situation au cours de l'année écoulée.</p>""}, {'', ""<p>Menée par Checkmarx, un fournisseur d'outils de test de sécurité des applications, l'enquête révèle également que 100 % des personnes interrogées ont connaissance d'une violation de leur chaîne d'approvisionnement en logiciels survenue à un moment donné dans le passé.</p>""}, {'', '<p>Sans surprise, l’enquête révèle que les trois quarts des répondants (75 %) sont désormais soit très préoccupés (39 %), soit préoccupés (36 %) par la sécurité de leur chaîne d’approvisionnement en logiciels.</p>'}, {'', '<p>En outre, plus de la moitié (57 %) des entreprises interrogées ont déclaré que la sécurité de la chaîne d’approvisionnement en logiciels était un domaine prioritaire ou important, et 54 % d’entre elles envisageaient d’utiliser ou d’étudier un type de solution. Cependant, selon l’enquête, seuls 7 % ont acquis et mis en œuvre un outil ou une plateforme pour sécuriser spécifiquement leur chaîne d’approvisionnement en logiciels.</p>'}, {'', ""<p>Du côté positif, la moitié (50 %) des personnes interrogées demandent désormais des nomenclatures de logiciels (SBOM) aux entités qui fournissent des logiciels à leur organisation, mais moins de la moitié (47 %) ont déclaré que leur organisation pouvait mettre en œuvre efficacement les SBOM, selon l'enquête.</p>""}, {'', ""<p>Renny Shen, vice-président du marketing de portefeuille chez Checkmarx, a déclaré que le défi réside dans l'ampleur des efforts nécessaires pour sécuriser les chaînes d'approvisionnement en logiciels. En plus des SBOM et du déploiement d'outils pour découvrir les vulnérabilités et les logiciels malveillants, les équipes DevSecOps doivent également, par exemple, adopter des initiatives de confiance zéro pour sécuriser l'accès à leur chaîne d'approvisionnement en logiciels.</p>""}, {'', '<p>Il n’existe pas d’outil ou de plateforme unique permettant aux équipes DevSecOps d’atteindre cet objectif, a-t-il ajouté.</p>'}, {'', '<p>En général, la sécurisation des logiciels reste un défi car même si une vulnérabilité est découverte, une équipe DevSecOps peut ne pas être en mesure de la corriger, a noté Shen. Par exemple, plus de la moitié (56 %) des applications déployées sont basées sur des packages de code open source dont les entreprises dépendent de mainteneurs externes pour la mise à jour.</p>'}, {'', '<p>Le point positif est que la sécurité de la chaîne d’approvisionnement logicielle est devenue une question de haut niveau, a déclaré Shen. Le défi est que de nombreuses organisations n’ont pas encore défini un ensemble d’indicateurs clés de performance (KPI) qui leur permettent de s’assurer que les meilleures pratiques DevSecOps sont adoptées. À l’heure actuelle, l’accent n’est pas suffisamment mis sur la sécurité des applications, de sorte que les développeurs continueront de consacrer la majeure partie de leur temps et de leurs efforts à l’écriture de nouveau code plutôt qu’à la correction du code existant, a-t-il noté.</p>'}, {'', '<p>Malheureusement, la plupart des entreprises ne donnent pas la priorité à la sécurité des applications. Tant qu’il n’y aura pas de changement fondamental dans les motivations, il est peu probable que les développeurs consacrent plus de temps à la correction des vulnérabilités. Des efforts ont été faits pour transférer davantage de responsabilités en matière de sécurité vers les développeurs, mais le succès a été mitigé. Les développeurs ont généralement accès à davantage d’outils, mais à moins que ces outils ne fassent apparaître des problèmes au fur et à mesure que les développeurs écrivent du code, il est peu probable qu’ils aient un impact significatif sur l’amélioration de l’état général de la sécurité des applications.</p>'}, {'', '<p>Espérons que l’essor de l’intelligence artificielle générative (IA) simplifiera bientôt la recherche et la correction des vulnérabilités, par exemple en créant automatiquement le correctif requis. En attendant, toutefois, sécuriser les chaînes d’approvisionnement en logiciels exige un niveau de vigilance qui, malgré les meilleures intentions, reste difficile à atteindre et à maintenir.</p>'}]"
Fauna ajoute un outil déclaratif pour mettre à jour la base de données des homonymes,"[{'', ""<p>Fauna a ajouté la capacité de maintenir une application stricte du schéma à sa base de données relationnelle de documents à l'aide d'un langage déclaratif qui simplifie la mise à jour des bases de données dans le contexte d'un flux de travail DevOps.</p>""}, {'', ""<p>Alors que Fauna a permis aux équipes informatiques d'étendre une base de données de documents pour ajouter des tables et des lignes nécessitant généralement une base de données relationnelle, Fauna Schema ajoute la structure, l'application et la fiabilité d'un schéma généralement associé aux bases de données relationnelles traditionnelles.</p>""}, {'', ""<p>Eric Berg, PDG de Fauna, a déclaré que l'ajout de cette approche renforce l'intérêt d'utiliser une base de données Fauna qui simplifie la gestion des bases de données pouvant être utilisées pour prendre en charge plusieurs types d'applications. En outre, Fauna permet désormais aux schémas de base de données d'évoluer en fonction des besoins de l'entreprise grâce à une nouvelle fonctionnalité de types de documents qui permet aux développeurs de définir et d'appliquer des structures de schéma directement dans la base de données. Les types de documents peuvent prendre en charge le typage statique et dynamique, ce qui permet aux développeurs de commencer avec une base de données de documents sans schéma avant d'ajouter des contrôles de type plus stricts d'une manière qui ne nécessite pas la mise hors ligne d'une base de données, a déclaré Berg.</p>""}, {''}, {'', '<p>Au cœur de cette capacité se trouve le langage Fauna Schema Language (FSL), qui permet aux développeurs de définir de manière déclarative leurs modèles de domaine, leurs contrôles d’accès et leur logique métier dans un langage lisible par l’homme dans un ensemble de fichiers qui peuvent être gérés parallèlement à leur code d’application.</p>'}, {'', ""<p>Grâce à un référentiel de code tel que GitHub, les équipes DevOps peuvent désormais coordonner les mises à jour des schémas et des codes d'application via un pipeline d'intégration continue/livraison continue (CI/CD). De plus, les équipes informatiques peuvent exploiter des outils d'infrastructure en tant que code (IaC) tels que Pulumi et Terraform pour automatiser le déploiement et la gestion de leur configuration de base de données Fauna.</p>""}, {'', ""<p>Fauna a également ajouté une fonctionnalité de champs calculés qui permet de générer dynamiquement les valeurs des champs des documents en fonction des expressions définies par l'utilisateur. Cette approche permet aux développeurs de créer des champs dont les valeurs sont calculées au moment de la requête pour permettre une composition dynamique des objets et des valeurs des champs. Ces champs peuvent être indexés, ce qui offre une plus grande flexibilité dans les modèles de requête afin que les développeurs puissent définir avec précision leur modèle de données dans la base de données au lieu de les définir dans des couches de mappage exécutées en dehors de la base de données.</p>""}, {'', ""<p>Le codage de cette logique de base de données au sein de la base de données permet aux équipes de développement de définir des jointures et des sous-requêtes entre collections comme un élément de leur conception de schéma pour générer dynamiquement des données et des modèles d'objet en temps réel. Cette capacité évite les inconvénients de performances inhérents à une stratégie de dénormalisation des données requise par d'autres bases de données de documents.</p>""}, {'', '<p>On ne sait pas exactement dans quelle mesure les organisations informatiques s’orientent vers la consolidation des bases de données, mais avec l’essor des bases de données documentaires, il est devenu relativement simple pour les développeurs d’ajouter une base de données à presque n’importe quel type d’application. Le défi auquel les organisations sont confrontées est qu’à mesure que les bases de données documentaires évoluent, elles deviennent plus difficiles à gérer. Il n’est pas rare que les développeurs confient la responsabilité de la gestion des bases de données documentaires à une équipe DevOps qui a souvent un intérêt direct dans la consolidation.</p>'}, {'', '<p>Après tout, plus il y a de bases de données basées sur des architectures disparates à gérer, plus il devient difficile de gérer les flux de travail DevOps à n’importe quel niveau d’échelle significatif.</p>'}]"
Pourquoi l'observabilité des risques commerciaux devient essentielle pour les applications modernes,"[{'', '<p>La technologie évolue plus vite que jamais et oblige les entreprises à mettre à jour rapidement leurs stratégies numériques, faute de quoi elles risquent de prendre du retard et de devenir obsolètes. En adoptant des technologies cloud natives, les changements d’infrastructure modernes permettent aux entreprises d’évoluer au-delà des locaux pour profiter de la flexibilité et de la capacité accrues du cloud. Il s’agit toutefois d’une mise en garde. Cette adoption rapide a conduit à des surfaces d’attaque étendues dans le domaine, rendant les applications plus vulnérables aux risques de sécurité, ce qui pourrait avoir un impact négatif sur la réputation ou les revenus d’une entreprise. Cette transformation rapide a laissé les technologues sans les outils et les informations nécessaires pour obtenir une visibilité complète sur leur nouvelle infrastructure hybride.</p>'}, {'', ""<p>En conséquence, les entreprises connaissent une augmentation des menaces de sécurité au sein de leurs environnements Kubernetes. Selon Red Hat, 90 % des entreprises ont connu au moins un incident de sécurité lié à Kubernetes au cours des 12 derniers mois, ce qui a entraîné une perte de revenus ou de clients pour 37 % des répondants. De plus, Aqua Security a enquêté sur les clusters Kubernetes appartenant à plus de 350 organisations, projets open source et particuliers, et a découvert qu'ils n'étaient tous pas protégés, avec au moins 60 % d'entre eux piratés.</p>""}, {'', '<p>La gestion réussie des infrastructures d’applications modernes nécessite une visibilité unifiée, qui est limitée dans les approches traditionnelles de sécurité des applications et de surveillance des vulnérabilités. Pour faire face aux vulnérabilités croissantes, les développeurs et les équipes de sécurité doivent d’abord cesser de fonctionner en silos en s’assurant qu’ils collaborent tout au long du processus de développement des applications. Les équipes doivent également envisager d’adopter de nouvelles solutions qui leur fourniront une vue plus unifiée d’une infrastructure d’applications fragmentée. Cela leur permettra d’identifier rapidement les vulnérabilités au sein du parc informatique, la probabilité d’exploitation et les risques pour leur entreprise.</p>'}, {'', '<h3>L’observabilité des risques commerciaux est essentielle</h3>'}, {'', ""<p>Une visibilité totale sur les infrastructures hybrides est essentielle pour soutenir le développement et le déploiement sécurisés des applications modernes. Les techniciens ont besoin d'une vue d'ensemble complète des menaces de sécurité, notamment de leur localisation et de la manière dont elles pourraient affecter l'application. En repérant et en corrélant les risques de sécurité potentiels sur différentes entités d'application, telles que les transactions commerciales, les conteneurs, les pods, les services et les charges de travail, les techniciens peuvent rapidement résoudre le problème et réduire le temps moyen de correction (MTTR).</p>""}, {'', ""<p>Toutefois, le contexte commercial élargit ces données de sécurité traditionnelles. En plus de fournir la visibilité nécessaire pour localiser et évaluer les menaces de sécurité, le contexte commercial aide les technologues à hiérarchiser et à corriger les risques en fonction de leur impact potentiel sur l'entreprise.</p>""}, {'', '<p>L’observabilité des risques métiers combine les données de performances des applications et le contexte métier avec la détection des vulnérabilités et les renseignements sur la sécurité, ce qui permet d’identifier les risques métiers. Grâce à l’observabilité des risques métiers, les techniciens peuvent générer un score de risque métier pour toutes les vulnérabilités afin d’aider les équipes à hiérarchiser les problèmes en fonction de leur gravité, qui pourraient causer le plus de dommages aux aspects financiers ou à la réputation de l’entreprise.</p>'}, {'', '<p>Fondamentalement, l’observabilité des risques métiers unifie les équipes d’applications et de sécurité autour d’un référentiel central de données complètes sur la disponibilité, les performances et la sécurité des applications. À l’ère des menaces zero-day, qui nécessitent une collaboration interfonctionnelle pour des déploiements sécurisés d’applications contemporaines, l’observabilité des risques métiers établit un cadre pour la mise en œuvre d’une approche DevSecOps au sein du service informatique. En intégrant la sécurité dans le cycle de vie des applications dès sa création, les équipes de développement adhèrent aux priorités de sécurité les plus critiques de l’organisation et intègrent des mesures de sécurité robustes dans chaque ligne de code. Cela aboutit à des applications renforcées et à une gestion de la sécurité simplifiée, avant la publication, pendant le déploiement et après la publication.</p>'}, {'', '<h3>L’adoption de l’observabilité des risques commerciaux prend de l’ampleur</h3>'}, {'', '<p>Le passage aux infrastructures hybrides et cloud natives a donné naissance à des environnements plus vulnérables, ce qui a accru la demande d’observabilité des risques commerciaux. Selon une étude de Cisco, le passage à une approche de sécurité pour l’ensemble de la pile applicative a révélé que 93 % des technologues reconnaissent désormais l’importance de contextualiser la sécurité et de hiérarchiser les correctifs de vulnérabilité en fonction de l’impact potentiel sur l’entreprise.</p>'}, {'', ""<p>Pour faire face aux niveaux de menace plus élevés dans les environnements Kubernetes, il est essentiel que les entreprises adoptent des solutions et des méthodes de travail innovantes. Gartner prévoit que 95 % des nouvelles charges de travail numériques s'appuieront sur des plateformes cloud natives d'ici 2025, ce qui signifie que les défis liés aux infrastructures fragmentées sont susceptibles de s'accroître. Les conséquences d'une éventuelle faille de sécurité pouvant être préjudiciables à une entreprise, les entreprises doivent sérieusement envisager de mettre en œuvre une solution d'observabilité.</p>""}, {'', ""<p>Le parc informatique moderne devient de plus en plus vulnérable aux menaces de sécurité et les techniciens ont besoin d'une forme de défense essentielle : une visibilité et un contexte commercial unifiés. L'observabilité des risques commerciaux constitue une forme de défense essentielle pour les organisations, permettant aux équipes informatiques de surveiller, de hiérarchiser et de répondre rapidement aux problèmes de sécurité, d'accélérer les temps de réponse et de protéger systématiquement leurs organisations et leurs clients.</p>""}]"
Backslash Security étend la portée de la plateforme de sécurité des applications,"[{'', ""<p>Backslash Security a ajouté aujourd'hui la prise en charge de C, C++, Ruby, Rust et Scala ainsi que des intégrations avec des instances sur site de plates-formes DevOps de GitHub et GitLab à sa plate-forme de sécurité des applications.</p>""}, {'', '<p>De plus, les équipes DevSecOps peuvent désormais détecter les « packages fantômes » non définis ou contrôlés par le développeur de l’application d’origine. La plateforme Backslash Security détecte ces packages fantômes dans le code des logiciels open source, même s’ils ne sont pas déclarés dans les fichiers manifestes, afin d’identifier les packages transitifs vulnérables.</p>'}, {'', '<p>Une interface utilisateur remaniée permet de comprendre plus facilement quelles vulnérabilités sont utilisées et donc exploitables dans leur base de code.</p>'}, {'', '<p>Les équipes DevSecOps peuvent créer des workflows qui génèrent automatiquement des tickets et des notifications pour Jira, Monday.com, ServiceNow, Slack et Microsoft Teams.</p>'}, {'', '<p>Enfin, Backslash ajoute la prise en charge des contrôles d’accès basés sur les rôles pour rendre sa plateforme plus sécurisée.</p>'}, {'', ""<p>Amit Bismut, responsable de la gestion des produits chez Backslash Security, a déclaré que la prise en charge de ces langages de programmation étendra la portée d'une plate-forme conçue pour remplacer les outils de sécurité des applications statiques (SAST) et les outils d'analyse de composition logicielle (SCA) hérités par une plate-forme de sécurité des applications intégrée unique qui gère les secrets, génère des nomenclatures de logiciels (SBOM) et est intégrée à Vulnerability Exploitability Exchange (VEX).</p>""}, {'', '<p>L’objectif général est d’éliminer le nombre de faux positifs générés qui augmentent le niveau de charge cognitive des développeurs en fournissant aux équipes DevSecOps une plateforme de sécurité des applications qui simplifie l’identification et la priorisation des efforts de correction, a-t-il ajouté. Cette approche élimine le flot d’alertes dont les développeurs sont inondés lorsque les organisations, au lieu de transférer la responsabilité vers la gauche, se contentent de « charger vers la gauche », a déclaré Bismut.</p>'}, {'', '<p>Backslash Security prenait auparavant en charge Golang, Java, JavaScript et Python en plus de fournir des intégrations avec les services cloud de GitHub, GitLab et Microsoft.</p>'}, {'', ""<p>L'ajout de la prise en charge de langages de programmation supplémentaires et de plates-formes sur site permettra aux équipes DevSecOps de gérer plus facilement la sécurité des applications dans des environnements informatiques hybrides constitués de différentes classes d'applications ayant toutes plusieurs dépendances, a déclaré Bismut.</p>""}, {''}, {'', '<p>La plupart des développeurs d’applications n’ont aujourd’hui que peu ou pas d’expertise en cybersécurité. De nombreuses entreprises tentent de transférer la responsabilité de la sécurité des applications aux développeurs, mais sans les bons outils à leur disposition, il est peu probable qu’elles y parviennent. Il est encore assez courant qu’une attaque par injection SQL relativement simple soit utilisée avec succès pour compromettre un environnement d’application, a noté Bismut.</p>'}, {'', '<p>La plateforme Backslash permet aux développeurs de comprendre plus facilement comment les flux de travail de leur code pourraient être exploités, au lieu de compter sur des professionnels de la cybersécurité pour découvrir des vulnérabilités qui pourraient ne pas être présentes dans un environnement de production, a-t-il ajouté.</p>'}, {'', '<p>Il ne fait aucun doute que les cybercriminels continueront de se concentrer sur l’exploitation des faiblesses des chaînes d’approvisionnement de logiciels. Les attaques ne sont peut-être pas très sophistiquées, mais compte tenu des chances de réussite actuelles, les cibles sont trop tentantes pour être ignorées. Le problème auquel les entreprises doivent faire face est le niveau de responsabilité croissant que représentent ces failles de sécurité, car les gouvernements du monde entier commencent à tenir les entreprises davantage responsables de la sécurité des applications qu’elles déploient.</p>'}]"
Harness acquiert Split Software pour la gestion des fonctionnalités,"[{'', ""<p>Harness a annoncé aujourd'hui l'acquisition de Split Software dans le cadre d'un effort visant à étendre les capacités de gestion des fonctionnalités de son portefeuille DevOps.</p>""}, {'', ""<p>Comme pour tous les outils complémentaires de Harness, la plate-forme de gestion des fonctionnalités développée par Split Software continuera d'être disponible sur plusieurs plates-formes d'intégration continue/livraison continue (CI/CD) même après que Harness aura terminé son intégration.</p>""}, {'', ""<p>Harish Doddala, vice-président senior et directeur général de Harness, a déclaré que Split Software ajoute une capacité d'expérimentation qui va au-delà des capacités de gestion des fonctionnalités existantes actuellement fournies pour permettre aux équipes DevOps de mieux gérer plusieurs projets de développement logiciel.</p>""}, {'', ""<p>Trevor Stuart, président de Split Software, a déclaré que ces capacités d'expérimentation ont été créées pour permettre aux organisations de fournir progressivement une gamme de fonctionnalités à différents types d'utilisateurs finaux après qu'une application a déjà été déployée dans un environnement de production.</p>""}, {'', '<p>Cette capacité deviendra encore plus cruciale à mesure que les organisations intégreront plusieurs modèles d’intelligence artificielle (IA) pour répondre à différents cas d’utilisation, a noté Stuart.</p>'}, {'', '<h3>Utilisation de la gestion des fonctionnalités dans les environnements de production</h3>'}, {'', ""<p>La gestion des fonctionnalités est déjà largement utilisée dans les tests A/B, mais le nombre d'organisations utilisant la gestion des fonctionnalités pour fournir différents niveaux d'expériences applicatives dans les environnements de production est relativement rare.</p>""}, {''}, {'', '<p>Cependant, avec l’essor du commerce numérique, le nombre d’organisations qui fournissent des fonctionnalités supplémentaires à un coût plus élevé au sein d’une application n’a cessé d’augmenter.</p>'}, {'', ""<p>Plutôt que de devoir faire appel à plusieurs fournisseurs pour fournir cette capacité, Harness s'oriente désormais vers l'invocation de la gestion des fonctionnalités en tant que code aux côtés de tous les autres services intégrés à sa plateforme CI/CD, y compris les modèles d'IA qu'elle fournit pour accélérer la création et le déploiement d'applications à des niveaux d'échelle plus élevés.</p>""}, {'', '<p>Les équipes de développement d’applications utilisent bien sûr la gestion des fonctionnalités depuis les années 1970 pour isoler le développement de divers composants dans une branche sur laquelle il est possible de travailler et de tester sans impacter la version principale sur laquelle l’application est basée. La gestion des fonctionnalités permet en effet aux équipes de développement d’expérimenter l’ajout de nouvelles fonctionnalités d’une manière qui ne perturbe pas l’application. Lorsqu’un projet est terminé, cette branche est généralement fusionnée dans la version principale ou peut être déployée en tant que microservice qui peut être invoqué via une interface de programmation d’application (API) par d’autres modules avec l’application ou une autre application externe.</p>'}, {'', '<h3>Drapeaux de fonctionnalité</h3>'}, {'', ""<p>Au cœur de cette capacité se trouvent ce que l'on appelle des indicateurs de fonctionnalité, également appelés bascules de fonctionnalité ou commutateurs de fonctionnalité, qui permettent d'activer ou de désactiver dynamiquement des services en fonction de la personne qui y accède. Plutôt que d'être limités au développement d'applications, ces indicateurs de fonctionnalité sont désormais utilisés dans les environnements de production pour permettre la fourniture continue de plusieurs types de services numériques à différentes classes d'utilisateurs d'un service d'application particulier.</p>""}, {'', '<p>Pour atteindre cet objectif, les équipes DevOps doivent naturellement travailler en étroite collaboration avec les chefs de produit afin de déterminer les fonctionnalités qui seront mises à disposition pour s’aligner sur le modèle commercial adopté. Le défi, comme toujours, consiste à s’assurer que ces services offrent une expérience utilisateur supérieure sans créer un ensemble de dépendances qui ne peuvent pas être facilement gérées indépendamment du reste de l’application.</p>'}]"
vFunction ajoute l'observabilité en temps réel et l'outil GenAI à sa plateforme de modernisation,"[{'', ""<p>vFunction, la plateforme d'observabilité architecturale pilotée par l'IA, a ajouté aujourd'hui l'observabilité en temps réel et l'assistant GenAI.</p>""}, {'', ""<p>vFunction a ajouté aujourd'hui la capacité d'observer les architectures informatiques en temps réel à une plate-forme qui convertit automatiquement le code d'application monolithique en un ensemble de microservices.</p>""}, {'', ""<p>En outre, vFunction a ajouté un assistant d'intelligence artificielle générative (IA) qui identifie les problèmes et fait ressortir les recommandations au fur et à mesure des modifications apportées aux environnements d'application. Basé sur le service ChatGPT-4 créé par OpenAI, l'assistant vFunction est conçu pour simplifier la correction et la refactorisation des applications.</p>""}, {'', '<h3>Exploiter OpenTelemetry</h3>'}, {'', ""<p>Moti Rafalin, PDG de vFunction, a déclaré que toutes ces capacités s'appuient sur OpenTelemetry, un logiciel agent open source développé sous les auspices de la Cloud Native Computing Foundation (CNCF) pour collecter des données de télémétrie.</p>""}, {'', '<p>On ne sait pas exactement combien d’applications sont instrumentées à l’aide d’OpenTelemetry. Cependant, une enquête menée par vFunction auprès de 1\xa0000 professionnels de l’ingénierie logicielle suggère que les organisations qui cherchent à rationaliser la gestion des environnements applicatifs ont clairement besoin d’une visibilité accrue. 80 % d’entre elles ont déclaré que ces fonctionnalités étaient extrêmement ou très précieuses, et près de quatre sur dix (39 %) ont reconnu qu’elles manquaient de visibilité sur leur architecture informatique. Au total, 40 % ont déclaré qu’elles préconisaient une approche « shift left » pour répondre de manière proactive aux problèmes de résilience plus tôt dans le cycle de développement.</p>'}, {'', '<p>Un pourcentage similaire (41 %) prévoit également de tirer parti de l’IA générative pour améliorer les performances et l’évolutivité des applications, selon l’enquête.</p>'}, {'', ""<p>L'enquête révèle également que plus de la moitié (51 %) des répondants travaillent pour des organisations qui consacrent plus d'un quart de leur budget informatique et d'ingénierie annuel à la correction des applications, y compris aux efforts de refactorisation et de réarchitecture. Plus des trois quarts (77 %) ont mis en œuvre des initiatives à l'échelle de l'entreprise pour traiter la dette technique.</p>""}, {'', '<p>Parmi les personnes interrogées, 57 % travaillent pour des organisations qui allouent plus d’un quart de leur budget informatique à la remédiation de la dette technique impliquant des applications monolithiques, contre 49 % pour les architectures de microservices. Plus de la moitié (53 %) ont également indiqué que leur organisation avait retardé des migrations technologiques majeures ou des mises à niveau de plateforme en raison de problèmes de productivité liés aux microservices. L’enquête révèle également que 44 % ont indiqué que la complexité des applications monolithiques, qui se traduit par un manque de limites claires dans les domaines et une diminution de la modularité, constitue un défi majeur.</p>'}, {'', ""<p>Dans un avenir proche, la plupart des environnements applicatifs seront constitués d'applications monolithiques et basées sur des microservices. Le défi auquel les organisations sont confrontées est que, à mesure que des modifications sont apportées aux environnements applicatifs, le niveau d'éloignement par rapport à l'architecture d'origine commence à augmenter.</p>""}, {'', ""<h3>Appliquer l'IA pour réduire la dérive</h3>""}, {'', '<p>Dans le même temps, les entreprises accumulent une dette technique, car les problèmes qui doivent être résolus doivent être traités ultérieurement. Malheureusement, bon nombre de ces problèmes ne sont jamais résolus, tout simplement parce que les nouveaux projets de développement d’applications obligent les équipes DevOps à consacrer des ressources limitées. Cependant, à mesure que de plus en plus d’applications sont instrumentées, il devient possible d’appliquer l’IA pour réduire à la fois la dérive et la dette technique.</p>'}, {''}, {'', ""<p>Chaque organisation doit déterminer dans quelle mesure elle souhaite moderniser les applications existantes ou les remplacer ou les supprimer complètement. Cependant, il existe de nombreux cas où les applications existantes sont encore bien trop critiques pour être remplacées. Le défi consiste alors à savoir dans quelle mesure mettre à jour ces applications, de manière à les rendre plus résilientes et plus flexibles à mesure que les besoins de l'entreprise continuent d'évoluer.</p>""}]"
StarTree présente l'observabilité de la base de données Apache Pinot,"[{'', '<p>StarTree améliore considérablement sa plate-forme de base de données en temps réel en ajoutant la prise en charge des requêtes pour les métriques, les journaux et les traces.</p>'}, {'', ""<p>StarTree a aujourd'hui présenté ses arguments pour devenir la base sur laquelle l'observabilité est obtenue dans les environnements informatiques en ajoutant la prise en charge des métriques, des journaux et des traces à sa plate-forme de base de données en temps réel fournie sous forme de service cloud.</p>""}, {'', ""<p>En outre, la société met également à disposition un service StarTree ThirdEye permettant de détecter les anomalies et d'identifier la cause profonde des problèmes d'application. Elle a également mis à disposition un niveau de calcul sans serveur gratuit sur son service cloud.</p>""}, {'', '<h3>Alternative viable à la base de données héritée</h3>'}, {'', ""<p>Annoncé lors de la conférence Real-Time Analytics Summit, StarTree Cloud est basé sur Apache Pinot, une base de données en colonnes open source optimisée pour l'analyse en temps réel, développée à l'origine par LinkedIn. StarTree, via une préversion privée, indique qu'elle considère Apace Pinot comme une alternative viable à la base de données héritée, sur laquelle les équipes DevOps utilisent des plateformes d'observabilité pour dépanner les environnements informatiques.</p>""}, {'', ""<p>StarTree, pour faire progresser Apache Pinot, ajoute désormais également une capacité d'indexation vectorielle à Apache Pinot ainsi que la prise en charge d'une interface de programmation d'application (API) d'écriture pour faciliter la synchronisation en temps réel avec les pipelines d'extraction, de transformation et de chargement (ELT) et des intégrations supplémentaires à Grafana et Tableau pour améliorer les visualisations.</p>""}, {'', ""<p>Chinmay Soman, responsable produit chez StarTree, a déclaré que le problème avec les bases de données existantes est qu'elles ne sont pas conçues pour s'adapter au niveau de performance requis par les environnements d'application modernes.</p>""}, {'', ""<h3>Les premiers jours de l'observabilité</h3>""}, {'', '<p>Il est encore tôt pour parvenir à l’observabilité dans les environnements applicatifs, mais de nombreuses équipes DevOps vont désormais bien au-delà de la simple surveillance d’un ensemble de mesures prédéterminées. Les plateformes d’observabilité permettent aux équipes DevOps de lancer des requêtes qui facilitent le dépannage des applications. Toutes les équipes DevOps ne savent pas quelles requêtes examiner. Avec l’essor de l’intelligence artificielle (IA), il devient de plus en plus possible de s’appuyer sur des algorithmes d’apprentissage automatique pour analyser les environnements applicatifs à l’aide de mesures, de journaux et de traces stockés dans une plateforme d’observabilité.</p>'}, {'', '<p>Peu d’équipes DevOps construiront elles-mêmes une plateforme d’observabilité, mais les capacités des bases de données sous-jacentes sur lesquelles elles reposent sont essentielles, a déclaré Soman. Les équipes DevOps doivent s’assurer que les bases de données utilisées aujourd’hui dans ces plateformes seront capables d’évoluer pour répondre aux exigences de traitement en temps réel qui ne feront qu’augmenter à mesure que des applications plus complexes seront déployées dans des environnements informatiques distribués, a-t-il ajouté.</p>'}, {'', '<p>Entre-temps, StarTree s’oriente vers davantage de capacités d’observabilité via le service de base de données géré qu’il fournit via le cloud.</p>'}, {''}, {'', '<p>Ce n’est qu’une question de temps avant que les équipes DevOps aient besoin de niveaux de visibilité plus élevés pour gérer les environnements d’applications modernes en temps réel. Les entreprises étant de plus en plus dépendantes des logiciels, même le plus petit problème peut désormais avoir un impact financier majeur. Le défi auquel les équipes DevOps sont confrontées est bien sûr de trouver le financement nécessaire pour acquérir ces capacités.</p>'}, {'', '<p>En attendant, les équipes DevOps devraient au moins commencer à évaluer lesquels des nombreux outils de surveillance utilisés aujourd’hui pourraient un jour être rationalisés par une plateforme d’observabilité, qui à terme pourrait réduire les coûts totaux en rationalisant les flux de travail DevOps qui sont aujourd’hui souvent plus décousus que quiconque ne veut l’admettre.</p>'}]"
Postman met à disposition un outil d'IA pour la plateforme de gestion des API,"[{'', ""<p>Postman a aujourd'hui mis à disposition de manière générale un outil d'intelligence artificielle (IA) générative pour sa plateforme de création et de gestion d'interfaces de programmation d'applications (API) qui crée automatiquement des tests et de la documentation.</p>""}, {'', ""<p>Annoncé lors d'une conférence POST/CON 2024, Postbot est inclus dans la version 11 de la plateforme Postman qui ajoute également des espaces de travail qui simplifient la collaboration des équipes d'ingénierie logicielle.</p>""}, {'', ""<p>Le PDG de Postman, Abhinav Asthana, a déclaré que l'objectif global est d'améliorer la qualité des API en cours de développement tout en réduisant le niveau de friction qui se produit lors du développement et de la mise à jour des API.</p>""}, {'', '<p>Les développeurs utilisent déjà largement les outils d’IA pour écrire du code, mais Postman utilise une combinaison de grands modèles de langage (LLM) open source et de LLM d’Open AI et de Microsoft pour générer automatiquement des tests et de la documentation en fonction des données collectées par sa plateforme. En plus de faciliter la création de tests API à l’aide d’une interface conversationnelle, la qualité globale de ces tests s’améliorera grâce à des LLM spécifiquement formés à l’aide des données de Postman, a noté Asthana.</p>'}, {'', '<h3>Les développeurs doivent s’appuyer sur des outils d’IA</h3>'}, {'', '<p>C’est un point crucial, car le rapport entre les développeurs et le nombre de services qui exposent les API est beaucoup trop élevé, a déclaré Asthana. Les développeurs devront s’appuyer davantage sur les outils d’IA pour créer et gérer les API, car le nombre de services qu’ils sont censés gérer continue d’augmenter régulièrement.</p>'}, {''}, {'', '<p>Il n’est pas toujours évident de savoir qui, au sein des organisations, gère les API. Les développeurs créent sans aucun doute des API, mais la gestion de ces dernières peut être confiée à une équipe DevOps. À mesure que de plus en plus d’organisations adoptent l’ingénierie de plateforme comme méthodologie de gestion des flux de travail DevOps à grande échelle, il devient de plus en plus probable que les API soient gérées de manière centralisée.</p>'}, {'', '<p>En fin de compte, l’essor des applications basées sur les microservices a considérablement augmenté le nombre d’API à gérer. Les API jouent un rôle crucial dans la mise à disposition des données en externe pour d’autres applications. Cependant, la plupart des API créées aujourd’hui servent à intégrer tous les microservices qui composent un environnement applicatif moderne. Ces API, en grande partie internes, augmentent à un rythme qui rend difficile leur gestion à grande échelle pour les développeurs individuels. En conséquence, de nombreuses organisations sont désormais confrontées à un défi de taille en matière de gestion des changements d’API, a noté Asthana.</p>'}, {'', '<p>Chaque organisation devra déterminer dans quelle mesure la gestion des microservices est durable. Plutôt que de se laisser entraîner dans des débats philosophiques, de nombreuses organisations déploient un mélange de microservices et d’applications monolithiques qui exposent également une API. En effet, les applications monolithiques deviennent de gros microservices. Quelle que soit l’approche, le nombre d’API, y compris les API malveillantes et zombies dont les organisations n’ont pas conscience de l’existence ou qui ont tout simplement été oubliées, est vertigineux. À bien des égards, les API en l’absence d’un cadre de gestion deviennent rapidement une trop bonne chose.</p>'}, {'', '<p>Il existe plusieurs façons de gérer les API, mais il est essentiel de se rappeler que l’approche adoptée doit s’intégrer parfaitement aux outils utilisés pour créer ces API en premier lieu.</p>'}]"
"Exploration des plateformes Low/No-Code, GenAI, Copilots et Générateurs de code","[{'', '<p>L’émergence des plateformes low/no-code remet en question les notions traditionnelles d’expertise en codage. L’époque où le codage était un ensemble de compétences exclusives réservées à quelques personnes averties est révolue. Les plateformes low/no-code ont démocratisé le développement de logiciels. Elles permettent aux personnes issues de milieux non informatiques ou techniques de traduire leurs idées commerciales en applications sans avoir besoin de maîtriser des langages de programmation complexes.</p>'}, {'', ""<p>Ces plateformes low/no-code, avec leurs interfaces glisser-déposer conviviales et leurs composants pré-intégrés, ont simplifié le développement d'applications. Imaginez créer une application pour maison intelligente en quelques clics et glisser-déposer : les plateformes low/no-code rendent cela possible.</p>""}, {'', '<p>Ajoutez à cela l’intelligence artificielle générative (Gen AI) et les nouvelles pratiques de développement changent considérablement le domaine.</p>'}, {'', ""<h3>L'impact de la génération d'IA sur les plateformes Low-Code</h3>""}, {'', ""<p>Les plateformes low-code devraient continuer à évoluer, devenant plus intelligentes, plus adaptables et plus intégrées au cycle de vie du développement logiciel. On assistera à une évolution vers des plateformes qui utilisent l'IA de génération pour comprendre l'intention des utilisateurs, automatiser les tâches de routine et générer des extraits de code complexes.</p>""}, {'', ""<p>Gartner prédit que, d'ici 2026, plus de 80 % des entreprises utiliseront les API et modèles GenAI ou déploieront des applications compatibles GenAI dans des environnements de production, contre moins de 5 % début 2023.</p>""}, {'', '<p>La combinaison du low-code et de l’IA générative permettra aux développeurs d’automatiser les tâches de routine et de se concentrer sur la logique et la créativité de haut niveau. L’IA générative agit comme un multiplicateur qui peut accélérer et optimiser les opérations de développement. McKinsey estime que l’IA générative pourrait ajouter entre 2,6 et 4,4 billions de dollars par an à l’économie mondiale d’ici 2040.</p>'}, {'', '<h3>Un autre outil : les copilotes IA et les générateurs de code</h3>'}, {'', ""<p>Une autre application prometteuse des grands modèles de langage (LLM) de l'IA est celle des assistants basés sur l'IA. C'est là qu'interviennent les copilotes et les générateurs de code, des assistants IA qui simplifient le codage.</p>""}, {'', '<p>Alors que les plateformes low/no-code ont transformé le développement d’applications, les copilotes et les générateurs de code ajoutent une couche supplémentaire d’efficacité.</p>'}, {'', '<p>Les copilotes IA utilisent une interface de chat pour simplifier le codage via une interface de chat, où les développeurs articulent leurs exigences de code dans des invites en langage naturel, et le copilote génère le code en conséquence. Ces assistants fournissent une assistance en temps réel, en proposant des suggestions contextuelles et des extraits de code, et ils peuvent anticiper les bugs potentiels. De plus, ils peuvent expliquer les blocs de code, générer des tests unitaires et proposer des correctifs de bugs, ce qui améliore le processus de codage. Les copilotes IA sont comme des assistants de confiance, prêts à vous aider dans des tâches de codage spécifiques ou à générer du code complexe - et ils le font rapidement.</p>'}, {'<h3>La grande question</h3>', ''}, {'', '<p>Avec l’essor des plateformes low/no-code, les compétences requises dans le secteur technologique vont évoluer. L’époque où la maîtrise de plusieurs langages de programmation était indispensable est révolue.</p>'}, {'', '<p>Les outils low/no-code occupant une place centrale, cela signifie-t-il que le codage devient obsolète ?</p>'}, {'', '<p>Un grand non.</p>'}, {'', '<p>Si les connaissances de base en programmation restent précieuses, elles ne constituent plus la seule condition requise pour créer des applications innovantes. Grâce aux plateformes low/no-code qui démocratisent le développement d’applications, des personnes d’horizons divers peuvent exploiter la technologie pour créer des solutions efficaces.</p>'}, {'', '<p>GenAI, les copilotes et les générateurs de code rendent-ils les plateformes low/no-code obsolètes\xa0?</p>'}, {'', '<p>Encore une fois, non.</p>'}, {'', '<p>Si GenAI, copilots et générateurs de code excellent dans certaines tâches spécifiques, les plateformes low/no code offrent une solution holistique, combinant conception d’interface visuelle, intégration de bases de données et bien plus encore. Ce sont les couteaux suisses du développement, répondant à un large éventail de besoins au-delà de la simple génération de code.</p>'}, {'', '<p>L’IA de génération, les copilotes et les générateurs de code ne sont pas là pour remplacer les plateformes low/no-code ou les pratiques de codage traditionnelles. Au contraire, ils sont positionnés pour les compléter et les améliorer. La fusion de ces technologies est très prometteuse pour simplifier le développement de logiciels, accroître l’efficacité et ouvrir de nouvelles voies d’innovation.</p>'}, {'', '<h3>Le dernier mot</h3>'}, {'', '<p>Grâce à de meilleures suggestions de code et à des capacités de test automatisées, les plateformes low/no-code offrent des solutions de haute qualité qui répondent aux exigences des utilisateurs férus de technologie d’aujourd’hui.</p>'}, {'', '<p>Grâce à la combinaison passionnante des plateformes low/no-code et de GenAI, l’avenir du développement logiciel est prometteur. Si ces avancées peuvent bouleverser le codage traditionnel, elles offrent également des opportunités d’innovation et de créativité.</p>'}, {'', '<p>Il est temps pour nous d’accepter ce changement et de construire un avenir meilleur, une ligne de code (ou son absence) à la fois.</p>'}]"
Comment migrer une plateforme d'observabilité vers l'open source,"[{'', ""<p>Il est difficile d'obtenir une observabilité complète des applications d'entreprise. Si des services tiers comme New Relic ou Datadog facilitent l'observabilité de bout en bout, à mesure que votre application devient plus complexe, les données de télémétrie le deviennent également, ce qui entraîne une augmentation des coûts. La migration vers une pile open source vous permet de contrôler les données de télémétrie et de réduire les coûts d'observabilité, malgré les défis liés aux engagements des fournisseurs de services existants.</p>""}, {'', ""<p>La migration vers des solutions open source est simple avec une architecture et des piles technologiques simples. Cependant, la transition de l'ensemble de la pile d'observabilité vers l'open source exige une planification méticuleuse, des évaluations d'outils/frameworks, des tests, une analyse des risques et une communication entre les équipes, en particulier compte tenu du nombre de microservices, de la forte dépendance au cloud et de la diversité des langages et frameworks à prendre en compte.</p>""}, {'', ""<p>Une plateforme d'observabilité cloud absorbe généralement 20 à 30 % des dépenses globales d'infrastructure, mais dans certains cas, elle peut atteindre 50 à 60 %. Si vos dépenses d'observabilité dépassent 50 %, la transition vers des solutions open source serait une option judicieuse pour atténuer les coûts de l'infrastructure technologique.</p>""}, {'', '<p>Depuis deux ans, je travaille à la migration de la plateforme d’observabilité d’un fournisseur de solutions propriétaire vers une pile open source. Plongeons-nous dans les étapes clés nécessaires à une telle migration.</p>'}, {'', '<h3>Finaliser les données et systèmes clés de télémétrie</h3>'}, {'', ""<p>Les plateformes d'observabilité gérées offrent des informations complètes sur l'état du système, ce qui nécessite un volume important de données de télémétrie de haute qualité. Par défaut, les agents chargés de capturer et de transmettre les données de télémétrie sont configurés pour recueillir autant d'informations que possible, ce qui facilite la création de tableaux de bord et de rapports complets. Cependant, cette collecte de données extensive contribue au coût global.</p>""}, {'', '<p>Sélectionnez les systèmes que vous devez réellement surveiller. Une application d’entreprise typique comprend des bases de données, des bases de données de mise en cache, des orchestrateurs de conteneurs et de nombreux services cloud. Les ingénieurs surveillent souvent plus de services que nécessaire, ce qui entraîne des complexités et des coûts inutiles. Il suffit souvent de rationaliser le périmètre de surveillance aux systèmes essentiels.</p>'}, {'', '<p>D’après mon expérience, la quantité de données de télémétrie requise est nettement inférieure à celle que les services tiers récupèrent généralement. Vous pouvez facilement diviser ces collectes de données en deux catégories\xa0: les données indispensables et les données utiles. Les données «\xa0indispensables\xa0» correspondent aux quatre signaux d’or bien établis\xa0:</p>'}, {'', '<li>Latence : le temps nécessaire pour répondre à une requête</li>'}, {'', ""<li>Trafic : le volume de requêtes qu'un système traite actuellement</li>""}, {'', ""<li>Le taux d'erreur : le nombre de requêtes qui échouent ou renvoient des réponses inattendues</li>""}, {'', '<li>Saturation des ressources : le pourcentage de ressources disponibles consommées</li>'}, {'', ""<p>Ces signaux fondamentaux servent de base aux tableaux de bord et aux alertes clés. Les plateformes d'observabilité construites sur ces signaux couvrent 80 % des cas d'utilisation typiques.</p>""}, {'', '<p>Il est essentiel de comprendre les exigences en matière de métadonnées de chaque signal. L’étendue des métadonnées capturées a un impact direct sur la complexité et le coût de la plateforme d’observabilité. Par exemple, lors de la surveillance de la latence d’un service à l’autre, tenez compte de la nécessité d’adresses IP de service, d’ID d’instance ou d’informations d’en-tête.</p>'}, {'', '<h3>Sélectionnez la pile concernée</h3>'}, {'', ""<p>Les plateformes d'observabilité gérées telles que New Relic et Datadog offrent une surveillance complète de divers composants d'entreprise. Cependant, lorsque vous migrez vers une pile open source, vous devez évaluer et intégrer les piles d'outils pour répondre à diverses exigences de surveillance.</p>""}, {'', ""<p>Un aspect important à prendre en compte est la mise à l'échelle : comment gérer l'énorme quantité de données générées chaque minute sur tous ces systèmes. Concentrez-vous sur deux fronts : la sélection de la pile pour le stockage et le traitement des données de télémétrie (journaux, métriques et traces) et la conception de méthodes pour capturer et transmettre les données de télémétrie à partir de divers systèmes.</p>""}, {'', '<h4>Pile pour les bûches</h4>'}, {'', ""<p>Vous avez besoin d'une pile capable de traiter et de stocker de manière efficace et économique le volume considérable de journaux générés par le système. Par le passé, j'utilisais la pile ELK pour stocker et rechercher des journaux, mais il s'agit d'une solution générique et non conçue spécifiquement pour les journaux.</p>""}, {'', '<p>Je recommande Loki de Grafana pour sa gestion efficace des gros volumes de journaux et LogQL, un langage proche de PromQL. Si vous connaissez PromQL, la navigation dans des données de journaux volumineuses avec LogQL est simple et intuitive.</p>'}, {'', '<h4>Pile pour les métriques</h4>'}, {'', ""<p>Prometheus est une base de données populaire pour le stockage de données de mesures de séries chronologiques. Cependant, elle présente des limitations liées à la mise à l'échelle ; elle ne peut pas être mise à l'échelle horizontalement. Des alternatives comme Thanos, Mimir de Grafana et VictoriaMetrics offrent de meilleures solutions prêtes à l'emploi pour la mise à l'échelle horizontale.</p>""}, {'', '<p>J’ai effectué quelques recherches et j’ai finalement choisi Mimir de Grafana pour stocker les métriques dans un projet de migration similaire. Cette décision était basée sur la capacité de stockage à distance de Mimir, ainsi que sur son architecture évolutive et hautement disponible.</p>'}, {'', '<h3>Pile de traçage</h3>'}, {'', '<p>Le traçage distribué est indispensable dans une architecture de microservices, pour identifier facilement les goulots d’étranglement en termes de latence et pour résoudre les bugs liés aux performances. Tempo de Grafana pourrait être une bonne option pour plusieurs raisons :</p>'}, {'', ""<li>Il peut s'intégrer de manière transparente en tant que backend pour le tableau de bord de Grafana. La consolidation des tableaux de bord dans Grafana permet d'éviter de naviguer entre plusieurs applications. Changer de plateforme uniquement pour le traçage tout en utilisant Grafana pour d'autres aspects de l'observabilité ne serait pas pratique.</li>""}, {'', ""<li>Tempo offre une expérience similaire à TraceQL lors de l'interaction avec la base de données, simplifiant le processus d'apprentissage et réduisant considérablement la courbe d'apprentissage.</li>""}, {'', '<li>Tempo est compatible avec les protocoles de traçage open source les plus répandus, tels que Zipkin et Jaeger. Si une équipe utilise déjà ces protocoles, la transition vers Tempo de Grafana se fera plus facilement.</li>'}, {'', ""<p>Vous devez installer un agent sur le système pour capturer et transmettre les données de télémétrie. Ces étapes varient en fonction du système surveillé. Les configurations d'applications d'entreprise classiques impliquent la surveillance des intégrations cloud, la surveillance des processus, des hôtes d'infrastructure, des clusters Kubernetes et la surveillance des applications. Parmi les possibilités :</p>""}, {'', '<li>Intégrations cloud : cela inclut la surveillance des services cloud tels que SQS, SNS, EMR et EC2.</li>'}, {'', ""<li>Surveillance des processus : cela implique la surveillance des processus exécutés sur des machines bare-metal. Avec l'avènement de la dockerisation et de Kubernetes, il existe désormais une manière standardisée de démarrer l'application. Dans le passé, il n'y avait pas de mécanisme fixe. Par exemple, pour exécuter une application Java, vous pouviez utiliser une application Java, la commande « java -jar », Tomcat ou OS systemctl. Dans le cas de node, il pourrait s'agir de npm ou de PM2. Chaque équipe ou service peut avoir sa propre façon de démarrer le processus.</li>""}, {'', ""<li>Hôte d'infrastructure\xa0: cela implique de surveiller la machine elle-même pour des mesures telles que l'utilisation du processeur, la mémoire, les E/S de disque et les E/S réseau ou de vérifier si la machine est hors ligne.</li>""}, {'', '<li>Surveillance Kubernetes\xa0: vous devez régulièrement surveiller un cluster Kubernetes, par exemple dans les cas où Kubernetes ne parvient pas à planifier un pod en raison de ressources insuffisantes.</li>'}, {'', ""<li>Surveillance des applications : cette surveillance se concentre sur la supervision des services créés par l'équipe. Chaque service peut différer dans son approche de développement et son choix de pile technologique, mais du point de vue de l'observabilité, ils sont généralement traités de la même manière.</li>""}, {'', ""<li>Surveillance du navigateur et des appareils mobiles\xa0: ces mesures garantissent des performances et une expérience utilisateur optimales sur différentes plateformes. La surveillance du navigateur comprend le suivi des temps de chargement des pages, des performances de rendu, des erreurs JavaScript et de l'utilisation des ressources. Pour les appareils mobiles, elle comprend la surveillance des plantages d'applications, de la latence, de l'utilisation de la batterie, des requêtes réseau et des mesures spécifiques à l'appareil.</li>""}, {'', ""<p>Avant l'avènement d'OpenTelemetry, il n'existait aucune méthode standardisée de surveillance des applications. OpenTelemetry est né de la fusion de deux projets antérieurs, OpenTracing et OpenCensus. Il s'agit d'un cadre indépendant des fournisseurs et des outils permettant d'instrumenter des applications ou des systèmes indépendamment de la langue, de l'infrastructure ou de l'environnement d'exécution. OpenTelemetry représente un effort communautaire important, et sa popularité et sa stabilité ne cessent de croître.</p>""}, {'', ""<h3>Valider la pile d'observabilité sur l'architecture d'application</h3>""}, {'', ""<p>Les applications varient en termes d'architecture et de stades de développement. Pour répondre à cette diversité, vous avez besoin de plusieurs preuves de concept (POC) sur différents systèmes, tels que l'infrastructure, le service cloud et les applications back-end et front-end.</p>""}, {'', ""<p>La migration d'une plateforme d'observabilité d'une pile technologique à une autre est simple lorsque l'architecture de l'application est monolithique. Cependant, lorsque l'architecture sous-jacente est constituée de microservices, cela devient difficile.</p>""}, {'', ""<p>Bien que l'architecture des microservices offre une flexibilité dans le choix de différentes piles technologiques pour la création de services, la conception d'une solution standard pour la capture de données de télémétrie devient un défi en raison de la diversité des piles technologiques impliquées.</p>""}, {'', '<p>Il est nécessaire de réaliser des POC pour différentes combinaisons technologiques afin de créer un outil en libre-service que les équipes individuelles peuvent suivre pour migrer facilement leurs services. Par exemple, si votre service utilise Java 11, Spring Boot 3.x.x et PostgreSQL, ces POC peuvent fournir des étapes standardisées pour permettre la surveillance des applications.</p>'}, {'', '<h3>Migrer les composants principaux</h3>'}, {'', '<p>La mise en place de la plateforme d’observabilité implique la configuration de la pile pour les métriques, les journaux et les traces, ainsi que l’installation des agents nécessaires. L’un des avantages de la migration de la plateforme d’observabilité est qu’il n’est pas nécessaire de transférer les anciennes données de télémétrie. Pendant la phase de test, vous disposerez d’une période de chevauchement pour accumuler suffisamment de données utiles pour créer des tableaux de bord et des alertes spécifiques.</p>'}, {'', ""<p>La migration des alertes et des tableaux de bord vers le nouveau système est essentielle, même lorsque la migration des données n'est pas nécessaire. En effet, les modifications apportées aux techniques sous-jacentes de capture des mesures peuvent modifier les noms des mesures et, par la suite, toutes les expressions de requête utilisées dans les alertes et les tableaux de bord. Certaines mesures peuvent ne pas être disponibles dans certains scénarios, en particulier les mesures dérivées qui peuvent être recréées à partir de mesures sous-jacentes.</p>""}, {'', ""<p>Bien que la migration manuelle des expressions de requête soit une option, un processus manuel est sujet à des erreurs et prend du temps pour un grand nombre d'alertes. J'ai connu une situation similaire avec des numéros d'alerte allant de 100 à 400.</p>""}, {'', '<p>Pour simplifier la migration, nous avons développé un script Node.js qui convertit par programmation les expressions de requête de New Relic en expressions Prometheus. Le script a effectué les étapes de haut niveau suivantes\xa0:</p>'}, {'', ""<li>Connecté au serveur API New Relic et récupéré toutes les alertes configurées pour l'intégration cloud</li>""}, {'', ""<li>Conversion des expressions d'alerte New Relic en expressions d'alerte PromQL</li>""}, {'', ""<li>J'ai écrit toutes les expressions d'alerte Prometheus dans le fichier YAML</li>""}, {'', '<p>La mise en œuvre de cette technique nous a permis de migrer avec succès toutes les alertes et les tableaux de bord en quatre ou cinq jours, un processus qui nécessiterait normalement un mois de travail manuel. De plus, les scripts ont considérablement réduit le risque d’erreur humaine.</p>'}, {'', '<h3>Tester la migration</h3>'}, {'', '<p>Vérifiez chaque alerte et chaque tableau de bord. Exécutez à la fois les anciennes et les nouvelles plateformes pendant les tests. Le fonctionnement en parallèle permet de tester en profondeur les métriques.</p>'}, {'', ""<p>Cependant, l'exécution simultanée des deux systèmes peut entraîner une baisse des performances en raison de l'envoi de données de télémétrie par les services à deux emplacements. De plus, il peut y avoir une légère augmentation des coûts jusqu'à la transition complète.</p>""}, {'', '<p>L’exécution des deux systèmes en parallèle fournit une configuration idéale pour garantir que les tableaux de bord et les alertes migrés fonctionnent correctement.</p>'}, {'', ""<p>Il est simple de tester les alertes déclenchées au cours de ce processus, mais évaluer celles qui ne se sont pas encore déclenchées constitue un défi. Dans le cadre d'un projet, j'ai réduit la valeur seuil de chaque alerte et testé ses fonctionnalités. Environ 90 % des alertes générées par des scripts personnalisés ont fonctionné de manière transparente, et seulement 10 % ont nécessité quelques ajustements manuels.</p>""}, {'', ""<h3>Migrer d'autres composants associés</h3>""}, {'', ""<p>D'autres systèmes dépendent des données générées par la plateforme d'observabilité. Par exemple :</p>""}, {'', ""<li>Systèmes de notification comme le courrier électronique, le canal Slack, etc., qui alertent en cas d'incident</li>""}, {'', '<li>Les outils de gestion des incidents comme PagerDuty offrent un moyen simplifié de gérer les incidents.</li>'}, {'', ""<p>Ces systèmes s'appuient sur la charge utile de l'alerte pour fonctionner correctement. Leurs ressources doivent être mises à jour, car un changement dans la plateforme d'observabilité entraîne un changement dans la charge utile de l'alerte. Mettez à jour les modèles des systèmes de notification pour vous assurer qu'ils s'intègrent parfaitement à la nouvelle charge utile de l'alerte.</p>""}, {'', '<p>De même, les outils de gestion des incidents comme PagerDuty nécessitent des modifications des règles de routage, des politiques d’escalade et de la planification. Heureusement, des outils de migration open source prêts à l’emploi sont disponibles pour faciliter la migration de PagerDuty vers OnCall de Grafana. Cependant, dans d’autres solutions, vous n’aurez peut-être pas accès à des outils de migration prêts à l’emploi. Dans de tels cas, une migration manuelle ou la rédaction de scripts peuvent être nécessaires.</p>'}, {'', '<h3>Calendrier de la migration</h3>'}, {'', ""<p>La planification d'une migration implique la finalisation des données et des systèmes de télémétrie clés, la sélection des piles pertinentes et la validation de la pile d'observabilité sur l'architecture de l'application. Cela prend généralement environ trois semaines.</p>""}, {'', ""<p>La durée du projet dépend de facteurs tels que le nombre d'équipes impliquées et la quantité de services actifs. Par exemple, une migration vers un environnement open source avec 100 microservices et la participation de 10 équipes différentes peut prendre quatre mois.</p>""}, {'', '<h3>Migration terminée\xa0!</h3>'}, {'', '<p>La transition de votre plateforme d’observabilité vers une pile open source offre une voie prometteuse pour réduire les coûts et renforcer le contrôle des données de télémétrie. Cependant, cette migration exige une planification et une exécution méticuleuses, englobant des étapes essentielles telles que la priorisation des fonctionnalités, la sélection de la pile, les POC, la migration des composants principaux, les tests et la migration des systèmes associés. Malgré les défis posés par la diversité des architectures et des technologies, une approche systématique, une évaluation complète et une collaboration entre les équipes peuvent faciliter un processus de migration fluide et garantir son succès.</p>'}]"
Rocket Software rend les tests des applications iSeries plus sûrs,"[{'', ""<p>Cette semaine, Rocket Software a étendu sa plateforme DevOps pour les plateformes iSeries d'IBM afin de simplifier les tests d'applications de manière à garantir que les données sensibles ne soient pas exposées par inadvertance aux développeurs d'applications.</p>""}, {'', ""<p>Julianna Cammarano, vice-présidente des produits pour les applications chez Rocket Software, a déclaré que la mise à jour permet aux équipes DevOps d'appliquer des politiques qui empêchent les développeurs de pouvoir consulter des données sensibles, telles que les numéros de carte de crédit ou de sécurité sociale, qui pourraient entraîner une violation de la conformité.</p>""}, {'', '<p>La capacité de masquage permet aux équipes DevOps de continuer à fournir l’accès aux données nécessaires pour tester les applications sans risquer d’éventuelles amendes qui pourraient leur être infligées en cas d’échec à un audit, a-t-elle ajouté. En l’absence de cette capacité, les organisations qui doivent se conformer à un certain nombre de réglementations ne sont pas en mesure de donner aux développeurs l’accès aux ensembles de données exécutés dans des environnements de production pour tester leurs applications, a déclaré Cammarano.</p>'}, {''}, {'', ""<p>Comme c'est le cas avec la plupart des plateformes IBM existantes, l'adoption des meilleures pratiques DevOps est inégale. De nombreuses organisations utilisent désormais un mélange de méthodologies en cascade et DevOps pour créer et déployer des applications. Dans certains cas, une application rarement mise à jour se prête davantage à une méthodologie en cascade qu'à un flux de travail DevOps conçu pour permettre aux organisations de mettre à jour fréquemment leurs logiciels.</p>""}, {'', ""<p>Le défi auquel sont confrontées les entreprises qui utilisent des applications iSeries est d'attirer les développeurs et les ingénieurs logiciels vers une plateforme héritée, a souligné Cammarano. La seule façon de les inciter à travailler sur une plateforme héritée est de s'assurer qu'ils ont accès à des outils DevOps modernes facilement accessibles via un portail centralisé, a-t-elle ajouté.</p>""}, {'', ""<p>On ne sait pas exactement combien de nouvelles applications sont en cours de création pour la plateforme iSeries, mais il existe encore un nombre important d'applications héritées exécutées dans des environnements de production que les organisations peuvent choisir de moderniser plutôt que de remplacer, surtout si le coût de remplacement de ces applications est prohibitif, a déclaré Cammarano.</p>""}, {'', ""<p>IBM, pour sa part, semble déterminé à assurer le support de la plateforme iSeries jusqu'en 2032 et au-delà, puisqu'IBM utilise désormais la même architecture de serveur développée pour ses plateformes Unix P-Series pour construire les serveurs iSeries de nouvelle génération. Cette approche garantit que les performances des applications iSeries continueront de s'améliorer à mesure qu'IBM investira davantage dans les processeurs RISC utilisés actuellement sur les deux plateformes.</p>""}, {'', '<p>Bien entendu, rares sont les entreprises qui ne créent et ne déploient pas également des applications sur d’autres plateformes. Celles qui disposent de plateformes iSeries cherchent généralement à adopter un ensemble cohérent de workflows DevOps pouvant être appliqués à plusieurs plateformes qui continuent de fonctionner dans des environnements informatiques sur site, principalement parce que la migration des applications iSeries vers une autre plateforme nécessite beaucoup de temps et d’efforts.</p>'}, {'', '<p>En fin de compte, chaque organisation doit déterminer le niveau d’adoption de DevOps qui correspond le mieux à ses besoins, en fonction de son degré de dépendance à une plateforme spécifique. Une chose est sûre : de nombreuses organisations, à mesure que les processus de développement et de déploiement d’applications s’automatisent, découvriront que DevOps est un parcours sans fin.</p>'}]"
Grafana Labs rend l'observabilité plus accessible,"[{'', '<p>Aujourd’hui, lors d’un événement GrafanaCon, Grafana Labs a dévoilé une mise à jour de sa plateforme de visualisation principale Grafana. La société propose un module Explore Metric qui, selon elle, peut identifier la cause profonde d’un problème, et ce, sans que personne n’ait à lancer une requête sur la plateforme open source Prometheus qui est au cœur de la plateforme d’observabilité Grafana Cloud.</p>'}, {'', '<p>Ce n’est pas la seule amélioration. La version 11 de Grafana facilite également la modification et le partage des tableaux de bord, en plus de rationaliser le nombre d’alertes qu’un seul incident peut générer. De plus, Grafana Labs met désormais à disposition une édition organisée du logiciel OpenTelemetry baptisée Grafana Alloy, qu’elle étendra et prendra en charge parallèlement au reste de son portefeuille de logiciels open source.</p>'}, {'', '<p>Enfin, la société met à jour la gestion des journaux Loki pour simplifier la rationalisation du stockage des données des journaux.</p>'}, {'', ""<p>Thomas Wilkie, directeur technique de Grafana Labs, a déclaré que collectivement, ces mises à jour permettront à un plus large éventail de professionnels de l'informatique, y compris les développeurs d'applications, d'invoquer plus facilement les capacités d'une plateforme Grafana Cloud basée sur un logiciel open source.</p>""}, {''}, {'', '<p>À mesure que les environnements informatiques deviennent plus complexes, le besoin de plateformes d’observabilité devient plus prononcé. Les équipes informatiques ont généralement accès à une multitude d’outils de surveillance pour suivre des mesures prédéfinies, mais leur capacité à lancer des requêtes pour faire apparaître la cause profonde d’un problème est limitée. À mesure que les plateformes d’observabilité continuent d’évoluer, la frontière entre observabilité et surveillance continue de s’estomper, a déclaré Wilkie.</p>'}, {'', '<p>Il est moins évident de savoir dans quelle mesure les plateformes d’observabilité pourraient permettre aux équipes informatiques de rationaliser les outils de surveillance existants. En attendant, le rythme auquel les équipes informatiques adoptent l’observabilité varie considérablement d’une organisation à l’autre. Les équipes informatiques ont besoin de suffisamment d’expertise pour lancer des requêtes pertinentes afin d’obtenir le plein retour sur investissement d’une plateforme d’observabilité. En l’absence de cette expertise, l’acquisition d’une plateforme d’observabilité devient moins intéressante à court terme.</p>'}, {'', '<p>En théorie, un jour viendra où les algorithmes d’apprentissage automatique et d’autres formes d’intelligence artificielle (IA) permettront aux entreprises d’identifier plus facilement les problèmes sans avoir à élaborer de requêtes. Mais il est peu probable que l’IA élimine le besoin d’ingénieurs en fiabilité des sites (SRE), a déclaré Wilkie. Au contraire, l’IA simplifiera l’intégration de nouveaux membres dans une équipe DevOps tout en déployant plus d’applications que jamais, a-t-il ajouté.</p>'}, {'', '<p>L’adoption de l’IA et de l’observabilité est encore très récente, mais une convergence inévitable se produira. L’objectif général devrait être de faciliter le dépannage des environnements d’applications hautement distribués qui couvrent aujourd’hui les applications cloud, sur site et en périphérie. C’est essentiel, car à mesure que les environnements d’application deviennent de plus en plus distribués, la taille globale de la plupart des équipes informatiques chargées de les gérer n’augmente pas. Par conséquent, le rythme auquel les flux de travail DevOps peuvent être automatisés s’avérera crucial.</p>'}, {'', '<p>Dans l’intervalle, les équipes informatiques doivent s’attendre à ce qu’il devienne plus difficile de trier les environnements informatiques complexes avant que cela ne devienne plus simple grâce à l’essor de l’observabilité, de l’automatisation et de l’IA.</p>'}, {'', '<p>Crédit photo : tito pixel sur Unsplash</p>'}]"
La gestion moderne des applications nécessite une visibilité plus approfondie sur Internet,"[{'', '<p>La plupart des équipes informatiques utilisent régulièrement une gamme d’outils pour surveiller les performances des applications. Mais à une époque où les applications s’exécutent partout, du cloud à la périphérie du réseau, la visibilité offerte est limitée. Les mesures et analyses qui apparaissent sont à peu près comparables à la vision à quelques mètres seulement lorsque l’on conduit un véhicule par une nuit pluvieuse. La gestion des applications modernes nécessite plutôt des plateformes offrant l’équivalent de capacités radar et sonar, afin que les équipes informatiques aient une meilleure visibilité sur les environnements applicatifs qui restent aujourd’hui bien trop voilés pour être gérés efficacement.</p>'}, {'', ""<p>Pour atteindre cet objectif, il faut mieux comprendre l'impact des différents services Internet sur les performances des applications. Presque toutes les applications dépendent de la qualité d'un ou plusieurs de ces services. Plus les services applicatifs qui dépendent d'Internet sont nombreux, plus il est probable que quelqu'un rencontre des problèmes intermittents ayant un impact sur les performances de manière difficile à comprendre, et encore moins à prévoir.</p>""}, {'', '<p>Malheureusement, la plupart des équipes informatiques ont aujourd’hui une capacité limitée à discerner l’impact des performances des services Internet sur leurs applications. Il existe bien sûr des outils de surveillance des performances Internet (IPM) capables de faire apparaître des mesures de performances réseau. Le défi et l’opportunité sont désormais de faire apparaître ces mesures dans le contexte de toutes les autres données de télémétrie que les équipes DevOps collectent à partir des différentes plateformes de gestion des performances des applications (APM) et d’observabilité sur lesquelles elles s’appuient pour surveiller et dépanner les environnements applicatifs.</p>'}, {'<h3>Pourquoi les applications deviennent de plus en plus distribuées</h3>', ''}, {'', '<p>Tout d’abord, avec l’essor du cloud computing, suivi de la prolifération des applications SaaS (Software-as-a-Service) et désormais de l’edge computing, les applications deviennent de plus en plus distribuées de jour en jour. Le problème est que chaque application est appelée sur un réseau qui ajoute de la latence, et les plateformes APM et d’observabilité existantes ne peuvent pas l’analyser efficacement.</p>'}, {'', '<p>La vitesse à laquelle les applications deviennent de plus en plus distribuées ne fera qu’augmenter. De plus en plus de données sont traitées et analysées à la périphérie du réseau à mesure que les entreprises déploient des applications en temps quasi réel. Au lieu de transférer les données brutes vers le cloud, ces applications traitent et analysent les données au moment où elles sont créées et consommées. Ce n’est qu’à ce moment-là qu’une application diffuse ou transmet les résultats agrégés à un service cloud ou à un environnement informatique sur site pour une analyse plus approfondie.</p>'}, {'', '<p>Cependant, la plupart des équipes DevOps manquent de visibilité sur l’impact des services Internet sur les applications plus hautement distribuées, au-delà du service cloud ou de l’environnement informatique sur site dans lequel elles ont historiquement déployé leurs applications.</p>'}, {'', '<h3>Angles morts sur Internet</h3>'}, {'', '<p>Il existe trois grandes catégories d’angles morts qui ont un impact sur les performances des applications distribuées. La première, et sans doute la plus opaque, est celle des services fournis par des fournisseurs tiers. Qu’il s’agisse de réseaux de diffusion de contenu (CDN) ou d’applications SaaS (Software-as-a-Service), chacun de ces services est contrôlé par un fournisseur de services externe qui n’autorise généralement pas une équipe DevOps à collecter des données de télémétrie en déployant un logiciel agent dans ses environnements informatiques. Au mieux, ils peuvent exposer une interface de programmation d’application (API) pour permettre une approche sans agent de collecte de données, mais cette méthode n’offre généralement pas le niveau de contrôle requis pour optimiser les performances des applications.</p>'}, {'', ""<p>Le deuxième grand angle mort se situe au niveau de l'appareil. Les erreurs de configuration dans les navigateurs peuvent avoir un impact significatif sur les temps de chargement, ce qui a un impact négatif sur les performances des applications, par exemple.</p>""}, {'', '<p>Enfin, diverses plateformes et protocoles allant des serveurs de noms de domaine (DNS) aux services de publication et d’abonnement peuvent avoir un impact sur les applications d’une manière qu’une plateforme APM ou d’observabilité ne détectera pas.</p>'}, {'', ""<p>La seule façon de fournir le niveau de visibilité requis est de tirer parti d'une plateforme IPM qui collecte des données de télémétrie à partir d'un réseau mondial, composé de milliers de points de présence (PoP), pour déterminer précisément quel niveau de service Internet est fourni à un point de terminaison donné.</p>""}, {'', '<h3>Contexte et contrôle</h3>'}, {'', ""<p>L'intégration des données de télémétrie collectées via un IPM a fourni aux équipes DevOps un contexte pour mieux dépanner les applications distribuées. Au lieu de se demander si un certain morceau de code est à blâmer, une équipe DevOps peut immédiatement déterminer si le trafic Internet passant par un emplacement spécifique crée une latence supplémentaire ou bloque complètement l'accès.</p>""}, {'', '<p>Forte de ces informations, l’équipe peut contacter le fournisseur de services Internet pour résoudre le problème ou, si nécessaire, rediriger l’application via un autre service pour garantir la disponibilité de l’application.</p>'}, {'', '<p>Quelle que soit l’approche, l’époque où les équipes DevOps étaient soumises aux aléas des services réseau, sur lesquels elles n’avaient que peu ou pas d’influence, touche à sa fin.</p>'}, {'', '<h3>DevOps rencontre Netops</h3>'}, {'', '<p>L’intégration des plateformes IPM et APM ou d’observabilité crée une base sur laquelle les meilleures pratiques de gestion des opérations réseau (NetOps) et DevOps peuvent enfin être fusionnées. Au lieu de devoir réunir des « salles de guerre » pour découvrir la cause profonde d’un problème, les deux équipes pourront instantanément visualiser dans quelle mesure les services réseau peuvent avoir un impact négatif sur les services réseau. Tout ce temps passé à deviner quelle pourrait être la cause d’un problème peut désormais être consacré à la résolution du problème.</p>'}, {'', '<p>À une époque où les applications n’ont jamais été aussi dépendantes des services réseau, la nécessité de converger la gestion des flux de travail NetOps et DevOps n’a jamais été aussi critique.</p>'}, {'', '<h3>Conclusion</h3>'}, {'', '<p>La plupart des équipes DevOps savent pertinemment qu’il existe souvent des écarts majeurs entre ce que pensent les équipes DevOps qui s’appuient sur les plateformes APM et d’observabilité et ce qui se passe réellement dans un environnement informatique distribué. Le problème est qu’elles n’ont généralement pas la visibilité nécessaire pour déterminer la meilleure marche à suivre pour résoudre le problème. À mesure que les IPM s’intègrent davantage aux plateformes APM et d’observabilité, les angles morts du réseau qui empêchaient auparavant de découvrir la cause profonde d’un problème seront bientôt presque tous éliminés.</p>'}, {'', '<p>Bien entendu, chaque équipe DevOps doit déterminer dans quelle mesure l’obtention de ces informations est une priorité. Cependant, compte tenu du nombre de problèmes que les équipes DevOps tentent de résoudre et qui peuvent être attribués à un service Internet, il est clair que le niveau global de stress et de travail auquel elles sont régulièrement confrontées pourrait être considérablement réduit en offrant simplement une meilleure visibilité réseau sur ces services.</p>'}]"
Créer une plateforme d'observabilité open source,"[{'', ""<p>Les entreprises sont soumises à une pression constante pour garantir la disponibilité de leur infrastructure informatique et de leurs applications tout au long de l'année. La complexité des architectures modernes (conteneurs, cloud hybride, SOA, microservices, etc.) ne cesse de croître, générant d'énormes volumes de journaux ingérables. Nous avons besoin d'outils intelligents de gestion des performances des applications (APM) et d'observabilité pour atteindre l'excellence de la production et atteindre les objectifs de disponibilité et de disponibilité. Il s'agit notamment d'analyser l'état des applications, les performances et l'expérience utilisateur. L'adoption de techniques d'apprentissage automatique pour identifier les anomalies et les modèles de comportement permettra de détecter rapidement la cause profonde et de respecter les accords de niveau de service (SLA) des clients.</p>""}, {'', '<p>Le marché des outils d’APM et d’observabilité est sans aucun doute en plein essor. Ces outils ingèrent plusieurs flux de données de télémétrie et constituent de puissantes plateformes d’analyse fournissant des informations essentielles sur la santé des applications et des infrastructures, y compris les performances du système. Les équipes de développement de logiciels qui adoptent l’observabilité sont bien mieux équipées pour publier leur code d’application de manière itérative. Selon une étude de « MarketsandMarkets », la taille du marché des outils et plateformes d’observabilité devrait passer de 2,4 milliards de dollars en 2023 à plus de 4 milliards de dollars d’ici 2028, à un taux de croissance annuel composé (TCAC) de 11,7 %.</p>'}, {'', ""<h3>Qu'est-ce que l'observabilité ?</h3>""}, {'', ""<p>L'observabilité est la capacité à collecter des données sur les applications distribuées, l'infrastructure et la communication entre ses composants et services internes et externes, permettant aux équipes de déboguer leurs systèmes avec diligence. Elle permet aux équipes d'ingénierie de fiabilité du site (SRE), d'ingénierie logicielle et d'exploitation d'analyser l'impact sur le client et de trier une panne de service. L'observabilité et la surveillance sont parfois utilisées de manière interchangeable. L'observabilité (proactive) rend les données accessibles et vous permet de poser n'importe quelle question sur le système pour savoir plus en profondeur comment le code se comporte. La surveillance (réactive) est la tâche de collecte et d'affichage de ces données et la capacité de déterminer l'état général du système. L'observabilité peut être divisée en trois piliers clés : les journaux, les traces et les métriques, qui sont essentiels pour l'observabilité SRE.</p>""}, {'', ""<p>• Les journaux nous aident à diagnostiquer les problèmes et à nous dire pourquoi ils se sont produits.• Les traces nous aident à isoler les problèmes et à nous dire où ils se sont produits.• Les métriques nous aident à détecter les problèmes et à nous dire ce qui s'est passé.</p>""}, {'', '<h3>Outils, capacités et défis du marché</h3>'}, {'', '<p>Le quadrant magique de Gartner pour l’APM et l’observabilité a identifié plus de 20 produits de fournisseurs offrant des capacités d’APM et d’observabilité, y compris des déploiements auto-hébergés, gérés par le fournisseur ou SaaS. Ces produits offrent de nombreuses fonctionnalités, notamment des mesures de performances des applications, la surveillance et les alertes sur les événements, la traçabilité, la détection des anomalies et des vulnérabilités, etc.</p>'}, {'', ""<p>Une application métier d'entreprise comprend des applications développées en interne (telles que .NET, Java, Python, SQL, NoSQL DB, etc.), des produits tiers/prêts à l'emploi (tels que Salesforce, HubSpot, etc.) et des intégrations (telles que Stripe, PayPal, etc.). Les applications développées en interne sont hébergées dans un centre de données sur site ou par des fournisseurs de cloud comme AWS, GCP ou Azure. Les produits prêts à l'emploi sont basés sur SaaS ou intégrés via des API. Il existe des applications hautement distribuées couvrant des dizaines et des centaines de nœuds, de services et d'instances.</p>""}, {'', ""<p>• Trop d'outils : les applications d'entreprise utilisent divers outils pour surveiller l'état et les performances des applications (tels que New Relic, Data Dog, etc.), la journalisation des erreurs (comme Splunk) et les outils fournis par les fournisseurs de cloud (tels que CloudWatch). Ces produits se chevauchent en termes de fonctionnalités et la maintenance et la gestion de ces outils (approvisionnement, courbe d'apprentissage, etc.) peuvent s'avérer fastidieuses.</p>""}, {'', ""<p>• Volume de données imprévisible : imaginez le volume de données d'observabilité (journaux, traces, métriques) collectées en fonction du trafic applicatif, de l'utilisation, de la dépendance aux produits externes, etc. La quantité de stockage de données requise pour consolider ces flux de données peut rapidement devenir incontrôlable.</p>""}, {'', ""<p>• La tarification est complexe : ces produits de fournisseurs proposent également différents modèles de tarification, tels que la facturation par hôte (comme Splunk, Data Dog, Dynatrace), par utilisateur (comme New Relic), par ingestion (comme SumoLogic, AppDynamics). La complexité des modèles de tarification rend difficile la comparaison du coût total de possession (TCO) entre les fournisseurs et la détermination de l'outil adapté à vos besoins et à votre budget.</p>""}, {'', '<h3>Pourquoi choisir une plateforme d’observabilité open source\xa0?</h3>'}, {'', ""<p>Les outils d'observabilité open source visent à fournir une approche standard et indépendante du fournisseur pour l'ingestion, la transformation et l'envoi de données à un backend d'observabilité. Les outils d'observabilité open source peuvent être une alternative pour économiser sur les coûts de licence et consolider plusieurs outils APM avec l'outil qui correspond à vos besoins et à votre budget.</p>""}, {'', '<p>Cependant, la maintenance de systèmes open source peut nécessiter des efforts de mise en place et de maintenance et augmentera vos coûts opérationnels initiaux. Mais à long terme, vous économiserez sur les frais de licence et éviterez la dépendance vis-à-vis des fournisseurs et les accords contractuels.</p>'}, {'', ""<p>Gartner prédit que, d'ici 2025, 70 % des nouvelles applications cloud natives utiliseront une instrumentation open source plutôt que des agents spécifiques aux fournisseurs pour une meilleure interopérabilité, et 70 % des nouvelles applications cloud natives adopteront OpenTelemetry pour l'observabilité plutôt que des agents spécifiques aux fournisseurs et des kits de développement logiciel (SDK).</p>""}, {'', ""<h3>Observabilité à grande échelle grâce à l'écosystème Open Source</h3>""}, {'', ""<p>Le paysage open source de l'observabilité est assez dynamique. Il existe plusieurs outils open source de la Cloud Native Computing Foundation (CNCF) pour l'observabilité et la surveillance. Cet article se concentrera principalement sur le framework OpenTelemetry et la pile technologique LGTM.</p>""}, {'', '<p>OpenTelemetry\xa0: le défi «\xa0trop d’outils\xa0» décrit ci-dessus pose un nouveau problème dans la collecte de données de télémétrie. Chaque fournisseur d’outils possède ses propres API, SDK, agents et collecteurs pour les journaux, les métriques et les traces. Nous avons besoin d’une collecte de télémétrie unifiée utilisant le framework OpenTelemetry pour créer et gérer les données de télémétrie telles que les journaux, les traces et les métriques.</p>'}, {'', ""<p>Le projet OTEL, sous l'égide de la CNCF, propose un ensemble unifié d'API, de SDK et d'outils indépendants des fournisseurs pour générer et collecter des données de télémétrie et les exporter vers divers outils d'analyse. Vous disposez d'une API et d'un SDK par langage de programmation pour extraire les données d'observabilité de votre application, d'un collecteur standard, d'un protocole de transmission (OTLP) et bien plus encore.</p>""}, {'', ""<p>LGTM\xa0: l'observabilité et la surveillance open source les plus populaires sont mises en œuvre à l'aide de la pile technologique LGTM.</p>""}, {'', ""<p>Dans la pile LGTM, nous exploitons\xa0: • Loki pour l'agrégation des journaux • Tableaux de bord Grafana pour les visualisations de télémétrie • Tempo (ou Jaeger) pour l'agrégation des traces • Prometheus géré pour l'agrégation des métriques</p>""}, {'', '<h3>Conclusion</h3>'}, {'', '<p>L’observabilité consiste à avoir une visibilité complète sur vos systèmes et à associer les indicateurs commerciaux aux données techniques. La surveillance consiste à comprendre si les choses fonctionnent correctement, et l’AIOps consiste à tirer un sens de cette visibilité. L’observabilité et la surveillance sont essentielles pour garantir le bon fonctionnement des applications et respecter les SLA des clients. En conclusion, en investissant dans des frameworks OTel open source et des outils LGTM, les équipes SRE peuvent surveiller efficacement leurs applications et obtenir des informations sur le comportement du système et les problèmes potentiels. Ces outils offrent une rentabilité et une personnalisation adaptées à des exigences spécifiques. Ils favorisent la neutralité des fournisseurs, ce qui peut être essentiel pour éviter la dépendance vis-à-vis d’un fournisseur.</p>'}]"
L'enquête révèle un manque de progrès significatif en matière d'observabilité,"[{'', '<p>Une enquête mondiale menée auprès de 500 professionnels de l’informatique suggère que les organisations ne progressent pas beaucoup dans leur capacité à observer véritablement les environnements applicatifs, d’autant plus qu’ils deviennent chaque jour plus complexes.</p>'}, {'', ""<p>L'enquête, menée par Logz.io, un fournisseur d'une plateforme d'observabilité, a révélé que seulement un répondant sur dix a déclaré disposer d'une observabilité complète de ses environnements d'application.</p>""}, {'', '<p>Asaf Yigal, directeur technique de Logz.io, a déclaré que même si de plus en plus d’équipes DevOps collectent des logs, des mesures et des traces, la plupart d’entre elles n’ont pas encore déterminé comment exploiter toutes les données collectées. Au fur et à mesure que les données sont ingérées, elles ne doivent pas seulement être stockées ; elles doivent également être corrélées aux différents services qui composent une application, a-t-il noté.</p>'}, {'', ""<p>Plus les entreprises déploient des applications cloud natives dans des environnements de production, plus ce problème devient urgent. Tous les microservices qui composent ces applications génèrent désormais une quantité massive de données de télémétrie qui génèrent plus d'alertes que jamais.</p>""}, {'', '<p>Sans surprise, les plus grands défis rencontrés par les organisations lors de la gestion des clusters Kubernetes dans des environnements de production sont la surveillance/le dépannage (40 %), suivis de près par la sécurité (37 %) et la mise en réseau (33 %).</p>'}, {'', '<p>De nombreuses entreprises ne disposent tout simplement pas des compétences nécessaires pour gérer des applications cloud natives. Près de la moitié des répondants (48 %) ont spécifiquement cité le manque de connaissances comme le plus grand défi qu’ils ont rencontré lorsqu’ils ont essayé d’observer ces types d’applications. Du point de vue de la gestion informatique, la plupart des microservices utilisent tous le même modèle de base, il est donc difficile pour les équipes DevOps d’identifier les microservices susceptibles d’avoir le plus grand impact sur les objectifs de niveau de service (SLO) et les accords de niveau de service (SLA) en cas de perturbation, a noté Yigal.</p>'}, {'', '<p>En l’absence de capacité à déterminer la cause réelle d’un problème, les alertes se multiplient et la fatigue s’accroît, ce qui finit par entraîner des niveaux d’épuisement plus élevés au sein de l’équipe DevOps, a noté Yigal. En fait, 82 % des personnes interrogées ont déclaré que leur délai moyen de résolution (MTTR) lors des incidents de production était supérieur à une heure.</p>'}, {'', ""<p>L'enquête a également révélé que plus de la moitié des répondants (52 %) travaillaient pour des organisations qui tentaient simultanément de maîtriser les coûts de surveillance. Plus des trois quarts (76 %) des répondants ont également indiqué que les outils OpenTelemetry (OTEL) ou centrés sur OTEL étaient au moins quelque peu importants pour leur stratégie globale d'observabilité.</p>""}, {'', '<p>En outre, 87 % des personnes interrogées ont déclaré que leur organisation utilise déjà une certaine forme d’ingénierie de plateforme pour gérer les flux de travail DevOps à grande échelle.</p>'}, {'', '<p>On ne sait pas exactement à quelle vitesse les entreprises adoptent les outils et les plateformes d’observabilité, mais trop d’équipes DevOps n’ont pas la visibilité nécessaire pour identifier la cause profonde d’un problème. Par conséquent, des variantes d’un même problème continuent souvent de se manifester parce que les efforts de correction précédents n’ont tout simplement pas été assez approfondis pour résoudre le problème principal.</p>'}, {'', '<p>Bien sûr, il se peut qu’un jour les algorithmes d’apprentissage automatique, ainsi que d’autres formes d’intelligence artificielle (IA), permettent de faire apparaître plus facilement ces problèmes. Le défi, en attendant, est de poser dès aujourd’hui les bases de l’observabilité pour donner accès aux données qui seront nécessaires à l’entraînement de ces modèles d’IA.</p>'}]"
Oracle cherche à accélérer le rythme de l'innovation Java,"[{'', '<p>Avec la dernière version de Java, il devrait devenir plus simple pour les équipes DevOps qui créent et déploient des applications avec le langage de programmation le plus utilisé dans l’entreprise d’innover plus rapidement.</p>'}, {'', ""<p>Georges Saab, vice-président senior d'Oracle Java Platform, a déclaré que Java 22 est unique dans l'histoire de Java dans le sens où il est spécifiquement conçu pour permettre aux développeurs d'invoquer des fonctionnalités disponibles en version bêta et en préversion tout en continuant à travailler avec les autres fonctions prises en charge dans le vénérable langage de programmation. L'objectif global est d'accélérer le rythme de l'innovation dans les environnements basés sur les applications Java, a-t-il ajouté.</p>""}, {'', ""<p>La dernière édition du Java Development Kit (JDK), par exemple, donne accès en avant-première aux fonctionnalités développées dans le cadre d'une initiative du projet Amber qui simplifie l'expression de chaînes qui incluent des valeurs calculées lors de l'exécution tout en améliorant la sécurité des programmes qui composent des chaînes à partir de valeurs fournies par l'utilisateur et les transmettent à d'autres systèmes.</p>""}, {'', ""<p>Parallèlement, une initiative du projet Loom, également disponible en version préliminaire, rationalise la gestion et l'annulation des erreurs d'une manière qui améliore l'observabilité à l'aide d'une interface de programmation d'application (API) qui permet une concurrence structurée, tandis qu'une capacité Scoped Values \u200b\u200boffre un moyen de partager des données immuables au sein et entre les threads.</p>""}, {'', ""<p>Une initiative du projet Panama ajoute une API vectorielle qui promet de compiler des instructions au moment de l'exécution lorsque, par exemple, on utilise des techniques de génération augmentée de récupération (RAG) pour étendre les modèles d'intelligence artificielle (IA).</p>""}, {'', ""<p>En termes de fonctionnalités supplémentaires désormais généralement prises en charge, Java 22 ajoute une API pour permettre aux programmes Java d'interagir en toute sécurité avec le code et les données exécutés en mémoire en dehors de l'environnement d'exécution Java.</p>""}, {'', ""<p>Plus important encore, du point de vue des performances des applications, la dernière itération du langage rationalise les processus de collecte des déchets en mémoire pour réduire le nombre de fois où une application Java pourrait autrement avoir besoin d'être mise en pause.</p>""}, {'', ""<p>Malgré le nombre de langages de programmation alternatifs existants, la majorité des applications d'entreprise continuent d'être développées en Java. Si de nombreux développeurs connaissent au moins plusieurs langages de programmation, la plupart ont tendance à privilégier un seul langage de programmation principal. Les développeurs Java, en particulier, ont montré peu d'envie d'abandonner ce langage, même si d'autres langages de programmation, tels que JavaScript, sont apparus et sont soit plus simples à utiliser, soit, comme dans le cas de Rust, peuvent offrir des capacités de sécurité plus robustes.</p>""}, {'', '<p>Il est moins évident de savoir à quel rythme les développeurs mettent à niveau leurs applications vers la dernière version. Dans certains cas, les développeurs utilisent encore la version 8, 11 ou, plus récemment, 17. Il faudra peut-être un certain temps avant que la majorité des développeurs Java soient prêts à passer à Java 22. En outre, les équipes DevOps devront alors décider quand la masse critique sera suffisante pour justifier la mise à niveau des environnements d’exécution afin de prendre en charge une autre itération de Java.</p>'}, {'', '<p>D’une manière ou d’une autre, que ce soit via une édition de Java fournie par Oracle ou un autre fournisseur, le langage de programmation n’est pas prêt d’être supplanté. Le défi consiste désormais à fournir un support non seulement pour les différentes versions de Java qui peuvent être exécutées, mais aussi, inévitablement, pour tous les autres artefacts logiciels écrits dans différents langages qui circulent désormais également régulièrement dans les pipelines DevOps.</p>'}]"
Une enquête montre que l'adoption de l'environnement de développement cloud prend de l'ampleur,"[{'', '<p>Une enquête mondiale a révélé que même si 95 % des développeurs et des chefs d’entreprise connaissent les environnements de développement cloud (CDE), les raisons qui les poussent à en adopter un varient considérablement. L’enquête a interrogé 223 développeurs et chefs d’entreprise travaillant pour des organisations comptant plus de 2 000 employés et 250 développeurs.</p>'}, {'', ""<p>Menée par Coder, un fournisseur d'un CDE, l'enquête a par exemple révélé que si 15 % des personnes interrogées ont cité l'optimisation du processus de configuration de l'environnement de développement comme raison de l'adoption d'un CDE, 14 % ont cité la simplification des tâches de gestion telles que l'intégration ou l'amélioration de l'accès à distance.</p>""}, {'', ""<p>Selon Rob Whiteley, PDG de Coder, même si les CDE améliorent clairement la productivité des équipes de développement d'applications et renforcent la sécurité en réduisant la quantité de code source exécuté sur les ordinateurs portables et de bureau, les organisations souhaitant adopter les CDE doivent faire face à une résistance culturelle importante. Les développeurs d'applications ont historiquement mis en place leurs propres environnements de développement sur leurs propres machines locales. Cette approche signifie cependant qu'en plus de rendre la collaboration plus difficile, les organisations doivent également consacrer beaucoup de temps et d'efforts au verrouillage de chaque point de terminaison utilisé par leurs développeurs, a noté Whiteley.</p>""}, {'', ""<p>En fait, ces problèmes sont la raison pour laquelle de nombreuses équipes d'ingénierie de plateforme formées pour centraliser la gestion des flux de travail DevOps mettent en œuvre des CDE, a-t-il ajouté. L'objectif ultime est d'établir un ensemble de garde-fous tout en permettant aux développeurs de répondre eux-mêmes à leurs besoins en outils, a noté Whiteley.</p>""}, {'', ""<p>En fait, de nombreuses équipes DevOps ont déjà créé une version maison d'un CDE qu'elles maintiennent actuellement. Coder et d'autres fournisseurs de CDE préconisent l'utilisation d'une plateforme prise en charge par un fournisseur plutôt que de consacrer des ressources à la prise en charge d'un CDE personnalisé, a déclaré Whiteley.</p>""}, {'', '<p>Les workflows DevOps continuent d’évoluer à l’ère de l’intelligence artificielle (IA), et ce n’est qu’une question de temps avant que davantage d’entreprises adoptent les CDE, a-t-il ajouté. Le volume considérable de données avec lesquelles les développeurs travailleront rendra impossible l’utilisation continue de machines locales pour créer des applications, a déclaré M. Whiteley.</p>'}, {'', '<p>Quelle que soit la motivation à l’origine de l’adoption des CDE, des changements culturels importants sont en train d’être apportés aux flux de travail DevOps, qui doivent évoluer à mesure que le développement d’applications continue de devenir de plus en plus distribué. À l’ère post-COVID-19, il n’est plus rare que les équipes de développement d’applications soient composées de professionnels de l’informatique travaillant dans des fuseaux horaires différents. Cela crée un besoin urgent de tirer parti des services cloud pour fournir le niveau de collaboration que ces équipes doivent atteindre.</p>'}, {'', '<p>On ne sait pas exactement combien d’environnements de développement de code ont été déployés, mais comme le rythme de création et de déploiement des applications continue de s’accélérer, l’époque où les entreprises pouvaient permettre à chaque développeur de gérer son propre environnement de développement touche à sa fin. Les entreprises doivent déterminer le plus rapidement possible si le code créé sera réellement exécuté dans un environnement de production. Plus on passe de temps à essayer de concilier les différences entre les environnements de développement et l’environnement de production dans lequel l’application s’exécute, plus il faudra de temps pour mener à bien un projet donné.</p>'}, {'', '<p>Malheureusement, le temps est la seule chose que la plupart des équipes DevOps ne peuvent plus se permettre de consacrer à des initiatives de développement d’applications qui, en plus de se multiplier, deviennent également beaucoup plus complexes à gérer chaque jour qui passe.</p>'}]"
